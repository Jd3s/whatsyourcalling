0,Flexible electronic skin aids human-machine interactions,"Human skin contains sensitive nerve cells that detect pressure, temperature and other sensations that allow tactile interactions with the environment. To help robots and prosthetic devices attain these abilities, scientists are trying to develop electronic skins. Now researchers report a new method in ACS Applied Materials & Interfaces that creates an ultrathin, stretchable electronic skin, which could be used for a variety of human-machine interactions.Electronic skin could be used for many applications, including prosthetic devices, wearable health monitors, robotics and virtual reality. A major challenge is transferring ultrathin electrical circuits onto complex 3D surfaces and then having the electronics be bendable and stretchable enough to allow movement. Some scientists have developed flexible ""electronic tattoos"" for this purpose, but their production is typically slow, expensive and requires clean-room fabrication methods such as photolithography. Mahmoud Tavakoli, Carmel Majidi and colleagues wanted to develop a fast, simple and inexpensive method for producing thin-film circuits with integrated microelectronics.In the new approach, the researchers patterned a circuit template onto a sheet of transfer tattoo paper with an ordinary desktop laser printer. They then coated the template with silver paste, which adhered only to the printed toner ink. On top of the silver paste, the team deposited a gallium-indium liquid metal alloy that increased the electrical conductivity and flexibility of the circuit. Finally, they added external electronics, such as microchips, with a conductive ""glue"" made of vertically aligned magnetic particles embedded in a polyvinyl alcohol gel. The researchers transferred the electronic tattoo to various objects and demonstrated several applications of the new method, such as controlling a robot prosthetic arm, monitoring human skeletal muscle activity and incorporating proximity sensors into a 3D model of a hand.Video: https://www.youtube.com/watch?time_continue=15&v=NCOwWEMEQN8Story Source:Materials provided by American Chemical Society. ",0.0977066,0.133768,0.126426,0.173748,0.190782,0.13077,0.161687,0,0
1,Ultra-light gloves let users 'touch' virtual objects,"Engineers and software developers around the world are seeking to create technology that lets users touch, grasp and manipulate virtual objects, while feeling like they are actually touching something in the real world.Scientists at EPFL and ETH Zurich have just made a major step toward this goal with their new haptic glove, which is not only lightweight -- under 8 grams per finger -- but also provides feedback that is extremely realistic. The glove is able to generate up to 40 Newtons of holding force on each finger with just 200 Volts and only a few milliWatts of power. It also has the potential to run on a very small battery. That, together with the glove's low form factor (only 2 mm thick), translates into an unprecedented level of precision and freedom of movement.""We wanted to develop a lightweight device that -- unlike existing virtual-reality gloves -- doesn't require a bulky exoskeleton, pumps or very thick cables,"" says Herbert Shea, head of EPFL's Soft Transducers Laboratory (LMTS).The scientists' glove, called DextrES, has been successfully tested on volunteers in Zurich and will be presented at the upcoming ACM Symposium on User Interface Software and Technology (UIST).Fabric, metal strips and electricity DextrES is made of nylon with thin elastic metal strips running over the fingers. The strips are separated by a thin insulator. When the user's fingers come into contact with a virtual object, the controller applies a voltage difference between the metal strips causing them to stick together via electrostatic attraction -- this produces a braking force that blocks the finger's or thumb's movement. Once the voltage is removed, the metal strips glide smoothly and the user can once again move his fingers freely.Tricking your brain For now the glove is powered by a very thin electrical cable, but thanks to the low voltage and power required, a very small battery could eventually be used instead. ""The system's low power requirement is due to the fact that it doesn't create a movement, but blocks one,"" explains Shea. The researchers also need to conduct tests to see just how closely they have to simulate real conditions to give users a realistic experience. ""The human sensory system is highly developed and highly complex. We have many different kinds of receptors at a very high density in the joints of our fingers and embedded in the skin. As a result, rendering realistic feedback when interacting with virtual objects is a very demanding problem and is currently unsolved. Our work goes one step in this direction, focusing particularly on kinesthetic feedback,"" says Otmar Hilliges, head of the Advanced Interactive Technologies Lab at ETH Zurich.In this joint research project, the hardware was developed by EPFL at its Microcity campus in Neuchetel, and the virtual reality system was created by ETH Zurich, which also carried out the user tests.""Our partnership with the EPFL lab is a very good match. It allows us to tackle some of the longstanding challenges in virtual reality at a pace and depth that would otherwise not be possible,"" adds Hilliges.The next step will be to scale up the device and apply it to other parts of the body using conductive fabric. ""Gamers are currently the biggest market, but there are many other potential applications -- especially in healthcare, such as for training surgeons. The technology could also be applied in augmented reality,"" says Shea.Story Source:Materials provided by Ecole Polytechnique Federale de Lausanne. ",0.0773093,0.0834841,0.123452,0.204952,0.109267,0.136706,0.144828,0,0
2,The cart before the horse: A new model of cause and effect,"Natural little scientists, human babies love letting go of things and watching them fall. Baby's first experiment teaches them about more than the force of gravity. It establishes the concept of causality -- the relationship between cause and effect that all human knowledge depends on. Let it go, it falls. The cause must precede its effect in time, as scientist from Galileo in the 16th Century to Clive Granger in 1969 defined causality.But in many cases, this one-way relationship between cause and effect fails to accurately describe reality. In a recent paper in Nature Communications, scientists led by Albert C. Yang, MD, PhD, of Beth Israel Deaconess Medical Center, introduce a new approach to causality that moves away from this temporally linear model of cause and effect.""The reality in the real-world is that cause and effect are often reciprocal, as in the feedback loops seen in physiologic/endocrine pathways, neuronal regulation, ecosystems, and even the economy,"" said Albert C. Yang, MD, PhD, a scientist in the Division of Interdisciplinary Medicine and Biotechnology. ""Our new causal method allows for mutual or two-way causation, in which the effect of a cause can feed back to the cause itself simultaneously.""Yang and colleagues' new approach defines causality independently from time. Their covariation principle of cause and effect defines cause as that which when present, the effect follows, and that which when removed, the effect is removed. The team demonstrates the new approach by applying it to predator and prey systems. Moreover, Yang and colleagues showed that their model can work well in systems where other causality methods cannot work.""I would expect the method to represent a breakthrough of causal assessment of observational data,"" said Yang. ""It can be applied to a wide range of causal questions in the scientific field.""Story Source:Materials provided by Beth Israel Deaconess Medical Center. ",0.120846,0.140117,0.171083,0.224315,0.163692,0.178455,0.189955,0,0
3,"Beyond deep fakes: Transforming video content into another video's style, automatically Applications include movie production, self-driving cars, VR content","Researchers at Carnegie Mellon University have devised a way to automatically transform the content of one video into the style of another, making it possible to transfer the facial expressions of comedian John Oliver to those of a cartoon character, or to make a daffodil bloom in much the same way a hibiscus would.Because the data-driven method does not require human intervention, it can rapidly transform large amounts of video, making it a boon to movie production. It can also be used to convert black-and-white films to color and to create content for virtual reality experiences.""I think there are a lot of stories to be told,"" said Aayush Bansal, a Ph.D. student in CMU's Robotics Institute. Film production was his primary motivation in helping devise the method, he explained, enabling movies to be produced more quickly and cheaply. ""It's a tool for the artist that gives them an initial model that they can then improve,"" he added.The technology also has the potential to be used for so-called ""deep fakes,"" videos in which a person's image is inserted without permission, making it appear that the person has done or said things that are out of character, Bansal acknowledged.""It was an eye opener to all of us in the field that such fakes would be created and have such an impact,"" he said. ""Finding ways to detect them will be important moving forward.""Bansal will present the method today at ECCV 2018, the European Conference on Computer Vision, in Munich. His co-authors include Deva Ramanan, CMU associate professor of robotics.Transferring content from one video to the style of another relies on artificial intelligence. In particular, a class of algorithms called generative adversarial networks (GANs) have made it easier for computers to understand how to apply the style of one image to another, particularly when they have not been carefully matched.In a GAN, two models are created: a discriminator that learns to detect what is consistent with the style of one image or video, and a generator that learns how to create images or videos that match a certain style. When the two work competitively -- the generator trying to trick the discriminator and the discriminator scoring the effectiveness of the generator -- the system eventually learns how content can be transformed into a certain style.A variant, called cycle-GAN, completes the loop, much like translating English speech into Spanish and then the Spanish back into English and then evaluating whether the twice-translated speech still makes sense. Using cycle-GAN to analyze the spatial characteristics of images has proven effective in transforming one image into the style of another.That spatial method still leaves something to be desired for video, with unwanted artifacts and imperfections cropping up in the full cycle of translations. To mitigate the problem, the researchers developed a technique, called Recycle-GAN, that incorporates not only spatial, but temporal information. This additional information, accounting for changes over time, further constrains the process and produces better results.The researchers showed that Recycle-GAN can be used to transform video of Oliver into what appears to be fellow comedian Stephen Colbert and back into Oliver. Or video of John Oliver's face can be transformed a cartoon character. Recycle-GAN allows not only facial expressions to be copied, but also the movements and cadence of the performance.The effects aren't limited to faces, or even bodies. The researchers demonstrated that video of a blooming flower can be used to manipulate the image of other types of flowers. Or clouds that are crossing the sky rapidly on a windy day can be slowed to give the appearance of calmer weather.Such effects might be useful in developing self-driving cars that can navigate at night or in bad weather, Bansal said. Obtaining video of night scenes or stormy weather in which objects can be identified and labeled can be difficult, he explained. Recycle-GAN, on the other hand, can transform easily obtained and labeled daytime scenes into nighttime or stormy scenes, providing images that can be used to train cars to operate in those conditions.Story Source:Materials provided by Carnegie Mellon University. Original written by Byron Spice. ",0.0642505,0.0853253,0.11208,0.242163,0.107735,0.137418,0.15904,0,0
4,A new way to see stress -- using supercomputers Stressed materials show asymmetric distributions in simulations on Comet and Jetstream supercomputers through XSEDE allocations,"It's easy to take a lot for granted. Scientists do this when they study stress, the force per unit area on an object. Scientists handle stress mathematically by assuming it to have symmetry. That means the components of stress are identical if you transform the stressed object with something like a turn or a flip. Supercomputer simulations show that at the atomic level, material stress doesn't behave symmetrically. The findings could help scientists design new materials such as glass or metal that doesn't ice up.That's according to a study published September of 2018 in the Proceedings of the Royal Society A. Study co-author Liming Xiong summarized the two main findings. ""The commonly accepted symmetric property of a stress tensor in classical continuum mechanics is based on certain assumptions, and they will not be valid when a material is resolved at an atomistic resolution."" Xiong continued that ""the widely used atomic Virial stress or Hardy stress formulae significantly underestimate the stress near a stress concentrator such as a dislocation core, a crack tip, or an interface, in a material under deformation."" Liming Xiong is an Assistant Professor in the Department of Aerospace Engineering at Iowa State University.Xiong and colleagues treated stress in a different way than classical continuum mechanics, which assumes that a material is infinitely divisible such that the moment of momentum vanishes for the material point as its volume approaches zero. Instead, they used the definition by mathematician A.L. Cauchy of stress as the force per unit area acting on three rectangular planes. With that, they conducted molecular dynamics simulations to measure the atomic-scale stress tensor of materials with inhomogeneities caused by dislocations, phase boundaries and holes.The computational challenges, said Xiong, swell up to the limits of what's currently computable when one deals with atomic forces interacting inside a tiny fraction of the space of a raindrop. ""The degree of freedom that needs to be calculated will be huge, because even a micron-sized sample will contain billions of atoms. Billions of atomic pairs will require a huge amount of computation resource,"" said Xiong.What's more, added Xiong, is the lack of a well-established computer code that can be used for the local stress calculation at the atomic scale. His team used the open source LAMMPS Molecular Dynamics Simulator, incorporating the Lennard-Jones interatomic potential and modified through the parameters they worked out in the paper. ""Basically, we're trying to meet two challenges,"" Xiong said. ""One is to redefine stress at an atomic level. The other one is, if we have a well-defined stress quantity, can we use supercomputer resources to calculate it?""Xiong was awarded supercomputer allocations on XSEDE, the Extreme Science and Engineering Discovery Environment, funded by the National Science Foundation. That gave Xiong access to the Comet system at the San Diego Supercomputer Center; and Jetstream, a cloud environment supported by Indiana University, the University of Arizona, and the Texas Advanced Computing Center.""Jetstream is a very suitable platform to develop a computer code, debug it, and test it,"" Xiong said. ""Jetstream is designed for small-scale calculations, not for large-scale ones. Once the code was developed and benchmarked, we ported it to the petascale Comet system to perform large-scale simulations using hundreds to thousands of processors. This is how we used XSEDE resources to perform this research,"" Xiong explained.The Jetstream system is a configurable large-scale computing resource that leverages both on-demand and persistent virtual machine technology to support a much wider array of software environments and services than current NSF resources can accommodate.""The debugging of that code needed cloud monitoring and on-demand intelligence resource allocation,"" Xiong recalled. ""We needed to test it first, because that code was not available. Jetstream has a unique feature of cloud monitoring and on-demand intelligence resource allocation. These are the most important features for us to choose Jetstream to develop the code.""""What impressed our research group most about Jetstream,"" Xiong continued, ""was the cloud monitoring. During the debugging stage of the code, we really need to monitor how the code is performing during the calculation. If the code is not fully developed, if it's not benchmarked yet, we don't know which part is having a problem. The cloud monitoring can tell us how the code is performing while it runs. This is very unique,"" said Xiong.The simulation work, said Xiong, helps scientists bridge the gap between the micro and the macro scales of reality, in a methodology called multiscale modeling. ""Multiscale is trying to bridge the atomistic continuum. In order to develop a methodology for multiscale modeling, we need to have consistent definitions for each quantity at each level... This is very important for the establishment of a self-consistent concurrent atomistic-continuum computational tool. With that tool, we can predict the material performance, the qualities and the behaviors from the bottom up. By just considering the material as a collection of atoms, we can predict its behaviors. Stress is just a stepping stone. With that, we have the quantities to bridge the continuum,"" Xiong said.Xiong and his research group are working on several projects to apply their understanding of stress to design new materials with novel properties. ""One of them is de-icing from the surfaces of materials,"" Xiong explained. ""A common phenomenon you can observe is ice that forms on a car window in cold weather. If you want to remove it, you need to apply a force on the ice. The force and energy required to remove that ice is related to the stress tensor definition and the interfaces between ice and the car window. Basically, the stress definition, if it's clear at a local scale, it will provide the main guidance to use in our daily life.""Xiong sees great value in the computational side of science. ""Supercomputing is a really powerful way to compute. Nowadays, people want to speed up the development of new materials. We want to fabricate and understand the material behavior before putting it into mass production. That will require a predictive simulation tool. That predictive simulation tool really considers materials as a collection of atoms. The degree of freedom associated with atoms will be huge. Even a micron-sized sample will contain billions of atoms. Only a supercomputer can help. This is very unique for supercomputing,"" said Xiong.Story Source:Materials provided by University of Texas at Austin, Texas Advanced Computing Center. ",0.0430301,0.0714947,0.103949,0.168893,0.0764837,0.0811732,0.131416,0,0
5,Virtual reality could serve as powerful environmental education tool,"Utter the words ""ocean acidification"" in mixed company, and you'll probably get blank stares. Although climate change has grown steadily in the public consciousness, one of its most insidious impacts -- a widespread die-off of marine ecosystems driven by carbon dioxide emissions -- remains relatively unknown.Enter virtual reality. In a new study, published Nov. 30 in Frontiers in Psychology, researchers at Stanford and the University of Oregon discovered that VR can be a powerful tool for improving environmental learning gains and attitudes. The researchers found that experiencing a simulation of ocean acidification's effects spurred meaningful gains in people's understanding of the issue.""I believe virtual reality is a powerful tool that can help the environment in so many ways,"" said study co-author Jeremy Bailenson, the Thomas More Storke Professor of Communication. ""Changing the right minds can have a huge impact.""New gear, wider reachWith the advent of affordable consumer-grade gear from companies such as Oculus Rift, Samsung and Microsoft, potential audiences for VR are expanding far beyond Stanford's multimillion-dollar Virtual Human Interaction Lab.Working with co-author Roy Pea, the David Jacks Professor of Education and director of Stanford's Human-Sciences and Technologies Advanced Research Institute, Bailenson and his team brought the Stanford Ocean Acidification Experience to more than 270 high school students, college students and adults.In one such test, high school seniors in a marine biology class at Sacred Heart Preparatory in Atherton, California, took on new virtual identities in the simulation (which is free to download). Each became a pink coral on a rocky underwater reef throbbing with urchins, bream, snails and other creatures.By the end of the simulation -- which fast-forwards to what the reef will look like at the end of this century -- those brilliantly varied and colorful species have disappeared. They are replaced by slimy green algae and the silver Salema Porgy -- a fish that will likely thrive in more acidic waters. The simulation is based on the work of Fiorenza Micheli, the David and Lucile Packard Professor of Marine Science at Stanford.Eventually, the viewer's virtual coral skeleton disintegrates. ""If ocean acidification continues, ecosystems like your rocky reef, a world that was once full of biological diversity, will become a world of weeds,"" the narration intones.Connected to the environmentThe simulation was effective at making users feel a connection with their bodies, according to researchers who tracked the students' movements. Some of the students swiveled their heads and twisted their bodies during the simulation.""It's pretty cool, pretty responsive,"" said 18-year-old Cameron Chapman. ""I definitely felt like I was underwater.""""It was way more realistic than I expected,"" said fellow high school senior Alexa Levison. ""I'm a visual learner. Seeing ocean acidification happen is different than just hearing about it.""After the experience, the Sacred Heart students' scores on questions about ocean acidification causes and mechanisms increased by almost 150 percent and they retained that knowledge when tested several weeks later. In all of the study's in-school experiments, participants demonstrated increasing knowledge about ocean acidification as their time in the VR learning environment grew longer.""Across age groups, learning settings and learning content, people understand the processes and effect of ocean acidification after a short immersive VR experience,"" said study lead author David Markowitz, a graduate student at the time of the research, now an assistant professor at the University of Oregon.Increasing motivation""We don't know whether a VR experience results in more learning compared to the same materials presented in other media,"" Bailenson said. ""What we do know is that it increases motivation -- people are thrilled to do it, much more so than opening a textbook -- and because of the richness of the data recorded by the VR system, you can tweak the learning materials in real time based on how well someone is learning.""Bailenson is taking his VR experience beyond the classroom. He has been sending researchers with VR headsets to flea markets and libraries to show the ocean acidification experience. Also, it is part of a permanent virtual reality exhibition at the Tech Museum of Innovation in San Jose, California. He is also collaborating with companies to incorporate environment-themed VR into video games.Although Bailenson is becoming more confident in the generalizability of the work, he acknowledges the need for replications to test how robust it is and to determine how long the effects endure. Questions remain about the effects of repeated VR exposure and how they persist over time. Research has yet to incorporate a broad demographic sample that spans variables such as age, income and education.Despite these unknowns, co-author Brian Perone, a graduate student at the time of the research, said he is optimistic about the value of VR in education. ""When done right, these experiences can feel real, and can give learners a lasting sense of connectedness,"" he said.Bailenson is also a senior fellow at the Stanford Woods Institute for the Environment. Micheli is also co-director of the Stanford Center for Ocean Solutions and a senior fellow at the Stanford Woods Institute for the Environment. Co-authors also include Rob Laha, a postdoctoral scholar at the time of research.Funding for this research was provided by the Gordon and Betty Moore Foundation.Video: https://www.youtube.com/watch?v=RqEKy3cvRGoStory Source:Materials provided by Stanford University. Original written by Rob Jordan. ",0.0550112,0.094105,0.153228,0.230074,0.0938472,0.114683,0.155316,0,0
6,Making it easier to transform freeform 2D sketching into 3D Models Novel method for modeling diverse freeform shapes by drawing intuitive and expressive sparse 2D sketches,"A new computational approach, built on data-driven techniques, is making it possible to turn simple 2D sketch into a realistic 3D shape, with little or no user input necessary.A global team of computer scientists from University of Hong Kong, Microsoft and University of British Columbia, have developed a novel framework, combining CNNs, or convolutional neural network techniques, with geometric inference methods to compute sketches and their corresponding 3D shapes, faster and more intuitively than existing methods.""Our tool can be used by artists to quickly and easily draft freeform shapes that can serve as base shapes and be further refined with advanced modeling software,"" says Hao Pan, researcher at Microsoft Research Asia who led the work. ""Novice users may also use it as an accessible tool to make stylish contents for their 3D creations and virtual worlds.""The novel method builds on the team's previous sketch-based 3D modeling work -- BendSketch: Modeling Freeform Surfaces through 2D Sketching* -- a purely geometric-based method which requires annotated sketch inputs. Adds Pan, ""We want to develop a method that is both intelligent -- in the sense of requiring as few user inputs as possible and doing all needed inference automatically, and generic -- so that we do not have to train a new machine learning model for each specific object category.""Pan's collaborators Changjian Li and Wenping Wang at University of Hong Kong; Yang Liu and Xin Tong at Microsoft Research Asia; and Alla Sheffer at University of British Columbia, will present at SIGGRAPH Asia 2018 in Tokyo 4 December to 7 December. The annual conference features the most respected technical and creative members in the field of computer graphics and interactive techniques, and showcases leading edge research in science, art, gaming and animation, among other sectors.It is a challenge to easily, automatically transform a sparse, imperfect 2D sketch to a robust 3D shape. The key challenge the researchers addressed in this work is the significant information disparity presented between the stages of 'sketch' and '3D' model. This is where the artist would fill in features such as bumps, bends, creases, valleys, ridges on the surface drawings to create a 3D design. For each sketch, demonstrated in their paper, the method uses a CNN to predict the 3D surface patch.The researchers' CNN is trained on a large dataset generated by rendering sketches of various 3D shapes using non-photorealistic line rendering, or NPR, method that automatically mimics human sketching of freeform shapes -- shapes, including that of a teddy bear, a bird or a fish, for example. The researchers validated their new approach, compared it with existing methods and evaluated its performance. Results demonstrated that their method provides a new way to produce 3D modeling from 2D sketches in a user-friendly and more robust manner.One example, included in their paper, involved novice users who were tested on the new method. The users have little knowledge or training about sketching or 3D modeling. They were given a 20-minutes training demo on the new method. They then were asked to create three target shapes -- bird, teddy bear and dolphin -- of varying complexity. The users managed to complete the tasks, ranging from just 5 minutes to 20 minutes total. As noted in the paper, the novice participants found the sketches easy to produce using the researchers' tool, and some also wanted to explore with it more.In future work, the researchers intend to explore several possibilities and extensions of their algorithm. For example, currently their method models a surface patch each time by sketching in view, but it would be beneficial to be able to model a complete shape by sketching both front and back patches in a single view. They also hope to enhance the tool so that he can handle multiple scale modeling, so that large-scale shapes and fine-level details can be sketched with ease.Video: https://www.youtube.com/watch?v=Fz-f-e51zRQStory Source:Materials provided by Association for Computing Machinery. ",0.0600179,0.0790939,0.135012,0.278142,0.0944054,0.115187,0.158128,0,0
7,Virtual library of 1 million new macrolide scaffolds could help speed drug discovery,"Researchers from North Carolina State University have created the largest publicly available virtual library of macrolide scaffolds. The library -- called V1M -- contains chemical structures and computed properties for 1 million macrolide scaffolds with potential for use as antibiotics or cancer drugs.""As chemists, we're only able to look at a tiny portion of the actual chemical universe,"" says Denis Fourches, assistant professor of chemistry at NC State and corresponding author of a paper describing the work. ""If we were trying to synthesize and test all possible individual chemicals of interest, it would take way too much time and be too expensive. So we have to use computers to explore those unknown parts of the chemical space.""One way that chemists utilize computers is by using them to enumerate, or virtually generate, new molecules and compare their predicted properties to those of existing drugs. This in silico screening process quickly and inexpensively identifies compounds with desirable properties that experimental chemists can then synthesize and test.Macrolides are a family of chemicals mainly used as antibiotics and anti-cancer drugs. Their unique ring structure enables them to bind to difficult protein targets. Some of them are considered drugs of last resort, especially for drug-resistant bacteria.""Macrolides are natural products,"" Fourches says. ""These chemicals are produced by bacteria as a means to kill other bacteria. But it takes 20 to 25 chemical steps to synthesize these very complex compounds, which is a time-consuming and expensive process. So if you want to find new compounds, computer simulation is by far the fastest way to do it.""Fourches and his colleagues created a computer software program called the PKS Enumerator, which generates very large libraries of virtual chemical analogues of macrolide drugs. The software uses chemical building blocks extracted from a set of 18 known bioactive macrolides, breaking each one down into its component chemical parts, and then reshuffling them to create new compounds according to a series of rules and user-constraints. The resulting library of new macrolides -- V1M -- classifies the new compounds by size, weight, topology and hydrogen bond donors and acceptors.""We wanted to create a virtual library of completely new chemicals that no one has probably ever synthesized, but those compounds still needed to be similar enough to known macrolide drugs in order to make this library relevant for the research community,"" Fourches says. ""V1M is the first public domain library of these new macrolides, which are all chemically similar to the 18 known bioactive macrolides we analyzed. Hopefully other researchers can use the library to further screen and identify some compounds that may be useful in drug discovery.""The research appears in the Journal of Cheminformatics. NC State graduate student Phyo Phyo Kyaw Zin is first author. Gavin Williams, associate professor of chemistry at NC State, also contributed to the work.Story Source:Materials provided by North Carolina State University. ",0.0754351,0.158079,0.145162,0.226818,0.117807,0.136896,0.182659,0,0
8,"To replicate physical objects for virtual reality,  just turn on your smartphone ","Capturing and reproducing realistic, real-world objects for any virtual environment is complex and time-consuming. Imagine using a conventional camera with a built-in flash -- from any mobile device or off-the-shelf digital camera -- to simplify this task. A global team of computer scientists have developed a novel method that replicates physical objects for the virtual and augmented reality space just using a point-and-shoot camera with a flash, without the need for additional, and oftentimes expensive, supporting hardware.""To faithfully reproduce a real-world object in the VR/AR environment, we need to replicate the 3D geometry and appearance of the object,"" says Min H. Kim, associate professor of computer science at KAIST in South Korea and lead author of the research. ""Traditionally, this has been either done manually by 3D artists, which is a labor-intensive task, or by using specialized, expensive hardware. Our method is straightforward, cheaper and efficient, and reproduces realistic 3D objects by just taking photos from a single camera with a built-in flash.""Kim and his collaborators, Diego Gutierrez, professor of computer science at Universidad de Zaragoza in Spain, and KAIST PhD students Giljoo Nam and Joo Ho Lee, will present this new work at SIGGRAPH Asia 2018 in Tokyo 4 December to 7 December. The annual conference features the most respected technical and creative members in the field of computer graphics and interactive techniques, and showcases leading edge research in science, art, gaming and animation, among other sectors.Existing approaches for the acquisition of physical objects require specialized hardware setups to achieve geometry and appearance modeling of the desired objects. Those setups might include a 3D laser scanner or multiple cameras, or a lighting dome with more than a hundred light sources. In contrast, this new technique only needs a single camera, to produce high-quality outputs.""Many traditional methods using a single camera can capture only the 3D geometry of objects, but not the complex reflectance of real-world objects, given by the SVBRDF,"" notes Kim. SVBRDF, which stands for spatially-varying bidirectional reflectance distribution functions, is key in obtaining an object's real-world shape and appearance. ""Using only 3D geometry cannot reproduce the realistic appearance of the object in the AR/VR environment. Our technique can capture high-quality 3D geometry as well as its material appearance so that the objects can be realistically rendered in any virtual environment.""The group demonstrated their framework using a digital camera, the Nikon D7000 and the built-in camera of an Android mobile phone, in a series of examples in their paper, ""Practical SVBRDF Acquisition of 3D Objects with Unstructured Flash Photography."" The novel algorithm, which does not require any input geometry of the target object, successfully captured the geometry and appearance of 3D objects with basic, flash photography and reproduced consistent results. Examples that were showcased in the work included diverse set of objects that spanned a wide range of geometries and materials, including metal, wood, plastic, ceramic, resin and paper, and comprised of complex shapes like a finely detailed mini-statute of Nefertiti.In future work, the researchers hope to further simplify the capture process or extending the method to include dynamic geometry or larger scenes, for instance.Story Source:Materials provided by Association for Computing Machinery. ",0.0759356,0.102628,0.156518,0.254735,0.129575,0.13661,0.171997,0,0
9,A big step toward the practical application of 3D holography with high- performance computers,"Japanese computer scientists have succeeded in developing a special purpose computer that can project high-quality three-dimensional (3D) holography as a video. The research team led by Tomoyoshi Ito, who is a professor at the Institute for Global Prominent Research, Chiba University, has been working to increase the speed of the holographic projections by developing new hardware.Holography has a long history. Since 1960 when the first laser was invented, many works involving laser holograms have been produced. For digitalizing these analog technologies and developing electron holography techniques to project 3D holography images as video, computing powers with more than 10 frames per second and 1 trillion pixels per frame are required. Therefore, hardware development, as well as corresponding software development, represents some of the biggest challenges for researchers in this field.Also, to make a 3D object from two-dimensional (2D) data, it is necessary to properly consider several factors including the binocular parallax, motion parallax, convergence angle, focus adjustment, and estimates made based on human experience. Currently, general 3D televisions (TVs) use the binocular parallax for the stereoscopy, but children cannot use this technology because it has the potential to damage their health, a risk that is related to the difference between the distances that a brain perceives and those that the eyes' focus on. Many researchers in various countries around the world, including Ito in Japan, have been investing in video holography, which may allow more people to enjoy 3D TVs safely.Ito, who is an astronomer and a computer scientist, began working on specially designed computers for holography, called HORN, in 1992. The HORN-8, which adopts a calculation method called the ""amplitude type"" for adjusting the intensity of light, was recognized as the world's fastest computer for holography in a publication in the international science journal Nature Electronics on April 17, 2018.With the newly developed ""phase type"" HORN-8, the calculation method for adjusting the phase of light was implemented, and the researchers were successful at projecting holography information as a 3D video with high-quality images. This research was published in Optics Express on September 28, 2018.""We have been developing the high-speed computers for 3D holography by implementing the knowledge of information engineering and the technology of electrical and electronic engineering and by learning insights from computer science and optical methods,"" Ito reflected. ""This is a result of the interdisciplinary approach of our research that has been conducted for over 25 years with the commendable effort by our students who have been studying at our lab.""Takashi Nishitsuji, a former student of Ito's lab and now assistant professor at Tokyo Metropolitan University, who led the experiment, said ""HORN-8 is the fruit of many people's wisdom, skills, and efforts. We want to continue the research of HORN and try other methods from various perspectives for its practical application.""In the latest phase type of HORN-8, eight chips are mounted on the FPGA (Field Programmable Gate Array) board. This enables one to avoid a bottleneck problem for the processing speed with the calculation method, by which the chips are prevented from communicating with each other. With this approach, HORN-8 increases the computing speed in proportion to the number of chips, so that it can project video holography more clearly.Story Source:Materials provided by Chiba University. ",0.0517617,0.0611159,0.098804,0.206543,0.100857,0.0895839,0.118746,0,0
10,Novel machine learning technique for simulating the every day task of dressing,"Putting on clothes is a daily, mundane task that most of us perform with little or no thought. We may never take into consideration the multiple steps and physical motions involved when we're getting dressed in the mornings. But that is precisely what needs to be explored when attempting to capture the motion of dressing and simulating cloth for computer animation.Computer scientists from the Georgia Institute of Technology and Google Brain, Google's artificial intelligence research arm, have devised a novel computational method, driven by machine learning techniques, to successfully and realistically simulate the multi-step process of putting on clothes. When dissected, the task of dressing is quite complex, and involves several different physical interactions between the character and his or her clothing, primarily guided by the person's sense of touch.Creating animation of a character putting on clothing is challenging due to the complex interactions between the character and the simulated garment. Most work in highly constrained character animation deals with static environments which don't react very much to the motion of the character, notes the researchers. In contrast, clothing can respond immediately and drastically to small changes in the position of the body; clothing has the tendency to fold, stick and cling to the body, making haptic, or touch sensation, essential to the task.Another unique challenge about dressing is that it requires the character to perform a prolonged sequence of motion involving a diverse set of subtasks, such as grasping the front layer of a shirt, tucking a hand into the shirt opening and pushing a hand through a sleeve.""Dressing seems easy to many of us because we practice it every single day. In reality, the dynamics of cloth make it very challenging to learn how to dress from scratch,"" says Alexander Clegg, lead author of the research and a computer science PhD student at the Georgia Institute of Technology. ""We leverage simulation to teach a neural network to accomplish these complex tasks by breaking the task down into smaller pieces with well-defined goals, allowing the character to try the task thousands of times and providing reward or penalty signals when the character tries beneficial or detrimental changes to its policy.""The researchers' method then updates the neural network one step at a time to make the discovered positive changes more likely to occur in the future. ""In this way, we teach the character how to succeed at the task,"" notes Clegg.Clegg and his collaborators at Georgia Tech include computer scientists Wenhao Yu, Greg Turk and Karen Liu. Together with Google Brain researcher Jie Tan, the group will present their work at SIGGRAPH Asia 2018 in Tokyo 4 December to 7 December. The annual conference features the most respected technical and creative members in the field of computer graphics and interactive techniques, and showcases leading edge research in science, art, gaming and animation, among other sectors.In this study, the researchers demonstrated their approach on several dressing tasks: putting on a t-shirt, throwing on a jacket and robot-assisted dressing of a sleeve. With the trained neural network, they were able to achieve complex reenactment of a variety of ways an animated character puts on clothes. Key is incorporating the sense of touch into their framework to overcome the challenges in cloth simulation. The researchers found that careful selection of the cloth observations and the reward functions in their trained network are crucial to the framework's success. As a result, this novel approach not only enables single dressing sequences but a character controller that can successfully dress under various conditions.""We've opened the door to a new way of animating multi-step interaction tasks in complex environments using reinforcement learning,"" says Clegg. ""There is still plenty of work to be done continuing down this path, allowing simulation to provide experience and practice for task training in a virtual world."" In expanding this work, the team is currently collaborating with other researchers in Georgia Tech's Healthcare Robotics lab to investigate the application of robotics for dressing assistance.The researchers' method then updates the neural network one step at a time to make the discovered positive changes more likely to occur in the future. ""In this way, we teach the character how to succeed at the task,"" notes Clegg.Clegg and his collaborators at Georgia Tech include computer scientists Wenhao Yu, Greg Turk and Karen Liu. Together with Google Brain researcher Jie Tan, the group will present their work at SIGGRAPH Asia 2018 in Tokyo 4 December to 7 December. The annual conference features the most respected technical and creative members in the field of computer graphics and interactive techniques, and showcases leading edge research in science, art, gaming and animation, among other sectors.In this study, the researchers demonstrated their approach on several dressing tasks: putting on a t-shirt, throwing on a jacket and robot-assisted dressing of a sleeve. With the trained neural network, they were able to achieve complex reenactment of a variety of ways an animated character puts on clothes. Key is incorporating the sense of touch into their framework to overcome the challenges in cloth simulation. The researchers found that careful selection of the cloth observations and the reward functions in their trained network are crucial to the framework's success. As a result, this novel approach not only enables single dressing sequences but a character controller that can successfully dress under various conditions.""We've opened the door to a new way of animating multi-step interaction tasks in complex environments using reinforcement learning,"" says Clegg. ""There is still plenty of work to be done continuing down this path, allowing simulation to provide experience and practice for task training in a virtual world."" In expanding this work, the team is currently collaborating with other researchers in Georgia Tech's Healthcare Robotics lab to investigate the application of robotics for dressing assistance.Story Source:Materials provided by Association for Computing Machinery. ",0.0628929,0.0981197,0.12894,0.296404,0.101795,0.132924,0.182376,0,0
11,Model of quantum artificial life on quantum computer,"A scenario of artificial intelligence could see the emergence of circumstances in which models of simple organisms could be capable of experiencing the various phases of life in a controlled virtual environment. This is what has been designed by the QUTIS research group at the UPV/EHU's Department of Physical Chemistry, but the scenario is that of quantum computers: an artificial life protocol that encodes quantum behaviours belonging to living systems, such as self-replication, mutation, interaction between individuals, birth and death, and has been executed on an IBM ibmqx4 cloud quantum computer. This is the first experimental realization on a quantum computer of a quantum algorithm of artificial life following Darwin's laws of evolution.The algorithm follows a protocol that the researchers refer to as biomimetic and which encodes quantum behaviours adapted to the same behaviours of living systems. Quantum biomimetics involves reproducing in quantum systems certain properties exclusive to living beings, and this research group had previously managed to imitate life, natural selection, learning and memory by means of quantum systems. This research aimed, as the authors themselves describe, ""to design a set of quantum algorithms based on the imitation of biological processes, which take place in complex organisms, and transfer them to a quantum scale, so we were only trying to imitate the key aspects in these processes.""Quantum artificial life with a promising futureIn the scenario of artificial life that they designed, a set of models of simple organisms are capable of accomplishing the most common phases of life in a controlled virtual environment, and have proven that microscopic quantum systems are able to encode quantum characteristics and biological behaviours that are normally associated with living systems and natural selection.The models of organism designed were coined as units of quantum life, each one of which is made up of two qubits that act as genotype and phenotype, respectively, and where the genotype contains the information that describes the type of living unit, and this information is transmitted from generation to generation. By contrast, the phenotype, the characteristics displayed by individuals, are determined by genetic information as well as by the interaction of the individuals themselves with the environment.To be able to regard the systems as organisms of artificial life, the basic characteristics of Darwinian evolution that were simulated by these systems were birth and its evolution, self-replication, interaction between individuals and the environment, which gradually degrades the phenotype of the individual as it ages and ends in a state representing death. The protocol also considers interaction between individuals as well as mutations, which are implemented in random rotations of individual qubits.This experimental test represents the consolidation of the theoretical framework of quantum artificial life in an evolutionary sense, but as the model is scaled up to more complex systems, it will be possible ""to implement more accurate quantum emulations with growing complexity towards quantum supremacy,"" as the authors pointed out.In the same way, they expect these units of artificial life and their possible applications to have profound implications for the community of quantum simulation and quantum computing in a range of quantum platforms, be they trapped ions, photonic systems, neutral atoms or superconductor circuits. According to Enrique Solano, director of the QUTIS group and leader of this project, ""the bases have been established for addressing different levels of classical and quantum complexity. For example, one could consider the growth of populations of quantum individuals with gender criteria, their life aims both as individuals and as groups, automated behaviours without external controls, quantum robotics processes, intelligent quantum systems, until the threshold of quantum supremacy that could only be reached by a quantum computer can be overcome. What would emerge after that would be terribly risky questions, such as guessing the microscopic origin of life itself, the intelligent development of individuals and societies, or addressing the origin of awareness and animal and human creativity. This is only the start; we are at the beginning of the 21st century and we will have many fantasy dreams and questions that we will be able to respond to.""Story Source:Materials provided by University of the Basque Country. ",0.0865233,0.144118,0.1631,0.265276,0.141247,0.157188,0.204684,0,0
12,Virtual reality simulation of a supermassive black hole,"The black hole at the centre of our galaxy, Sagittarius A*, has been visualised in virtual reality for the first time. The details are described in an article published in the open access journal Computational Astrophysics and Cosmology.Scientists at Radboud University, The Netherlands and Goethe University, Germany used recent astrophysical models of Sagittarius A* to create a series of images that were then put together to create a 360 degree virtual reality simulation of the black hole, that can be viewed on widely available VR consoles. The authors suggest that this virtual reality simulation could be useful for studying black holes.Jordy Davelaar, corresponding author, said: ""Our virtual reality simulation creates one of the most realistic views of the direct surroundings of the black hole and will help us to learn more about how black holes behave. Traveling to a black hole in our lifetime is impossible, so immersive visualizations like this can help us understand more about these systems from where we are.""The authors also suggest that the virtual reality simulation could help encourage the general public, including children, to take an interest in astrophysics.Davelaar said: ""The visualisations that we produced have a great potential for outreach. We used them to introduce children to the phenomenon of black holes, and they really learned something from it. This suggests that immersive virtual reality visualizations are a great tool to show our work to a broader audience, even when it involves very complicated systems like black holes.""Heino Falcke, Professor at Radboud University adds: ""We all have a picture in our head of how black holes supposedly look, but science has progressed and we can now make much more accurate renderings -- and these black holes look quite different from what we are used to. These new visualisations are just the start, more to come in the future.""Story Source:Materials provided by BioMed Central. ",0.13483,0.160405,0.197564,0.302708,0.191493,0.183436,0.226704,0,0
13,Electronic skin points the way north Sensors give humans magnetoception,"While birds are able to naturally perceive the Earth's magnetic field and use it for orientation, humans have so far not come close to replicate this feat -- at least, until now. Researchers at the Helmholtz-Zentrum Dresden-Rossendorf (HZDR) in Germany have developed an electronic skin (e-skin) with magnetosensitive capabilities, sensitive enough to detect and digitize body motion in the Earth's magnetic field. As this e-skin is extremely thin and malleable, it can easily be affixed to human skin to create a bionic analog of a compass. This might not only help people with orientation issues, but also facilitate interaction with objects in virtual and augmented reality. The results have been published in the journal Nature Electronics.Just swipe your hand to the left and the virtual panda on the screen will start making its way towards the bottom left. Swipe your hand to the right and you can make the black-and-white animal face the opposite direction. This demonstration is reminiscent of the famous scene from the movie Minority Report where lead actor Tom Cruise controls a computer with nothing but hand gestures. What was science fiction 16 years ago, has now become reality thanks to Dr. Denys Makarov and his team of HZDR researchers. Neither bulky gloves, cumbersome glasses, nor sophisticated camera systems are required to control the panda's path. All it takes is a sliver of polymer foil, no more than a thousandth of a millimeter thick, attached to a finger -- and the Earth's magnetic field.""The foil is equipped with magnetic field sensors that can pick up geomagnetic fields,"" says the lead author Gilbert Santiago Caeen Bermedez. ""We are talking about 40 to 60 microtesla -- that is 1,000 times weaker than a magnetic field of a typical fridge magnet."" This is the first demonstration of highly compliant electronic skins capable of controlling virtual objects relying on the interaction with geomagnetic fields. The previous demonstrations still required the use of an external permanent magnet: ""Our sensors enable the wearer to continuously ascertain his orientation with respect to the earth's magnetic field. Therefore, if he or the body part hosting the sensor changes orientation, the sensor captures the motion, which is then transferred and digitized to operate in the virtual world.""Just like a regular compassThe sensors, which are ultrathin strips of the magnetic material permalloy, work on the principle of the so-called anisotropic magneto-resistive effect, as Caeen Bermedez explains: ""It means that the electric resistance of these layers changes depending on their orientation in relation to an outer magnetic field. In order to align them specifically with the Earth's magnetic field, we decorated these ferromagnetic strips with slabs of conductive material, in this case gold, arranged at a 45-degree angle. Thus, the electric current can only flow at this angle, which changes the response of the sensor to render it most sensitive around very small fields. The voltage is strongest when the sensors point north and weakest when they point south."" The researchers conducted outdoor experiments to demonstrate that their idea works in practical settings.With a sensor attached to his index finger, the user started out from the north, first heading west, then south and back again -- causing the voltage to rise and fall again accordingly. The cardinal directions that were displayed matched those shown on a traditional compass used as a reference. ""This shows that we were able to develop the first soft and ultrathin portable sensor which can reproduce the functionality of a conventional compass and prospectively grant artificial magnetoception to humans,"" Bermedez says. But that is not all. The researchers were also able to transfer the principle to virtual reality, using their magnetic sensors to control a digital panda in the computer game engine, Panda3D.In these experiments, pointing to the north corresponded to a movement of the panda to the left, pointing to the south to a movement to the right. When the hand was on the left, i.e. magnetic north, the panda in the virtual world started moving in that direction. When it swiped in the opposite direction, the animal turned on its heels. ""We were able to transfer the real-world geomagnetic stimuli straight into the virtual realm,"" Denys Makarov summarizes. As the sensors can withstand extreme bending and twisting without losing their functionality, the researchers see great potential for the practical use of their sensors not only as a way to access virtual reality. ""Psychologists, for instance, could study the effects of magnetoception in humans more precisely, without bulky devices or cumbersome experimental setups, which are prone to bias the results,"" Gilbert Santiago Caeen Bermedez projects.Story Source:Materials provided by Helmholtz-Zentrum Dresden-Rossendorf. ",0.0889783,0.0785369,0.104661,0.13304,0.101688,0.161483,0.120671,0,0
14,"There's real skill in fantasy sports Most fantasy sports are based on skill, not luck","If you've ever taken part in the armchair sport of fantasy football and found yourself at the top of your league's standings at the end of the season, a new MIT study suggests your performance -- however far removed from any actual playing field -- was likely based on skill rather than luck.Those looking for ways to improve their fantasy game will have to look elsewhere: The study doesn't identify any specific qualities that make one fantasy player more skilled over another. Instead, the researchers found, based on the win/loss records of thousands of fantasy players over multiple seasons, that the game of fantasy football is inherently a contest that rewards skill.""Some [fantasy] players may know more about statistics, rules of the game, which players are injured, effects of weather, and a host of other factors that make them better at picking players -- that's the skill in fantasy sports,"" says Anette ""Peko"" Hosoi, associate dean of engineering at MIT. ""We ask, does that skill have an impact on the outcome of the [fantasy] game? In our analysis, the signal for skill in the data is very clear.""Other fantasy sports such as baseball, basketball, and hockey also appear to be games of skill -- considerably more so than activities based on pure chance, such as coin-flipping. What ultimately do these results mean for the average fantasy player?""They probably can't use our study to assemble better sports teams,"" says Hosoi, who is also the Neil and Jane Pappalardo Professor of Mechanical Engineering. ""But they can use it to talk better smack when they're at the top of their standings.""The team's findings appear this week in the Society for Industrial and Applied Mathematics Review. Hosoi's co-authors are first author Daniel Getty, a graduate student in MIT's Department of Aeronautics and Astronautics; graduate student Hao Li; former graduate student Charles Gao; and Masayuki Yano of the University of Toronto.A fantasy gambleHosoi and her colleagues began looking into the roles of skill and chance in fantasy sports several years ago, when they were approached by FanDuel, the second largest company in the daily fantasy sports industry. FanDuel provides online platforms for more than 6 million registered users, who use the site to create and manage fantasy teams -- virtual teams made up of real players of professional sports, which fantasy players can pick and draft to their fantasy team. Players can pit their team against other virtual teams, and whether a team wins or loses depends on how the real players perform in actual games in a given day or week.In recent years, the question has arisen as to whether fantasy sports are a potential form of online gambling. Under a federal law known as the Unlawful Internet Gambling Enforcement Act, or UIGEA, online players of games such as poker are prohibited from transmitting across state lines funds won through gambling activities using the internet. The law exempts fantasy sports, stating that the game is not a form of betting or wagering.However, the UIGEA was not drafted to alter the legality of internet wagering, which is, for the most part, determined by individual states. As fantasy sports -- and fantasy football in particular -- have grown more popular, with prominent ads on commercial and cable television, a handful of states have questioned the legality of fantasy sports and the companies that enable them.Gambling, of course, is defined as any money-exchanging activity that depends mostly on chance. Fantasy sports would not be considered a form of gambling if it were proven to be more of a contest of skill.""That is the question that FanDuel wanted us to investigate: Have they designed the contest such that skill is rewarded? If so, then these contests should be classified as games of skill, and are not gambling,"" Hosoi says. ""They gave us all of their data, and asked whether we could determine the relative role of skill and luck in the outcomes.""Tests of skill and chanceThe team analyzed daily fantasy competitions played on FanDuel during the 2013 and 2014 seasons, in baseball, basketball, hockey, and football. In their analysis, the researchers followed guidelines laid out originally by economist and ""Freakonomics"" author Steven Levitt, along with Thomas Miles and Andrew Rosenfield. In a research paper they wrote in 2012, the economists sought to determine whether a game -- in this case, poker -- was based more on skill than on chance.They reasoned that if a game were more skill-based, then a player's performance should be persistent. It might be good or bad, but it would remain relatively constant over multiple rounds.To test this in the context of fantasy sports, Hosoi's team looked at the win/loss record of every fantasy player in FanDuel's dataset, over one season. For each (anonymized) player, the researchers calculated the fraction of wins the player experienced over the first half of the season versus the second half. They then represented each player's performance over an entire season as a single dot on a graph whose vertical and horizontal axes represented the win fraction for the first and second halves of the season, respectively.If a given fantasy sport were based more on skill, then a individual player's win fraction should be approximately the same -- be it 90 percent or 10 percent -- for the first and second halves of the season. When every player's performance is plotted on the same graph, it should roughly resemble a line, indicating a prevalence of skill. On the other hand, if the game were one of chance, every player should have around a 50 percent win fraction, which on the graph would look more like a circular cloud.For every fantasy sport, the researchers found the graph skewed more linear versus circular, indicating games of skill rather than chance.The researchers tested a second hypothesis proposed by Levitt: If a game is based on chance, then every player should have the same expected outcome, just as flipping a coin has the same probability for landing heads versus tails. To test this idea, the team split the fantasy player population into two groups: those that played a large number of games, versus those who only participated in a few.""Even when you correct for biases, like people who quit after losing a lot of games in a row, you find there's a statistically higher win fraction for people who play a lot versus a little, regardless of the [type of] fantasy sport, which is indicative of skill,"" Hosoi says.The last test, again proposed by Levitt, was to see whether a player's actions had any impact on the game's outcome. If the answer is yes, then the game must be one of skill.""So we looked at how the actual playing population on FanDuel performed, versus a random algorithm,"" Hosoi says.The researchers devised an algorithm that created randomly generated fantasy teams from the same pool of players that were available to the FanDuel users. The algorithm was designed to follow the rules of the game and to be relatively smart in how it generated each team.""We ran hundreds of thousands of games, and looked at the scores of actual fantasy players, versus scores of computer-generated fantasy players,"" Hosoi says. ""And you see again that the fantasy players beat the computer-generated ones, indicating that there must be some skill involved.""Sports on a spectrumTo put their findings in perspective, the researchers plotted the results of each fantasy sport on a spectrum of luck versus skill. Along this spectrum, they also included each fantasy sport's real counterpart, along with other activities, such as coin flipping, based entirely on chance, and cyclocross racing, which hinges almost entirely on skill.For the most part, success while playing both fantasy sports and real sports skewed more toward skill, with baseball and basketball, both real and virtual, being more skill-based compared to hockey and football.Hosoi reasons that skill may play a relatively large role in basketball because the sport encompasses more than 80 games in a season.""That's a lot of games, and there are a lot of scoring opportunities in each game,"" Hosoi says. ""If you get a lucky basket, it doesn't matter too much. Whereas in hockey, there are so few scoring opportunities that if you get a lucky goal it makes a difference, and luck can play a much larger role.""Hosoi says the team's results will ultimately be useful in characterizing fantasy sports, both in and out of the legal system.""This is one piece of evidence [courts] have to weigh,"" Hosoi says. ""What I can give them is a quantitative analysis of where [fantasy sports] sit on the skill/luck spectrum. It's mostly skill, but there's always a little bit of luck.""Story Source:Materials provided by Massachusetts Institute of Technology. Original written by Jennifer Chu. ",0.143305,0.156492,0.189183,0.336267,0.187218,0.209951,0.239511,0,0
15,Good news for immersive journalism: Look at your audience,"Immersive journalism uses virtual reality (VR) to put the viewer directly into a news event. But how can you help someone to personally engage and interact with a story, when they cannot alter the narrative?A new study provides an elegant solution: researchers found that small changes suggestive of a possibility for interaction -- in this case, having a virtual character acknowledge your presence when you look towards them -- can have a significant positive impact on peoples' sense of presence in a VR news scenario.""We Wait""Published in Frontiers in Robotics and AI, the study used a VR experience based on BBC News reports of refugees migrating to the EU. The animated scenario, called ""We Wait,"" gives viewers an immersive experience via VR goggles of the plight of refugees waiting to be taken illegally by boat to Europe, across a dangerous stretch of sea.""We were interested in the level of presence people felt watching the scenario, and the extent to which this might prompt them to seek further information about the refugee crisis,"" says study co-author Professor Mel Slater of the University of Barcelona.""By presence we refer to the illusions of place -- the feeling of being in the virtual space -- and plausibility, that the unfolding events were really happening.""I was there, it was realSlater and colleagues assessed the illusions of place and plausibility experienced by participants shown one of four different versions of the ""We Wait"" scenario. In one version, the participant is given a virtual body which they can see when they look down, and when they look at virtual characters these occasionally look back at them. The other three versions lack one or both of these features, i.e. the participant receives no body and/or no acknowledgement.The most striking result was the effect of acknowledgement by the virtual characters.""The simple exchange of glances with the characters greatly enhanced participants' sense of presence in the scenario, by making it more plausibly real"" says Slater.Embodiment in a virtual avatar also increased presence somewhat, by enhancing the illusion of place as well as plausibility. Most promising of all, the virtual body also led participants to seek further information about the refugee crisis -- by visiting a website provided during the study -- for longer after the VR experience.""It is as if there was the implicit inference by participants: These were real people, I was amongst them -- so I had better ?nd out more about what is going on,"" posits Slater.Director of Audiences for the BBC, Nick North, said: ""We know how difficult it can be to drive audiences to online content; from all the BBC's research to date we might expect a small proportion of the audience to go online to a related website after watching a TV programme, for example. So, whilst this was a small study, a 25% conversion rate from the We Wait VR experience is very impressive, and potentially indicative of the significant impact VR could have at scale.""The body of evidenceThese results demonstrate that minimal changes can be helpful in engendering follow-up and further interest in an immersive news story, without compromising faithful reporting.""Have characters respond to actions of the participants, in a way that does not change the narrative,"" recommends study co-author Professor Anthony Steed of UCL, UK.""Also, give the participant a body. Previous findings show that the illusion of embodiment is made much stronger by multisensory stimulation -- where a person observes their virtual body moving and being touched in synchrony with their own movements and sensations -- and it is likely that had we included this, the gain in presence and tendency to follow-up would have been greater,"" adds Steed.An outline of the scenario is available on the BBC Taster and readers can experience the version with embodiment and character responsiveness by downloading ""We Wait"" from the Oculus Store.Story Source:Materials provided by Frontiers. ",0.0939267,0.114358,0.165266,0.267978,0.134036,0.164158,0.185093,0,0
16,Augmented reality may assist cardiologists plan and perform complex procedures,"Augmented reality (AR), a technology that superimposes computer-generated information on a user's view of the real world, offers a new platform to help physicians better visualize complex medical data, particularly before and during medical procedures. A new self-contained AR device aims to provide an immersive AR experience in which surgeons can interactively explore data in three dimensions.Jihye Jang, a PhD Candidate at the Cardiac Magnetic Resonance (MR) Center at Beth Israel Deaconess Medical Center (BIDMC), and colleagues assessed AR's potential to help cardiologists visualize myocardial scarring in the heart as they perform ventricular tachycardia ablation or other electrophysiological interventions. Myocardial scarring can occur in people who experience a heart attack and also stems from the surgical repair of congenital heart disease. The team's findings, published in PLOS ONE, demonstrate that the new augmented reality technology confers a number of advantages.""Augmented reality allows physicians to superimpose images, such as MRI or CT scans, as a guide during therapeutic intervention,"" said Jang. ""Our report shows exciting potential that having this complex 3D scar information through augmented reality during the intervention may help guide treatment and ultimately improve patient care. Physicians can now use AR to view 3D cardiac MR information with a touchless interaction in sterile environment.""By projecting three dimensional imagery onto a glass screen worn like a diving mask on the surgeon's face, AR provides 3D depth perception and allows surgeons to interact with the medical data without physically touching a screen or computer mouse, maintaining a sterile environment and reducing the risk of infection. In Jang and colleague's pilot study, the researchers applied the augmented reality technique as they generated holographic 3D scar in five animal models that underwent controlled infarction and electrophysiological study.3D holographic visualization of the scar was performed to assist assessment of the complex 3D scar architecture. An operator and mapping specialist viewed the holographic 3D scar during electrophysiological study, and completed the perceived usefulness questionnaire in the six-item usefulness scale and found it useful to have scar information during the intervention. The user could interactively explore 3D myocardial scar in the augmented reality environment that allows for the combination of holographic 3D LGE data interacting with any real-world environments, such as a surgical suite or patient's body.""Our report is one of the first efforts to test augmented reality in cardiovascular electrophysiological intervention,"" said Jang. ""Our next steps will expand the use of AR into treatments for arrhythmia by merging the scar information with electrophysiology data.""The senior corresponding author of the paper is Reza Nezafat, PhD, scientific director of the Cardiac Magnetic Resonance Center at BIDMC. Other co-authors include colleagues from BIDMC, University of Pennsylvania and Technical University of Munich.This work was supported by the National Institutes of Health [1R01HL129185, 1R21HL127650]; and the American Heart Association [15EIA22710040]. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.Story Source:Materials provided by Beth Israel Deaconess Medical Center. ",0.104938,0.127728,0.170454,0.260649,0.154948,0.161835,0.187025,0,0
17,Motion sickness vs. cybersickness: Two different problems or the same condition? Findings of a new study contradict previous research,"Contrary to previous research, severe motion sickness and cybersickness -- a type of motion sickness that stems from exposure to virtual reality -- may be considered the same clinical condition, according to researchers. The findings, the first to study both conditions in the same group of people, are published ahead of print in the Journal of Applied Physiology.Motion sickness is a common consequence of ""sensory mismatch."" This happens when what a person sees, feels and senses don't match up, such as when there is a conflict between sensory channels that define body orientation and position in space. The disconnect occurs between the eyes and the vestibular system, which controls the workings of the inner ear, overall balance and orientation of a person in a physical space. Because motion sickness does not involve only the eyes, research has shown that people who are blind can still experience ""classic"" motion sickness. Cybersickness has been thought to be a sub-type of motion sickness because it does not involve the vestibular system and is triggered only by visual stimuli. However, the two conditions share many of the same symptoms, including nausea, sweating, dizziness and fatigue.Researchers from the University of Newcastle in Australia studied the physiological responses to motion sickness and cybersickness in 30 young adult volunteers. Two different trials were separated by at least one week. One trial consisted of exposure to a vestibular stimulus: being blindfolded and riding a motorized rotating chair while tilting their heads at regular intervals. For the visual stimulus trial, the participants ""rode"" a virtual reality rollercoaster. Both trials were designed to span a maximum of 15 minutes. Volunteers were instructed to continue for as long as they could tolerate uncomfortable symptoms. During both trials, the researchers measured the participants' sweat rate through sensors placed on the skin of their foreheads. The volunteers completed questionnaires before and after the study, including a post-trial questionnaire that rated the severity of their discomfort.Only one of the study participants was able to complete the full 15 minutes of either trial, indicating that the majority of the group experienced advanced or severe motion sickness and cybersickness throughout the trials. Nausea, dizziness and feeling hot and sweaty were the symptoms most often reported on the post-trial questionnaire. There was little difference in self-reported severity rating and in objective physiological measures between the motion and cybersickness trials, suggesting ""the clinical picture of advanced motion sickness (assessed as a spectrum of symptoms and as their intensity) is very similar, independently of whether it is induced by pure visual or pure vestibular stimuli. This conclusion contradicts previously published results,"" the researchers wrote.These results may have practical significance that could affect public safety, the research team noted. ""Simple and relatively inexpensive [virtual reality] technology [may be used] for occupational pre-selection tests in those professions where motion sickness is an exclusion criterion or represents a common occupational hazard (e.g., pilots, drivers of public transport, crane operators, etc.).""Story Source:Materials provided by American Physiological Society. ",0.105566,0.109946,0.151672,0.212486,0.120691,0.169518,0.17558,0,0
18,Roadmap for quantum internet development,"A quantum internet may very well be the first quantum information technology to become reality. Researchers at QuTech in Delft, The Netherlands, today published a comprehensive guide towards this goal in Science. It describes six phases, starting with simple networks of qubits that could already enable secure quantum communications -- a phase that could be reality in the near future. The development ends with networks of fully quantum-connected quantum computers. In each phase, new applications become available such as extremely accurate clock synchronization or integrating different telescopes on Earth in one virtual 'supertelescope'. This work creates a common language that unites the highly interdisciplinary field of quantum networking towards achieving the dream of a world-wide quantum internet.A quantum internet will revolutionize communication technology by exploiting phenomena from quantum physics, such as entanglement. Researchers are working on technology that enables the transmission of quantum bits between any two points on earth. Such quantum bits can be '0' and '1' at the same time, and can be 'entangled': their fates are merged in such a way that an operation on one of the qubits instantly affects the state of the other.This brings two features which are provably out of reach for the Internet that we know today. The first is that entanglement allows improved coordination between distant sites. This makes it extremely suitable for tasks such as clock synchronization or the linking of distant telescopes to obtain better images. The second is that entanglement is inherently secure. If two quantum bits are maximally entangled, then nothing else in the universe can have any share in that entanglement. This feature makes entanglement uniquely suitable for applications that require security and privacy.Many other applications of a quantum internet are already known, and more are likely to be discovered as the first networks come online. Researchers at QuTech, a collaboration between Delft University of Technology and the Netherlands organisation for applied scientific research TNO have now set forth stages of quantum internet development distinguished by technological capabilities and corresponding applications.The lowest stage of a true quantum network -- a prepare and measure network -- allows the end-to-end delivery of quantum bits between any two network nodes, one quantum bit at a time. This is already sufficient to support many cryptographic applications of a quantum network. The highest stage is the long-term goal of connecting large quantum computers on which arbitrary quantum applications can be executed.In addition to providing a guide to further development, the work sets challenges both to engineering efforts and to the development of applications. ""On the one hand, we would like to build ever more advanced stages of such at network,"" says Stephanie Wehner, lead author of the work, ""On the other hand, quantum software developers are challenged to reduce the requirements of application protocols so they can be realized already with the more modest technological capabilities of a lower stage."" Co-author Ronald Hanson adds: ""This work establish a much-needed common language between the highly interdisciplinary field of quantum networking spanning physics, computer science and engineering.""The first true quantum networks, allowing the end-to-end transmission of quantum bits, are expected to be realized in the coming years, heralding the dawn of a large-scale quantum internet.Story Source:Materials provided by Delft University of Technology. ",0.0948352,0.128251,0.136377,0.289585,0.148343,0.161246,0.190218,0,0
19,Virtual reality can help make people more compassionate compared to other media,"A Stanford-developed virtual reality experience, called ""Becoming Homeless,"" is helping expand research on how this new immersive technology affects people's level of empathy.According to new Stanford research, people who saw in virtual reality, also known as VR, what it would be like to lose their jobs and homes developed longer-lasting compassion toward the homeless compared to those who explored other media versions of the VR scenario, like text. These findings are set to publish Oct. 17 in PLOS ONE.""Experiences are what define us as humans, so it's not surprising that an intense experience in VR is more impactful than imagining something,"" said Jeremy Bailenson, a professor of communication and a co-author of the paper.Emerging research, nascent technologyMany enthusiasts of virtual reality view the immersive technology as ""the ultimate empathy machine"" that can help people relate to each other better than novels, TV shows or films can.But there is little research examining how exactly this emerging technology can alter people's attitudes.""About 10 million headsets have been sold in the U.S. over the past two years. So, many people now have access to VR experiences. But we still don't know much about how VR affects people,"" said Fernanda Herrera, a graduate student in the Department of Communication and the lead author of the research paper. ""This research is an important step in figuring out how much of an effect this technology can have on people's level of empathy in the long term.""Past research on VR and empathy has shown mixed results and used small sample sizes composed mostly of college students, Herrera said. In addition, previous studies have not examined the long-term effect of VR on empathy -- beyond one week.As part of the research, Herrera, along with Stanford psychology scholar Jamil Zaki, Bailenson and psychology graduate student Erika Weisz, conducted two two-month-long studies with more than 560 participants, age 15 to 88 and representing at least eight ethnic backgrounds. Researcher Elise Ogle was also a co-author on the paper.During the research, some participants were shown ""Becoming Homeless,"" a seven-minute VR experience developed by Stanford's Virtual Human Interaction Lab.In ""Becoming Homeless,"" a narrator guides participants through several interactive VR scenarios that would happen if they lost their jobs. In one scene, the participant has to look around an apartment to select items to sell in order to pay the rent. In another scene, the participant finds shelter on a public bus and has to protect belongings from being stolen by a stranger.The researchers found that participants who underwent ""Becoming Homeless"" were more likely to have enduring positive attitudes toward the homeless than people who did other tasks, such as reading a narrative or interacting with a two-dimensional version of the scenario on a desktop computer. The same people were also more likely to sign a petition in support of affordable housing, according to the research.""Taking the perspective of others in VR produces more empathy and prosocial behaviors in people immediately after going through the experience and over time in comparison to just imagining what it would be like to be in someone else's shoes,"" Herrera said. ""And that is an exciting finding.""Measuring empathy over timeEmpathy, the ability to share and understand someone else's emotions, is a critical part of meaningful social interactions, according to scholars. It has been shown to increase people's understanding of one another and to motivate positive social behaviors, such as donating, volunteering or cooperating with others.""We tend to think of empathy as something you either have or don't have,"" said Zaki, an assistant professor of psychology and a co-author of the paper. ""But lots of studies have demonstrated that empathy isn't just a trait. It's something you can work on and turn up or down in different situations.""The studies' results showed that participants in the ""Becoming Homeless"" condition were significantly more likely to agree with statements like ""Our society does not do enough to help homeless people."" They were also more likely to say that they personally cared ""very much"" about the plight of homeless people. The research also showed that their empathetic attitudes toward the homeless endured.In addition, according to the first study, 82 percent of participants in the VR condition signed a petition supporting affordable housing versus 67 percent of the people who read a narrative that asked them to imagine becoming homeless.In the second study, 85 percent in the VR condition signed the petition in comparison to 63 percent who read the narrative. Of participants who went through the two-dimensional version of the VR experience, 66 percent signed the petition.""What's special about this research is that it gives us longitudinal evidence that VR changes attitudes and behaviors of people in a positive way,"" Bailenson said.More research aheadNot all empathy exercises that introduce perspectives of different groups produce positive effects, the researchers said. For example, previous research has shown that when people are asked to take the perspective of their competitors, they become less empathetic toward them.Similarly, the format of a VR experience also matters when considering how it might alter people's attitudes, Herrera said.Herrera, Bailenson and other researchers are working on other studies to figure out the nuances of VR's effects on people.But for now, Herrera and her team are excited about the evidence that they have gathered in their new study.""Long after our studies were complete, some research participants emailed me to reflect on how they started becoming more involved in the issue afterward. One of them befriended a homeless person in their community and wrote me again once that person found a home,"" Herrera said. ""It was really inspiring to see that positive, lasting impact.""The research was supported by a grant from the Robert Wood Johnson Foundation.Story Source:Materials provided by Stanford University. Original written by Alexandra Shashkevich. ",0.0787957,0.112569,0.161361,0.26105,0.124353,0.151165,0.170109,0,0
20,Supermassive black holes and supercomputers,"The Big Bang has captured our imagination like no other theory in science: the magnificent, explosive birth of our Universe. But do you know what came next?Around 100 million years of darkness.When the cosmos eventually lit up its very first stars, they were bigger and brighter than any that have followed. They shone with UV light so intense, it turned the surrounding atoms into ions. The Cosmic Dawn -- from the first star to the completion of this 'cosmic reionization', lasted roughly one billion years.""Where did these stars come from? And how did they become the galaxies -- the Universe teeming with radiation and plasma -- that we see today? These are our driving questions,"" says Professor Michael Norman, Director of the San Diego Supercomputer Center and lead author of a new review published in Frontiers in Astronomy and Space Sciences.The Universe in a BoxResearchers like Professor Norman solve mathematical equations in a cubic virtual universe.""We have spent over 20 years using and refining this software, to better understand the Cosmic Dawn.""To start, code was created which allowed formation of the first stars in the universe to be modeled. These equations describe the movement and chemical reactions inside gas clouds in a universe before light, and the immense gravitational pull of a much larger but invisible mass of mysterious dark matter.""These clouds of pure hydrogen and helium collapsed under gravity to ignite single, massive stars -- hundreds of times heavier than our Sun,"" explains Norman.The very first heavy elements formed in the pressure-cooker cores of the first stars: just a smidgen of lithium and beryllium. But with the death of these short-lived giants -- collapsing and exploding into dazzling supernovae -- metals as heavy as iron were created in abundance and sprayed into space.Equations were added to the virtual Universe to model enrichment of gas clouds with these newly formed metals -- which drove formation of a new type of star.""The transition was rapid: within 30 million years, virtually all new stars were metal-enriched.""This is despite the fact that chemical enrichment was local and slow, leaving more than 80% of the virtual Universe metal-free by the end of the simulation.""Formation of metal-free giant stars did not stop entirely -- small galaxies of these stars should exist where there is enough dark matter to cool pristine clouds of hydrogen and helium.""But without this huge gravitational pull, the intense radiation from existing stars heats gas clouds and tears apart their molecules. So in most cases, the metal-free gas collapses entirely to form a single, supermassive black hole.""From stars to galaxies""The new generations of stars that formed in galaxies are smaller and far more numerous, because of the chemical reactions made possible with metals,"" Norman observes.The increased number of reactions in gas clouds allowed them to fragment and form multiple stars via 'metal line cooling': tracts of decreased gas density, where combining elements gain room to radiate their energy into space -- instead of each other.At this stage we have the first objects in the universe that can rightfully be called galaxies: a combination of dark matter, metal-enriched gas, and stars.""The first galaxies are smaller than expected because intense radiation from young, massive stars drives dense gas away from star-forming regions.""In turn, radiation from the very smallest galaxies contributed significantly to cosmic reionization.""These hard-to-detect but numerous galaxies can therefore account for the predicted end date of the Cosmic Dawn -- i.e., when cosmic reionization was complete.Thinking outside the boxNorman and colleagues explain how some groups are overcoming computing limitations in these numerical simulations by importing their ready-made results, or by simplifying parts of a model less relevant to the outcomes of interest.""These semi-analytical methods have been used to more accurately determine how long massive metal-free early stars were being created, how many should still be observable, and the contribution of these -- as well as black holes and metal-enriched stars -- to cosmic reionization.""The authors also highlight areas of uncertainty that will drive a new generation of simulations, using new codes, on future high-performance computing platforms.""These will help us to understand the role of magnetic fields, X-rays and space dust in gas cooling, and the identity and behavior of the mysterious dark matter that drives star formation.""Story Source:Materials provided by Frontiers. ",0.0998523,0.151474,0.142174,0.178133,0.12846,0.162665,0.192354,0,0
21,New reservoir computer marks first-ever microelectromechanical neural network application,"As artificial intelligence has become increasingly sophisticated, it has inspired renewed efforts to develop computers whose physical architecture mimics the human brain. One approach, called reservoir computing, allows hardware devices to achieve the higher-dimension calculations required by emerging artificial intelligence. One new device highlights the potential of extremely small mechanical systems to achieve these calculations.A group of researchers at the Universite de Sherbrooke in Quebec, Canada, reports the construction of the first reservoir computing device built with a microelectromechanical system (MEMS). Published in the Journal of Applied Physics, from AIP Publishing, the neural network exploits the nonlinear dynamics of a microscale silicon beam to perform its calculations. The group's work looks to create devices that can act simultaneously as a sensor and a computer using a fraction of the energy a normal computer would use.The article appears in a special topic section of the journal devoted to ""New Physics and Materials for Neuromorphic Computation,"" which highlights new developments in physical and materials science research that hold promise for developing the very large-scale, integrated ""neuromorphic"" systems of tomorrow that will carry computation beyond the limitations of current semiconductors today.""These kinds of calculations are normally only done in software, and computers can be inefficient,"" said Guillaume Dion, an author on the paper. ""Many of the sensors today are built with MEMS, so devices like ours would be ideal technology to blur the boundary between sensors and computers.""The device relies on the nonlinear dynamics of how the silicon beam, at widths 20 times thinner than a human hair, oscillates in space. The results from this oscillation are used to construct a virtual neural network that projects the input signal into the higher dimensional space required for neural network computing.In demonstrations, the system was able to switch between different common benchmark tasks for neural networks with relative ease, Dion said, including classifying spoken sounds and processing binary patterns with accuracies of 78.2 percent and 99.9 percent respectively.""This tiny beam of silicon can do very different tasks,"" said Julien Sylvestre, another author on the paper. ""It's surprisingly easy to adjust it to make it perform well at recognizing words.""Sylvestre said he and his colleagues are looking to explore increasingly complicated computations using the silicon beam device, with the hopes of developing small and energy-efficient sensors and robot controllers.Story Source:Materials provided by American Institute of Physics. ",0.0624354,0.0770512,0.107316,0.233932,0.113341,0.117298,0.151172,0,0
22,"Studies show potential of virtual reality to reduce children's anxiety, pain ","Innovative virtual reality technologies hold promise in reducing children's anxiety and pain before and after medical procedures and surgery, suggest two studies being presented at the ANESTHESIOLOGYe 2018 annual meeting.One study found virtual reality that induces a hypnotic state helped manage postoperative anxiety and pain in children. A second study suggests that a novel virtual reality program that has the ability to adjust cognitive load (the amount of information that is being processed) and reorient the program's display when a child changes position during medical procedures may reduce their anxiety and perception of pain.Postoperative virtual reality hypnosis reduces need for anxiety, pain medicationsThe first study found virtual reality hypnosis (VRH) reduced anxiety, total postoperative opioid consumption and vomiting in children after scoliosis surgery.Twenty-one children were included in the study. Ten children received VRH support after surgery in addition to usual postoperative pain management, which included patient-controlled analgesia with opioids, while 11 children did not receive VRH.""It's been well documented that reducing preoperative anxiety correlates with reducing postoperative pain,"" said study co-author Girish P. Joshi, M.D., professor of anesthesiology and pain management at the University of Texas Southwestern Medical School, Dallas. ""To our knowledge, this is the first study to assess the postoperative use of virtual reality hypnosis in children. Our goal is not only to decrease pain, but also to decrease opioid use.""The children receiving VRH participated in one session per day for 20 minutes during the first 72 hours after surgery. They wore a helmet and goggles linked to software, which uses virtual reality hypnosis technology to decrease stress and anxiety after painful medical procedures. The children chose a scenario -- beach, submarine or walking in the mountains -- and heard soothing speech and music to induce hypnosis.The VRH group had a significantly lower need for supplemental treatment -- only 20 percent in the VRH group required pain medication (IV morphine), compared to 62.5 percent in the control group, while 37.5 percent of the VRH group required anti-anxiety medication, compared to 100 percent in the control group. Total postoperative IV morphine consumption over 72 hours after surgery was significantly less in the VRH group -- 1.18 mg/kg compared to 2.15 mg/kg in the control group.""Morphine is associated with a sedating effect that may prevent patients from eating and walking sooner,"" Dr. Joshi said. ""The significant reduction in morphine use seen in patients given VRH means they can have a quicker recovery.""Children in the VRH group were able to sit up and/or walk more quickly after surgery (23 hours compared to 33 hours in the control group), another important sign of recovery, noted Dr. Joshi. Additionally, children in the VRH group experienced less vomiting (40 percent vs. 83.3 percent in the control group) and their urinary catheters were removed sooner (20 hours vs. 41 hours in the control group). Total hospital length of stay was 125.5 hours in the VRH group compared to 137.7 hours in the control group.""We are very encouraged by the initial findings in this pilot study,"" said Dr. Joshi. ""Next we need to conduct a randomized controlled study. Our hope is that virtual reality hypnosis will one day be used for most patients undergoing all types of surgeries to reduce postoperative pain and speed recovery.""Using virtual reality to reduce children's anxiety, perception of painThe second study examined over 600 patients, ages 6 to 18 years old, who were given a novel virtual reality (VR) game, which allows the user or clinician to reorient the game play in real time and adjust cognitive load to increase its ability to distract patients, so their perception of pain is decreased.""We have known for years that pain and anxiety are highly correlated,"" said study co-author Thomas J. Caruso, M.D., clinical associate professor, anesthesiology, perioperative and pain medicine at Stanford University, California. ""If someone is extremely anxious about something that is about to occur, they are more likely to report pain. Our goal is to distract our pediatric patients to reduce their anxiety and perception of pain.""The program was launched in 2017 as part of CHARIOT (childhood anxiety reduction through innovation and technology), a Stanford University program that uses VR in a variety of settings, both in the hospital and clinic, to distract patients during periods of high anxiety such as IV placement, blood draws, port access or dressing changes. The researchers developed the new VR platform in response to a problem they had encountered when using other VR programs -- when a patient changed position, such as going from lying down to sitting, existing VR technology did not change position along with them.""During a typical VR experience, when you look up, you are looking at the sky,"" explained study lead author Samuel Rodriguez, M.D., clinical assistant professor, anesthesiology, perioperative and pain medicine at Stanford. ""The reorientation feature that we developed allows the horizon line to be reoriented every time the patient's head position changes.""The program also adjusts the patient's cognitive load, which means using faster music, more colors or increased game play to increase the game's ability to distract in order to reduce anxiety. It is used primarily before and during minor medical procedures such as IV placement, wound care, cast or surgical pin removal and nasal endoscopy, typically for 2 to 20 minutes.""Preliminary results have suggested our new VR game decreases anxiety and patients' perception of pain,"" Dr. Caruso said. ""The randomized trial is ongoing, and the results have not yet been finalized. There have been no reports of side effects or patient intolerance in a wide range of inpatient and outpatient settings. We are currently expanding use of the technology into our institution's obstetrics service line for epidural placement in laboring mothers as well as for physical therapy.""The authors note that while further studies are needed to quantify the anxiety-reducing and pain-reducing effects, their case series demonstrates the feasibility of this VR experience as a non-pharmacological treatment that can distract patients safely while limiting patient movement in a wide variety of clinical settings.Story Source:Materials provided by American Society of Anesthesiologists. ",0.128229,0.131371,0.184795,0.271463,0.15769,0.221417,0.209439,0,0
23,Smart devices could soon tap their owners as a battery source,"The world is edging closer to a reality where smart devices are able to use their owners as an energy resource, say experts from the University of Surrey.In a study published by the Advanced Energy Materials journal, scientists from Surrey's Advanced Technology Institute (ATI) detail an innovative solution for powering the next generation of electronic devices by using Triboelectric Nanogenerators (TENGs). Along with human movements, TENGs can capture energy from common energy sources such as wind, wave, and machine vibration.A TENG is an energy harvesting device that uses the contact between two or more (hybrid, organic or inorganic) materials to produce an electric current.Researchers from the ATI have provided a step-by-step guide on how to construct the most efficient energy harvesters. The study introduces a ""TENG power transfer equation"" and ""TENG impedance plots,"" tools which can help improve the design for power output of TENGs.Professor Ravi Silva, Director of the ATI, said: ""A world where energy is free and renewable is a cause that we are extremely passionate about here at the ATI (and the University of Surrey) -- TENGs could play a major role in making this dream a reality. TENGs are ideal for powering wearables, internet of things devices and self-powered electronic applications. This research puts the ATI in a world leading position for designing optimized energy harvesters.""Ishara Dharmasena, PhD student and lead scientist on the project, said: ""I am extremely excited with this new study which redefines the way we understand energy harvesting. The new tools developed here will help researchers all over the world to exploit the true potential of triboelectric nanogenerators, and to design optimised energy harvesting units for custom applications.""Story Source:Materials provided by University of Surrey. ",0.109879,0.128297,0.139457,0.195302,0.133217,0.171325,0.162345,0,0
24,Virtual reality motion sickness may be predicted and counteracted,"Researchers at the University of Waterloo have made progress towards predicting who is likely to feel sick from virtual reality technology.In a recent study, the researchers found they could predict whether an individual will experience cybersickness (motion sickness caused by virtual reality) by how much they sway in response to a moving visual field. The researchers think that this knowledge will help them to develop counteractions to cybersickness.Cybersickness involves nausea and discomfort that can last for hours after participating in virtual reality (VR) applications, which have become prevalent in gaming, skills training and clinical rehabilitation.""Despite decreased costs and significant benefits offered by VR, a large number of users are unable to use the technology for more than a brief period because it can make them feel sick,"" Seamas Weech, a postdoctoral research fellow at the Department of Kinesiology and lead author of the paper. ""Our results show that this is partly due to differences in how individuals use vision to control their balance. By refining our predictive model, we will be able to rapidly assess an individual's tolerance for virtual reality and tailor their experience accordingly.""In conducting their work, the researchers collected several sensorimotor measures, such as balance control and self-motion sensitivity, from 30 healthy participants aged 18-30.The researchers then exposed the participants to VR with the aim of predicting the severity of motion sickness. Using a regression model, they significantly predicted how much cybersickness participants experienced after being exposed to a zero-gravity space simulator in VR.""Knowing who might suffer from cybersickness, and why, allows us to develop targeted interventions to help reduce, or even prevent, the onset of symptoms,"" said Michael Barnett-Cowan, neuroscience professor in the Department of Kinesiology and senior author of the paper. ""Considering this technology is in a growth phase with industries such as gaming, design, medicine and automotive starting to use it, understanding who is negatively impacted and how to help them is crucial.""Story Source:Materials provided by University of Waterloo. ",0.0923188,0.113729,0.154996,0.257683,0.124313,0.169099,0.179516,0,0
25,Scientists use artificial neural networks to predict new stable materials,"Artificial neural networks -- algorithms inspired by connections in the brain -- have ""learned"" to perform a variety of tasks, from pedestrian detection in self-driving cars, to analyzing medical images, to translating languages. Now, researchers at the University of California San Diego are training artificial neural networks to predict new stable materials.""Predicting the stability of materials is a central problem in materials science, physics and chemistry,"" said senior author Shyue Ping Ong, a nanoengineering professor at the UC San Diego Jacobs School of Engineering. ""On one hand, you have traditional chemical intuition such as Linus Pauling's five rules that describe stability for crystals in terms of the radii and packing of ions. On the other, you have expensive quantum mechanical computations to calculate the energy gained from forming a crystal that have to be done on supercomputers. What we have done is to use artificial neural networks to bridge these two worlds.""By training artificial neural networks to predict a crystal's formation energy using just two inputs -- electronegativity and ionic radius of the constituent atoms -- Ong and his team at the Materials Virtual Lab have developed models that can identify stable materials in two classes of crystals known as garnets and perovskites. These models are up to 10 times more accurate than previous machine learning models and are fast enough to efficiently screen thousands of materials in a matter of hours on a laptop. The team details the work in a paper published Sept. 18 in Nature Communications.""Garnets and perovskites are used in LED lights, rechargeable lithium-ion batteries, and solar cells. These neural networks have the potential to greatly accelerate the discovery of new materials for these and other important applications,"" noted first author Weike Ye, a chemistry Ph.D. student in Ong's Materials Virtual Lab.The team has made their models publicly accessible via a web application at http://crystals.ai. This allows other people to use these neural networks to compute the formation energy of any garnet or perovskite composition on the fly.The researchers are planning to extend the application of neural networks to other crystal prototypes as well as other material properties.Story Source:Materials provided by University of California - San Diego. ",0.0659809,0.136178,0.131829,0.31213,0.124088,0.124023,0.188739,0,0
26,3-D virtual simulation gets to the 'heart' of irregular heartbeats,"In a proof of concept study, scientists at Johns Hopkins report they have successfully performed 3D personalized virtual simulations of the heart to accurately identify where cardiac specialists should electrically destroy cardiac tissue to stop potentially fatal irregular and rapid heartbeats in patients with scarring in the heart. The retrospective analysis of 21 patients and prospective study of five patients with ventricular tachycardia, the researchers say, demonstrate that 3D simulation-guided procedures are worthy of expanded clinical trials.Results of the study are described in the Sept. 3 issue of Nature Biomedical Engineering.""Cardiac ablation, or the destruction of tissue to stop errant electrical impulses, has been somewhat successful but hampered by a lot of guesswork and variability in the way that physicians figure out which locations to zap with a catheter,"" says Natalia Trayanova, Ph.D., the Murray B. Sachs Professor in the Department of Biomedical Engineering at The Johns Hopkins University Schools of Engineering and Medicine. ""Our new study results suggest we can remove a lot of the guesswork, standardize treatment and decrease the variability in outcomes, so that patients remain free of arrhythmia in the long term,"" she adds.When a normal heart contracts to pump blood throughout the body, a wave of electrical signals flows through the heart, stimulating each cardiac cell to contract -- one after the other -- in a normal rhythm. After the heart contracts, it relaxes and refills with blood.In people with ventricular tachycardia, the electrical signals in the heart's lower chambers misfire and get stuck within the fist-size organ, crippling the relaxation and refilling process and producing rapid and irregular pulses -- or arrhythmias -- linked to an estimated 300,000 sudden cardiac deaths in the U.S. each year.Numerous drugs are available to treat and manage so-called infarct-related ventricular tachycardia, but side effects and limitations of the drugs have increased focus on other interventions, especially the potential of cardiac ablation that essentially ""rewires"" the electrical signaling that gives rise to the arrhythmias. Trayanova says current estimates indicate that cardiac ablation is successful anywhere between 50 and 88 percent of the time, but outcomes are difficult to predict.To perform a traditional ablation, doctors thread a catheter through blood vessels to reach the heart, and use radiofrequency waves to destroy regions in the heart tissue believed to sustain and propagate erratic electrical waves. Mapping of the heart's electrical functioning with a catheter is used to locate likely problem areas, but as Trayanova notes, precise pinpointing of those tissues has been a challenge.In a bid to locate arrhythmias more precisely, Trayanova and her research team developed 3D personalized computational models of patients' hearts based on contrast-enhanced clinical MRI images. Each heart tissue cell in the model generates electrical signals with the aid of mathematical equations representing how heart cells behave when they are healthy, or when they are semiviable when near the scar. By poking the patient's virtual heart with small electrical signals in different locations, the computer program then determines whether the heart develops an arrhythmia and the location of the tissue that perpetuates it. Using the model, Trayanova then simulates an ablation to that area of the heart and runs the computer program over and over to find multiple locations that doctors should ablate on the actual patient.Among the experiments in the current study, Trayanova and her team used MRI images to create personalized heart models of 21 people who previously had successful cardiac ablation procedures for infarct-related ventricular tachycardia at The Johns Hopkins Hospital between 2006 and 2017. The 3D modeling of these patients correctly identified and predicted the locations where physicians ablated heart tissue. In five patients, the amount of ablated tissue identified by the 3D model was smaller overall -- in some cases, more than 10 times smaller -- than the area that was destroyed during the patients' procedures.Next, the research team tested the 3D simulation to guide cardiac ablation treatments for three patients with ventricular tachycardia at the University of Utah and two patients at the University of Pennsylvania. Two patients who received the simulation-guided ablation procedure have remained free of tachycardia throughout follow-up periods of 23 and 21 months. One patient who had the simulation procedure remained free of tachycardia after two months of follow up. In two patients, the virtual heart approach predicted that tachycardias would not be inducible -- this was confirmed during the clinical procedure, so cardiac ablation was not performed.With this prospective test, the research team demonstrated the feasibility of integrating a computer-simulated prediction into the clinical routine. The patient is scanned approximately 24 hours or less before the procedure. Then, the simulation is created and a prediction is made of where physicians should perform the ablation. Finally, the predicted set of ablation targets is imported into the mapping system before the patient's procedure so that the ablation catheter is navigated directly to the predicted targets.The study represents the first attempt to incorporate personalized simulation predictions as part of anti-arrhythmia treatment. The researchers believe that implementing these predictions will cut down the lengthy and invasive cardiac mapping process and reduce complications experienced by patients. The technology could also reduce the need for repeat procedures through its ability to make the infarcted heart incapable of creating new arrhythmias.""It's an exciting blend of engineering and medicine,"" says Trayanova.""One of the main challenges of catheter ablation is that we are performing procedures on very sick patients with advanced heart disease who have multiple areas in their heart that could sustain arrhythmias,"" says Jonathan Chrispin, M.D., Robert E. Meyerhoff Assistant Professor of Medicine at the Johns Hopkins University School of Medicine, who will lead the clinical trials of this technology. ""We are excited to begin testing Trayanova's approach in a prospective clinical trial. We are hopeful that it can help us achieve our overarching goal of improving quality of life for patients suffering from treatment-resistant ventricular tachycardia.""Trayanova says the results of a clinical trial are needed to validate the promise of personalized simulation guidance for infarct-related ablation treatments. Further clinical study planned at The Johns Hopkins Hospital was recently approved by the Food and Drug Administration under an investigational device exemption.In addition to Trayanova, other scientists who conducted the experiments and clinical studies and contributed to the research include Adityo Prakosa, Hermenegild Arevalo, Dongdong Deng, Patrick Boyle, Plamen Nikolov, Hiroshi Ashikaga, Carolyn Park, Henry Halperin, Robert Blake III and Jonathan Chrispin from Johns Hopkins; Joshua Blauer, Elyar Ghafoori, Rob MacLeod, Frederick Han and Ravi Ranjan from the University of Utah; and David Callans and Saman Nazarian from the University of Pennsylvania.The research was funded by the National Institutes of Health's Director's Pioneer Award (DP1-HL123271).Story Source:Materials provided by Johns Hopkins Medicine. ",0.104358,0.151199,0.174498,0.250539,0.148202,0.172423,0.20648,0,0
27,Proactive approach to defending computer systems,"A team of researchers from the U.S. Army Research Laboratory, the University of Canterbury in New Zealand and the Gwangju Institute of Science and Technology in the Republic of Korea have taken a step toward the development of moving target defense techniques in software-defined networks. This is a demanding cybersecurity research topic, scientists said.This collaboration grew out of efforts of ARL researchers Dr. Jin-Hee Cho (now at Virginia Tech in the Department of Computer Science), Dr. Terrence J. Moore and Dr. Frederica Nelson reaching out to researchers in Asia Pacific regions through the international collaborative program administrated by the U.S. Army International Technology Center-Pacific.Cyberattacks on computer systems are becoming more common. Any company with information on a computer system connected to the internet might become a victim from someone or some group who wants to steal or destroy the company's data for their own use, or for ransom.This is possible because of the way the Internet is set up, researchers said. In order to access content on a website, a computer needs to know where to ask for the information. Websites have an address, what is known as an internet protocol, or IP, address; however, these are not just used for websites. Every computer connected to the internet has an IP address.Cyber attackers have time to discover the IP addresses of the computers they think might have valuable information and attack them using code that is more commonly known as computer viruses or worms.If the computer or system being attacked has a security system, such as a firewall or anti-virus software, it might be able to recognize some code as being bad and prevent itself from being infected.What cyber attackers do is slightly modify their bad code so it is not recognized until the computer's security system is updated or patched.Essentially, the typical defensive response to these attacks is passive, the researchers said. The attackers have time to prepare, plan and execute their attacks, whereas the potential victims are left reacting only after an intruder breaks into a computer system.Recently, a new proactive type of defense is being considered to protect important information in computer systems. This approach is known as moving target defense, or MTD.""The concept of MTD has been introduced with the aim of increasing the adversary's confusion or uncertainty by dynamically changing the attack surface, which consists of the reachable and exploitable vulnerabilities,"" Cho said. ""MTD can lead to making the adversary's intelligence gained from previous monitoring no longer useful and accordingly results in poor attack decisions.""The basic idea as it applies to IP addresses on computer networks is this: Change the IP address of the computer frequently enough so the attacker loses sight of where his victim is; however, this can be expensive, so the approach taken by the researchers in the collaboration here uses something known as software-defined networking.This lets computers keep their real IP addresses fixed, but masks them from the rest of the internet with virtual IP addresses that are frequently changing.Moore added that as the adage suggests, it is harder to hit a moving target.""MTD increases uncertainty and confuses the adversary, as time is no longer an advantage,"" Moore said. ""The adversary has to expend more resources, such as time and/or computational power, to discover vulnerabilities of a target system, but will experience more difficulty in exploiting any vulnerabilities found in the past since their location or accessibility is constantly changing.""According to Professor Hyuk Lim at GIST in the Republic of Korea, this proactive defense approach provides defense services before attackers get into a target system.""Taking actions proactively requires extra overhead to add another layer of defense strength,"" Kim said. ""Hence, deploying the proactive defense and security mechanisms is not for free, but brings a cost because the system needs to constantly change the attack surface such as IP addresses. This cost can be mitigated to some extent by leveraging the technology called 'Software-Defined Networking'. The SDN technology provides highly efficient programmatic and dynamic management of the network policy by removing the network control from individual devices in a network to a centralized controller. The network configuration can be defined by the SDN controller, enabling more reliable and responsive network operations under variable conditions.""Nelson explained the reason why these SDN-based MTD techniques are critical to supporting the vision of the Army and warfighters.""The key technology of SDN-based MTD techniques, under development by the research team, is highly relevant to support the warfighters' mission execution by proactively thwarting potential attacks, which can protect the defense system so that the warfighters can properly execute the mission in the presence of highly dynamic, hostile and innovative adversaries within contested tactical environments,"" Nelson said.The UC team in New Zealand led the effort of developing the MTD technology called the Flexible Random Virtual IP Multiplexing, namely FRVM.""In FRVM, while the real IP address of a server-host remains unchanged but stays hidden, a virtual IP address of the server-host keeps being randomly and periodically changed where the IP mapping/remapping (i.e., called multiplexing/demultiplexing) is performed by an SDN controller,"" said Dilli P. Sharma, a doctoral student in Prof. DongSeong Kim's cybersecurity research group at UC, New Zealand. ""This effectively forces the adversary to play the equivalent of an honest shell game. However, instead of guessing among three shells (IP addresses) to find a pea (a running network service), the adversary must guess among 65,536 shells, given address space2^16. This MTD protocol is novel because it provides high flexibility to have multiple, random, time-variant IP addresses in a host, which implies the adversary will require more time to discover an IP address of the target host.""In this research, the team formulated the architecture and communication protocols for the proposed IP (de)multiplexing-based MTD to be applied in SDN environments.The team also validated the effectiveness of the FRVM under various degrees of scanning attacks in terms of the attack success probability.The preliminary results evaluating the FRVM were presented at the 17th Institute of Electrical and Electronics Engineers International Conference on Trust, Security and Privacy in Computing and Communications, or TrustCom'18, held in New York in August.""Our next step is to study the trade-off in the FRVM between the dual conflicting goals of system security and performance, as proactive defense may introduce adverse effects when running MTD techniques while achieving enhanced security,"" Kim said.DP Sharma, D-S Kim, S Yoon, H Lim, J-H Cho, TJ Moore, ""FRVM: Flexible random virtual ip multiplexing in software-defined networks,"" 2018 17th IEEE International Conference On Trust, Security And Privacy In Computing And Communications, New York City, July 31- August 3, 2018, p. 579-587.Sharma DP, Kim DS, Yoon S, Lim H, Cho JH, Moore TJ. FRVM: Flexible Random Virtual IP Multiplexing in Software-Defined Networks. arXiv preprint arXiv:1807.09343. 2018 Jul 18.Story Source:Materials provided by U.S. Army Research Laboratory. ",0.0538417,0.0643556,0.107727,0.292254,0.109108,0.0929265,0.137779,0,0
28,Could AI robots develop prejudice on their own?,"Showing prejudice towards others does not require a high level of cognitive ability and could easily be exhibited by artificially intelligent machines, new research has suggested.Computer science and psychology experts from Cardiff University and MIT have shown that groups of autonomous machines could demonstrate prejudice by simply identifying, copying and learning this behaviour from one another.It may seem that prejudice is a human-specific phenomenon that requires human cognition to form an opinion of, or to stereotype, a certain person or group.Though some types of computer algorithms have already exhibited prejudice, such as racism and sexism, based on learning from public records and other data generated by humans, this new work demonstrates the possibility of AI evolving prejudicial groups on their own.The new findings, which have been published in the journal Scientific Reports, are based on computer simulations of how similarly prejudiced individuals, or virtual agents, can form a group and interact with each other.In a game of give and take, each individual makes a decision as to whether they donate to somebody inside of their own group or in a different group, based on an individual's reputation as well as their own donating strategy, which includes their levels of prejudice towards outsiders.As the game unfolds and a supercomputer racks up thousands of simulations, each individual begins to learn new strategies by copying others either within their own group or the entire population.Co-author of the study Professor Roger Whitaker, from Cardiff University's Crime and Security Research Institute and the School of Computer Science and Informatics, said: ""By running these simulations thousands and thousands of times over, we begin to get an understanding of how prejudice evolves and the conditions that promote or impede it.""Our simulations show that prejudice is a powerful force of nature and through evolution, it can easily become incentivised in virtual populations, to the detriment of wider connectivity with others. Protection from prejudicial groups can inadvertently lead to individuals forming further prejudicial groups, resulting in a fractured population. Such widespread prejudice is hard to reverse.""The findings involve individuals updating their prejudice levels by preferentially copying those that gain a higher short term payoff, meaning that these decisions do not necessarily require advanced cognitive abilities.""It is feasible that autonomous machines with the ability to identify with discrimination and copy others could in future be susceptible to prejudicial phenomena that we see in the human population,"" Professor Whitaker continued.""Many of the AI developments that we are seeing involve autonomy and self-control, meaning that the behaviour of devices is also influenced by others around them. Vehicles and the Internet of Things are two recent examples. Our study gives a theoretical insight where simulated agents periodically call upon others for some kind of resource.""A further interesting finding from the study was that under particular conditions, which include more distinct subpopulations being present within a population, it was more difficult for prejudice to take hold.""With a greater number of subpopulations, alliances of non-prejudicial groups can cooperate without being exploited. This also diminishes their status as a minority, reducing the susceptibility to prejudice taking hold. However, this also requires circumstances where agents have a higher disposition towards interacting outside of their group,"" Professor Whitaker concluded.Story Source:Materials provided by Cardiff University. ",0.0445547,0.0705559,0.107454,0.234066,0.0826188,0.100546,0.13873,0,0
29,A new theory for phantom limb pain points the way to more effective treatment,"Dr Max Ortiz Catalan of Chalmers University of Technology, Sweden, has developed a new theory for the origin of the mysterious condition, 'phantom limb pain'. Published in the journal Frontiers in Neurology, his hypothesis builds upon his previous work on a revolutionary treatment for the condition, that uses machine learning and augmented reality.Phantom limb pain is a poorly understood phenomenon, in which people who have lost a limb can experience severe pain, seemingly located in that missing part of the body. The condition can be seriously debilitating and can drastically reduce the sufferer's quality of life. But current ideas on its origins cannot explain clinical findings, nor provide a comprehensive theoretical framework for its study and treatment.Now, Max Ortiz Catalan, Associate Professor at Chalmers University of Technology, has published a paper that offers up a promising new theory -- one that he terms 'stochastic entanglement'.He proposes that after an amputation, neural circuitry related to the missing limb loses its role and becomes susceptible to entanglement with other neural networks -- in this case, the network responsible for pain perception.""Imagine you lose your hand. That leaves a big chunk of 'real estate' in your brain, and in your nervous system as a whole, without a job. It stops processing any sensory input, it stops producing any motor output to move the hand. It goes idle -- but not silent,"" explains Max Ortiz Catalan.Neurons are never completely silent. When not processing a particular job, they might fire at random. This may result in coincidental firing of neurons in that part of the sensorimotor network, at the same time as from the network of pain perception. When they fire together, that will create the experience of pain in that part of the body.""Normally, sporadic synchronised firing wouldn't be a big deal, because it's just part of the background noise, and it won't stand out,"" continues Max Ortiz Catalan. ""But in patients with a missing limb, such event could stand out when little else is going on at the same time. This can result in a surprising, emotionally charged experience -- to feel pain in a part of the body you don't have. Such a remarkable sensation could reinforce a neural connection, make it stick out, and help establish an undesirable link.""Through a principle known as 'Hebb's Law' -- 'neurons that fire together, wire together' -- neurons in the sensorimotor and pain perception networks become entangled, resulting in phantom limb pain. The new theory also explains why not all amputees suffer from the condition- the randomness, or stochasticity, means that simultaneous firing may not occur, and become linked, in all patients.In the new paper, Max Ortiz Catalan goes on to examine how this theory can explain the effectiveness of Phantom Motor Execution (PME), the novel treatment method he previously developed. During PME treatment, electrodes attached to the patient's residual limb pick up electrical signals intended for the missing limb, which are then translated through AI algorithms, into movements of a virtual limb in real time. The patients see themselves on a screen, with a digitally rendered limb in place of their missing one, and can then control it just as if it were their own biological limb . This allows the patient to stimulate and reactivate those dormant areas of the brain.""The patients can start reusing those areas of brain that had gone idle. Making use of that circuitry helps to weaken and disconnect the entanglement to the pain network. It's a kind of 'inverse Hebb's law' -- the more those neurons fire apart, the weaker their connection. Or, it can be used preventatively, to protect against the formation of those links in the first place,"" he says.The PME treatment method has been previously shown to help patients for whom other therapies have failed. Understanding exactly how and why it can help is crucial to ensuring it is administered correctly and in the most effective manner. Max Ortiz Catalan's new theory could help unravel some of the mysteries surrounding phantom limb pain, and offer relief for some of the most affected sufferers.Story Source:Materials provided by Chalmers University of Technology. ",0.114133,0.128859,0.165532,0.27948,0.143586,0.2204,0.202474,0,0
30,Android child's face strikingly expressive Quantitative approach adds rich nuance to the expressions of their robot child face,"Japan's affection for robots is no secret. But is the feeling mutual in the country's amazing androids? We may now be a step closer to giving androids greater facial expressions to communicate with.While robots have featured in advances in healthcare, industrial, and other settings in Japan, capturing humanistic expression in a robotic face remains an elusive challenge. Although their system properties have been generally addressed, androids' facial expressions have not been examined in detail. This is owing to factors such as the huge range and asymmetry of natural human facial movements, the restrictions of materials used in android skin, and of course the intricate engineering and mathematics driving robots' movements.A trio of researchers at Osaka University has now found a method for identifying and quantitatively evaluating facial movements on their android robot child head. Named Affetto, the android's first-generation model was reported in a 2011 publication. The researchers have now found a system to make the second-generation Affetto more expressive. Their findings offer a path for androids to express greater ranges of emotion, and ultimately have deeper interaction with humans.The researchers reported their findings in the journal Frontiers in Robotics and AI.""Surface deformations are a key issue in controlling android faces,"" study co-author Minoru Asada explains. ""Movements of their soft facial skin create instability, and this is a big hardware problem we grapple with. We sought a better way to measure and control it.""The researchers investigated 116 different facial points on Affetto to measure its three-dimensional movement. Facial points were underpinned by so-called deformation units. Each unit comprises a set of mechanisms that create a distinctive facial contortion, such as lowering or raising of part of a lip or eyelid. Measurements from these were then subjected to a mathematical model to quantify their surface motion patterns.While the researchers encountered challenges in balancing the applied force and in adjusting the synthetic skin, they were able to employ their system to adjust the deformation units for precise control of Affetto's facial surface motions.""Android robot faces have persisted in being a black box problem: they have been implemented but have only been judged in vague and general terms,"" study first author Hisashi Ishihara says. ""Our precise findings will let us effectively control android facial movements to introduce more nuanced expressions, such as smiling and frowning.""Story Source:Materials provided by Osaka University. ",0.106598,0.13832,0.174881,0.309374,0.147028,0.176319,0.252617,0,0
31,Artificial fly brain can tell who's who,"Despite the simplicity of their visual system, fruit flies are able to reliably distinguish between individuals based on sight alone. This is a task that even humans who spend their whole lives studying Drosophila melanogaster struggle with. Researchers have now built a neural network that mimics the fruit fly's visual system and can distinguish and re-identify flies. This may allow the thousands of labs worldwide that use fruit flies as a model organism to do more longitudinal work, looking at how individual flies change over time. It also provides evidence that the humble fruit fly's vision is clearer than previously thought.In an interdisciplinary project funded by a Canadian Institute for Advanced Research (CIFAR) Catalyst grant, researchers at the University of Guelph and the University of Toronto, Mississauga combined expertise in fruit fly biology with machine learning to build a biologically-based algorithm that churns through low-resolution videos of fruit flies in order to test whether it is physically possible for a system with such constraints to accomplish such a difficult task.Fruit flies have small compound eyes that take in a limited amount of visual information, an estimated 29 units squared. The traditional view has been that once the image is processed by a fruit fly, it is only able to distinguish very broad features. But a recent discovery that fruit flies can boost their effective resolution with subtle biological tricks has led researchers to believe that vision could contribute significantly to the social lives of flies. This, combined with the discovery that the structure of their visual system looks a lot like a Deep Convolutional Network (DCN), led the team to ask: ""can we model a fly brain that can identify individuals?""Their computer program has the same theoretical input and processing ability as a fruit fly and was trained on video of a fly over two days. It was then able to reliably identify the same fly on the third day with an F1 score (a measure that combines precision and recall) of 0.75. Impressively, this is only slightly worse than scores of 0.85 and 0.83 for algorithms without the constraints of fly-brain biology. For comparison, when given the easier task of matching the 'mugshot' of a fly to a field of 20 others, experienced human fly biologists only managed a score of 0.08. Random chance would score 0.05.According to Jon Schneider, the first author of the paper being published in PLOS ONE this week, this study points to ""the tantalizing possibility that rather than just being able to recognize broad categories, fruit flies are able to distinguish individuals. So when one lands next to another, it's ""Hi Bob, Hey Alice."" ""Graham Taylor, a machine learning specialist and CIFAR Azrieli Global Scholar in the Learning in Machines and Brains program, was excited by the prospect of beating humans at a visual task. ""A lot of Deep Neural Network applications try to replicate and automate human abilities like facial recognition, natural language processing, or song identification. But rarely do they go beyond human capacity. So it's exciting to find a problem where algorithms can outperform humans.""The experiments took place in the University of Toronto Mississauga lab of Joel Levine, a senior fellow in the CIFAR Child & Brain Development program. He has high hopes for the future of research like this. ""The approach of pairing deep learning models with nervous systems is incredibly rich. It can tell us about the models, about how neurons communicate with each other, and it can tell us about the whole animal. That's sort of mind blowing. And it's unexplored territory.""Schneider summed up what it was like working between disciplines: ""Projects like this are a perfect arena for neurobiologists and machine learning researchers to work together to uncover the fundamentals of how any system -- biological or otherwise -- learns and processes information.""Story Source:Materials provided by Canadian Institute for Advanced Research. ",0.100406,0.116135,0.151392,0.336884,0.143434,0.165537,0.204286,0,0
32,Robot masters human balancing act Novel approach to human-like balance in a biped robot,"When walking in a crowded place, humans typically aren't thinking about how we avoid bumping into one another. We are built to use a gamut of complex skill sets required to execute these types of seemingly simple motions.Now, thanks to researchers in the Cockrell School of Engineering at The University of Texas at Austin, robots may soon be able to experience similar functionality. Luis Sentis, associate professor in the Department of Aerospace Engineering and Engineering Mechanics, and his team in the Human Centered Robotics Laboratory have successfully demonstrated a novel approach to human-like balance in a biped robot.Their approach has implications for robots that are used in everything from emergency response to defense to entertainment. The team will present their work this week at the 2018 International Conference on Intelligent Robots and Systems (IROS2018), the flagship conference in the field of robotics.By translating a key human physical dynamic skill -- maintaining whole-body balance -- into a mathematical equation, the team was able to use the numerical formula to program their robot Mercury, which was built and tested over the course of six years. They calculated the margin of error necessary for the average person to lose one's balance and fall when walking to be a simple figure -- 2 centimeters.""Essentially, we have developed a technique to teach autonomous robots how to maintain balance even when they are hit unexpectedly, or a force is applied without warning,"" Sentis said. ""This is a particularly valuable skill we as humans frequently use when navigating through large crowds.""Sentis said their technique has been successful in dynamically balancing both bipeds without ankle control and full humanoid robots.Dynamic human-body-like movement is far harder to achieve for a robot without ankle control than for one equipped with actuated, or jointed, feet. So, the UT Austin team used an efficient whole-body controller developed by integrating contact-consistent rotators (or torques) that can effectively send and receive data to inform the robot as to the best possible move to make next in response to a collision. They also applied a mathematical technique -- often used in 3D animation to achieve realistic-looking movements from animated characters -- known as inverse kinematics, along with low-level motor position controllers.Mercury may have been tailored to the specific needs of its creators, but the fundamental equations underpinning this technique in our understanding of human locomotion are, in theory, universally applicable to any comparable embodied artificial intelligence (AI) and robotics research.Like all the robots developed in Sentis' lab, the biped is anthropomorphic -- designed to mimic the movement and characteristics of humans.""We choose to mimic human movement and physical form in our lab because I believe AI designed to be similar to humans gives the technology greater familiarity,"" Sentis said. ""This, in turn, will make us more comfortable with robotic behavior, and the more we can relate, the easier it will be to recognize just how much potential AI has to enhance our lives.""The research was funded by the Office of Naval Research and UT, in partnership with Apptronik Systems, a company of which Sentis is co-founder.The University of Texas at Austin is committed to transparency and disclosure of all potential conflicts of interest. The university investigator who led this research, Luis Sentis, has submitted required financial disclosure forms with the university. Sentis is co-founder, chairman and chief scientific officer of Apptronik Systems, a robotics company in which he has equity ownership. The company was spun out of the Human Centered Robotics Lab at The University of Texas at Austin in 2016. The lab, which developed all equations and algorithms described in this news release, worked with Meka to develop the original robot in 2011. Apptronik designed new electronic systems for it in 2018.Video: https://www.youtube.com/watch?v=wahMNyBHKL4Story Source:Materials provided by University of Texas at Austin. ",0.0549289,0.0640108,0.097318,0.219739,0.0802896,0.107567,0.166811,0,0
33,Tiny soft robot with multilegs paves way for drugs delivery in human body,"A novel tiny, soft robot with caterpillar-like legs capable of carrying heavy loads and adaptable to adverse environment was developed from a research led by City University of Hong Kong (CityU). This mini delivery-robot could pave way for medical technology advancement such as drugs delivery in human body.Around the world, there has been research about developing soft milli-robots. But the CityU's new design with multi-legs helps reduce friction significantly, so that the robot can move efficiently inside surfaces within the body lined with, or entirely immersed in, body fluids such as blood or mucus.The research findings have been published in the latest issue of the scientific journal Nature Communications, titled ""A Bio-inspired Multilegged Soft Millirobot that Functions in Both Dry and Wet Conditions.""Bio-inspired robot designWhat makes this milli-robot stand out is its hundreds of less than 1 mm long pointed legs that looks like short tiny hair. This unique design was not a random choice. The research team has studied the leg structures of hundreds of ground animals including those with 2, 4, 8 or more legs, in particular the ratio between leg-length and the gap between the legs. And from there, they got their inspiration.""Most animals have a leg-length to leg-gap ratio of 2:1 to 1:1. So we decided to create our robot using 1:1 proportion,"" explains Dr Shen Yajing, Assistant Professor at CityU's Department of Biomedical Engineering (BME), who led the research.The robot's body thickness measures approximately 0.15 mm, with each conical leg measuring 0.65 mm long and the gap between the legs measuring approximately 0.6 mm, making the leg-length-to-gap ratio around 1:1. Moreover, the robot's pointed legs have greatly reduced their contact area and hence the friction with the surface. Laboratory tests showed that the multi-legged robot has 40 times less friction than a limbless robot in both wet and dry environment.Apart from the multi-leg design, the materials also matter. The robot is fabricated with a silicon material called polydimethylsiloxane (PDMS) embedded with magnetic particles which enables it to be remotely controlled by applying electromagnetic force. ""Both the materials and the mutli-leg design greatly improve the robot's hydrophobic property. Besides, the rubbery piece is soft and can be cut easily to form robots of various shapes and sizes for different applications,"" says Professor Wang Zuankai at CityU's Department of Mechanical Engineering (MNE) who conceived this research idea and initiated the collaboration among the researchers.Moving at ease in harsh environmentControlled by a magnetic manipulator used in experiments, the robot can move in both a flap propulsion pattern and an inverted pendulum pattern, meaning that it can use its front feet to flap forward as well as swinging the body by standing on the left and right feet alternately to advance respectively.""The rugged surface and changing texture of different tissues inside the human body make transportation challenging. Our multi-legged robot shows an impressive performance in various terrains and hence open wide applications for drug delivery inside the body,"" says Professor Wang.The research team further proved that when facing an obstacle ten times higher than its leg length, the robot, with its deformable soft legs, is capable to lift up one end of its body to form an angle of up to 90-degree and cross the obstacle easily. And the robot can increase its speed by increasing the electromagnetic frequency applied.The robot also shows a remarkable loading ability. Laboratory tests showed that the robot was capable of carrying a load 100 times heavier than itself, a strength comparable to an ant, one of the strongest Hercules in nature, or to a human being able to easily lift a 26-seated mini-bus.The amazingly strong carrying capability, efficient locomotion and good obstacle-crossing ability make this milli-robot extremely suitable for applications in a harsh environment, for example delivering a drug to a designated spot through the digestive system, or carrying out medical inspection,"" adds Dr Shen.Before conducting further tests in animals and eventually in humans, the research teams are further developing and refining their research in three aspects, namely finding a biodegradable material, studying new shapes, and adding extra features.""We are hoping to create a biodegradable robot in the next two to three years so it will decompose naturally after its meds delivery mission,"" says Dr Shen.The co-first authors of the paper are Lu Haojian from BME Department and Dr Zhang Mei from MNE Department of CityU. The other co-authors include Yang Yuanyuan from BME Department of CityU, and Professor Huang Qiang and Professor Toshio Fukuda from the Beijing Institute of Technology.Story Source:Materials provided by City University of Hong Kong. ",0.081514,0.10304,0.152115,0.174757,0.106838,0.144296,0.183817,0,0
34,Computers successfully trained to identify animals in photos,"A computer model developed at the University of Wyoming by UW researchers and others has demonstrated remarkable accuracy and efficiency in identifying images of wild animals from camera-trap photographs in North America.The artificial-intelligence breakthrough, detailed in a paper published in the scientific journal Methods in Ecology and Evolution, is described as a significant advancement in the study and conservation of wildlife. The computer model is now available in a software package for Program R, a widely used programming language and free software environment for statistical computing.""The ability to rapidly identify millions of images from camera traps can fundamentally change the way ecologists design and implement wildlife studies,"" says the paper, whose lead authors are recent UW Department of Zoology and Physiology Ph.D. graduate Michael Tabak and Ryan Miller, both of the U.S. Department of Agriculture's Center for Epidemiology and Animal Health in Fort Collins, Colo.The study builds on UW research published earlier this year in the Proceedings of the National Academy of Sciences (PNAS) in which a computer model analyzed 3.2 million images captured by camera traps in Africa by a citizen science project called Snapshot Serengeti. The artificial-intelligence technique called deep learning categorized animal images at a 96.6 percent accuracy rate, the same as teams of human volunteers achieved, at a much more rapid pace than did the people.In the latest study, the researchers trained a deep neural network on Mount Moran, UW's high-performance computer cluster, to classify wildlife species using 3.37 million camera-trap images of 27 species of animals obtained from five states across the United States. The model then was tested on nearly 375,000 animal images at a rate of about 2,000 images per minute on a laptop computer, achieving 97.6 percent accuracy -- likely the highest accuracy to date in using machine learning for wildlife image classification.The computer model also was tested on an independent subset of 5,900 images of moose, cattle, elk and wild pigs from Canada, producing an accuracy rate of 81.8 percent. And it was 94 percent successful in removing ""empty"" images (without any animals) from a set of photographs from Tanzania.The researchers have made their model freely available in a software package in Program R. The package, ""Machine Learning for Wildlife Image Classification in R (MLWIC),"" allows other users to classify their images containing the 27 species in the dataset, but it also allows users to train their own machine learning models using images from new datasets.The lead author of the PNAS article, recent UW computer science Ph.D. graduate Mohammad Sadegh (Arash) Norouzzadeh, is one of multiple contributors to the new paper in Methods in Ecology and Evolution. Other participating researchers from UW are Department of Computer Science Associate Professor Jeff Clune and postdoctoral researcher Elizabeth Mandeville of the Wyoming Cooperative Fish and Wildlife Research Unit.Other organizations represented in the research group are the USDA's National Wildlife Research Center, Arizona State University, California's Tejon Ranch Conservancy, the University of Georgia, the University of Florida, Colorado Parks and Wildlife, the University of Saskatchewan and the University of Montana.Story Source:Materials provided by University of Wyoming. ",0.07581,0.122569,0.154169,0.289451,0.127881,0.126387,0.183121,0,0
35,Smarter AI: Machine learning without negative data,"A research team from the RIKEN Center for Advanced Intelligence Project (AIP) has successfully developed a new method for machine learning that allows an AI to make classifications without what is known as ""negative data,"" a finding which could lead to wider application to a variety of classification tasks.Classifying things is critical for our daily lives. For example, we have to detect spam mail, fake political news, as well as more mundane things such as objects or faces. When using AI, such tasks are based on ""classification technology"" in machine learning -- having the computer learn using the boundary separating positive and negative data. For example, ""positive"" data would be photos including a happy face, and ""negative"" data photos that include a sad face. Once a classification boundary is learned, the computer can determine whether a certain data is positive or negative. The difficulty with this technology is that it requires both positive and negative data for the learning process, and negative data are not available in many cases (for instance, it is hard to find photos with the label, ""this photo includes a sad face,"" since most people smile in front of a camera.)In terms of real-life programs, when a retailer is trying to predict who will make a purchase, it can easily find data on customers who purchased from them (positive data), but it is basically impossible to obtain data on customers who did not purchase from them (negative data), since they do not have access to their competitors' data. Another example is a common task for app developers: they need to predict which users will continue using the app (positive) or stop (negative). However, when a user unsubscribes, the developers lose the user's data because they have to completely delete data regarding that user in accordance with the privacy policy to protect personal information.According to lead author Takashi Ishida from RIKEN AIP, ""Previous classification methods could not cope with the situation where negative data were not available, but we have made it possible for computers to learn with only positive data, as long as we have a confidence score for our positive data, constructed from information such as buying intention or the active rate of app users. Using our new method, we can let computers learn a classifier only from positive data equipped with confidence.""Ishida proposed, together with researcher Niu Gang from his group and team leader Masashi Sugiyama, that they let computers learn well by adding the confidence score, which mathematically corresponds to the probability whether the data belongs to a positive class or not. They succeeded in developing a method that can let computers learn a classification boundary only from positive data and information on its confidence (positive reliability) against classification problems of machine learning that divide data positively and negatively.To see how well the system functioned, they used it on a set of photos that contains various labels of fashion items. For example, they chose ""T-shirt,"" as the positive class and one other item, e.g., ""sandal,"" as the negative class. Then they attached a confidence score to the ""T-shirt"" photos. They found that without accessing the negative data (e.g., ""sandal"" photos), in some cases, their method was just as good as a method that involves using positive and negative data.According to Ishida, ""This discovery could expand the range of applications where classification technology can be used. Even in fields where machine learning has been actively used, our classification technology could be used in new situations where only positive data can be gathered due to data regulation or business constraints. In the near future, we hope to put our technology to use in various research fields, such as natural language processing, computer vision, robotics, and bioinformatics.""Story Source:Materials provided by RIKEN. ",0.0769478,0.10289,0.142919,0.356435,0.14148,0.141404,0.191921,0,0
36,"Aquatic animals that jump out of water inspire leaping robots Aquatic animals' maximum jumping height is related to their body size, while 'entrained water mass' plays a limiting role","Ever watch aquatic animals jump out of the water and wonder how they manage to do it in such a streamlined and graceful way? A group of researchers who specialize in water entry and exit in nature had the same question and are exploring the specific physical conditions required for animals to successfully leap out of water.During the American Physical Society's Division of Fluid Dynamics 71st Annual Meeting, which will take place Nov. 18-20 at the Georgia World Congress Center in Atlanta, Georgia, Sunghwan Jung, an associate professor of biology and environmental engineering at Cornell University, and one of his students, Brian Chang, will present their work designing a robotic system inspired by jumping copepods (tiny crustaceans) and frogs to illuminate some of the fluid dynamics at play when aquatic animals jump.""We collected data about aquatic animals of different sizes -- from about 1 millimeter to tens of meters -- jumping out of water, and were able to reveal how their maximum jumping heights are related to their body size,"" said Jung.In nature, animals frequently move in and out of water for various purposes -- including escaping predators, catching prey, or communicating. ""But since water is 1,000 times denser than air, entering or exiting water requires a lot of effort, so aquatic animals face mechanical challenges,"" Jung said.As an object -- like a dolphin or a copepod -- jumps through water, mass is added to it -- a quantity referred to as ""entrained water mass."" This entrained water mass is incorporated and gets swept along in the flow off aquatic animals' bodies. The group discovered that entrained water mass is important because it limits the animals' maximum jumping height.""We're trying to understand how biological systems are able to smartly figure out and overcome these challenges to maximize their performance, which might also shed light on engineering systems to enter or exit air-water interfaces,"" Jung said.Most aquatic animals are streamlined, limiting entrained water mass's effect, so water slides easily off their bodies. ""That's why they're such good jumpers,"" said Jung. ""But when we made and tested a robotic system similar to jumping animals, it didn't jump as much as animals. Why? Our robot isn't as streamlined and carries a lot of water with it. Imagine getting out of a swimming pool with a wet coat -- you might not be able to walk due to the water weight.""The group's robot features a simple design akin to a door hinge with a rubber band. A rubber band is wrapped around a 3D-printed door hinge's outer perimeter, while a tiny wire that holds the door hinge allows it to flip back when fluid is pushed downward. ""This robot shows the importance of entrained water while an object jumps out of the water,"" he said.Next up, the group will modify and advance their robotic system so that it can jump out of the water at higher heights similar to those reached by animals like copepods or frogs. ""This system might then be able to be used for surveillance near water basins,"" said Jung.Story Source:Materials provided by American Physical Society. ",0.128977,0.140826,0.197272,0.193653,0.14524,0.173172,0.19793,0,0
37,AI capable of outlining in a single chart information from thousands of scientific papers,"NIMS and the Toyota Technological Institute at Chicago have jointly developed a Computer-Aided Material Design (CAMaD) system capable of extracting information related to fabrication processes and material structures and properties -- factors vital to material design -- and organizing and visualizing the relationship between them. The use of this system enables information from thousands of scientific and technical articles to be summarized in a single chart, rationalizing and expediting material design.The performance of a material is determined by its properties. Because a material's properties are greatly influenced by its structure and by the fabrication process that controls the structure, understanding the relationships between factors affecting material properties of interest and associated material structures and fabrication processes is vital to rationalizing and expediting the development of materials with desirable performance. Materials informatics -- an information science-based approach to materials research -- allows the relationships between these factors to be extracted from large amounts of data using deep learning. However, because the collection of large amounts of data on materials through experiments and database construction is labor-intensive, it had been difficult to use materials informatics to integrate process-structure-property-performance relationships into material design.This research group has developed a system able to extract and identify relationships between factors related to processes, structures and properties vital to material design by instructing computers to read the text of scientific articles -- rather than numerical data on materials -- using natural language processing and weekly supervised deep learning. The material designers initially select several material properties relevant to desirable material performance. Based on these selections, the computer then extracts relevant information, determines the type and strength of relationships between material structures relevant to the desirable properties and factors related to structure-controlling fabrication processes and generates a chart to visualize these relationships. For example, if a steel designer selects ""strength"" and ""ductility"" as material properties of interest, the computer produces a chart illustrating the relationship between structural and process factors relevant to composite microstructures known to influence these two properties.In this pioneering effort, we actively integrated natural language processing and deep learning into material design. We have publicized the AI source code developed in this study for use by others, free of charge, to promote related research.Story Source:Materials provided by National Institute for Materials Science, Japan. ",0.0781953,0.130664,0.184248,0.24176,0.141986,0.125299,0.18502,0,0
38,Codebreaker Turing's theory explains how shark scales are patterned,"A system proposed by world war two codebreaker Alan Turing more than 60 years ago can explain the patterning of tooth-like scales possessed by sharks, according to new research.Scientists from the University of Sheffield's Department of Animal and Plant Sciences found that Turing's reaction-diffusion theory -- widely accepted as the patterning method in mouse hair and chicken feathers -- also applies to shark scales.The findings can explain how the pattern of shark scales has evolved to reduce drag whilst swimming, thereby saving energy during movement. Scientists believe studying the patterning could help to design new shark-inspired materials to improve energy and transport efficiency.Turing, forefather of the computer, came up with the reaction-diffusion system which was published in 1952, two years before his death. His equations describe how molecular signals can interact to form complex patterns.In the paper, published today (7 November 2018) in the journal Science Advances, researchers compared the patterning of shark scales to that of chicken feathers.They found that the same core genes underlying feather patterning also underlie the development of shark scales and suggest these genes may be involved in the patterning of other diverse vertebrate skin structures, such as spines and teeth.Dr Gareth Fraser, formerly of the University of Sheffield and now at the University of Florida, said: ""We started looking at chicks and how they develop their feathers. We found these very nice lines of gene expression that pattern where these spots appear that eventually grow into feathers. We thought maybe the shark does a similar thing, and we found two rows on the dorsal surface, which start the whole process.""We teamed up with a mathematician to figure out what the pattern is and whether we can model it. We found that shark skin denticles are precisely patterned through a set of equations that Alan Turing -- the mathematician, computer scientist and the code breaker -- came up with.""These equations describe how certain chemicals interact during animal development and we found that these equations explain the patterning of these units.""Researchers also demonstrated how tweaking the inputs of Turing's system can result in diverse scale patterns comparable to those seen in shark and ray species alive today.They suggest that natural variations to Turing's system may have enabled the evolution of different traits within these animals, including the provision of drag reduction and defensive armour.Rory Cooper, PhD student at the University of Sheffield, said: ""Sharks belong to an ancient vertebrate group, long separated from most other jawed vertebrates. Studying their development gives us an idea of what skin structures may have looked like early in vertebrate evolution.""We wanted to learn about the developmental processes that control how these diverse structures are patterned, and therefore the processes which facilitate their various functions.""Scientists used a combination of techniques including reaction-diffusion modelling to create a simulation based on Turing's equations, to demonstrate that his system can explain shark scale patterning, when the parameters are tuned appropriately.Mr Cooper added: ""Scientists and engineers have been trying to create shark-skin inspired materials to reduce drag and increase efficiency during locomotion, of both people and vehicles, for many years.""Our findings help us to understand how shark scales are patterned, which is essential for enabling their function in drag reduction.Therefore, this research helps us to understand how these drag reductive properties first arose in sharks, and how they change between different species.""Patterning is one important aspect that contributes to achieving drag reduction in certain shark species. Another is the shape of individual scales. Researchers now want to examine the developmental processes which underlie the variation of shape both within and between different shark species.""Understanding how both these factors contribute towards drag reduction will hopefully lead towards the production of improved, widely applicable shark-inspired materials capable of reducing drag and saving energy,"" added Mr Cooper.Story Source:Materials provided by University of Sheffield. ",0.0852902,0.109905,0.122921,0.194424,0.105518,0.137248,0.149762,0,0
39,Could machines using artificial intelligence make doctors obsolete?,"Artificial intelligence systems simulate human intelligence by learning, reasoning, and self correction. This technology has the potential to be more accurate than doctors at making diagnoses and performing surgical interventions, says Jerg Goldhahn, MD, MAS, deputy head of the Institute for Translational Medicine at ETH Zurich, Switzerland.It has a ""near unlimited capacity"" for data processing and subsequent learning, and can do this at a speed that humans cannot match.Increasing amounts of health data, from apps, personal monitoring devices, electronic medical records, and social media platforms are being brought together to give machines as much information as possible about people and their diseases. At the same time machines are ""reading"" and taking account of the rapidly expanding scientific literature.""The notion that today's physicians could approximate this knowledge by keeping abreast of current medical research while maintaining close contacts with their patients is an illusion not least because of the sheer volume of data,"" says Goldhahn.Machine learning is also not subject to the same level of potential bias seen in human learning that reflects cultural influences and links with particular institutions, for example.While the ability to form relationships with patients is often presented as an argument in favour of human doctors, this may also be their ""Achilles heel,"" Goldhahn points out. Trust is important to patients but machines and systems can be more trustworthy than humans if they can be regarded as unbiased and without conflicts of interest.Furthermore, some patients, particularly younger ones and those with minor conditions, may rate correct diagnosis higher than empathy or continuity of care, he says. ""In some very personal situations the services of a robot could help patients avoid feeling shame.The key challenges for today's healthcare systems are rising costs and insufficient numbers of doctors. ""Introducing AI-driven systems could be cheaper than hiring and training new staff, Goldhahn says. ""They are also universally available, and can even monitor patients remotely.""Doctors as we now know them will become obsolete eventually.""But Vanessa Rampton at the McGill Institute for Health and Social Policy in Montreal, Canada and Professor Giatgen Spinas at University Hospital in Zerich, Switzerland, maintain that machines will never replace doctors entirely because the interrelational quality of the doctor-patient relationship is vital and cannot be replicated.They agree that machines will increasingly be able to perform tasks that human doctors do today, such as diagnosis and treatment, but say doctors will remain because they are better at dealing with the patient as a whole person.Doctors can relate to the patient as a fellow human being and can gain holistic knowledge of their illness as it relates to the patient's life, they say.A doctor-patient relationship where the doctor thinks laterally and takes into account an individual patient's preferences, values and social circumstances is important for healing, particularly for complex conditions, when there are symptoms with no obvious cause, and if there is a high risk of adverse effects.""Feeling they've been heard by someone who understands the seriousness of the problem and whom they can trust can be crucial for patients,"" Rampton and Spinas argue.""Computers aren't able to care for patients in the sense of showing devotion or concern for the other as a person, because they are not people and do not care about anything. Sophisticated robots might show empathy as a matter of form, just as humans might behave nicely in social situations yet remain emotionally disengaged because they are only performing a social role.""Most importantly there will be no cure for some patients -- care will be about helping them have the best quality of life possible with their condition and for the longest time. ""Here doctors are irreplaceable,"" they emphasise. ""Robots cannot understand our concern with relating illness to the task of living a life.""Regulated and well implemented, machines that learn have the potential to bring huge benefit to patients, but who wants to receive a terminal diagnosis from a robot, ask Michael Mittelman and colleagues in a patient commentary?""Patients need to be cared for by people, especially when we are ill and at our most vulnerable. A machine will never be able to show us true comfort,"" they say.They acknowledge that AI may have the potential to become a highly useful and innovative aide in healthcare, but they hope there will always be room for humanity -- human healthcare professionals.""Ultimately, no one wants to be told he or she is dying by an entity that can have no understanding of what that means. We see AI as the servant rather than the director of our medical care,"" they conclude.Story Source:Materials provided by BMJ. ",0.063338,0.086969,0.159392,0.311655,0.110674,0.13258,0.182389,0,0
40,Empathetic machines favored by skeptics but might creep out believers,"Most people would appreciate a chatbot that offers sympathetic or empathetic responses, according to a team of researchers, but they added that reaction may rely on how comfortable the person is with the idea of a feeling machine.In a study, the researchers reported that people preferred receiving sympathetic and empathetic responses from a chatbot -- a machine programmed to simulate a conversation -- than receiving a response from a machine without emotions, said S. Shyam Sundar, James P. Jimirro Professor of Media Effects and co-director of the Media Effects Research Laboratory. People express sympathy when they feel compassion for a person, whereas they express empathy when they are actually feeling the same emotions of the other person, said Sundar.As healthcare providers look for ways to cut costs and improve service, he added these findings could help developers create conversational technologies that encourage people to share information about their physical and mental health states, for example.""Increasingly, as we have more and more chatbots and more AI-driven conversational agents in our midst,"" said Sundar. ""And, as more people begin to turn to their smart speaker or chatbot on a health forum for advice, or for social and emotional support, the question becomes: To what extent should these chatbots and smart speakers express human-like emotions?""While machines today cannot truly feel either sympathy or empathy, developers could program these cues into current chatbot and voice assistant technology, according to the researchers who report their findings in the current issue of Cyberpsychology, Behavior & Social Networking.However, chatbots may become too personal for some people, said Bingjie Liu, a doctoral candidate in mass communications, who worked with Sundar on the study. She said that study participants who were leery of conscious machines indicated they were impressed by the chatbots that were programmed to deliver statements of sympathy and empathy.""The majority of people in our sample did not really believe in machine emotion, so, in our interpretation, they took those expressions of empathy and sympathy as courtesies,"" said Liu. ""When we looked at people who have different beliefs, however, we found that people who think it's possible that machines could have emotions had negative reactions to these expressions of sympathy and empathy from the chatbots.""The researchers recruited 88 volunteers from a university and Amazon Mechanical Turk, an online task platform. The volunteers were asked to interact with one of four different online health service chatbots programmed to deliver responses specific to one of four conditions set up by the researchers: sympathy, two different types of empathy -- cognitive empathy and affective empathy -- or, an advice-only control condition.In the sympathetic version, the chatbot responded with a statement, such as, ""I am sorry to hear that."" The chatbot programmed for cognitive empathy, which acknowledged the user's feelings, might say, ""That issue can be quite disturbing."" A chatbot that expressed affective empathy might respond with a sentence that showed the machine understood how and why a user felt the way they did, such as, ""I understand your anxiety about the situation.""The researchers said that affective empathy and sympathy worked the best.""We found that the cognitive empathy -- where the response is somewhat detached and it's approaching the problem from a thoughtful, but almost antiseptic way -- did not quite work,"" said Sundar."" Of course, chatbots and robots do that quite well, but that is also the stereotype of machines. And it doesn't seem to be as effective. What seems to work best is affective empathy, or an expression of sympathy.""In a previous study, the researchers asked participants to just read the script of the conversation between a human subject and a machine. They found similar effects on the use of sympathy and empathy in messages.The researchers said that future research could examine how the sympathetic and empathetic interactions work for different issues beyond health and sexuality, as well as investigate how people feel if humanlike machines and robots deliver those types of responses.""We want to see if this is a consistent pattern in how humans react to machine emotions,"" said Liu.Story Source:Materials provided by Penn State. Original written by Matt Swayne. ",0.109639,0.137135,0.175609,0.353677,0.173438,0.175412,0.223885,0,0
41,New method peeks inside the 'black box' of artificial intelligence Researchers help explain why machine learning algorithms sometimes generate nonsensical answers,"Artificial intelligence -- specifically, machine learning -- is a part of daily life for computer and smartphone users. From autocorrecting typos to recommending new music, machine learning algorithms can help make life easier. They can also make mistakes.It can be challenging for computer scientists to figure out what went wrong in such cases. This is because many machine learning algorithms learn from information and make their predictions inside a virtual ""black box,"" leaving few clues for researchers to follow.A group of computer scientists at the University of Maryland has developed a promising new approach for interpreting machine learning algorithms. Unlike previous efforts, which typically sought to ""break"" the algorithms by removing key words from inputs to yield the wrong answer, the UMD group instead reduced the inputs to the bare minimum required to yield the correct answer. On average, the researchers got the correct answer with an input of less than three words.In some cases, the researchers' model algorithms provided the correct answer based on a single word. Frequently, the input word or phrase appeared to have little obvious connection to the answer, revealing important insights into how some algorithms react to specific language. Because many algorithms are programmed to give an answer no matter what -- even when prompted by a nonsensical input -- the results could help computer scientists build more effective algorithms that can recognize their own limitations.The researchers will present their work on November 4, 2018 at the 2018 Conference on Empirical Methods in Natural Language Processing.""Black-box models do seem to work better than simpler models, such as decision trees, but even the people who wrote the initial code can't tell exactly what is happening,"" said Jordan Boyd-Graber, the senior author of the study and an associate professor of computer science at UMD. ""When these models return incorrect or nonsensical answers, it's tough to figure out why. So instead, we tried to find the minimal input that would yield the correct result. The average input was about three words, but we could get it down to a single word in some cases.""In one example, the researchers entered a photo of a sunflower and the text-based question, ""What color is the flower?"" as inputs into a model algorithm. These inputs yielded the correct answer of ""yellow."" After rephrasing the question into several different shorter combinations of words, the researchers found that they could get the same answer with ""flower?"" as the only text input for the algorithm.In another, more complex example, the researchers used the prompt, ""In 1899, John Jacob Astor IV invested $100,000 for Tesla to further develop and produce a new lighting system. Instead, Tesla used the money to fund his Colorado Springs experiments.""They then asked the algorithm, ""What did Tesla spend Astor's money on?"" and received the correct answer, ""Colorado Springs experiments."" Reducing this input to the single word ""did"" yielded the same correct answer.The work reveals important insights about the rules that machine learning algorithms apply to problem solving. Many real-world issues with algorithms result when an input that makes sense to humans results in a nonsensical answer. By showing that the opposite is also possible -- that nonsensical inputs can also yield correct, sensible answers -- Boyd-Graber and his colleagues demonstrate the need for algorithms that can recognize when they answer a nonsensical question with a high degree of confidence.""The bottom line is that all this fancy machine learning stuff can actually be pretty stupid,"" said Boyd-Graber, who also has co-appointments at the University of Maryland Institute for Advanced Computer Studies (UMIACS) as well as UMD's College of Information Studies and Language Science Center. ""When computer scientists train these models, we typically only show them real questions or real sentences. We don't show them nonsensical phrases or single words. The models don't know that they should be confused by these examples.""Most algorithms will force themselves to provide an answer, even with insufficient or conflicting data, according to Boyd-Graber. This could be at the heart of some of the incorrect or nonsensical outputs generated by machine learning algorithms -- in model algorithms used for research, as well as real-world algorithms that help us by flagging spam email or offering alternate driving directions. Understanding more about these errors could help computer scientists find solutions and build more reliable algorithms.""We show that models can be trained to know that they should be confused,"" Boyd-Graber said. ""Then they can just come right out and say, 'You've shown me something I can't understand.'""In addition to Boyd-Graber, UMD-affiliated researchers involved with this work include undergraduate researcher Eric Wallace; graduate students Shi Feng and Pedro Rodriguez; and former graduate student Mohit Iyyer (M.S. '14, Ph.D. '17, computer science).The research presentation, ""Pathologies of Neural Models Make Interpretation Difficult,"" Shi Feng, Eric Wallace, Alvin Grissom II, Pedro Rodriguez, Mohit Iyyer, and Jordan Boyd-Graber, will be presented at the 2018 Conference on Empirical Methods in Natural Language Processing on November 4, 2018.This work was supported by the Defense Advanced Research Projects Agency (Award No. HR0011-15-C-011) and the National Science Foundation (Award No. IIS1652666). The content of this article does not necessarily reflect the views of these organizations.Story Source:Materials provided by University of Maryland. ",0.047562,0.0710776,0.103831,0.36586,0.0931494,0.120162,0.175436,0,0
42,"Shape-shifting robots perceive surroundings, make decisions for first time ","General-purpose robots have plenty of limitations. They can be expensive and cumbersome. They often accomplish only a single type of task.But modular robots -- composed of several interchangeable parts, or modules -- are far more flexible. If one part breaks, it can be removed and replaced. Components can be rearranged as needed -- or better yet, the robots can figure out how to reconfigure themselves, based on the tasks they're assigned and the environments they're navigating.Now, a Cornell University-led team has developed modular robots that can perceive their surroundings, make decisions and autonomously assume different shapes in order to perform various tasks -- an accomplishment that brings the vision of adaptive, multipurpose robots a step closer to reality.""This is the first time modular robots have been demonstrated with autonomous reconfiguration and behavior that is perception-driven,"" said Hadas Kress-Gazit, associate professor of mechanical and aerospace engineering at Cornell and principal investigator on the project.The results of this research were published in Science Robotics.The robots are composed of wheeled, cube-shaped modules that can detach and reattach to form new shapes with different capabilities. The modules have magnets to attach to each other, and Wi-Fi to communicate with a centralized system.Other modular robot systems have successfully performed specific tasks in controlled environments, but these robots are the first to demonstrate fully autonomous behavior and reconfigurations based on the task and an unfamiliar environment, Kress-Gazit said.""I want to tell the robot what it should be doing, what its goals are, but not how it should be doing it,"" she said. ""I don't actually prescribe, 'Move to the left, change your shape.' All these decisions are made autonomously by the robot.""The work was funded by the National Science Foundation.Story Source:Materials provided by Cornell University. Original written by Melanie Lefkowitz. ",0.113918,0.133341,0.155869,0.278977,0.143791,0.179557,0.24601,0,0
43,Artificial intelligence may fall short when analyzing data across multiple health systems,"Artificial intelligence (AI) tools trained to detect pneumonia on chest X-rays suffered significant decreases in performance when tested on data from outside health systems, according to a study conducted at the Icahn School of Medicine at Mount and published in a special issue of PLOS Medicine on machine learning and health care. These findings suggest that artificial intelligence in the medical space must be carefully tested for performance across a wide range of populations; otherwise, the deep learning models may not perform as accurately as expected.As interest in the use of computer system frameworks called convolutional neural networks (CNN) to analyze medical imaging and provide a computer-aided diagnosis grows, recent studies have suggested that AI image classification may not generalize to new data as well as commonly portrayed.Researchers at the Icahn School of Medicine at Mount Sinai assessed how AI models identified pneumonia in 158,000 chest X-rays across three medical institutions: the National Institutes of Health; The Mount Sinai Hospital; and Indiana University Hospital. Researchers chose to study the diagnosis of pneumonia on chest X-rays for its common occurrence, clinical significance, and prevalence in the research community.In three out of five comparisons, CNNs' performance in diagnosing diseases on X-rays from hospitals outside of its own network was significantly lower than on X-rays from the original health system. However, CNNs were able to detect the hospital system where an X-ray was acquired with a high-degree of accuracy, and cheated at their predictive task based on the prevalence of pneumonia at the training institution. Researchers found that the difficulty of using deep learning models in medicine is that they use a massive number of parameters, making it challenging to identify specific variables driving predictions, such as the types of CT scanners used at a hospital and the resolution quality of imaging.""Our findings should give pause to those considering rapid deployment of artificial intelligence platforms without rigorously assessing their performance in real-world clinical settings reflective of where they are being deployed,"" says senior author Eric Oermann, MD, Instructor in Neurosurgery at the Icahn School of Medicine at Mount Sinai. ""Deep learning models trained to perform medical diagnosis can generalize well, but this cannot be taken for granted since patient populations and imaging techniques differ significantly across institutions.""""If CNN systems are to be used for medical diagnosis, they must be tailored to carefully consider clinical questions, tested for a variety of real-world scenarios, and carefully assessed to determine how they impact accurate diagnosis,"" says first author John Zech, a medical student at the Icahn School of Medicine at Mount Sinai.This research builds on papers published earlier this year in the journals Radiology and Nature Medicine, which laid the framework for applying computer vision and deep learning techniques, including natural language processing algorithms, for identifying clinical concepts in radiology reports for CT scans.Story Source:Materials provided by The Mount Sinai Hospital / Mount Sinai School of Medicine. ",0.0742901,0.105805,0.145498,0.337657,0.140607,0.13161,0.200631,0,0
44,"Machines that learn language more like kids do Computer model could improve human-machine interaction, provide insight into how children learn language","Children learn language by observing their environment, listening to the people around them, and connecting the dots between what they see and hear. Among other things, this helps children establish their language's word order, such as where subjects and verbs fall in a sentence.In computing, learning language is the task of syntactic and semantic parsers. These systems are trained on sentences annotated by humans that describe the structure and meaning behind words. Parsers are becoming increasingly important for web searches, natural-language database querying, and voice-recognition systems such as Alexa and Siri. Soon, they may also be used for home robotics.But gathering the annotation data can be time-consuming and difficult for less common languages. Additionally, humans don't always agree on the annotations, and the annotations themselves may not accurately reflect how people naturally speak.In a paper being presented at this week's Empirical Methods in Natural Language Processing conference, MIT researchers describe a parser that learns through observation to more closely mimic a child's language-acquisition process, which could greatly extend the parser's capabilities. To learn the structure of language, the parser observes captioned videos, with no other information, and associates the words with recorded objects and actions. Given a new sentence, the parser can then use what it's learned about the structure of the language to accurately predict a sentence's meaning, without the video.This ""weakly supervised"" approach -- meaning it requires limited training data -- mimics how children can observe the world around them and learn language, without anyone providing direct context. The approach could expand the types of data and reduce the effort needed for training parsers, according to the researchers. A few directly annotated sentences, for instance, could be combined with many captioned videos, which are easier to come by, to improve performance.In the future, the parser could be used to improve natural interaction between humans and personal robots. A robot equipped with the parser, for instance, could constantly observe its environment to reinforce its understanding of spoken commands, including when the spoken sentences aren't fully grammatical or clear. ""People talk to each other in partial sentences, run-on thoughts, and jumbled language. You want a robot in your home that will adapt to their particular way of speaking ... and still figure out what they mean,"" says co-author Andrei Barbu, a researcher in the Computer Science and Artificial Intelligence Laboratory (CSAIL) and the Center for Brains, Minds, and Machines (CBMM) within MIT's McGovern Institute.The parser could also help researchers better understand how young children learn language. ""A child has access to redundant, complementary information from different modalities, including hearing parents and siblings talk about the world, as well as tactile information and visual information, [which help him or her] to understand the world,"" says co-author Boris Katz, a principal research scientist and head of the InfoLab Group at CSAIL. ""It's an amazing puzzle, to process all this simultaneous sensory input. This work is part of bigger piece to understand how this kind of learning happens in the world.""Co-authors on the paper are: first author Candace Ross, a graduate student in the Department of Electrical Engineering and Computer Science and CSAIL, and a researcher in CBMM; Yevgeni Berzak PhD '17, a postdoc in the Computational Psycholinguistics Group in the Department of Brain and Cognitive Sciences; and CSAIL graduate student Battushig Myanganbayar.Visual learnerFor their work, the researchers combined a semantic parser with a computer-vision component trained in object, human, and activity recognition in video. Semantic parsers are generally trained on sentences annotated with code that ascribes meaning to each word and the relationships between the words. Some have been trained on still images or computer simulations.The new parser is the first to be trained using video, Ross says. In part, videos are more useful in reducing ambiguity. If the parser is unsure about, say, an action or object in a sentence, it can reference the video to clear things up. ""There are temporal components -- objects interacting with each other and with people -- and high-level properties you wouldn't see in a still image or just in language,"" Ross says.The researchers compiled a dataset of about 400 videos depicting people carrying out a number of actions, including picking up an object or putting it down, and walking toward an object. Participants on the crowdsourcing platform Mechanical Turk then provided 1,200 captions for those videos. They set aside 840 video-caption examples for training and tuning, and used 360 for testing. One advantage of using vision-based parsing is ""you don't need nearly as much data -- although if you had [the data], you could scale up to huge datasets,"" Barbu says.In training, the researchers gave the parser the objective of determining whether a sentence accurately describes a given video. They fed the parser a video and matching caption. The parser extracts possible meanings of the caption as logical mathematical expressions. The sentence, ""The woman is picking up an apple,"" for instance, may be expressed as: ?xy. woman x, pick_up x y, apple y.Those expressions and the video are inputted to the computer-vision algorithm, called ""Sentence Tracker,"" developed by Barbu and other researchers. The algorithm looks at each video frame to track how objects and people transform over time, to determine if actions are playing out as described. In this way, it determines if the meaning is possibly true of the video.Connecting the dotsThe expression with the most closely matching representations for objects, humans, and actions becomes the most likely meaning of the caption. The expression, initially, may refer to many different objects and actions in the video, but the set of possible meanings serves as a training signal that helps the parser continuously winnow down possibilities. ""By assuming that all of the sentences must follow the same rules, that they all come from the same language, and seeing many captioned videos, you can narrow down the meanings further,"" Barbu says.In short, the parser learns through passive observation: To determine if a caption is true of a video, the parser by necessity must identify the highest probability meaning of the caption. ""The only way to figure out if the sentence is true of a video [is] to go through this intermediate step of, 'What does the sentence mean?' Otherwise, you have no idea how to connect the two,"" Barbu explains. ""We don't give the system the meaning for the sentence. We say, 'There's a sentence and a video. The sentence has to be true of the video. Figure out some intermediate representation that makes it true of the video.'""The training produces a syntactic and semantic grammar for the words it's learned. Given a new sentence, the parser no longer requires videos, but leverages its grammar and lexicon to determine sentence structure and meaning.Ultimately, this process is learning ""as if you're a kid,"" Barbu says. ""You see world around you and hear people speaking to learn meaning. One day, I can give you a sentence and ask what it means and, even without a visual, you know the meaning.""In future work, the researchers are interested in modeling interactions, not just passive observations. ""Children interact with the environment as they're learning. Our idea is to have a model that would also use perception to learn,"" Ross says.This work was supported, in part, by the CBMM, the National Science Foundation, a Ford Foundation Graduate Research Fellowship, the Toyota Research Institute, and the MIT-IBM Brain-Inspired Multimedia Comprehension project.Story Source:Materials provided by Massachusetts Institute of Technology. Original written by Rob Matheson. ",0.0415256,0.0626488,0.0989304,0.353612,0.0906405,0.0905517,0.16552,0,0
45,Humans help robots learn tasks,"In the basement of the Gates Computer Science Building at Stanford University, a screen attached to a red robotic arm lights up. A pair of cartoon eyes blinks. ""Meet Bender,"" says Ajay Mandlekar, PhD student in electrical engineering.Bender is one of the robot arms that a team of Stanford researchers is using to test two frameworks that, together, could make it faster and easier to teach robots basic skills. The RoboTurk framework allows people to direct the robot arms in real time with a smartphone and a browser by showing the robot how to carry out tasks like picking up objects. SURREAL speeds the learning process by running multiple experiences at once, essentially allowing the robots to learn from many experiences simultaneously.""With RoboTurk and SURREAL, we can push the boundary of what robots can do by combining lots of data collected by humans and coupling that with large-scale reinforcement learning,"" said Mandlekar, a member of the team that developed the frameworks.The group will be presenting RoboTurk and SURREAL Oct. 29 at the conference on robot learning in Zurich, Switzerland.Humans teaching robotsYuke Zhu, a PhD student in computer science and a member of the team, showed how the system works by opening the app on his iPhone and waving it through the air. He guided the robot arm -- like a mechanical crane in an arcade game -- to hover over his prize: a wooden block painted to look like a steak. This is a simple pick-and-place task that involves identifying objects, picking them up and putting them into the bin with the correct label.To humans, the task seems ridiculously easy. But for the robots of today, it's quite difficult. Robots typically learn by interacting with and exploring their environment -- which usually results in lots of random arm waving -- or from large datasets. Neither of these is as efficient as getting some human help. In the same way that parents teach their children to brush their teeth by guiding their hands, people can demonstrate to robots how to do specific tasks.However, those lessons aren't always perfect. When Zhu pressed hard on his phone screen and the robot released its grip, the wooden steak hit the edge of the bin and clattered onto the table. ""Humans are by no means optimal at this,"" Mandlekar said, ""but this experience is still integral for the robots.""Faster learning in parallelThese trials -- even the failures -- provide invaluable information. The demonstrations collected through RoboTurk will give the robots background knowledge to kickstart their learning. SURREAL can run thousands of simulated experiences by people worldwide at once to speed the learning process.""With SURREAL, we want to accelerate this process of interacting with the environment,"" said Linxi Fan, a PhD student in computer science and a member of the team. These frameworks drastically increase the amount of data for the robots to learn from.""The twin frameworks combined can provide a mechanism for AI-assisted human performance of tasks where we can bring humans away from dangerous environments while still retaining a similar level of task execution proficiency,"" said postdoctoral fellow Animesh Garg, a member of the team that developed the frameworks.The team envisions that robots will be an integral part of everyday life in the future: helping with household chores, performing repetitive assembly tasks in manufacturing or completing dangerous tasks that may pose a threat to humans.""You shouldn't have to tell the robot to twist its arm 20 degrees and inch forward 10 centimeters,"" said Zhu. ""You want to be able to tell the robot to go to the kitchen and get an apple.""Current members of the RoboTurk and SURREAL team include Ajay Mandlekar, Yuke Zhu, Linxi Fan, Animesh Garg and faculty members Fei-Fei Li and Silvio Savarese.Story Source:Materials provided by Stanford University. ",0.0812155,0.0915018,0.123698,0.309064,0.122371,0.149376,0.223004,0,0
46,Shielded quantum bits,"The researchers have found ways to shield electric and magnetic noise for a short time. This will make it possible to use spins as memory for quantum computers, as the coherence time is extended and many thousand computer operations can be performed during this interval. The study was published in the current issue of the journal Physical Review Letters.The technological vision of building a quantum computer does not only depend on computer and information science. New insights in theoretical physics, too, are decisive for progress in the practical implementation. Every computer or communication device contains information embedded in physical systems. ""In the case of a quantum computer, we use spin qubits, for example, to realize information processing,"" explains Professor Guido Burkard, who carries out his research in cooperation with colleagues from Princeton University. The theoretical findings that led to the current publication were largely made by the lead author of the study, doctoral researcher Maximilian Russ from the University of Konstanz.In the quest for the quantum computer, spin qubits and their magnetic properties are the centre of attention. To use spins as memory in quantum technology, they must be lined up, because otherwise they cannot be controlled specifically. ""Usually magnets are controlled by magnetic fields -- like a compass needle in the Earth's magnetic field', explains Guido Burkard. ""In our case the particles are extremely small and the magnets very weak, which makes it really difficult to control them."" The physicists meet this challenge with electric fields and a procedure in which several electrons, in this case four, form a quantum bit. Another problem they have to face is the electron spins, which are rather sensitive and fragile. Even in solid bodies of silicon they react to external interferences with electric or magnetic noise. The current study focuses on theoretical models and calculations of how the quantum bits can be shielded from this noise -- an important contribution to basic research for a quantum computer: If this noise can be shielded for even the briefest of times, thousands of computer operations can be carried out in these fractions of a second -- at least theoretically.The next step for the physicists from Konstanz will now be to work with their experimental colleagues towards testing their theory in experiments. For the first time, four instead of three electrons will be used in these experiments, which could, e.g., be implemented by the research partners in Princeton. While the Konstanz-based physicists provide the theoretical basis, the collaboration partners in the US perform the experimental part. This research is not the only reason why Konstanz is now on the map for qubit research. This autumn, for example, Konstanz attracted the internationally leading scientific community in this field for the ""4th School and Conference on Based Quantum Information Processing.""Story Source:Materials provided by University of Konstanz. ",0.0752255,0.100301,0.101377,0.200738,0.114713,0.150139,0.138166,0,0
47,Artificial intelligence controls quantum computers Neural networks enable learning of error correction strategies for computers based on quantum physics,"Quantum computers could solve complex tasks that are beyond the capabilities of conventional computers. However, the quantum states are extremely sensitive to constant interference from their environment. The plan is to combat this using active protection based on quantum error correction. Florian Marquardt, Director at the Max Planck Institute for the Science of Light, and his team have now presented a quantum error correction system that is capable of learning thanks to artificial intelligence.In 2016, the computer program AlphaGo won four out of five games of Go against the world's best human player. Given that a game of Go has more combinations of moves than there are estimated to be atoms in the universe, this required more than just sheer processing power. Rather, AlphaGo used artificial neural networks, which can recognize visual patterns and are even capable of learning. Unlike a human, the program was able to practise hundreds of thousands of games in a short time, eventually surpassing the best human player. Now, the Erlangen-based researchers are using neural networks of this kind to develop error-correction learning for a quantum computer.Artificial neural networks are computer programs that mimic the behaviour of interconnected nerve cells (neurons) -- in the case of the research in Erlangen, around two thousand artificial neurons are connected with one another. ""We take the latest ideas from computer science and apply them to physical systems,"" explains Florian Marquardt. ""By doing so, we profit from rapid progress in the area of artificial intelligence.""Artificial neural networks could outstrip other error-correction strategiesThe first area of application are quantum computers, as shown by the recent paper, which includes a significant contribution by Thomas Fesel, a doctoral student at the Max Planck Institute in Erlangen. In the paper, the team demonstrates that artificial neural networks with an AlphaGo-inspired architecture are capable of learning -- for themselves -- how to perform a task that will be essential for the operation of future quantum computers: quantum error correction. There is even the prospect that, with sufficient training, this approach will outstrip other error-correction strategies.To understand what it involves, you need to look at the way quantum computers work. The basis for quantum information is the quantum bit, or qubit. Unlike conventional digital bits, a qubit can adopt not only the two states zero and one, but also superpositions of both states. In a quantum computer's processor, there are even multiple qubits superimposed as part of a joint state. This entanglement explains the tremendous processing power of quantum computers when it comes to solving certain complex tasks at which conventional computers are doomed to fail. The downside is that quantum information is highly sensitive to noise from its environment. This and other peculiarities of the quantum world mean that quantum information needs regular repairs -- that is, quantum error correction. However, the operations that this requires are not only complex but must also leave the quantum information itself intact.Quantum error-correction is like a game of Go with strange rules""You can imagine the elements of a quantum computer as being just like a Go board,"" says Marquardt, getting to the core idea behind his project. The qubits are distributed across the board like pieces. However, there are certain key differences from a conventional game of Go: all the pieces are already distributed around the board, and each of them is white on one side and black on the other. One colour corresponds to the state zero, the other to one, and a move in a game of quantum Go involves turning pieces over. According to the rules of the quantum world, the pieces can also adopt grey mixed colours, which represent the superposition and entanglement of quantum states.When it comes to playing the game, a player -- we'll call her Alice -- makes moves that are intended to preserve a pattern representing a certain quantum state. These are the quantum error correction operations. In the meantime, her opponent does everything they can to destroy the pattern. This represents the constant noise from the plethora of interference that real qubits experience from their environment. In addition, a game of quantum Go is made especially difficult by a peculiar quantum rule: Alice is not allowed to look at the board during the game. Any glimpse that reveals the state of the qubit pieces to her destroys the sensitive quantum state that the game is currently occupying. The question is: how can she make the right moves despite this?Auxiliary qubits reveal defects in the quantum computerIn quantum computers, this problem is solved by positioning additional qubits between the qubits that store the actual quantum information. Occasional measurements can be taken to monitor the state of these auxiliary qubits, allowing the quantum computer's controller to identify where faults lie and to perform correction operations on the information-carrying qubits in those areas. In our game of quantum Go, the auxiliary qubits would be represented by additional pieces distributed between the actual game pieces. Alice is allowed to look occasionally, but only at these auxiliary pieces.In the Erlangen researchers' work, Alice's role is performed by artificial neural networks. The idea is that, through training, the networks will become so good at this role that they can even outstrip correction strategies devised by intelligent human minds. However, when the team studied an example involving five simulated qubits, a number that is still manageable for conventional computers, they were able to show that one artificial neural network alone is not enough. As the network can only gather small amounts of information about the state of the quantum bits, or rather the game of quantum Go, it never gets beyond the stage of random trial and error. Ultimately, these attempts destroy the quantum state instead of restoring it.One neural network uses its prior knowledge to train anotherThe solution comes in the form of an additional neural network that acts as a teacher to the first network. With its prior knowledge of the quantum computer that is to be controlled, this teacher network is able to train the other network -- its student -- and thus to guide its attempts towards successful quantum correction. First, however, the teacher network itself needs to learn enough about the quantum computer or the component of it that is to be controlled.In principle, artificial neural networks are trained using a reward system, just like their natural models. The actual reward is provided for successfully restoring the original quantum state by quantum error correction. ""However, if onliy the achievement of this long-term aim gave a reward, it would come at too late a stage in the numerous correction attempts,"" Marquardt explains. The Erlangen-based researchers have therefore developed a reward system that, even at the training stage, incentivizes the teacher neural network to adopt a promising strategy. In the game of quantum Go, this reward system would provide Alice with an indication of the general state of the game at a given time without giving away the details.The student network can surpass its teacher through its own actions""Our first aim was for the teacher network to learn to perform successful quantum error correction operations without further human assistance,"" says Marquardt. Unlike the school student network, the teacher network can do this based not only on measurement results but also on the overall quantum state of the computer. The student network trained by the teacher network will then be equally good at first, but can become even better through its own actions.In addition to error correction in quantum computers, Florian Marquardt envisages other applications for artificial intelligence. In his opinion, physics offers many systems that could benefit from the use of pattern recognition by artificial neural networks.Story Source:Materials provided by Max-Planck-Gesellschaft. ",0.073269,0.101644,0.112328,0.31486,0.1327,0.135018,0.184225,0,0
48,New tool streamlines the creation of moving pictures,"It's often easy to imagine balloons soaring or butterflies fluttering across a still image, but realizing this vision through computer animation is easier said than done. Now, a team of researchers has developed a new tool that makes animating such images much simpler.The tool is designed to animate similar elements within an image, such as balloons or raindrops, said Nora Willett, a graduate student in Princeton's Department of Computer Science and the lead author of a paper presenting the research. To do so, the user manually selects a subset of the repeating objects, then draws motion lines and specifies the frequency and velocity at which the objects should move. The tool's algorithm extracts similar objects in the image and separates them into their own layer for animation.""The main challenge in this system was to design an interface that allows the person and the computer to work together to create a plausible animation,"" said co-author Adam Finkelstein, a Princeton professor of computer science. ""The person provides clues about what aspects of the scene they would like to animate, and the computer removes much of the difficulty and tedium that would be required to create the animation completely by hand.""The new tool builds on the existing capabilities of the Autodesk SketchBook Motion animation app. To animate a still image with the currently available version of the app, a user must either produce the image completely from scratch, or work with an existing image using a program such as Adobe Photoshop to select different objects and separate them into layers before generating the animation.Developing an algorithm that could successfully identify repeating objects was surprising difficult, said Willett. While machine learning methods can reliably do this with photographs, training computers to recognize elements of drawings or paintings is less straightforward. ""There's such a wide range of drawing styles, and humans can create such fantastical things, that there's just not enough data to train a machine to recognize every single fantastical drawing,"" she said.To improve the user interface, the researchers worked with six users representing a range of experience levels with digital animation. Two users chose to animate their own artworks: One created a slowly swinging light within a photograph, while another animated a ring of avocado pieces circling around other food in a drawing.Willett's other projects at Princeton have focused on creating methods to enhance live animation of characters by adding secondary motion, such as movements of hair or clothing; and quickly swapping parts of a live animated character to change hand gestures or accessories. She discussed her background and demonstrated these methods during a 2017 Facebook Live event for Princeton Engineering.Willett presented the team's results on October 16 at the Association for Computing Machinery's Symposium on User Interface Software and Technology. She began working on the tool during an internship at Autodesk Research in Toronto. In addition to Finkelstein, other co-authors were Rubaiat Kazi, Michael Chen and George Fitzmaurice of Autodesk Research; and Tovi Grossman of Autodesk Research and the University of Toronto.Story Source:Materials provided by Princeton University, Engineering School. Original written by Molly Sharlach. ",0.0650928,0.0859964,0.128293,0.29428,0.102233,0.132744,0.174667,0,0
49,Where deep learning meets metamaterials Researchers devise new approach to streamlining design of nanoscale building blocks with endless applications,"Breakthroughs in the field of nanophotonics -- how light behaves on the nanometer scale -- have paved the way for the invention of ""metamaterials,"" human-made materials that have enormous applications, from remote nanoscale sensing to energy harvesting and medical diagnostics. But their impact on daily life has been hindered by a complicated manufacturing process with large margins of error.Now a new interdisciplinary Tel Aviv University study published in Light: Science and Applications demonstrates a way of streamlining the process of designing and characterizing basic nanophotonic, metamaterial elements. The study was led by Dr. Haim Suchowski of TAU's School of Physics and Astronomy and Prof. Lior Wolf of TAU's Blavatnik School of Computer Science and conducted by research scientist Dr. Michael Mrejen and TAU graduate students Itzik Malkiel, Achiya Nagler and Uri Arieli.""The process of designing metamaterials consists of carving nanoscale elements with a precise electromagnetic response,"" Dr. Mrejen says. ""But because of the complexity of the physics involved, the design, fabrication and characterization processes of these elements require a huge amount of trial and error, dramatically limiting their applications.""Deep Learning a key to precision manufacturing""Our new approach depends almost entirely on Deep Learning, a computer network inspired by the layered and hierarchical architecture of the human brain,"" Prof. Wolf explains. ""It's one of the most advanced forms of machine learning, responsible for major advances in technology, including speech recognition, translation and image processing. We thought it would be the right approach for designing nanophotonic, metamaterial elements.""The scientists fed a Deep Learning network with 15,000 artificial experiments to teach the network the complex relationship between the shapes of the nanoelements and their electromagnetic responses. ""We demonstrated that a 'trained' Deep Learning network can predict, in a split second, the geometry of a fabricated nanostructure,"" Dr. Suchowski says.The researchers also demonstrated that their approach successfully produces the novel design of nanoelements that can interact with specific chemicals and proteins.Broadly applicable results""These results are broadly applicable to so many fields, including spectroscopy and targeted therapy, i.e., the efficient and quick design of nanoparticles capable of targeting malicious proteins,"" says Dr. Suchowski. ""For the first time, a novel Deep Neural Network, trained with thousands of synthetic experiments, was not only able to determine the dimensions of nanosized objects but was also capable of allowing the rapid design and characterization of metasurface-based optical elements for targeted chemicals and biomolecules.""Our solution also works the other way around. Once a shape is fabricated, it usually takes expensive equipment and time to determine the precise shape that has actually been fabricated. Our computer-based solution does that in a split second based on a simple transmission measurement.""The researchers, who have also written a patent on their new method, are currently expanding their Deep Learning algorithms to include the chemical characterization of nanoparticles.Story Source:Materials provided by American Friends of Tel Aviv University. ",0.0587077,0.110189,0.120107,0.225855,0.1071,0.111926,0.158005,0,0
50,Clapping Music app reveals that changing rhythm isn't so easy,"Scientists at Queen Mary University of London have developed an app to understand why some rhythms are more difficult to perform than others.They collected and analysed a huge dataset of more than 100,000 people and found that changing rhythm is more difficult than playing a complex individual rhythm.The app challenges users to play Clapping Music -- a ground-breaking piece of music written by contemporary classical composer Steve Reich which is performed entirely by clapping.In the original piece, two people clap the same short rhythmic pattern while one shifts the pattern gradually until the patterns align again.In the app, developed with the London Sinfonietta and Touchpress, the device takes the part of the performer playing the static pattern and the user takes the part of the performer making the pattern transitions.Rather than clapping, players tap in a performance area in the lower part of the screen.Users from more than 100 countries have downloaded the app more than 250,000 times since the launch in July 2015. The research focused on the first 12 months, including 109,303 players and a total of 46 million rows of gameplay data.The results, published in the journal PLOS ONE, provide a new understanding of the musical factors that determine how easy or difficult it is to play a rhythm.Traditionally this has been studied in the lab with small numbers of participants reproducing simple artificial rhythms. These results extend the scope to performance of a real piece of music and a much broader demographic.They show that performance accuracy varied not with the complexity of the individual rhythms in the piece but with the complexity of transitioning between rhythms, which is not something that has been investigated in traditional lab-based music psychology.Lead author Dr Sam Duffy, from Queen Mary's School of Electronic Engineering and Computer Science, said: ""It is quite rare in music psychology to study the performance of rhythms that change, especially when played in ensemble with another player, and yet these are very common in actual music.""She added: ""In spite of the apparent simplicity of Clapping Music, it is a challenging piece to perform and the app generated a very large and complex set of data to analyse.""In previous research, theoretical models have proven useful in predicting individual rhythm reproduction performance for artificial rhythms in the laboratory. However, a striking finding from this research is that none of these measures predicted how accurately players performed the 12 different rhythms that make up Clapping Music.This could mean that these measures only apply to the simple artificial rhythms used in laboratory studies but a more likely interpretation is that they do apply to real music but have a small effect when compared to the much greater influence of transition difficulty between rhythms. This has implications for composers and also for the way in which rhythm is taught.In the app, there were three levels of difficulty -- easy, medium and hard: 19 per cent of players (21,603 players) completed an entire performance of the piece without making a mistake at some level and 5 per cent (5,685 players) completed the game at the hardest level. For completed games, average accuracy was 91 per cent at the hard level, 84 per cent at the medium level and 78 per cent at the easy level.The researchers hope the findings could lead to similar large-scale game-based approaches to investigating other aspects of music psychology than rhythm and even the psychology of other artistic areas such as music and dance.""We have shown that data collected from a game-based app can provide detailed empirical evidence of how pattern complexity influences ensemble rhythmic performance,"" said Dr Marcus Pearce, the project's senior author, also from Queen Mary's School of Electronic Engineering and Computer Science.Story Source:Materials provided by Queen Mary University of London. ",0.104932,0.125758,0.172023,0.278747,0.166484,0.170525,0.190971,0,0
51,Artificial intelligence helps reveal how people process abstract thought Study of deep neural networks suggests knowledge comes via sensory experience,"As artificial intelligence becomes more sophisticated, much of the public attention has focused on how successfully these technologies can compete against humans at chess and other strategy games. A philosopher from the University of Houston has taken a different approach, deconstructing the complex neural networks used in machine learning to shed light on how humans process abstract learning.""As we rely more and more on these systems, it is important to know how they work and why,"" said Cameron Buckner, assistant professor of philosophy and author of a paper exploring the topic published in the journal Synthese. Better understanding how the systems work, in turn, led him to insights into the nature of human learning.Philosophers have debated the origins of human knowledge since the days of Plato -- is it innate, based on logic, or does knowledge come from sensory experience in the world?Deep Convolutional Neural Networks, or DCNNs, suggest human knowledge stems from experience, a school of thought known as empiricism, Buckner concluded. These neural networks -- multi-layered artificial neural networks, with nodes replicating how neurons process and pass along information in the brain -- demonstrate how abstract knowledge is acquired, he said, making the networks a useful tool for fields including neuroscience and psychology.In the paper, Buckner notes that the success of these networks at complex tasks involving perception and discrimination has at times outpaced the ability of scientists to understand how they work.While some scientists who build neural network systems have referenced the thinking of British philosopher John Locke and other influential theorists, their focus has been on results rather than understanding how the networks intersect with traditional philosophical accounts of human cognition. Buckner set out to fill that void, considering the use of AI for abstract reasoning, ranging from strategy games to visual recognition of chairs, artwork and animals, tasks that are surprisingly complex considering the many potential variations in vantage point, color, style and other detail.""Computer vision and machine learning researchers have recently noted that triangle, chair, cat, and other everyday categories are so dif?cult to recognize because they can be encountered in a variety of different poses or orientations that are not mutually similar in terms of their low-level perceptual properties,"" Buckner wrote. ."" .. a chair seen from the front does not look much like the same chair seen from behind or above; we must somehow unify all these diverse perspectives to build a reliable chair-detector.""To overcome the challenges, the systems have to control for so-called nuisance variation, or the range of differences that commonly affect a system's ability to identify objects, sounds and other tasks -- size and position, for example, or pitch and tone. The ability to account for and digest that diversity of possibilities is a hallmark of abstract reasoning.The DCNNs have also answered another lingering question about abstract reasoning, Buckner said. Empiricists from Aristotle to Locke have appealed to a faculty of abstraction to complete their explanations of how the mind works, but until now, there hasn't been a good explanation for how that works. ""For the first time, DCNNs help us to understand how this faculty actually works,"" Buckner said.He began his academic career in computer science, studying logic-based approaches to artificial intelligence. The stark differences between early AI and the ways in which animals and humans actually solve problems prompted his shift to philosophy.Less than a decade ago, he said, scientists believed advances in machine learning would stop short of the ability to produce abstract knowledge. Now that machines are beating humans at strategic games, driverless cars are being tested around the world and facial recognition systems are deployed everywhere from cell phones to airports, finding answers has become more urgent.""These systems succeed where others failed,"" he said, ""because they can acquire the kind of subtle, abstract, intuitive knowledge of the world that comes automatically to humans but has until now proven impossible to program into computers.""Story Source:Materials provided by University of Houston. Original written by Jeannie Kever. ",0.0369604,0.0619774,0.0980202,0.342831,0.0853088,0.0873546,0.157541,0,0
52,"Model helps robots navigate more like humans do In simulations, robots move through new environments by exploring, observing, and drawing from learned experiences","When moving through a crowd to reach some end goal, humans can usually navigate the space safely without thinking too much. They can learn from the behavior of others and note any obstacles to avoid. Robots, on the other hand, struggle with such navigational concepts.MIT researchers have now devised a way to help robots navigate environments more like humans do. Their novel motion-planning model lets robots determine how to reach a goal by exploring the environment, observing other agents, and exploiting what they've learned before in similar situations. A paper describing the model was presented at this week's IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS).Popular motion-planning algorithms will create a tree of possible decisions that branches out until it finds good paths for navigation. A robot that needs to navigate a room to reach a door, for instance, will create a step-by-step search tree of possible movements and then execute the best path to the door, considering various constraints. One drawback, however, is these algorithms rarely learn: Robots can't leverage information about how they or other agents acted previously in similar environments.""Just like when playing chess, these decisions branch out until [the robots] find a good way to navigate. But unlike chess players, [the robots] explore what the future looks like without learning much about their environment and other agents,"" says co-author Andrei Barbu, a researcher at MIT's Computer Science and Artificial Intelligence Laboratory (CSAIL) and the Center for Brains, Minds, and Machines (CBMM) within MIT's McGovern Institute. ""The thousandth time they go through the same crowd is as complicated as the first time. They're always exploring, rarely observing, and never using what's happened in the past.""The researchers developed a model that combines a planning algorithm with a neural network that learns to recognize paths that could lead to the best outcome, and uses that knowledge to guide the robot's movement in an environment.In their paper, ""Deep sequential models for sampling-based planning,"" the researchers demonstrate the advantages of their model in two settings: navigating through challenging rooms with traps and narrow passages, and navigating areas while avoiding collisions with other agents. A promising real-world application is helping autonomous cars navigate intersections, where they have to quickly evaluate what others will do before merging into traffic. The researchers are currently pursuing such applications through the Toyota-CSAIL Joint Research Center.""When humans interact with the world, we see an object we've interacted with before, or are in some location we've been to before, so we know how we're going to act,"" says Yen-Ling Kuo, a PhD in CSAIL and first author on the paper. ""The idea behind this work is to add to the search space a machine-learning model that knows from past experience how to make planning more efficient.""Boris Katz, a principal research scientist and head of the InfoLab Group at CSAIL, is also a co-author on the paper.Trading off exploration and exploitationTraditional motion planners explore an environment by rapidly expanding a tree of decisions that eventually blankets an entire space. The robot then looks at the tree to find a way to reach the goal, such as a door. The researchers' model, however, offers ""a tradeoff between exploring the world and exploiting past knowledge,"" Kuo says.The learning process starts with a few examples. A robot using the model is trained on a few ways to navigate similar environments. The neural network learns what makes these examples succeed by interpreting the environment around the robot, such as the shape of the walls, the actions of other agents, and features of the goals. In short, the model ""learns that when you're stuck in an environment, and you see a doorway, it's probably a good idea to go through the door to get out,"" Barbu says.The model combines the exploration behavior from earlier methods with this learned information. The underlying planner, called RRT*, was developed by MIT professors Sertac Karaman and Emilio Frazzoli. (It's a variant of a widely used motion-planning algorithm known as Rapidly-exploring Random Trees, or RRT.) The planner creates a search tree while the neural network mirrors each step and makes probabilistic predictions about where the robot should go next. When the network makes a prediction with high confidence, based on learned information, it guides the robot on a new path. If the network doesn't have high confidence, it lets the robot explore the environment instead, like a traditional planner.For example, the researchers demonstrated the model in a simulation known as a ""bug trap,"" where a 2-D robot must escape from an inner chamber through a central narrow channel and reach a location in a surrounding larger room. Blind allies on either side of the channel can get robots stuck. In this simulation, the robot was trained on a few examples of how to escape different bug traps. When faced with a new trap, it recognizes features of the trap, escapes, and continues to search for its goal in the larger room. The neural network helps the robot find the exit to the trap, identify the dead ends, and gives the robot a sense of its surroundings so it can quickly find the goal.Results in the paper are based on the chances that a path is found after some time, total length of the path that reached a given goal, and how consistent the paths were. In both simulations, the researchers' model more quickly plotted far shorter and consistent paths than a traditional planner.Working with multiple agentsIn one other experiment, the researchers trained and tested the model in navigating environments with multiple moving agents, which is a useful test for autonomous cars, especially navigating intersections and roundabouts. In the simulation, several agents are circling an obstacle. A robot agent must successfully navigate around the other agents, avoid collisions, and reach a goal location, such as an exit on a roundabout.""Situations like roundabouts are hard, because they require reasoning about how others will respond to your actions, how you will then respond to theirs, what they will do next, and so on,"" Barbu says. ""You eventually discover your first action was wrong, because later on it will lead to a likely accident. This problem gets exponentially worse the more cars you have to contend with.""Results indicate that the researchers' model can capture enough information about the future behavior of the other agents (cars) to cut off the process early, while still making good decisions in navigation. This makes planning more efficient. Moreover, they only needed to train the model on a few examples of roundabouts with only a few cars. ""The plans the robots make take into account what the other cars are going to do, as any human would,"" Barbu says.Going through intersections or roundabouts is one of the most challenging scenarios facing autonomous cars. This work might one day let cars learn how humans behave and how to adapt to drivers in different environments, according to the researchers. This is the focus of the Toyota-CSAIL Joint Research Center work.""Not everybody behaves the same way, but people are very stereotypical. There are people who are shy, people who are aggressive. The model recognizes that quickly and that's why it can plan efficiently,"" Barbu says.More recently, the researchers have been applying this work to robots with manipulators that face similarly daunting challenges when reaching for objects in ever-changing environments.Story Source:Materials provided by Massachusetts Institute of Technology. Original written by Rob Matheson. ",0.0388245,0.0514857,0.0889877,0.266035,0.0686223,0.0847163,0.157187,0,0
53,A new brain-inspired architecture could improve how computers handle data and advance AI,"IBM researchers are developing a new computer architecture, better equipped to handle increased data loads from artificial intelligence. Their designs draw on concepts from the human brain and significantly outperform conventional computers in comparative studies. They report on their recent findings in the Journal of Applied Physics, from AIP Publishing.Today's computers are built on the von Neumann architecture, developed in the 1940s. Von Neumann computing systems feature a central processer that executes logic and arithmetic, a memory unit, storage, and input and output devices. Unlike the stovepipe components in conventional computers, the authors propose that brain-inspired computers could have coexisting processing and memory units.Abu Sebastian, an author on the paper, explained that executing certain computational tasks in the computer's memory would increase the system's efficiency and save energy.""If you look at human beings, we compute with 20 to 30 watts of power, whereas AI today is based on supercomputers which run on kilowatts or megawatts of power,"" Sebastian said. ""In the brain, synapses are both computing and storing information. In a new architecture, going beyond von Neumann, memory has to play a more active role in computing.""The IBM team drew on three different levels of inspiration from the brain. The first level exploits a memory device's state dynamics to perform computational tasks in the memory itself, similar to how the brain's memory and processing are co-located. The second level draws on the brain's synaptic network structures as inspiration for arrays of phase change memory (PCM) devices to accelerate training for deep neural networks. Lastly, the dynamic and stochastic nature of neurons and synapses inspired the team to create a powerful computational substrate for spiking neural networks.Phase change memory is a nanoscale memory device built from compounds of Ge, Te and Sb sandwiched between electrodes. These compounds exhibit different electrical properties depending on their atomic arrangement. For example, in a disordered phase, these materials exhibit high resistivity, whereas in a crystalline phase they show low resistivity.By applying electrical pulses, the researchers modulated the ratio of material in the crystalline and the amorphous phases so the phase change memory devices could support a continuum of electrical resistance or conductance. This analog storage better resembles nonbinary, biological synapses and enables more information to be stored in a single nanoscale device.Sebastian and his IBM colleagues have encountered surprising results in their comparative studies on the efficiency of these proposed systems. ""We always expected these systems to be much better than conventional computing systems in some tasks, but we were surprised how much more efficient some of these approaches were.""Last year, they ran an unsupervised machine learning algorithm on a conventional computer and a prototype computational memory platform based on phase change memory devices. ""We could achieve 200 times faster performance in the phase change memory computing systems as opposed to conventional computing systems."" Sebastian said. ""We always knew they would be efficient, but we didn't expect them to outperform by this much."" The team continues to build prototype chips and systems based on brain-inspired concepts.Story Source:Materials provided by American Institute of Physics. ",0.0614567,0.0928411,0.104915,0.327639,0.136956,0.125338,0.172525,0,0
54,Smart technology for synchronized 3D printing of concrete,"Scientists from Nanyang Technological University, Singapore (NTU Singapore) have developed a technology where two robots can work in unison to 3D-print a concrete structure.This method of concurrent 3D-printing, known as swarm printing, paves the way for a team of mobile robots to print even bigger structures in future.Developed by Assistant Professor Pham Quang Cuong and his team at NTU's Singapore Centre for 3D Printing, this new multi-robot technology was published in Automation in Construction, a top tier journal for civil engineering. The NTU scientist was also behind the Ikea Bot earlier this year where two robots assembled an Ikea chair in 8 min 55s.Using a specially formulated cement mix suitable for 3-D printing, this new development will allow for unique concrete designs currently not possible with conventional casting. Structures can also be produced on demand and in a much shorter period.Currently, 3D-printing of large concrete structures requires huge printers that are larger than the printed objects, which is unfeasible since most construction sites have space constraints.Having multiple mobile robots that can 3D print in sync means large structures like architectural features and specially-designed facades can be printed anywhere as long as there is enough space for the robots to move around the work site.The NTU robots 3D-printed a concrete structure measuring 1.86m x 0.46m x 0.13m in eight minutes. It took two days to harden and one week for it to achieve its full strength before it was ready for installation.""We envisioned a team of robots which can be transported to a work site, print large pieces of concrete structures and then move on to the next project once the parts have been printed,"" explained Asst Prof Pham from NTU's School of Mechanical and Aerospace Engineering.""This research builds on the knowledge we have acquired from developing a robot to autonomously assemble an Ikea chair. But this latest project is more complex in terms of planning, execution, and on a much larger scale.""Printing concrete structures concurrently with two mobile robots was a huge challenge, as both robots have to move into place and start printing their parts without colliding into each other.Printing the concrete structure in segments is also not acceptable, as joints between the two parts will not bond properly if the concrete does not overlap during the printing process.This multi-step process starts by having the computer map out the design to be printed and assign a specific part of the printing to a robot. It then uses a special algorithm to ensure that each of robot arm will not collide with another during the concurrent printing.Using precise location positioning, the robots then move into place and print the parts in good alignment, ensuring that the joints between the separate parts are overlapped. Finally, the mixing and pumping of the specialised liquid concrete mix have to be blended evenly and synchronised to ensure consistency.Professor Chua Chee Kai, Executive Director of the Singapore Centre for 3D Printing said disruptive Industry 4.0 technologies like additive manufacturing, can be advanced even further when combined with other innovative technologies like robotics, AI, materials science and green manufacturing techniques.""This multiple robot printing project is highly interdisciplinary, requiring roboticists to work with materials scientists to make printable concrete. To achieve the end result of a strong concrete structure, we had to combine their expertise with mechanical engineers and civil engineering experts.""""Such an innovation demonstrates to the industry what is feasible now, and prove what is possible in the future if we are creative in developing new technologies to augment conventional building and construction methods.""This research project was supported by the National Research Foundation, Singapore (NRF Singapore) and Sembcorp Design and Construction, one of the key industry research partners of SC3DP.Moving forward, the NTU research team will look at integrating even more robots to print larger scale structures, optimising printing algorithm for consistent performance and to improve the concrete material for faster curing.Story Source:Materials provided by Nanyang Technological University. ",0.0804502,0.103236,0.185106,0.19076,0.124046,0.134392,0.163405,0,0
55,Robots may need lizard-like tails for 'off-road' travel,"Robots may one day tackle obstacles and traverse uneven terrains thanks to collaborative research analysing the motion of lizards.The study, which featured a University of Queensland researcher, used a slow motion camera to capture the nuanced movement of eight species of Australian agamid lizards that run on two legs -- an action known as 'bipedal' movement.UQ School of Biological Sciences researcher Nicholas Wu said the study's findings challenged existing mathematical models based on the animals' movement.""There was an existing understanding that the backwards shift in these lizards' centre of mass, combined with quick bursts of acceleration, caused them to start running on two legs at a certain point,"" he said.""It's just like a motorcycle driver doing a 'wheelie'.""What we found though is that some lizards run bipedally sooner than expected, by moving their body back and winging their tail up.""This means that they could run bipedally for longer, perhaps to overcome obstacles in their path.""Lead author Christofer Clemente from the University of the Sunshine Coast said these results may have important implications for the design of bio-inspired robotic devices.""We're still teasing out why these species have evolved to run like this in the first place, but as we learn more, it's clear that these lessons from nature may be able to be integrated into robotics,"" Dr. Clemente said.""It's been suggested that this movement might have something to do with increasing vision in moments of urgency, by elevating the head at the same time and helping to navigate over obstacles.""Indeed, bipedalism would be advantageous for robots in specific habitats, for example, on open grasslands where, in nature, many bipedal running agamids are found.""If obstacle negotiation is indeed improved with bipedal locomotion, then we have shown how the tail and body can be moved to enable it sooner and for longer.""Maybe adding a tail to robots can help them go 'off-road' sooner.""The research is not only looking to the future, but to the past, by helping explain the evolution of bipedalism in dinosaurs and how they could have transitioned from walking on four legs to two legs.Story Source:Materials provided by University of Queensland. ",0.121143,0.130326,0.152951,0.23919,0.137138,0.177442,0.224214,0,0
56,New robot picks a peck of peppers and more,"The world's most advanced sweet pepper harvesting robot, developed in a consortium including Ben-Gurion University of the Negev (BGU) researchers, was introduced last week at the Research Station for Vegetable Production at St. Katelijne Waver in Belgium.SWEEPER is designed to operate in a single stem row cropping system, with non-clustered fruits and little leaf occlusion. Preliminary test results showed that by using a commercially available crop modified to mimic the required conditions, the robot currently harvests ripe fruit in 24 seconds with a success rate of 62 percent.The BGU team spearheaded efforts to improve the robot's ability to detect ripe produce using computer vision, and has played a role in defining the specifications of the robot's hardware and software interfaces, focusing on supervisory control activities.Polina Kurtser, a Ph.D. candidate in the BGU Department of Industrial Engineering and Management and member of the team, says robotic harvesting will revolutionize the economics of the agriculture industry and dramatically reduce food waste.""The Sweeper picks methodically and accurately,"" she says. ""When it is fully developed, it will enable harvesting 24/7, drastically reduce spoilage, cut labor costs and shield farmers from market fluctuations.""Additional research is needed to increase the robot's work speed to reach a higher harvest success rate. Based upon these latest results, the Sweeper consortium expects that a commercial sweet pepper harvesting robot will be available within four to five years, and that the technology could be adapted for harvesting other crops.North America is the second largest producer of sweet (bell) and chili peppers in the world with a 31 percent market share. In 2017 Europe accounted for more than half the world's pepper supply (53.2 percent) with exports valued at $2.7 billion.SWEEPER is a partnership between BGU, Wageningen University & Research, and pepper grower De Tuindershoek BV, in the Netherlands, Umea University in Sweden, and the Research Station for Vegetable Cultivation and Bogaerts Greenhouse Logistics in Belgium.The project has received funding from the European Union's Horizon 2020 research and innovation program under grant agreement No 644313.Video: https://www.youtube.com/watch?v=VbW1ZW8NC2EStory Source:Materials provided by American Associates, Ben-Gurion University of the Negev. ",0.107395,0.125291,0.172897,0.204519,0.131559,0.179466,0.177797,0,0
57,Researchers teach computers to see optical illusions,"Is that circle green or gray? Are the center lines straight or tilted?Optical illusions can be fun to experience and debate, but understanding how human brains perceive these different phenomena remains an active area of scientific research. For one class of optical illusions, called contextual phenomena, those perceptions are known to depend on context. For example, the color you think a central circle is depends on the color of the surrounding ring. Sometimes the outer color makes the inner color appear more similar, such as a neighboring green ring making a blue ring appear turquoise -- but sometimes the outer color makes the inner color appear less similar, such as a pink ring making a grey circle appear greenish.A team of Brown University computer vision experts went back to square one to understand the neural mechanisms of these contextual phenomena. Their study was published on Sept. 20 in Psychological Review.""There's growing consensus that optical illusions are not a bug but a feature,"" said Thomas Serre, an associate professor of cognitive, linguistic and psychological sciences at Brown and the paper's senior author. ""I think they're a feature. They may represent edge cases for our visual system, but our vision is so powerful in day-to-day life and in recognizing objects.""For the study, the team lead by Serre, who is affiliated with Brown's Carney Institute for Brain Science, started with a computational model constrained by anatomical and neurophysiological data of the visual cortex. The model aimed to capture how neighboring cortical neurons send messages to each other and adjust one another's responses when presented with complex stimuli such as contextual optical illusions.One innovation the team included in their model was a specific pattern of hypothesized feedback connections between neurons, said Serre. These feedback connections are able to increase or decrease -- excite or inhibit -- the response of a central neuron, depending on the visual context.These feedback connections are not present in most deep learning algorithms. Deep learning is a powerful kind of artificial intelligence that is able to learn complex patterns in data, such as recognizing images and parsing normal speech, and depends on multiple layers of artificial neural networks working together. However, most deep learning algorithms only include feedforward connections between layers, not Serre's innovative feedback connections between neurons within a layer.Once the model was constructed, the team presented it a variety of context-dependent illusions. The researchers ""tuned"" the strength of the feedback excitatory or inhibitory connections so that model neurons responded in a way consistent with neurophysiology data from the primate visual cortex.Then they tested the model on a variety of contextual illusions and again found the model perceived the illusions like humans.In order to test if they made the model needlessly complex, they lesioned the model -- selectively removing some of the connections. When the model was missing some of the connections, the data didn't match the human perception data as accurately.""Our model is the simplest model that is both necessary and sufficient to explain the behavior of the visual cortex in regard to contextual illusions,"" Serre said. ""This was really textbook computational neuroscience work -- we started with a model to explain neurophysiology data and ended with predictions for human psychophysics data.""In addition to providing a unifying explanation for how humans see a class of optical illusions, Serre is building on this model with the goal of improving artificial vision.State-of-the-art artificial vision algorithms, such as those used to tag faces or recognize stop signs, have trouble seeing context, he noted. By including horizontal connections tuned by context-dependent optical illusions, he hopes to address this weakness.Perhaps visual deep learning programs that take context into account will be harder to fool. A certain sticker, when stuck on a stop sign can trick an artificial vision system into thinking it is a 65-mile-per-hour speed limit sign, which is dangerous, Serre said.Story Source:Materials provided by Brown University. ",0.0549455,0.0716349,0.125823,0.34306,0.108383,0.10997,0.169308,0,0
58,Switching identities: Revolutionary insulator-like material also conducts electricity,"University of Wisconsin-Madison researchers have made a material that can transition from an electricity-transmitting metal to a nonconducting insulating material without changing its atomic structure.""This is quite an exciting discovery,"" says Chang-Beom Eom, professor of materials science and engineering. ""We've found a new method of electronic switching.""The new material could lay the groundwork for ultrafast electronic devices. Eom and his international team of collaborators published details of their advance today (Nov. 30, 2018) in the journal Science.Metals like copper or silver conduct electricity, whereas insulators like rubber or glass do not allow current to flow. Some materials, however, can transition from insulating to conducting.This transition usually means that the arrangement of a material's atoms and its conducting electrons must change in a coordinated way, but the atomic transition typically proceeds much more slowly than the smaller, lighter electrons that conduct electricity.A material that can switch to conducting electricity like a metal without moving its atoms could dramatically advance switching speeds of advanced devices, says Eom.""The metal-to-insulator transition is very important for switches and for logic devices with a one or a zero state,"" he says. ""We have the potential to use this concept to make very fast switches.""In their research, Eom and his collaborators answered a fundamental question that has bothered scientists for years: Can the electronic and structural transition be decoupled -- essentially, can the quickly changing electrons break out on their own and leave the atoms behind?They used a material called vanadium dioxide, which is a metal when it's heated and an insulator when it's at room temperature. At high temperatures, the atoms that make up vanadium dioxide are arranged in a regularly repeating pattern that scientists refer to as the rutile phase. When vanadium dioxide cools down to become an insulator, its atoms adopt a different pattern, called monoclinic.No naturally occurring substances conduct electricity when their atoms are in the monoclinic conformation. And it takes time for the atoms to rearrange when a material reaches the insulator-to-metal transition temperature.Crucially, vanadium dioxide transitions between a metal and an insulator at different temperatures depending upon the amount of oxygen present in the material. The researchers leveraged that fact to create two thin layers of vanadium dioxide -- one with a slightly lower transition temperature than the other -- sandwiched on top of each other, with a sharp interface between.When they heated the thin vanadium dioxide sandwich, one layer made the structural switch to become a metal. Atoms in the other layer remained locked into the insulating monoclinic phase. Surprisingly, however, that part of the material conducted electricity.Most importantly, the material remained stable and retained its unique characteristics.Although other research groups have attempted to create electrically conductive insulators, those materials lost their properties almost instantly -- persisting for mere femtoseconds, or a few thousandths of one trillionth of a second.The Eom team's material, however, is here to stay.""We were able to stabilize it, making it useful for real devices,"" says Eom.Key to their approach was the dual-layer, sandwich structure. Each layer was so thin that the interface between the two materials dominated how the entire stack behaved. It's a notion that Eom and colleagues plan to pursue further.""Designing interfaces could open up new materials,"" says Eom.The Wisconsin Alumni Research Foundation is assisting the researchers with patent filing.Story Source:Materials provided by University of Wisconsin-Madison. Original written by Sam Million-Weaver. ",0.0872906,0.165904,0.12175,0.15005,0.153663,0.13876,0.143068,0,0
59,Brain-computer interface enables people with paralysis to control tablet devices,"Tablets and other mobile computing devices are part of everyday life, but using them can be difficult for people with paralysis. New research from the BrainGate* consortium shows that a brain-computer interface (BCI) can enable people with paralysis to directly operate an off-the-shelf tablet device just by thinking about making cursor movements and clicks.In a study published November 21 in PLOS ONE, three clinical trial participants with tetraplegia, each of whom was using the investigational BrainGate BCI that records neural activity directly from a small sensor placed in the motor cortex, were able to navigate through commonly used tablet programs, including email, chat, music-streaming and video-sharing apps. The participants messaged with family, friends, members of the research team and their fellow participants. They surfed the web, checked the weather and shopped online. One participant, a musician, played a snippet of Beethoven's ""Ode to Joy"" on a digital piano interface.""For years, the BrainGate collaboration has been working to develop the neuroscience and neuroengineering know-how to enable people who have lost motor abilities to control external devices just by thinking about the movement of their own arm or hand,"" said Dr. Jaimie Henderson, a senior author of the paper and a Stanford University neurosurgeon. ""In this study, we've harnessed that know-how to restore people's ability to control the exact same everyday technologies they were using before the onset of their illnesses. It was wonderful to see the participants express themselves or just find a song they want to hear.""The investigational BrainGate BCI includes a baby aspirin-sized implant that detects the signals associated with intended movements produced in the brain's motor cortex. Those signals are then decoded and routed to external devices. BrainGate researchers and other groups using similar technologies have shown that the device can enable people to move robotic arms or to regain control of their own limbs, despite having lost motor abilities from illness or injury. This study from the collaboration includes scientists, engineers and physicians from Brown University's Carney Institute for Brain Science, the Providence Veterans Affairs Medical Center (PVAMC), Massachusetts General Hospital (MGH) and Stanford University.Two of the participants in this latest study had weakness or loss of movement of their arms and legs due to amyotrophic lateral sclerosis (ALS), a progressive disease affecting the nerves in the brain and spine that control movement. The third participant was paralyzed due to a spinal cord injury. All were enrolled in a clinical trial aimed at assessing the safety and feasibility of the investigational BrainGate system.For this study, neural signals from the BrainGate BCI were routed to a Bluetooth interface configured to work like a wireless mouse. The virtual mouse was then paired to an unmodified Google Nexus 9 tablet. The participants were then asked to perform a set of tasks designed to see how well they were able to navigate within a variety of commonly used apps, and move from app to app. The participants browsed through music selections on a streaming service, searched for videos on YouTube, scrolled through a news aggregator and composed emails and chats.The study showed that participants were able to make up to 22 point-and-click selections per minute while using a variety of apps. In text apps, the participants were able to type up to 30 effective characters per minute using standard email and text interfaces.The participants reported finding the interface intuitive and fun to use, the study noted. One said, ""It felt more natural than the times I remember using a mouse."" Another reported having ""more control over this than what I normally use.""The researchers were pleased to see how quickly the participants used the tablet interface to explore their hobbies and interests.""It was great to see our participants make their way through the tasks we asked them to perform, but the most gratifying and fun part of the study was when they just did what they wanted to do -- using the apps that they liked for shopping, watching videos or just chatting with friends,"" said lead author Dr. Paul Nuyujukian, a bioengineer at Stanford. ""One of the participants told us at the beginning of the trial that one of the things she really wanted to do was play music again. So to see her play on a digital keyboard was fantastic.""The fact that the tablet devices were entirely unaltered and had all preloaded accessibility software turned off was an important part of the study, the researchers said.""The assistive technologies that are available today, while they're important and useful, are all inherently limited in terms of either the speed of use they enable, or the flexibility of the interface,"" said Krishna Shenoy, a senior author of the paper and an electrical engineer and neuroscientist at Stanford University and Howard Hughes Medical Institute. ""That's largely because of the limited input signals that are available. With the richness of the input from the BCI, we were able to just buy two tablets on Amazon, turn on Bluetooth and the participants could use them with our investigational BrainGate system right out of the box.""The researchers say that the study also has the potential to open important new lines of communication between patients with severe neurological deficits and their health care providers.""This has great potential for restoring reliable, rapid and rich communication for somebody with locked-in syndrome who is unable to speak,"" said Jose Albites Sanabria, who performed this research as a graduate student in biomedical engineering at Brown University. ""That not only could provide increased interaction with their family and friends, but can provide a conduit for more thoroughly describing ongoing health issues with caregivers.""As a neuroscientist and practicing critical care neurologist, senior author Dr. Leigh Hochberg of Brown University, Massachusetts General Hospital and the Providence VA Medical Center sees tremendous potential for the restorative capabilities of BCIs exemplified in this study.""When I see somebody in the neuro-intensive care unit who has had an acute stroke and has lost the ability to move or communicate, I'd like to be able to say, 'I'm very sorry this has happened, but we can restore your ability to use the technologies you were using before this happened, and you'll be able to use them again tomorrow,'"" Hochberg said. ""And we are getting closer to being able to tell someone who has been diagnosed with ALS, 'even while we continue to seek out a cure, you will never lose the ability to communicate.' This work is a step toward those goals.""Video: https://www.youtube.com/watch?time_continue=47&v=O6Qw3EDBPhgStory Source:Materials provided by Brown University. ",0.0581411,0.0847395,0.114083,0.235337,0.121422,0.104427,0.147934,0,0
60,AI systems shed light on root cause of religious conflict: Humanity is not naturally violent,"Artificial intelligence can help us to better understand the causes of religious violence and to potentially control it, according to a new Oxford University collaboration. The study is one of the first to be published that uses psychologically realistic AI -- as opposed to machine learning.The study published in The Journal for Artificial Societies and Social Simulation, combined computer modelling and cognitive psychology to create an AI system able to mimic human religiosity, allowing them to better understand the conditions, triggers and patterns for religious violence.The study is built around the question of whether people are naturally violent, or if factors such as religion can cause xenophobic tension and anxiety between different groups, that may or may not lead to violence?The findings reveal that people are a peaceful species by nature. However, in a wide range of contexts they are willing to endorse violence -- particularly when others go against the core beliefs which define their identity.Although the research focuses on specific historic events, the findings can be applied to any occurrence of religious violence, and used to understand the motivations behind it. Particularly events of radicalised Islam, when people's patriotic identity conflicts with their religions one, e.g. the Boston bombing and London terror attacks. The team hope that the results can be used to support governments to address and prevent social conflict and terrorism.Conducted by a cohort of researchers from universities including Oxford, Boston University and the University of Agder, Norway, the paper does not explicitly simulate violence, but, instead focuses on the conditions that enabled two specific periods of xenophobic social anxiety, that then escalated to extreme physical violence.The conflict commonly referred to as the Northern Ireland Troubles is regarded as one of the most violent periods in Irish history. The conflict, involving the British army and various Republican and Loyalist paramilitary groups, spanned three decades, claimed the lives of approximately 3,500 people and saw a further 47,000 injured.Although a much shorter period of tension, the 2002 Gujurat riots of India were equally devastating. The three-day period of inter-communal violence between the Hindu and Muslim communities in the western Indian state of Gujarat, began when a Sabarmarti Express train filled with Hindu pilgrims, stopped in the, predominantly Muslim town of Godhra, and ended with the deaths of more than 2,000 people.Of the study's use of psychologically realistic AI, Justin said: '99% of the general public are most familiar with AI that uses machine learning to automate human tasks like -- classifying something, such as tweets to be positive or negative etc., but our study uses something called multi-agent AI to create a psychologically realistic model of a human, for example -- how do they think, and particularly how do we identify with groups? Why would someone identify as Christian, Jewish or Muslim etc. Essentially how do our personal beliefs align with how a group defines itself?'To create these psychologically realistic AI agents, the team use theories in cognitive psychology to mimic how a human being would naturally think and process information. This is not a new or radical approach -- but it is the first time it has been applied physically in research. There is an entire body of theoretical literature that compares the human mind to a computer programme -- but no one has taken this information and physically programmed it into a computer, it has just been an analogy. The team programmed these rules for cognitive interaction within their AI programme, to show how an individual's beliefs match up with a group situation.They did this by looking at how humans process information against their own personal experiences. Combining some AI models (mimicking people) that have had positive experiences with people from other faiths, and others that have had negative or neutral encounters. They did this to study the escalation and de-escalation of violence over time, and how it can, or cannot be managed.To represent everyday society and how people of different faiths interact in the real world, they created a simulated environment and populated it with hundreds -- or thousands (or millions), of the human model agents. The only difference being that these 'people' all have slightly different variables -- age, ethnicity etc.The simulated environments themselves have a basic design. Individuals have a space that they exist in, but within this space there is a certain probability that they will interact with environmental hazards, such as natural disasters and disease etc. and at some point, each other.The findings revealed that the most common conditions that enable long periods of mutually escalating xenophobic tension occur when social hazards, such as outgroup members who deny the group's core beliefs or sacred values, overwhelm people to the point that they can no longer deal with them. It is only when people's core belief systems are challenged, or they feel that their commitment to their own beliefs is questioned, that anxiety and agitations occur. However, this anxiety only led to violence in 20% of the scenarios created -- all of which were triggered by people from either outside of the group, or within, going against the group's core beliefs and identity.Some religions have a tendency to encourage extreme displays of devotion to a chosen faith, and this can then take the form of violence against a group or individual of another faith, or someone who has broken away from the group.'While other research has tried to use traditional AI and machine learning approaches to understand religious violence, they have delivered mixed results and issues regarding biases against minority communities in machine learning also raise ethical issues. The paper marks the first time that multi-agent AI has been used to tackle this question and create psychologically realistic computer models.Justin said: 'Ultimately, to use AI to study religion or culture, we have to look at modelling human psychology because our psychology is the foundation for religion and culture, so the root causes of things like religious violence rest in how our minds process the information that our world presents it.'Understanding the root cause of religious violence allows people to use the model to both contain and minimise these conflicts, as well as increase them. However, used effectively, this research can be a positive tool that supports stable societies and community integration.Off the back of this project the team have recently secured funding for a new two-year project, at the Center for Modeling Social Systems in Kristiansand, Norway that studies demographic shifts related to immigration and integration in Europe such as the Roma in Slovakia, and the resettlement of Syrian refugees in Lesbos to Norway, in order to help the Norwegian government to optimise the integration process.Story Source:Materials provided by University of Oxford. ",0.0519426,0.0740064,0.127417,0.256362,0.0879892,0.110996,0.143437,0,0
61,"Integrated quantum chip operations possible, tests show ","Quantum computers that are capable of solving complex problems, like drug design or machine learning, will require millions of quantum bits -- or qubits -- connected in an integrated way and designed to correct errors that inevitably occur in fragile quantum systems.Now, an Australian research team has experimentally realised a crucial combination of these capabilities on a silicon chip, bringing the dream of a universal quantum computer closer to reality.They have demonstrated an integrated silicon qubit platform that combines both single-spin addressability -- the ability to 'write' information on a single spin qubit without disturbing its neighbours -- and a qubit 'read-out' process that will be vital for quantum error correction.Moreover, their new integrated design can be manufactured using well-established technology used in the existing computer industry.The team is led by Scientia Professor Andrew Dzurak of the University of New South Wales in Sydney, a program leader at the Centre of Excellence for Quantum Computation and Communication Technology (CQC2T) and Director of the NSW node of the Australian National Fabrication Facility.Last year, Dzurak and colleagues published a design for a novel chip architecture that could allow quantum calculations to be performed using silicon CMOS (complementary metal-oxide-semiconductor) components -- the basis of all modern computer chips.In their new study, published today in the journal Nature Communications, the team combine two fundamental quantum techniques for the first time, confirming the promise of their approach.Dzurak's team had also previously shown that an integrated silicon qubit platform can operate with single-spin addressability -- the ability to rotate a single spin without disturbing its neighbours.They have now shown that they can combine this with a special type of quantum readout process known as Pauli spin blockade, a key requirement for quantum error correcting codes that will be necessary to ensure accuracy in large spin-based quantum computers. This new combination of qubit readout and control techniques is a central feature of their quantum chip design.""We've demonstrated the ability to do Pauli spin readout in our silicon qubit device but, for the first time, we've also combined it with spin resonance to control the spin,"" says Dzurak.""This is an important milestone for us on the path to performing quantum error correction with spin qubits, which is going to be essential for any universal quantum computer.""""Quantum error correction is a key requirement in creating large-scale useful quantum computing because all qubits are fragile, and you need to correct for errors as they crop up,"" says lead author, Michael Fogarty, who performed the experiments as part of his PhD research with Professor Dzurak at UNSW.""But this creates significant overhead in the number of physical qubits you need in order to make the system work,"" notes Fogarty.Dzurak says, ""By using silicon CMOS technology we have the ideal platform to scale to the millions of qubits we will need, and our recent results provide us with the tools to achieve spin qubit error-correction in the near future.""""It's another confirmation that we're on the right track. And it also shows that the architecture we've developed at UNSW has, so far, shown no roadblocks to the development of a working quantum computer chip.""""And, what's more, one that can be manufactured using well-established industry processes and components.""CQC2T'S Unique Approach Using SiliconWorking in silicon is important not just because the element is cheap and abundant, but because it has been at the heart of the global computer industry for almost 60 years. The properties of silicon are well understood and chips containing billions of conventional transistors are routinely manufactured in big production facilities.Three years ago, Dzurak's team published in the journal Nature the first demonstration of quantum logic calculations in a real silicon device with the creation of a two-qubit logic gate -- the central building block of a quantum computer.""Those were the first baby steps, the first demonstrations of how to turn this radical quantum computing concept into a practical device using components that underpin all modern computing,"" says Professor Mark Hoffman, UNSW's Dean of Engineering.""Our team now has a blueprint for scaling that up dramatically.""We've been testing elements of this design in the lab, with very positive results. We just need to keep building on that -- which is still a hell of a challenge, but the groundwork is there, and it's very encouraging.""It will still take great engineering to bring quantum computing to commercial reality, but clearly the work we see from this extraordinary team at CQC2T puts Australia in the driver's seat,"" he added.Other authors of the new Nature Communications paper are UNSW researchers Kok Wai Chan, Bas Hensen, Wister Huang, Tuomo Tanttu, Henry Yang, Arne Laucht, Fay Hudson and Andrea Morello, as well as Menno Veldhorst of QuTech and TU Delft, Thaddeus Ladd of HRL Laboratories and Kohei Itoh of Japan's Keio University.Commercialising CQC2T'S Intellectual PropertyIn 2017, a consortium of Australian governments, industry and universities established Australia's first quantum computing company to commercialise CQC2T's world-leading intellectual property.Operating out of new laboratories at UNSW, Silicon Quantum Computing Pty Ltd (SQC) has the target of producing a 10-qubit demonstration device in silicon by 2022, as the forerunner to creating a silicon-based quantum computer.The work of Dzurak and his team will be one component of SQC realising that ambition. UNSW scientists and engineers at CQC2T are developing parallel patented approaches using single atom and quantum dot qubits.In May 2018, the then Prime Minister of Australia, Malcolm Turnbull, and the President of France, Emmanuel Macron, announced the signing of a Memorandum of Understanding (MoU) addressing a new collaboration between SQC and the world-leading French research and development organisation, Commissariat e l'Energie Atomique et aux Energies Alternatives (CEA).The MoU outlined plans to form a joint venture in silicon-CMOS quantum computing technology to accelerate and focus technology development, as well as to capture commercialisation opportunities -- bringing together French and Australian efforts to develop a quantum computer.The proposed Australian-French joint venture would bring together Dzurak's team, located at UNSW, with a team led by Dr Maud Vinet from CEA, who are experts in advanced CMOS manufacturing technology, and who have also recently demonstrated a silicon qubit made using their industrial-scale prototyping facility in Grenoble.It is estimated that industries comprising approximately 40% of Australia's current economy could be significantly impacted by quantum computing.Possible applications include software design, machine learning, scheduling and logistical planning, financial analysis, stock market modelling, software and hardware verification, climate modelling, rapid drug design and testing, and early disease detection and prevention.Story Source:Materials provided by University of New South Wales. Original written by Wilson da Silva. ",0.0643386,0.089135,0.0969724,0.179978,0.115609,0.123271,0.125239,0,0
62,Online gaming addiction in men affects brain's impulse control,"Researchers using functional MRI (fMRI) have found differences in the brains of men and women who are addicted to online gaming, according to a new study presented today at the annual meeting of the Radiological Society of North America (RSNA).""Internet use is an integral part of the daily lives of many young adults, and a loss of control over Internet use could lead to various negative effects,"" said the study's senior author, Yawen Sun, M.D., diagnostic radiologist at the Department of Radiology of Ren Ji Hospital, affiliated with the Shanghai Jiao Tong University School of Medicine in Shanghai, China. ""Internet gaming disorder has become a major public health concern worldwide among both adolescents and young adults.""Internet gaming disorder (IGD) is a condition characterized by compulsive playing of online games to the exclusion of other interests. Individuals with IGD often suffer significant impairment or distress and may experience negative effects at work, in school or in relationships because of the amount of time they spend playing. They also show symptoms of withdrawal when not playing.While some evidence exists that IGD is more prevalent among men, there is little existing research on differences in the structure and function of the brains of men and women with the disorder.The researchers studied 32 men and 23 women with IGD. They performed resting-state fMRI on the study participants, along with 30 male and 22 female age-matched healthy controls. Resting-state fMRI allows a view of the brain activity when it is not focused on a particular task. The study looked at relationships between brain activity as seen on fMRI and scores on the Barratt Impulsiveness Scale-11, a commonly used test to assess behavioral inhibition.The results illuminated key differences between the men and women with IGD. Men with IGD showed alterations in regional- and network-level brain function. In particular, they had lower brain activity in the superior frontal gyrus, an area of the brain's prefrontal lobe that is important to impulse control. The women with IGD did not exhibit any of these brain alterations.""Our findings demonstrated that alterations in cerebral activity are observed in men with IGD, but not in women with IGD, and that the lower brain activity in the superior frontal gyrus in men with IGD may be associated with higher impulsivity,"" Dr. Sun said.This and other differences apparent in the study suggest that IGD may interact with gender-specific patterns of brain function in men and women.Different rates of maturation in men's and women's brains could also contribute to gender-specific alterations in IGD, Dr. Sun noted. For instance, the prefrontal cortex, which has a central role in executive function and inhibition, matures later in men.""Men have shown lower levels of impulse control in comparison with women, and their impulse control also increases more gradually,"" she said. ""Given the role of inhibitory control in the initiation of IGD, young men may tend to experiment with pathological Internet use to a greater degree than young women do.""A dysfunctional prefrontal cortex specifically in men with IGD may be associated with high impulsivity, a finding partly consistent with those of previous studies of substance addiction. The research adds to a growing body of literature linking the behavioral problems associated with IGD to those found in individuals with substance abuse issues.""However, it remains unclear whether the brain functional and structural changes found in IGD are gaming-induced or precursors for vulnerability,"" Dr. Sun said. ""I think future research should focus on using functional MRI to identify brain susceptibility factors relating to the development of IGD.""Internet, or online, gaming has grown tremendously over the past decades. It includes social gaming, mobile gaming, and multiplayer gaming, which generates billions of dollars in revenue in the U.S. alone. Recent surveys have reported that there are more than 55 million online console gamers in the U.S. According to data measurement company Nielsen, 162 million people, or roughly half the U.S. population, live in a household with a video game console.Story Source:Materials provided by Radiological Society of North America. ",0.113525,0.14469,0.178977,0.269722,0.146512,0.193235,0.196922,0,0
63,AI system may accelerate search for cancer discoveries,"Searching through the mountains of published cancer research could be made easier for scientists, thanks to a new AI system.The system, called LION LBD and developed by computer scientists and cancer researchers at the University of Cambridge, has been designed to assist scientists in the search for cancer-related discoveries. It is the first literature-based discovery system aimed at supporting cancer research. The results are reported in the journal Bioinformatics.Global cancer research attracts massive amounts of funding worldwide, and the scientific literature is now so huge that researchers are struggling to keep up with it: critical hypothesis-generating evidence is now often discovered long after it was published.Cancer is a complex class of diseases that are not completely understood and are the second-leading cause of death worldwide. Cancer development involves changes in numerous chemical and biochemical molecules, reactions and pathways, and cancer research is being conducted across a wide variety of scientific fields, which have variability in the way that they describe similar concepts.""As a cancer researcher, even if you knew what you were looking for, there are literally thousands of papers appearing every day,"" said Professor Anna Korhonen, Co-Director of Cambridge's Language Technology Lab who led the development of LION LBD in collaboration with Dr Masashi Narita at Cancer Research UK Cambridge Institute and Professor Ulla Stenius at Karolinska Institutet in Sweden. ""LION LBD uses AI to help scientists keep up-to-date with published discoveries in their field, but could also help them make new discoveries by combining what is already known in the literature by making connections between sources that may appear to be unrelated.""The 'LBD' in LION LBD stands for Literature-Based Discovery, a concept developed in the 1980s which seeks to make new discoveries by combing pieces of information from disconnected sources. The key idea behind the original version of LBD is that concepts that are never explicitly linked in the literature may be indirectly linked through intermediate concepts.The design of the LION LBD system allows real-time search to discover indirect associations between entities in a database of tens of millions of publications while preserving the ability of users to explore each mention in its original context.""For example, you may know that a cancer drug affects the behaviour of a certain pathway, but with LION LBD, you may find that a drug developed for a totally different disease affects the same pathway,"" said Korhonen.LION LBD is the first system developed specifically for the needs of cancer research. It has a particular focus on the molecular biology of cancer and uses state-of-the-art machine learning and natural language processing techniques, in order to detect references to the hallmarks of cancer in the text. Evaluations of the system have demonstrated its ability to identify undiscovered links and to rank relevant concepts highly among potential connections.The system is built using open data, open source and open standards, and is available as an interactive web-based interface or a programmable API.The researchers are currently working on extending the scope of LION-LBD to include further concepts and relations. They are also working closely with cancer researchers to help and improve the technology for end users.The system was developed in collaboration with University of Cambridge Language Technology Lab, Cancer Research UK Cambridge Institute, and Karolinska Institutet in Sweden, and was funded by the Medical Research Council.Story Source:Materials provided by University of Cambridge. The original story is licensed under a Creative Commons License. ",0.0778051,0.129235,0.146616,0.287162,0.125647,0.140369,0.190874,0,0
64,"Quantum computing at scale: Scientists achieve compact, sensitive qubit readout ","Professor Michelle Simmons' team at UNSW Sydney has demonstrated a compact sensor for accessing information stored in the electrons of individual atoms -- a breakthrough that brings us one step closer to scalable quantum computing in silicon.The research, conducted within the Simmons group at the Centre of Excellence for Quantum Computation and Communication Technology (CQC2T) with PhD student Prasanna Pakkiam as lead author, was published today in the journal Physical Review X (PRX).Quantum bits (or qubits) made from electrons hosted on single atoms in semiconductors is a promising platform for large-scale quantum computers, thanks to their long-lasting stability. Creating qubits by precisely positioning and encapsulating individual phosphorus atoms within a silicon chip is a unique Australian approach that Simmons' team has been leading globally.But adding in all the connections and gates required for scale up of the phosphorus atom architecture was going to be a challenge -- until now.""To monitor even one qubit, you have to build multiple connections and gates around individual atoms, where there is not a lot of room,"" says Professor Simmons. ""What's more, you need high-quality qubits in close proximity so they can talk to each other -- which is only achievable if you've got as little gate infrastructure around them as possible.""Compared with other approaches for making a quantum computer, Simmons' system already had a relatively low gate density. Yet conventional measurement still required at least 4 gates per qubit: 1 to control it and 3 to read it.By integrating the read-out sensor into one of the control gates the team at UNSW has been able to drop this to just two gates: 1 for control and 1 for reading.""Not only is our system more compact, but by integrating a superconducting circuit attached to the gate we now have the sensitivity to determine the quantum state of the qubit by measuring whether an electron moves between two neighbouring atoms,"" lead author Pakkiam states.""And we've shown that we can do this real-time with just one measurement -- single shot -- without the need to repeat the experiment and average the outcomes.""""This represents a major advance in how we read information embedded in our qubits,"" concludes Simmons. ""The result confirms that single-gate reading of qubits is now reaching the sensitivity needed to perform the necessary quantum error correction for a scalable quantum computer.""Australia's first quantum computing companySince May 2017, Australia's first quantum computing company, Silicon Quantum Computing Pty Limited (SQC), has been working to create and commercialise a quantum computer based on a suite of intellectual property developed at the Australian Centre of Excellence for Quantum Computation and Communication Technology (CQC2T).Co-located with CQC2T on the UNSW Campus in Sydney, SQC is investing in a portfolio of parallel technology development projects led by world-leading quantum researchers, including Australian of the Year and Laureate Professor Michelle Simmons. Its goal is to produce a 10-qubit demonstration device in silicon by 2022 as the forerunner to a commercial scale silicon-based quantum computer.SQC believes that quantum computing will ultimately have a significant impact across the global economy, with possible applications in software design, machine learning, scheduling and logistical planning, financial analysis, stock market modelling, software and hardware verification, climate modelling, rapid drug design and testing, and early disease detection and prevention.Created via a unique coalition of governments, corporations and universities, SQC is competing with some of the largest tech multinationals and foreign research laboratories.As well as developing its own proprietary technology and intellectual property, SQC will continue to work with CQC2T and other participants in the Australian and International Quantum Computing ecosystems, to build and develop a silicon quantum computing industry in Australia and, ultimately, to bring its products and services to global markets.Story Source:Materials provided by University of New South Wales. Original written by Isabelle Dubach. ",0.0806312,0.117854,0.135939,0.21972,0.151549,0.130368,0.157157,0,0
65,Let's draw!: New deep learning technique for realistic caricature art Data-driven method leverages deep learning AI for photo-to-caricature translation,"Caricature portrait drawing is a distinct art form where artists sketch a person's face in an exaggerated manner, most times to elicit humor. Automating this technique poses challenges due to the amount of intricate details and shapes involved and level of professional skills it takes to transform a person artistically from their real-life selves to a creatively exaggerated one.A team of computer scientists from City University of Hong Kong and Microsoft, have developed an innovative deep learning-based approach to automatically generate the caricature of a given portrait, and to enable users to do so efficiently and realistically.""Compared to traditional graphics-based methods which define hand-crafted rules, our novel approach leverages big data and machine learning to synthesize caricatures from thousands of examples drawn by professional artists,"" says Kaidi Cao, lead author, who is currently a graduate student in computer science at Stanford University but conducted the work during his internship at Microsoft. ""While existing style transfer methods have focused mainly on appearance style, our technique achieves both geometric exaggeration and appearance stylization involved in caricature drawing."" The method enables users to automate caricatures of portraits, and can be applied to tasks such as creating caricatured avatars for social media, and designing cartoon characters. The technique also has potential applications in marketing, advertising and journalism.Cao collaborated on the research with Jing Liao of City University of Hong Kong and Lu Yuan of Microsoft, and the three plans to present their work at SIGGRAPH Asia 2018 in Tokyo from 4 December to 7 December. The annual conference features the most respected technical and creative members in the field of computer graphics and interactive techniques, and showcases leading edge research in science, art, gaming and animation, among other sectors.In this work, the researchers turned to a well-known technique in machine learning, Generative Adversarial Network (GAN), for unpaired photo-to-caricature translation to generate caricatures that preserve the identity of the portrait. Called ""CariGANs,"" the computational framework precisely models geometric exaggeration in photos (shapes of faces, specific angles) and appearance stylization (look, feel, pencil strokes, shadowing) via two algorithms the researchers have labeled, CariGeoGAN and CariStyGAN.CariGeoGAN only models the geometry-to-geometry mapping from face photos to caricatures and CariStyGAN transfers the style appearance from caricatures to face photos without any deformation to the geometry of the original image. The two networks are separately trained for each task so that the learning procedure is more robust, notes the researchers. The CariGANs framework enables users to control the exaggeration degree in geometric and appearance style by dragging slides or giving an example caricature.Cao and collaborators conducted perceptual studies to evaluate their framework's ability to generate caricatures of portraits that are easily recognizable and not overly distorted in shape and appearance style. For example, one study assessed how well the identity of an image is preserved using the CariGANs method in comparison to existing methods for translating caricature art. They demonstrated, through several examples, that existing methods resulted in unrecognizable caricature translation. Study participants found it too difficult to match the resulting caricatures with the original subjects because the end results were far too exaggerated or unclear. The researchers' method successfully generated clearer, more accurate caricature depictions of portrait photos, as if they were hand drawn by a professional artist.Currently, the focus of this work has centered on caricatures of people, primarily headshots or portraits. In future work, the researchers intend to explore beyond facial caricature generation into full body or more complex scenes. They are also interested in designing improved human-computer interaction (HCI) systems that would give users more freedom and user control on the machine learning-generated results.Story Source:Materials provided by Association for Computing Machinery. ",0.0457527,0.0754946,0.131372,0.303222,0.086585,0.105124,0.163988,0,0
66,How much do you trust Dr. Google?,"Women experiencing signs of breast cancer vary in how they value, use, and trust 'Dr Google' when making sense of their symptoms, a new study in the journal Health, Risk & Society reports.Researchers from the University of Surrey, led by Dr Afrodita Marcu, investigated whether women sought health information online when experiencing potential breast cancer symptoms and, if so, whether they found it useful. Interviewing 27 women, aged between 47 and 67 years old, researchers found different levels of engagement with the internet for health information that were driven by a range of attitudes and levels of trust.Some women, particularly those with no formal educational qualifications or with fewer than two O levels, were found to be less positive about the usefulness of 'Dr Google' and were largely against using the internet for health information, claiming that this could lead to misdiagnosis or to unnecessary worry about what their symptoms might mean.Researchers also found that women, although open to using the internet for health information, reported feeling overwhelmed by what they found and became reluctant to conduct further searches. The majority of women who experienced such feelings went to see their GP, mostly because they felt that only a health care professional could resolve concerns about their symptoms and provide appropriate answers.Other women in the study were however confident in looking up information online about their breast changes and used it to interpret and act upon their symptoms. These women did not view online health information as problematic nor did they express mistrust in 'Dr Google.' Some even supplemented the information received from the GP by further investigations on the internet.Dr Afrodita Marcu, Research Fellow at the University of Surrey, said: ""The internet is a valuable source of medical information. However, it also contains a lot of poor quality information, or information which cannot be easily interpreted by lay people or applied to an individual situation, so it is not surprising that some people feel they cannot trust it.""The way that a person will capitalize on the internet for health purposes depends on many factors, like the nature of their symptoms or their fear about coming across misleading information, so we should not assume that 'Dr Google' is valuable and credible to all.""This study received funding from Cancer Research UK.Story Source:Materials provided by University of Surrey. Original written by Natasha Meredith. ",0.131137,0.181187,0.190003,0.300612,0.204262,0.180636,0.223338,0,0
67,Intelligent framework aims to optimize data transfer in 5G networks,"A North Carolina State University researcher has developed technology designed to allow cellular communication nodes in 5G systems to partition bandwidth more efficiently in order to improve end-to-end data transmission rates. In simulations, the tech is capable of meeting the international goal of 10 gigabits per second in peak performance areas.""End-to-end transfer means that the technology accounts for all of the connections between a data source and the end user,"" says Shih-Chun Lin, an assistant professor of electrical and computer engineering at NC State and author of a paper on the work.""My technology, incorporating both hardware and software, is a framework that takes into account data transfer rates, wired and wireless bandwidth availability, and the power of base stations -- or eNodeBs -- in a 5G network,"" Lin says. ""It then uses stochastic optimization modeling to determine the most efficient means of transferring and retrieving data -- and it does this very quickly, without using a lot of computing power.""Lin says that simulation testing of the framework is promising, and he and his research team are in the process of building a fully functional prototype.""The prototype will allow us to conduct tests on a 5G testbed platform, since full-scale 5G networks are not yet online,"" Lin says. ""But simulation results suggest that we'll be able to meet the 3GPP goal of 10 gigabits per second data transfer in peak coverage areas.""We are currently seeking industry partners to work with us on developing, testing and deploying the framework to better characterize its performance prior to widespread adoption of 5G networks,"" Lin says.The paper, ""End-to-End Network Slicing for 5G&B Wireless Software-Defined Systems,"" will be presented Dec. 11 at IEEE GLOBECOM'18, being held in Abu Dhabi, UAE.Story Source:Materials provided by North Carolina State University. ",0.101576,0.118556,0.171282,0.293986,0.162056,0.16498,0.191651,0,0
68,Automated technique for anime colorization using deep learning,"Japanese researchers from IMAGICA GROUP Inc., OLM Digital, Inc. and Nara Institute of Science and Technology (NAIST) have jointly developed a technique for automatic colorization in anime production.While the number of animation works produced in Japan has been increasing every year, the number of animators has remained almost unchanged. To promote efficiency and automation in anime production, the research team focused on the possibility of automating the colorization of trace images in the finishing process of anime production. By integrating the anime production technology and know-how of IMAGICA GROUP Inc. and OLM Digital, Inc. with the machine learning, computer graphics and vision technology of NAIST, the research team succeeded in developing the world's first technique for automatic colorization of Japanese anime production. The technique is based on recent advances of deep learning approaches that are nowadays widely applied in various fields.After the trace image cleaning in a pre-processing step, automatic colorization is performed according to the color script of the character using a deep learning-based image segmentation algorithm. The colorization result is refined in a post-process step using voting techniques for each closed region.This technique will be presented at SIGGRAPH ASIA 2018, an international conference on computer graphics and interactive techniques, to be held in Tokyo, Japan on Dec. 4-7.While this technique is still in the preliminary research stage, the research team will further improve its accuracy and validate it in production within the anime production studio. The product of this will be available for commercialization from 2020.Story Source:Materials provided by Nara Institute of Science and Technology. ",0.0923157,0.134105,0.156586,0.30471,0.140773,0.173865,0.202979,0,0
69,Healthcare providers -- not hackers -- leak more of your data,"Your personal identity may fall at the mercy of sophisticated hackers on many websites, but when it comes to health data breaches, hospitals, doctors offices and even insurance companies are oftentimes the culprits.New research from Michigan State University and Johns Hopkins University found that more than half of the recent personal health information, or PHI, data breaches were because of internal issues with medical providers -- not because of hackers or external parties.""There's no perfect way to store information, but more than half of the cases we reviewed were not triggered by external factors -- but rather by internal negligence,"" said John (Xuefeng) Jiang, lead author and associate professor of accounting and information systems at MSU's Eli Broad College of Business.The research, published in JAMA Internal Medicine, follows the joint 2017 study that showed the magnitude of hospital data breaches in the United States. The research revealed nearly 1,800 occurrences of large data breaches in patient information over a seven years, with 33 hospitals experiencing more than one substantial breach.For this paper, Jiang and co-author Ge Bai, associate professor at the John's Hopkins Carey Business School, dove deeper to identify triggers of the PHI data breaches. They reviewed nearly 1,150 cases between October 2009 and December 2017 that affected more than 164 million patients.""Every time a hospital has some sort of a data breach, they need to report it to the Department of Health and Human Services and classify what they believe is the cause,"" Jiang, the Plante Moran Faculty Fellow, said. ""These causes fell into six categories: theft, unauthorized access, hacking or an IT incident, loss, improper disposal or 'other.'""After reviewing detailed reports, assessing notes and reclassifying cases with specific benchmarks, Jiang and Bai found that 53 percent were the result of internal factors in healthcare entities.""One quarter of all the cases were caused by unauthorized access or disclosure -- more than twice the amount that were caused by external hackers,"" Jiang said. ""This could be an employee taking PHI home or forwarding to a personal account or device, accessing data without authorization, or even through email mistakes, like sending to the wrong recipients, copying instead of blind copying or sharing unencrypted content.""While some of the errors seem to be common sense, Jiang said that the big mistakes can lead to even bigger accidents and that seemingly innocuous errors can compromise patients' personal data.""Hospitals, doctors offices, insurance companies, small physician offices and even pharmacies are making these kinds of errors and putting patients at risk,"" Jiang said.Of the external breaches, theft accounted for 33 percent with hacking credited for just 12 percent.While some data breaches might result in minor consequences, such as obtaining the phone numbers of patients, others can have much more invasive effects. For example, when Anthem, Inc. suffered a data breach in 2015, 37.5 million records were compromised. Many of the victims were not notified immediately, so weren't aware of the situation until they went to file their taxes only to discover that a third-party fraudulently filed them with the data they obtained from Anthem.While tight software and hardware security can protect from theft and hackers, Jiang and Bai suggest health care providers adopt internal policies and procedures that can tighten processes and prevent internal parties from leaking PHI by following a set of simple protocols. The procedures to mitigate PHI breaches related to storage include transitioning from paper to digital medical records, safe storage, moving to non-mobile policies for patient-protected information and implementing encryption. Procedures related to PHI communication include mandatory verification of mailing recipients, following a ""copy vs. blind copy"" protocol (bcc vs cc) as well as encryption of content.""Not putting on the whole armor opened health care entities to enemy's attacks,"" Bai said. ""The good news is that the armor is not hard to put on if simple protocols are followed.""Next, Jiang and Bai plan to look even more closely at the kind of data that is hacked from external sources to learn what exactly digital thieves hope to steal from patient data.Story Source:Materials provided by Michigan State University. ",0.0543367,0.0865761,0.127258,0.316037,0.124913,0.0968313,0.167734,0,0
70,Using Skype to beat the blues,"Imagine your family has moved across the state or across country. You're retired, and your spouse has passed away. Lacking the social connections previous generations once found in church or fraternal organizations, it doesn't take much time to begin feeling isolated and alone.Social isolation and depression have become commonplace in older adults, with estimates suggesting almost 5 percent of adults aged 50 and above lived with major depression in 2015.What if you could address the problem through communication technology?A new study led by researchers at OHSU in Portland, Oregon, discovered that, of four online communication technologies, using video chat to connect with friends and family appeared to hold the most promise in staving off depression among seniors. Researchers compared four different types of online communication technologies -- video chat, email, social networks and instant messaging -- used by people 60 and older and then gauged their symptoms of depression based on survey responses two years later.The findings were published in the American Journal of Geriatric Psychiatry.""Video chat came out as the undisputed champion,"" said lead author Alan Teo, M.D., associate professor of psychiatry in the OHSU School of Medicine and a researcher at the VA Portland Health Care System. ""Older adults who used video chat technology such as Skype had significantly lower risk of depression.""Data were obtained through the Health and Retirement Study supported by the National Institute on Aging of the National Institutes of Health. Since 1992, the nationwide study has surveyed seniors every two years.The researchers identified 1,424 participants from the 2012 survey who completed a set of questions about technology use. These same participants also responded to a follow-up survey two years later that measured, among other things, depressive symptoms.Those who used email, instant messaging or social media platforms like Facebook had virtually the same rate of depressive symptoms compared with older adults who did not use any communication technologies. In contrast, researchers found that people who used video chat functions such as Skype and FaceTime had almost half the estimated probability of depressive symptoms, after adjusting for other factors that could confound results, such as pre-existing depression and level of education.""To our knowledge, this is the first study to demonstrate a potential link between use of video chat and prevention of clinically significant symptoms of depression over two years in older adults,"" the authors wrote.Researchers said video chat's appeal isn't necessarily surprising. Video chat engages users in face-to-face interactions rather than having them passively scrolling through a Facebook feed, for example.""I still maintain that face-to-face interaction is probably best of all,"" Teo said. ""However, if we're looking at the reality of modern American life, we need to consider these communication technologies. And when we do consider them and compare them, our findings indicate that I'm better off Skyping with my dad in Indiana than sending him a message on WhatsApp.""Story Source:Materials provided by Oregon Health & Science University. ",0.0799148,0.106304,0.161682,0.256628,0.139292,0.143998,0.163282,0,0
71,Bending light around tight corners without backscattering losses New photonic crystal waveguide based on topological insulators paves the way to build futuristic light-based computers,"Engineers at Duke University have demonstrated a device that can direct photons of light around sharp corners with virtually no losses due to backscattering, a key property that will be needed if electronics are ever to be replaced with light-based devices.The result was achieved with photonic crystals built on the concept of topological insulators, which won its discoverers a Nobel Prize in 2016. By carefully controlling the geometry of a crystal lattice, researchers can prevent light traveling through its interior while transmitting it perfectly along its surface.Through these concepts, the device accomplishes its near-perfect transmittance around corners despite being much smaller than previous designs.The Semiconductor Industry Association estimates that the number of electronic devices is increasing so rapidly that by the year 2040, there won't be enough power in the entire world to run them all. One potential solution is to turn to massless photons to replace the electrons currently used for transmitting data. Besides saving energy, photonic systems also promise to be faster and have higher bandwidth.Photons are already in use in some applications such as on-chip photonic communication. One drawback of the current technology, however, is that such systems cannot turn or bend light efficiently. But for photons to ever replace electrons in microchips, travelling around corners in microscopic spaces is a necessity.""The smaller the device the better, but of course we're trying to minimize losses as well,"" said Wiktor Walasik, a postdoctoral associate in electrical and computer engineering at Duke. ""There are a lot of people working to make an all-optical computing system possible. We're not there yet, but I think that's the direction we're going.""Previous demonstrations have also shown small losses while guiding photons around corners, but the new Duke research does it on a rectangular device just 35 micrometers long and 5.5 micrometers wide -- 100 times smaller than previously demonstrated ring-resonator based devices.In the new study, which appeared online on November 12 in the journal Nature Nanotechnology, researchers fabricated topological insulators using electron beam lithography and measured the light transmittance through a series of sharp turns. The results showed that each turn only resulted in the loss of a few percent.""Guiding light around sharp corners in conventional photonic crystals was possible before but only through a long laborious process tailored to a specific set of parameters,"" said Natasha Litchinitser, professor of electrical and computer engineering at Duke. ""And if you made even the tiniest mistake in its fabrication, it lost a lot of the properties you were trying to optimize.""""But our device will work no matter its dimensions or geometry of the photons' path and photon transport is 'topologically protected,'"" added Mikhail Shalaev, a doctoral student in Litchinitser's laboratory and first author of the paper. ""This means that even if there are minor defects in the photonic crystalline structure, the waveguide still works very well. It is not so sensitive to fabrication errors.""The researchers point out that their device also has a large operating bandwidth, is compatible with modern semiconductor fabrication technologies, and works at wavelengths currently used in telecommunications.The researchers are next attempting to make their waveguide dynamically tunable to shift the bandwidth of its operation. This would allow the waveguide to be turned on and off at will -- another important feature for all-optical photon-based technologies to ever become a reality.Story Source:Materials provided by Duke University. ",0.0704296,0.0933039,0.109809,0.155455,0.149458,0.112508,0.127622,0,0
72,Chemistry freed from space and time Automated optimization and synthesis of pharmaceuticals in the cloud,"Shopping on the internet, storing photos in the cloud, turning up a thermostat with an app -- all are commonplace. Now, the internet of things and the cloud are entering the world of chemical research and production, as reported in the journal Angewandte Chemie. Researchers have used remote servers in Japan to autonomously optimize conditions to synthesize drugs in a British laboratory. The process was controlled over the internet by researchers in the USA.Modern production processes cannot simply assemble a target molecule; they have to be economical, efficient, robust, and sustainable too. It is thus necessary to develop a variety of alternative synthetic routes, design tailored equipment, and find optimal processing parameters. This is impossible without a deep understanding of the reactions taking place and a vast amount of data collected under different conditions. In the areas of natural products synthesis and pharmaceuticals, the trend is toward automation of repeated reaction sequences and self-optimizing processes. These are based on machine learning and information feedback in the form of measurements obtained from observation of reactions.Researchers led by Steven V. Ley at the University of Cambridge (UK) and California State University Fullerton (USA) have now demonstrated that this approach can succeed across international borders and time zones -- by use of the cloud. Remote servers in Tokyo (Japan) autonomously developed optimal synthetic conditions for three pharmaceutical agents that were physically synthesized in laboratories in Cambridge (UK). The process was initiated, controlled, and monitored by researchers in Los Angeles (USA) over an internet connection. In this way it was possible for the machines to self optimize the individual synthetic steps for tramadol, lidocaine, and bupropion as representative sample substances, with minimal intervention by the operators over hours.In the case of tramadol, three parameters were varied: temperature, residence time, and the ratio of reactants. Guided by spectroscopic data, the control system carried out nine fully autonomous experiments over a period of three hours and identified optimized conditions for a maximized conversion with the highest possible throughput and little consumption of the starting materials.The autonomous character of this cloud-based approach makes specialized knowledge and equipment broadly available and uses these resources efficiently by avoiding redundancies and allowing for global collaborations in which distance is irrelevant.Story Source:Materials provided by Wiley. ",0.0653428,0.119107,0.113437,0.199179,0.104203,0.126305,0.143098,0,0
73,"What's next for smart homes: An 'Internet of Ears?' Next generation of smart homes envisions using changes in vibrations, sound and electrical field to improve energy consumption, monitor occupants' movements","Houses have been getting progressively ""smarter"" for decades, but the next generation of smart homes may offer what two Case Western Reserve University scientists are calling an ""Internet of Ears.""Today's smart home features appliances, entertainment systems, security cameras and lighting, heating and cooling systems that are connected to each other and the Internet. They can be accessed and controlled remotely by computer or smart-phone apps.The technology of interconnecting commercial, industrial or government buildings, someday even entire communities, is referred to as the ""Internet of Things,"" or IoT.But a pair of electrical engineering and computer science professors in the Case School of Engineering have been experimenting with a new suite of sensors. This system would read not only the vibrations, sounds -- and even the specific gait, or other movements -- associated with people and animals in a building, but also any subtle changes in the existing ambient electrical field.While still maybe a decade or so away, the home of the future could be a building that adjusts to your activity with only a few small, hidden sensors in the walls and floor and without the need for invasive cameras.A building that 'listens'""We are trying to make a building that is able to 'listen' to the humans inside,"" said Ming-Chun Huang, an assistant professor in electrical engineering and computer science.""We are using principles similar to those of the human ear, where vibrations are picked up and our algorithms decipher them to determine your specific movements. That's why we call it the 'Internet of Ears.'""Huang is leading the research related to human gait and motion tracking, while Soumyajit Mandal, the T. and A. Schroeder Assistant Professor of Electrical Engineering and Computer Science, focuses on vibration sensing and changes in the existing electrical field caused by the presence of humans or even pets.""There is actually a constant 60 Hz electrical field all around us, and because people are somewhat conductive, they short out the field just a little,"" Mandal said. ""So, by measuring the disturbance in that field, we are able to determine their presence, or even their breathing, even when there are no vibrations associated with sound.Huang and Mandal published details of their research in October at the IEEE Sensors conference in New Delhi, India. A longer version of their results will appear in the journal IEEE Transactions on Instrumentation and Measurement early next year.They've also tested the technology in conference rooms in the electrical engineering department on campus and in the Smart Living Lab at Ohio Living Breckenridge Village, a senior-living community in Willoughby, Ohio.Mandal said they have used as few as four small sensors in the walls and floor of a room. As for privacy concerns, Mandal said the system would not be able to identify individuals, although it could be calibrated to recognize the different gaits of people.Energy savings, building safetyThey expect the system could provide many benefits.""The first advantage will be energy efficiency for buildings, especially in lighting and heating, as the systems adjust to how humans are moving from one room to another, allocating energy more efficiently,"" Huang said.Another benefit could be the ability to track and measure a building's structural integrity and safety, based on human occupancy -- which would be critical in an earthquake or hurricane, for example, Huang said.""This hasn't really been explored as far as we've seen, but we know that humans create a dynamic load on buildings, especially in older buildings,"" Huang said. ""In collaboration with our colleague YeongAe Heo in the Civil Engineering department, we are trying to predict if there is going to be structural damage because of the increased weight or load based on the number of people on the floor or how they are distributed on that floor.""Story Source:Materials provided by Case Western Reserve University. ",0.0683134,0.073509,0.155301,0.172355,0.104454,0.13515,0.133725,0,0
74,Big data used to predict the future,"Technology is taking giant leaps and bounds, and with it, the information with which society operates daily. Nevertheless, the volume of data needs to be organized, analyzed and crossed to predict certain patterns. This is one of the main functions of what is known as 'Big Data', the 21st century crystal ball capable of predicting the response to a specific medical treatment, the workings of a smart building and even the behavior of the Sun based on certain variables.Researcher in the KIDS research group from the University of Cordoba's Department of Computer Science and Numerical Analysis were able to improve the models that predict several variables simultaneously based on the same set of input variables, thus reducing the size of data necessary for the forecast to be exact. One example of this is a method that predicts several parameters related to soil quality based on a set of variables such as crops planted, tillage and the use of pesticides.""When you are dealing with a large volume of data, there are two solutions. You either increase computer performance, which is very expensive, or you reduce the quantity of information needed for the process to be done properly,"" says researcher Sebastian Ventura, one of the authors of the research article.When building a predictive model there are two issues that need to be dealt with: the number of variables that come into play and the number of examples entered into the system for the most reliable results. With the idea that less is more, the study has been able to reduce the number of examples, by eliminating those that are redundant or ""noisy,"" and that therefore do not contribute any useful information for the creation of a better predictive model.As Oscar Reyes, the lead author of the research, points out ""we have developed a technique that can tell you which set of examples you need so that the forecast is not only reliable but could even be better."" In some databases, of the 18 that were analyzed, they were able to reduce the amount of information by 80% without affecting the predictive performance, meaning that less than half the original data was used. All of this, says Reyes, ""means saving energy and money in the building of a model, as less computing power is required."" In addition, it also means saving time, which is interesting for applications that work in real-time, since ""it doesn't make sense for a model to take half an hour to run if you need a prediction every five minutes.""As pointed out by the authors of the research, these systems that predict several variables simultaneously (which could be related to one another), based on several variables -known as multi-output regression models,- are gaining more notable importance due to the wide range of applications that ""could be analyzed under this paradigm of automatic learning,"" such as for example those related to healthcare, water quality, cooling systems for buildings and environmental studies.Story Source:Materials provided by University of Cerdoba. ",0.0486391,0.05817,0.103488,0.20558,0.0709043,0.107925,0.129744,0,0
75,Batteryless smart devices closer to reality,"Researchers at the University of Waterloo have taken a huge step towards making smart devices that do not use batteries or require charging.These battery-free objects, which feature an IP address for internet connectivity, are known as Internet of Things (IoT) devices. If an IoT device can operate without a battery it lowers maintenance costs and allows the device to be placed in areas that are off the grid.Many of these IoT devices have sensors in them to detect their environment, from a room's ambient temperature and light levels to sound and motion, but one of the biggest challenges is making these devices sustainable and battery-free.Professor Omid Abari, Postdoctoral Fellow Ju Wang and Professor Srinivasan Keshav from Waterloo's Cheriton School of Computer Science have found a way to hack radio frequency identification (RFID) tags, the ubiquitous squiggly ribbons of metal with a tiny chip found in various objects, and give the devices the ability to sense the environment.""It's really easy to do,"" said Wang. ""First, you remove the plastic cover from the RFID tag, then cut out a small section of the tag's antenna with scissors, then attach a sensor across the cut bits of the antenna to complete the circuit.""In their stock form, RFID tags provide only identification and location. It's the hack the research team has done -- cutting the tag's antenna and placing a sensing device across it -- that gives the tag the ability to sense its environment.To give a tag eyes, the researchers hacked an RFID tag with a phototransistor, a tiny sensor that responds to different levels of light.By exposing the phototransistor to light, it changed the characteristics of the RFID's antenna, which in turn caused a change in the signal going to the reader. They then developed an algorithm on the reader side that monitors change in the tag's signal, which is how it senses light levels.Among the simplest of hacks is adding a switch to an RFID tag so it can act as a keypad that responds to touch.""We see this as a good example of a complete software-hardware system for IoT devices,"" Abari said. ""We hacked simple hardware -- we cut RFID tags and placed a sensor on them. Then we designed new algorithms and combined the software and hardware to enable new applications and capabilities.""Our main contribution is showing how simple it is to hack an RFID tag to create an IoT device. It's so easy a novice could do it.""The research paper by Wang, Abari and Keshav titled, Challenge: RFID Hacking for Fun and Profit-ACM MobiCom, appeared in the Proceedings of the 24th Annual International Conference on Mobile Computing and Networking, October 29-November 2, 2018, New Delhi, India, 461- 70.Story Source:Materials provided by University of Waterloo. ",0.118234,0.132885,0.166174,0.264787,0.208481,0.176716,0.19118,0,0
76,Powered by windows: Enhanced power factor in transparent thermoelectric nanowire materials,"A research group led by Professor Yoshiaki Nakamura of Osaka University successfully developed a methodology for enhancing thermoelectric power factor while decreasing thermal conductivity. By introducing ZnO nanowires into ZnO films, the thermoelectric power factor became 3 times larger than that of ZnO film without ZnO nanowires.For the development of high-performance thermoelectric materials, expensive and toxic heavy elements have often been used; however, the high cost and toxicity have limited the social use of such thermoelectric materials. In this research, Nakamura and his team developed novel nanostructured films (Embedded-ZnO nanowire structure) composed of low-cost and environmentally-friendly ZnO. In the developed films, the thermoelectric power factor was increased by selective transmission of energetic electrons through nanowire interfaces with intentionally-controlled energy barriers, and thermal conductivity was decreased by scattering phonons at the nanowire interfaces. It is anticipated that the success of this research will lead to the realization of high-performance transparent thermoelectric devices which will enable energy recovery from transparent objects used all over the world, such as window glass and transparent electronic devices.Thermoelectric generation converting heat to electricity has drawn much attention as a new energy source. Window glass with differing indoor and outdoor temperatures is anticipated as a heat source for thermoelectric generation, requiring transparent thermoelectric materials with high thermoelectric performance. Thermoelectric performance requires a high Seebeck coefficient, high electrical conductivity, and low thermal conductivity. However, these three parameters are intercorrelated, leading to difficulty in performance enhancement. So far, expensive and toxic heavy element materials with low thermal conductivity have often been used for the development of high-performance thermoelectric materials, limiting the use of thermoelectric generation. On the other hand, low-cost and environmentally-friendly light element-based materials exhibit low thermoelectric performance due to their high thermal conductivity in general. However, it was reported that nanostructuring achieved a significant reduction of thermal conductivity, and light element-based materials could be candidates for thermoelectric materials. But, there is still another problem in that the nanostructure scattered not only phonons but also electrons, resulting in a reduction of thermoelectric power factor.Nakamura and his team successfully developed low-cost and environmentally-friendly ZnO films, including surface-controlled ZnO nanowire (Embedded-ZnO nanowire structure), for the first time in the world. Embedded-ZnO nanowire structure film with high optical transmittance in visible range is anticipated as a transparent thermoelectric material. In the structure, electron energy barrier height was controlled by modulating the dopant concentration at the nanowire interface, which enabled the increase of Seebeck coefficient due to selective transmission of high-energy electrons and scattering of low-energy electrons. High electrical conductivity is also anticipated because the ZnO crystal is epitaxially formed at the nanowire interface, leading to relatively high electrical conductivity of high-energy electrons. Furthermore, thermal conductivity is also decreased by an increase in phonon scattering at the nanowire interface.Embedded-ZnO nanowire structures with nanowire areal density of more than 4e109 cm-2 exhibited a thermoelectric power factor 3 times larger than that of ZnO film without nanowires. It was confirmed that the dopant concentration was modulated at the interfaces by transmission electron microscopy observation of nanowire interfaces. The measurements of Seebeck coefficient and electrical conductivity in low temperature range (<300 K) showed the anomalous behaviors attributed to the electron transport controlled by energy barrier height. Furthermore, the energy barrier height was found to be several tens of meV through theoretical analysis of the experimental data. In addition, thermal conductivity of the Embedded-ZnO nanowire structure was 20% smaller than that of ZnO film without nanowires because of the phonon scattering enhancement due to the introduction of the nanowire interface. These results indicate simultaneous successes: an increase of thermoelectric power factor and a decrease of thermal conductivity. The optical measurement showed that the structure had optical transmittance of about 60% in visible range, which is comparable to the value of a window of a building.In the future, it will be possible to greatly decrease the thermal conductivity of Embedded-ZnO nanowire structure by increasing nanowire areal density. The thermoelectric devices composed of films with this structure are expected to be realized and see widespread use due to their use of low-cost and environmentally-friendly ZnO. Furthermore, the concept of ""Modulating energy barrier height by controlling dopant concentration"" can be applied not only to ZnO but also to other promising materials, which will accelerate the development of various high-performance thermoelectric materials.Story Source:Materials provided by Osaka University. ",0.134714,0.201757,0.181639,0.202449,0.21424,0.165407,0.202461,0,0
77,"Summer birth and computer games linked to heightened short-sight risk in childhood But fertility treatment may protect against development of myopia, twin study suggests","Summer birth and hours spent playing computer games are linked to a heightened risk of developing short or near sightedness (myopia) in childhood, indicates a twin study, published online in the British Journal of Ophthalmology.But fertility treatment may be protective, the findings suggest.Myopia is defined as a refractive error, meaning that the eye can't focus light properly. The result is that close objects look clear, but distant ones appear blurred.It can be corrected with prescription glasses, laser surgery, or contact lenses, but the condition is linked to a heightened risk of visual impairment and sight loss in later life.And it is becoming increasingly common: 4.758 billion people worldwide are likely to be affected by 2050, up from 1.950 billion in 2010.Genes are thought to have a role, but they don't fully explain the rising prevalence. And given the rapid development of the eyes in early life, the researchers wanted to explore potential contributory environmental factors across the life course.They studied 1991 twins whose age was 16.7 years, on average. The twins were all born between 1994 and 1996 in the UK, and taking part in the long term Twins Early Development Study (TEDS).Opticians provided information from their eye tests about myopia, and the researchers analysed demographic, social, economic, educational and behavioural factors in the twin pairs from when these children were 2,3,4,7,8,10,12,14, and 16 years old, to capture critical stages of child and eye development.Parents and teachers filled in comprehensive questionnaires and the twins did web based assessments to provide a wide range of background and potentially relevant information on factors that might have influenced early life development.The average age at which children with myopia started wearing glasses to correct the condition was 11. Around one in 20 (5.4%) had a 'lazy eye' (amblyopia) and a similar proportion (nearly 4.5%) had a squint. Overall, one in four (26%) of the twins was myopic.The factors most strongly associated with the development of myopia across the various time points were the mother's educational attainment (university or postgraduate level), hours spent playing computer games, and being born during the summer.Hours spent playing computer games may not just be linked to close working, but also to less time outdoors-a factor that has previously been linked to heightened myopia risk.Educational level has also been linked to myopia, and as child in the UK born in the summer months will start school at a younger age than those born during the winter months, the researchers suggest that this earlier close work may speed up eye growth which is responsible for short-sightedness.Higher levels of household income and measures of intelligence, particularly verbal dexterity scores, were associated with heightened risk, but to a lesser extent.Fertility treatment seemed to afford protection against myopia and was associated with a 25-30 per cent lower risk. The researchers speculate that children born as a result of fertility treatment are often born smaller and slightly more premature, and may have some level of developmental delay, which might account for shorter eye length and less myopia.This is an observational study, and as such, can't establish cause, say the researchers, highlighting that future research may be able to look at the interplay between genetic susceptibility and environmental influences.In a linked editorial, Drs Mohamed Dirani, Jonathan Crowston, and Tien Wong, of, respectively, the Singapore National Eye Centre, Centre for Eye Research, Melbourne, Australia, and the Department of Surgery, University of Melbourne, point out that environmental factors are now thought to have a greater role than genetic ones.They add that the study involved data gathered before the explosion in digital media.""The rapid adoption of smart devices in children adds a new dimension to how we define and quantify near-work activity,"" they write...The role of smart devices, quantified as device screen time (DST) must also be investigated.""And children start using these devices at an increasingly younger age. ""The increased DST resulting from gaming, social media, and digital entertainment has led to a rise in sedentary behaviour, poor diet and a lack of outdoor activity,"" they suggest.""The use and misuse of smart devices, particularly in our paediatric populations, must be closely monitored to address the emerging phenomenon of digital myopia,"" they conclude.Story Source:Materials provided by BMJ. ",0.0607303,0.0759607,0.142353,0.195197,0.103697,0.114109,0.1309,0,0
78,Why some Wikipedia disputes go unresolved Study identifies reasons for unsettled editing disagreements and offers predictive tools that could improve deliberation,"Wikipedia has enabled large-scale, open collaboration on the internet's largest general-reference resource. But, as with many collaborative writing projects, crafting the content can be a contentious subject.Often, multiple Wikipedia editors will disagree on certain changes to articles or policies. One of the main ways to officially resolve such disputes is the Requests for Comment (RfC) process. Quarreling editors will publicize their deliberation on a forum, where other Wikipedia editors will chime in and a neutral editor will make a final decision.Ideally, this should solve all issues. But a novel study by MIT researchers finds debilitating factors -- such as excessive bickering and poorly worded arguments -- have led to about one-third of RfCs going unresolved.For the study, the researchers compiled and analyzed the first-ever comprehensive dataset of RfC conversations, captured over an eight-year period, and conducted interviews with editors who frequently close RfCs, to understand why they don't find a resolution. They also developed a machine-learning model that leverages that dataset to predict when RfCs may go stale. And, they recommend digital tools that could make deliberation and resolution more effective.""It was surprising to see a full third of the discussions were not closed,"" says Amy X. Zhang, a PhD candidate in MIT's Computer Science and Artificial Intelligence Laboratory (CSAIL) and co-author on the paper, which is being presented at this week's ACM Conference on Computer-Supported Cooperative Work and Social Computing. ""On Wikipedia, everyone's a volunteer. People are putting in the work, and they have interest ... and editors may be waiting on someone to close so they can get back to editing. We know, looking through the discussions, the job of reading through and resolving a big deliberation is hard, especially with back and forth and contentiousness. [We hope to] help that person do that work.""The paper's co-authors are: first author Jane Im, a graduate student at the University of Michigan's School of Information; Christopher J. Schilling of the Wikimedia Foundation; and David Karger, a professor of computer science and CSAIL researcher.(Not) finding closureWikipedia offers several channels to solve editorial disputes, which involve two editors hashing out their problems, putting ideas to a simple majority vote from the community, or bringing the debate to a panel of moderators. Some previous Wikipedia research has delved into those channels and back-and-forth ""edit wars"" between contributors. ""But RfCs are interesting, because there's much less of a voting mentality,"" Zhang says. ""With other processes, at the end of day you'll vote and see what happens. [RfC participants] do vote sometimes, but it's more about finding a consensus. What's important is what's actually happening in a discussion.""To file an RfC, an editor drafts a template proposal, based on a content dispute that wasn't resolved in an article's basic ""talk"" page, and invites comment by the broader community. Proposals run the gamut, from minor disagreements about a celebrity's background information to changes to Wikipedia's policies. Any editor can initiate an RfC and any editor -- usually, more experienced ones -- who didn't participate in the discussion and is considered neutral, may close a discussion. After 30 days, a bot automatically removes the RfC template, with or without resolution. RfCs can close formally with a summary statement by the closer, informally due to overwhelming agreement by participants, or be left stale, meaning removed without resolution.For their study, the researchers compiled a database consisting of about 7,000 RfC conversations from the English-language Wikipedia from 2011 to 2017, which included closing statements, author account information, and general reply structure. They also conducted interviews with 10 of Wikipedia's most frequent closers to better understand their motivations and considerations when resolving a dispute.Analyzing the dataset, the researchers found that about 57 percent of RfCs were formally closed. Of the remaining 43 percent, 78 percent (or around 2,300) were left stale without informal resolution -- or, about 33 percent of all the RfCs studied. Combining dataset analysis with the interviews, the researchers then fleshed out the major causes of resolution failure. Major issues include poorly articulated initial arguments, where the initiator is unclear about the issue or writes a deliberately biased proposal; excessive bickering during discussions that lead to more complicated, longer, argumentative threads that are difficult to fully examine; and simple lack of interest from third-party editors because topics may be too esoteric, among other factors.Helpful toolsThe team then developed a machine-learning model to predict whether a given RfC would close (formally or informally) or go stale, by analyzing more than 60 features of the text, Wikipedia page, and editor account information. The model achieved a 75 percent accuracy for predicting failure or success within one week after discussion started. Some more informative features for prediction, they found, include the length of the discussion, number of participants and replies, number of revisions to the article, popularity of and interest in the topic, experience of the discussion participants, and the level of vulgarity, negativity, and general aggression in the comments.The model could one day be used by RfC initiators to monitor a discussion as it's unfolding. ""We think it could be useful for editors to know how to a target their interventions,"" Zhang says. ""They could post [the RfC] to more [Wikipedia forums] or invite more people, if it looks like it's in danger of not being resolved.""The researchers suggest Wikipedia could develop tools to help closers organize lengthy discussions, flag persuasive arguments and opinion changes within a thread, and encourage collaborative closing of RfCs.In the future, the model and proposed tools could potentially be used for other community platforms that involve large-scale discussions and deliberations. Zhang points to online city-and community-planning forums, where citizens weigh in on proposals. ""People are discussing [the proposals] and voting on them, so the tools can help communities better understand the discussions ... and would [also] be useful for the implementers of the proposals.""Zhang, Im, and other researchers have now built an external website for editors of all levels of expertise to come together to learn from one another, and more easily monitor and close discussions. ""The work of closer is pretty tough,"" Zhang says, ""so there's a shortage of people looking to close these discussions, especially difficult, longer, and more consequential ones. This could help reduce the barrier to entry [for editors to become closers] and help them collaborate to close RfCs.""Story Source:Materials provided by Massachusetts Institute of Technology. Original written by Rob Matheson. ",0.0568984,0.080348,0.134482,0.277482,0.109683,0.108318,0.160345,0,0
79,Face the music: Explicit anti-piracy warnings are best deterrent Privacy threats are just as effective at scaring off illegal downloaders as big fines,"STOP! This is illegal. You may be monitored and fined.Did that get your attention? Good. Because according to a new UNLV study, this phrasing coupled with a graphic of a computer and download symbol with a prohibitive slash is the most effective way to stop music piracy.The research, borne out of dissertation work by UNLV psychology instructor Joanne Ullman, tested 220 college undergraduates' reactions to more than 70 symbols, action words, and warning phrases. Among the findings:""For some individuals, privacy may be just as important as a pecuniary consequence,"" researchers wrote in their paper, which was published last month during the Human Factors and Ergonomics Society 2018 Annual Meeting. ""Of course, the combination of both would heighten the potential consequence.""Ullman said she doesn't expect her findings will rid the entire world of music piracy. But she hopes to educate the public and to perhaps encourage lawmakers to streamline copyright laws or the music industry to create blanket streaming/download rules that better match consumer habits.Currently, the Copyright Act allows damages of $750 up to $150,000 per infringement. Moreover, under the Digital Millennium Copyright Act, internet service providers can disable repeat offenders' web access.""Technically, there's no music out there that's free,"" Ullman said, referencing the 2018 Music Modernization Act which for the first time enacts federal laws -- that will supersede a vague, contradictory, confusing patchwork of state laws protecting sound recordings -- and allow fully-protected music to enter public domain beginning in 2022. She noted that even the ""Happy Birthday"" song, protected under the vague older laws, didn't enter public domain until 2016 under a judge's ruling; previously, she said, movies and restaurants were among entities that would create their own version or avoid using the song altogether to avoid legal battles surrounding copyright infringement.Ullman says that, similar to jaywalking, music copyright laws are not regulated across the board and therefore could be subject to selective enforcement. And that could become a problem for the United States, which produces and distributes the most music in the world.Throughout history, Ullman said, copyright issues have become an issue whenever there's been modernization; for example, laws had to be created to protect works reproduced after the printing press was invented. ""There might be a correlation between our economic condition and how strict (the government or music companies) are with intellectual property. Economic recessions may actually lead to more stringent enforcement with respect to copyright laws,"" she said.With respect to the effectiveness of music piracy warnings, Ullman said we all have different levels of risk aversions because of our personality or profession.""I doubt these symbols will get someone to suddenly change their mind and say, 'Piracy is bad', "" she said. ""But at least being reminded that if you download something illegally you may be monitored or fined, we can think about our behavior and make thoughtful decisions. That's especially important in the online world where we're so task-focused; the pictorial can almost immediately remind you -- you don't even have to read anything -- and make you say, 'I should think before I act'.""Story Source:Materials provided by University of Nevada, Las Vegas. ",0.11097,0.132015,0.176488,0.286747,0.167313,0.159615,0.195387,0,0
80,Breakthrough in construction of computers for mimicking human brain,"A computer built to mimic the brain's neural networks produces similar results to that of the best brain-simulation supercomputer software currently used for neural-signaling research, finds a new study published in the open-access journal Frontiers in Neuroscience. Tested for accuracy, speed and energy efficiency, this custom-built computer named SpiNNaker, has the potential to overcome the speed and power consumption problems of conventional supercomputers. The aim is to advance our knowledge of neural processing in the brain, to include learning and disorders such as epilepsy and Alzheimer's disease.""SpiNNaker can support detailed biological models of the cortex -- the outer layer of the brain that receives and processes information from the senses -- delivering results very similar to those from an equivalent supercomputer software simulation,"" says Dr. Sacha van Albada, lead author of this study and leader of the Theoretical Neuroanatomy group at the Jelich Research Centre, Germany. ""The ability to run large-scale detailed neural networks quickly and at low power consumption will advance robotics research and facilitate studies on learning and brain disorders.""The human brain is extremely complex, comprising 100 billion interconnected brain cells. We understand how individual neurons and their components behave and communicate with each other and on the larger scale, which areas of the brain are used for sensory perception, action and cognition. However, we know less about the translation of neural activity into behavior, such as turning thought into muscle movement.Supercomputer software has helped by simulating the exchange of signals between neurons, but even the best software run on the fastest supercomputers to date can only simulate 1% of the human brain.""It is presently unclear which computer architecture is best suited to study whole-brain networks efficiently. The European Human Brain Project and Jelich Research Centre have performed extensive research to identify the best strategy for this highly complex problem. Today's supercomputers require several minutes to simulate one second of real time, so studies on processes like learning, which take hours and days in real time are currently out of reach."" explains Professor Markus Diesmann, co-author, head of the Computational and Systems Neuroscience department at the Jelich Research Centre.He continues, ""There is a huge gap between the energy consumption of the brain and today's supercomputers. Neuromorphic (brain-inspired) computing allows us to investigate how close we can get to the energy efficiency of the brain using electronics.""Developed over the past 15 years and based on the structure and function of the human brain, SpiNNaker -- part of the Neuromorphic Computing Platform of the Human Brain Project -- is a custom-built computer composed of half a million of simple computing elements controlled by its own software. The researchers compared the accuracy, speed and energy efficiency of SpiNNaker with that of NEST -- a specialist supercomputer software currently in use for brain neuron-signaling research.""The simulations run on NEST and SpiNNaker showed very similar results,"" reports Steve Furber, co-author and Professor of Computer Engineering at the University of Manchester, UK. ""This is the first time such a detailed simulation of the cortex has been run on SpiNNaker, or on any neuromorphic platform. SpiNNaker comprises 600 circuit boards incorporating over 500,000 small processors in total. The simulation described in this study used just six boards -- 1% of the total capability of the machine. The findings from our research will improve the software to reduce this to a single board.""Van Albada shares her future aspirations for SpiNNaker, ""We hope for increasingly large real-time simulations with these neuromorphic computing systems. In the Human Brain Project, we already work with neuroroboticists who hope to use them for robotic control.""Story Source:Materials provided by Frontiers. ",0.0721935,0.0892267,0.123047,0.35539,0.127481,0.130922,0.182277,0,0
81,Turning deep-learning AI loose on software development BAYOU learned to write code for programmers by studying billions of programs,"Computer scientists at Rice University have created a deep-learning, software-coding application that can help human programmers navigate the growing multitude of often-undocumented application programming interfaces, or APIs.Known as Bayou, the Rice application was created through an initiative funded by the Defense Advanced Research Projects Agency aimed at extracting knowledge from online source code repositories like GitHub. A paper on Bayou will be presented May 1 in Vancouver, British Columbia, at the Sixth International Conference on Learning Representations, a premier outlet for deep learning research. Users can try it out at askbayou.com.Designing applications that can program computers is a long-sought grail of the branch of computer science called artificial intelligence (AI).""People have tried for 60 years to build systems that can write code, but the problem is that these methods aren't that good with ambiguity,"" said Bayou co-creator Swarat Chaudhuri, associate professor of computer science at Rice. ""You usually need to give a lot of details about what the target program does, and writing down these details can be as much work as just writing the code.""Bayou is a considerable improvement,"" he said. ""A developer can give Bayou a very small amount of information -- just a few keywords or prompts, really -- and Bayou will try to read the programmer's mind and predict the program they want.""Chaudhuri said Bayou trained itself by studying millions of lines of human-written Java code. ""It's basically studied everything on GitHub, and it draws on that to write its own code.""Bayou co-creator Chris Jermaine, a professor of computer science who co-directs Rice's Intelligent Software Systems Laboratory with Chaudhuri, said Bayou is particularly useful for synthesizing examples of code for specific software APIs.""Programming today is very different than it was 30 or 40 years ago,"" Jermaine said. ""Computers today are in our pockets, on our wrists and in billions of home appliances, vehicles and other devices. The days when a programmer could write code from scratch are long gone.""Bayou architect Vijay Murali, a research scientist at the lab, said, ""Modern software development is all about APls. These are system-specific rules, tools, definitions and protocols that allow a piece of code to interact with a specific operating system, database, hardware platform or another software system. There are hundreds of APIs, and navigating them is very difficult for developers. They spend lots of time at question-answer sites like Stack Overflow asking other developers for help.""Murali said developers can now begin asking some of those questions at Bayou, which will give an immediate answer.""That immediate feedback could solve the problem right away, and if it doesn't, Bayou's example code should lead to a more informed question for their human peers,"" Murali said.Jermaine said the team's primary goal is to get developers to try to extend Bayou, which has been released under a permissive open-source license.""The more information we have about what people want from a system like Bayou, the better we can make it,"" he said. ""We want as many people to use it as we can get."" Bayou is based on a method called neural sketch learning, which trains an artificial neural network to recognize high-level patterns in hundreds of thousands of Java programs. It does this by creating a ""sketch"" for each program it reads and then associating this sketch with the ""intent"" that lies behind the program.When a user asks Bayou questions, the system makes a judgment call about what program it's being asked to write. It then creates sketches for several of the most likely candidate programs the user might want.""Based on that guess, a separate part of Bayou, a module that understands the low-level details of Java and can do automatic logical reasoning, is going to generate four or five different chunks of code,"" Jermaine said. ""It's going to present those to the user like hits on a web search. 'This one is most likely the correct answer, but here are three more that could be what you're looking for.'""Story Source:Materials provided by Rice University. Original written by Jade Boyd. ",0.0451335,0.0645882,0.10847,0.381115,0.101537,0.0958927,0.16891,0,0
82,"Largest known prime number discovered 50th known Mersenne prime ever found, on computer volunteered in collaborative project","The Great Internet Mersenne Prime Search (GIMPS) has discovered the largest known prime number, 277,232,917-1, having 23,249,425 digits. A computer volunteered by Jonathan Pace made the find on December 26, 2017. Jonathan is one of thousands of volunteers using free GIMPS software.The new prime number, also known as M77232917, is calculated by multiplying together 77,232,917 twos, and then subtracting one. It is nearly one million digits larger than the previous record prime number, in a special class of extremely rare prime numbers known as Mersenne primes. It is only the 50th known Mersenne prime ever discovered, each increasingly difficult to find. Mersenne primes were named for the French monk Marin Mersenne, who studied these numbers more than 350 years ago. GIMPS, founded in 1996, has discovered the last 16 Mersenne primes. Volunteers download a free program to search for these primes, with a cash award offered to anyone lucky enough to find a new prime. Prof. Chris Caldwell maintains an authoritative web site on the largest known primes, and has an excellent history of Mersenne primes.The primality proof took six days of non-stop computing on a PC with an Intel i5-6600 CPU. To prove there were no errors in the prime discovery process, the new prime was independently verified using four different programs on four different hardware configurations.Jonathan Pace is a 51-year old Electrical Engineer living in Germantown, Tennessee. Perseverance has finally paid off for Jon -- he has been hunting for big primes with GIMPS for over 14 years. The discovery is eligible for a $3,000 GIMPS research discovery award.GIMPS Prime95 client software was developed by founder George Woltman. Scott Kurowski wrote the PrimeNet system software that coordinates GIMPS' computers. Aaron Blosser is now the system administrator, upgrading and maintaining PrimeNet as needed. Volunteers have a chance to earn research discovery awards of $3,000 or $50,000 if their computer discovers a new Mersenne prime. GIMPS' next major goal is to win the $150,000 award administered by the Electronic Frontier Foundation offered for finding a 100 million digit prime number.Credit for this prime goes not only to Jonathan Pace for running the Prime95 software, Woltman for writing the software, Kurowski and Blosser for their work on the Primenet server, but also the thousands of GIMPS volunteers that sifted through millions of non-prime candidates. In recognition of all the above people, official credit for this discovery goes to ""J. Pace, G. Woltman, S. Kurowski, A. Blosser, et al.""The Great Internet Mersenne Prime Search (GIMPS) was formed in January 1996 by George Woltman to discover new world record size Mersenne primes. In 1997 Scott Kurowski enabled GIMPS to automatically harness the power of thousands of ordinary computers to search for these ""needles in a haystack."" Most GIMPS members join the search for the thrill of possibly discovering a record-setting, rare, and historic new Mersenne prime. The search for more Mersenne primes is already under way. There may be smaller, as yet undiscovered Mersenne primes, and there almost certainly are larger Mersenne primes waiting to be found. Anyone with a reasonably powerful PC can join GIMPS and become a big prime hunter, and possibly earn a cash research discovery award. All the necessary software can be downloaded for free at www.mersenne.org/download/.Story Source:Materials provided by Great Internet Mersenne Prime Search (GIMPS). ",0.144951,0.151209,0.190139,0.329401,0.175153,0.26343,0.224803,0,0
83,"Advanced computer technology and software turn species identification interactive Important group of biocontrol wasps from Central Europe are used to demonstrate the perks and advantages of modern, free-to-use software","Representing a group of successful biocontrol agents for various pest fruit flies, a parasitic wasp genus remains largely overlooked. While its most recent identification key dates back to 1969, many new species have been added since then. As if to make matters worse, this group of visually identical species most likely contains many species yet to be described as new to science.Having recently studied a species group of these wasps in Central Europe, scientists Fabian Klimmek and Hannes Baur of the Natural History Museum Bern, Switzerland, not only demonstrate the need for a knowledge update, but also showcase the advantages of modern taxonomic software able to analyse large amounts of descriptive and quantitative data.Published in the open access Biodiversity Data Journal, the team's taxonomic paper describes a new species -- Pteromalus capito -- and presents a discussion on the free-to-use Xper3, developed by the Laboratory of Informatics and Systematics of Pierre-and-Marie-Curie University. The software was used to create an openly available updated key for the species group Pteromalus albipennis.The fully illustrated interactive database covers 27 species in the group and 18 related species, in addition to a complete diagnosis, a large set of body measurements and a total of 585 images, displaying most of the characteristic features for each species.""Nowadays, advanced computer technology, measurement procedures and equipment allow more sophisticated ways to include quantitative characters, which greatly enhance the delimitation of cryptic species,"" explain the scientists.""Recently developed software for the creation of biological identification keys like Xper3, Lucid or Delta could have the potential to replace traditional paper-based keys.""To put the statement into context, the authors give an example with one of the studied wasp species, whose identification would take 16 steps if the previously available identification key were used, whereas only 6 steps were needed with the interactive alternative.One of the reasons tools like Xper3 are so fast and efficient is that the key's author can list all descriptive characters in a specific order and give them different weight in species delimitation. Thus, whenever an entomologist tries to identify a wasp specimen, the software will first run a check against the descriptors at the top, so that it can exclude non-matching taxons and provide a list of the remaining names. Whenever multiple names remain, a check further down the list is performed, until there is a single one left, which ought to be the one corresponding to the specimen. At any point, the researcher can access the chronology, in order to check for any potential mismatches without interrupting the process.Being the product of digitally available software, interactive identification keys are not only easy, quick and inexpensive to publish, but they are also simple to edit and build on in a collaborative manner. Experts from all around the world could update the key, as long as the author grants them specific user rights. However, regardless of how many times the database is updated, a permanent URL link will continue to provide access to the latest version at all times.To future-proof their key and its underlying data, the scientists have deposited all raw data files, R-scripts, photographs, files listing and prepared specimens at the research data Zenodo, created by OpenAIRE and CERN.Story Source:Materials provided by Pensoft Publishers. The original story is licensed under a Creative Commons License. ",0.062512,0.101277,0.119459,0.250825,0.103565,0.123939,0.154772,0,0
84,The big problem of small data: A new approach,"Big Data is all the rage today, but Small Data matters too! Drawing reliable conclusions from small datasets, like those from clinical trials for rare diseases or in studies of endangered species, remains one of the trickiest obstacles in statistics. Now, Cold Spring Harbor Laboratory (CSHL) researchers have developed a new way to analyze small data, one inspired by advanced methods in theoretical physics, but available as easy-to-use software.""Dealing with small datasets is a fundamental part of doing science,"" CSHL Assistant Professor Justin Kinney explained. The challenge is that, with very little data, it's not only hard to come to a conclusion; it's also hard to determine how certain your conclusions are.""It's important to not only produce the best guess for what's going on, but also to say, 'This guess is probably correct,'"" said Kinney.A good example is clinical drug trials.""When each data point is a patient, you will always be dealing with small datasets, and for very good reasons,"" he said. ""You don't want to test a treatment on more people than you have to before determining if the drug is safe and effective. It's really important to be able to make these decisions with as little data as possible.""Quantifying that certainty has been difficult because of the assumptions that common statistical methods make. These assumptions were necessary back when standard methods were developed, before the computer age. But these approximations, Kinney notes, ""can be catastrophic"" on small datasets.Now, Kinney's lab has crafted a modern computational approach called Density Estimation using Field Theory, or DEFT, that fixes these shortcomings. DEFT is freely available via an open source package called SUFTware.In their recent paper, published in Physical Review Letters, Kinney's lab demonstrates DEFT on two datasets: national health statistics compiled by the World Health Organization, and traces of subatomic particles used by physicists at the Large Hadron Collider to reveal the existence of the Higgs boson particle.Kinney says that being able to apply DEFT to such drastically diverse ""real-world"" situations -- despite its computations being inspired by theoretical physics -- is what makes the new approach so powerful.""Flexibility is a really good thing... We're now adapting DEFT to problems in survival analysis, the type of statistics used in clinical trials,"" Kinney said. ""Those new capabilities are going to be added to SUFTware as we continue developing this new approach to statistics.""Story Source:Materials provided by Cold Spring Harbor Laboratory. Original written by Brian Stallard. ",0.073843,0.0980193,0.156741,0.229724,0.105963,0.132322,0.16886,0,0
85,Guidelines for a standardized data format for use in cross-linguistic studies,"An international team of researchers, members of the Cross-Linguistic Data Formats Initiative (CLDF) led by the Max Planck Institute for the Science of Human History, has set out a proposal for new guidelines on cross-linguistic data formats, in order to facilitate sharing and data comparisons between the growing number of large linguistic databases worldwide. This format provides a software package, a basic ontology and usage examples.There is an increasing number of linguistic databases worldwide, raising the possibility of a vast network for potential comparative studies. However, these databases are generally created independently of each other, and often have a unique and narrow focus. This means that the formats used for encoding the data are often different and this creates real difficulties in effectively comparing data across databases.In an effort to resolves these issues, the Cross-Linguistic Data Formats Initiative (CLDF) was created. In a paper published in Scientific Data, the CLDF sets out proposed guidelines for a standardized format for linguistic databases, and also supplies a software package, a basic ontology and usage examples of best practices. The goal of this effort is to facilitate sharing and re-use of data in comparative linguistics.Standardizing data formats to facilitate sharing and reuseThe CLDF provides a data model underlying its recommendations that aims to be simple, yet expressive, and is based on the data model previously developed for the Cross-Linguistic Data project. This model has four main entities: (a) Languages; (b) Parameters; (c) Values; and (d) Sources. In the model, each Value is related to a Parameter and a Language, and can be based on multiple Sources. There are additionally References for Sources, and References can also have Contexts (which, for example, for printed references would be page numbers).The CLDF data model is a package format, in which a dataset would be made up of a set of data files containing tables, and a descriptive file that defines the relationships between the tables. Each linguistic data type would have a CLDF module and additional components, which would be the aspects of the data in the module that recur across multiple data types. The CLDF modules would also contain terms from the CLDF ontology. The ontology is a list of vocabulary that represents objects and properties with well-known semantics in comparative linguistics. This makes it possible for users to reference these terms in a uniform way.A software package to enable validation and manipulationThe CLDF specifications use common file formats -- such as CSV, JSON and BibTeX -- that are widely supported, with the goal that these files can easily be read and written on many platforms. Even more importantly, the standardized format will allow researchers without programming skills to access and manipulate the data with preexisting tools, rather than this ability being limited to researchers with sufficient programming skills to create their own tools. To facilitate this, the CLDF has created a ""cookbook"" repository for scripts for use with the CLDF specifications.""We want to bring access to these data and the ability to compare them to as many researchers as possible,"" states Johann-Mattis List of the Max Planck Institute for the Science of Human History. Robert Forkel, one of the driving forces behind the CLDF initiative, also notes that the CLDF format is not limited to linguistic data alone, but can also incorporate databases of cultural and geographic data, for example. ""CLDF may drastically facilitate the testing of questions regarding the interaction between linguistic, cultural, and environmental factors in linguistic and cultural evolution.""Story Source:Materials provided by Max Planck Institute for the Science of Human History. ",0.0792966,0.0973851,0.153806,0.34135,0.138808,0.134874,0.195605,0,0
86,"Sound, vibration recognition boost context-aware computing New methods help smart devices detect what's happening around them","Smart devices can seem dumb if they don't understand where they are or what people around them are doing. Carnegie Mellon University researchers say this environmental awareness can be enhanced by complementary methods for analyzing sound and vibrations.""A smart speaker sitting on a kitchen countertop cannot figure out if it is in a kitchen, let alone know what a person is doing in a kitchen,"" said Chris Harrison, assistant professor in CMU's Human-Computer Interaction Institute (HCII). ""But if these devices understood what was happening around them, they could be much more helpful.""Harrison and colleagues in the Future Interfaces Group will report today at the Association for Computing Machinery's User Interface Software and Technology Symposium in Berlin about two approaches to this problem -- one that uses the most ubiquitous of sensors, the microphone, and another that employs a modern-day version of eavesdropping technology used by the KGB in the 1950s.In the first case, the researchers have sought to develop a sound-based activity recognition system, called Ubicoustics. This system would use the existing microphones in smart speakers, smartphones and smartwatches, enabling them to recognize sounds associated with places, such as bedrooms, kitchens, workshops, entrances and offices.""The main idea here is to leverage the professional sound-effect libraries typically used in the entertainment industry,"" said Gierad Laput, a Ph.D. student in HCII. ""They are clean, properly labeled, well-segmented and diverse. Plus, we can transform and project them into hundreds of different variations, creating volumes of data perfect for training deep-learning models.""This system could be deployed to an existing device as a software update and work immediately,"" he added.The plug-and-play system could work in any environment. It could alert the user when someone knocks on the front door, for instance, or move to the next step in a recipe when it detects an activity, such as running a blender or chopping.The researchers, including Karan Ahuja, a Ph.D. student in HCII, and Mayank Goel, assistant professor in the Institute for Software Research, began with an existing model for labeling sounds and tuned it using sound effects from the professional libraries, such as kitchen appliances, power tools, hair dryers, keyboards and other context-specific sounds. They then synthetically altered the sounds to create hundreds of variations.Laput said recognizing sounds and placing them in the correct context is challenging, in part because multiple sounds are often present and can interfere with each other. In their tests, Ubicoustics had an accuracy of about 80 percent -- competitive with human accuracy, but not yet good enough to support user applications. Better microphones, higher sampling rates and different model architectures all might increase accuracy with further research.A video explaining Ubicoustics is available: https://www.youtube.com/watch?v=N5ZaBeB07u4In a separate paper, HCII Ph.D. student Yang Zhang, along with Laput and Harrison, describe what they call Vibrosight, which can detect vibrations in specific locations in a room using laser vibrometry. It is similar to the light-based devices the KGB once used to detect vibrations on reflective surfaces such as windows, allowing them to listen in on the conversations that generated the vibrations.""The cool thing about vibration is that it is a byproduct of most human activity,"" Zhang said. Running on a treadmill, pounding a hammer or typing on a keyboard all create vibrations that can be detected at a distance. ""The other cool thing is that vibrations are localized to a surface,"" he added. Unlike microphones, the vibrations of one activity don't interfere with vibrations from another. And unlike microphones and cameras, monitoring vibrations in specific locations makes this technique discreet and preserves privacy.This method does require a special sensor, a low-power laser combined with a motorized, steerable mirror. The researchers built their experimental device for about $80. Reflective tags -- the same material used to make bikes and pedestrians more visible at night -- are applied to the objects to be monitored. The sensor can be mounted in a corner of a room and can monitor vibrations for multiple objects.Zhang said the sensor can detect whether a device is on or off with 98 percent accuracy and identify the device with 92 percent accuracy, based on the object's vibration profile. It can also detect movement, such as that of a chair when someone sits in it, and it knows when someone has blocked the sensor's view of a tag, such as when someone is using a sink or an eyewash station.The Packard Foundation, Sloan Foundation and Qualcomm supported the work on Ubicoustics and Vibrosight, with additional funding from the Google Ph.D. Fellowship for Ubicoustics.Story Source:Materials provided by Carnegie Mellon University. ",0.063247,0.0796463,0.143144,0.223155,0.128302,0.115484,0.154605,0,0
87,Code of ethics doesn't influence decisions of software developers,"The world's largest computing society, the Association for Computing Machinery (ACM), updated its code of ethics in July 2018 -- but new research from North Carolina State University shows that the code of ethics does not appear to affect the decisions made by software developers.""We applauded the decision to update the ACM code of ethics, but wanted to know whether it would actually make a difference,"" says Emerson Murphy-Hill, co-author of a paper on the work and an adjunct associate professor of computer science at NC State.""This issue is timely, given the tech-related ethics scandals in the news in recent years, such as when Volkwagen manipulated its technology that monitored vehicle emissions. And developers will continue to face work-related challenges that touch on ethical issues, such as the appropriate use of artificial intelligence.""For the study, researchers developed 11 written scenarios involving ethical challenges, most of which were drawn from real-life ethical questions posted by users on the website Stack Overflow. The study included 105 U.S. software developers with five or more years of experience and 63 software engineering graduate students at a university. Half of the study participants were shown a copy of the ACM code of ethics, the other half were simply told that ethics are important as part of an introductory overview of the study. All study participants were then asked to read each scenario and state how they would respond to the scenario.""There was no significant difference in the results -- having people review the code of ethics beforehand did not appear to influence their responses,"" Murphy-Hill says.""While we believe maintaining an up-to-date, robust code of ethics is an admirable thing for ACM to do, we were unable to find any effect of the code of ethics on developer decision making. The question now becomes: What can the computing profession do to promote ethical behavior?""The paper, ""Does ACM's Code of Ethics Change Ethical Decision Making in Software Development?,"" will be presented Nov. 7 at the ACM Symposium on the Foundations of Software Engineering, being held in Lake Buena Vista, Fla. The paper was co-authored by Justin Smith, a Ph.D. student at NC State, and Andrew McNamara, a former graduate student at NC State.Story Source:Materials provided by North Carolina State University. ",0.105801,0.134224,0.194672,0.359822,0.153439,0.193054,0.232505,0,0
88,AI could help cities detect expensive water leaks,"Costly water losses in municipal water systems could be significantly reduced using sensors and new artificial intelligence (AI) technology.Developed by researchers at the University of Waterloo in collaboration with industry partners, the technology has the potential to detect even small leaks in pipes.It combines sophisticated signal processing techniques and AI software to identify telltale signs of leaks carried via sound in water pipes.The acoustic signatures are recorded by hydrophone sensors that can be easily and inexpensively installed in existing fire hydrants without excavation or taking them out of service.""This would allow cities to use their resources for maintenance and repairs much more effectively,"" said lead researcher Roya Cody, a civil engineering PhD candidate at Waterloo. ""They could be more proactive as opposed to reactive."" Municipal water systems in Canada lose an average of over 13 per cent of their clean water between treatment and delivery due to leaks, bursts and other issues. Countries with older infrastructure have even higher loss rates.Major problems such as burst pipes are revealed by pressure changes, volume fluctuations or water simply bubbling to the surface, but small leaks often go undetected for years.""In addition to the economic costs of wasting treated water, chronic leaks can create health hazards, do damage to the foundations of structures and deteriorate over time.""By catching small leaks early, we can prevent costly, destructive bursts later on,"" said Cody.Researchers are now doing field tests with the hydrant sensors after reliably detecting leaks as small as 17 litres a minute in the lab.They are also working on ways to pinpoint the location of leaks, which would allow municipalities to identify, prioritize and carry out repairs.""Right now they react to situations by sending workers out when there is flooding or to inspect a particular pipe if it's due to be checked because of its age,"" Cody said.The sensor technology works by pre-processing acoustic data using advanced signal processing techniques to highlight components associated with leaks.That makes it possible for machine learning algorithms to identify leaks by distinguishing their signs from the many other sources of noise in a water distribution system.Story Source:Materials provided by University of Waterloo. ",0.118993,0.125799,0.200578,0.222096,0.168408,0.18096,0.182568,0,0
89,AI improves doctors' ability to correctly interpret tests and diagnose lung disease,"Artificial intelligence (AI) can be an invaluable aid to help lung doctors interpret respiratory symptoms accurately and make a correct diagnosis, according to new research presented today (Wednesday) at the European Respiratory Society International Congress [1].Dr Marko Topalovic (PhD), a postdoctoral researcher at the Laboratory for Respiratory Diseases, Catholic University of Leuven (KU Leuven), Belgium, told the meeting that after training an AI computer algorithm using good quality data, it proved to be more consistent and accurate in interpreting respiratory test results and suggesting diagnoses than lung specialists.""Pulmonary function tests provide an extensive series of numerical outputs and their patterns can be hard for the human eye to perceive and recognise; however, it is easy for computers to manage large quantities of data like these and so we thought AI could be useful for pulmonologists. We explored if this was true with 120 pulmonologists from 16 hospitals. We found that diagnosis by AI was more accurate in twice as many cases as diagnosis by pulmonologists. These results show how AI can serve as a second opinion for pulmonologists when they are assessing and diagnosing their patients,"" he said.Pulmonary function tests (PFT) include: spirometry, which involves the patient breathing through a mouthpiece to measure the amount of air inhaled and exhaled; a body box or plethysmography test, which enables doctors to assess lung volume by measuring the pressure in a booth in which the patient is sitting and breathing through a mouthpiece; and a diffusion capacity test, which tests how well a patient's lungs are able to transfer oxygen and carbon dioxide to and from the bloodstream by testing the efficiency of the alveoli (small air sacks in the lungs). Results from these tests give doctors important information about the functioning of the lungs, but do not tell them what is wrong with the patient. This requires interpretation of the results in order to reach a diagnosis.In this study, the researchers used historical data from 1430 patients from 33 Belgian hospitals. The data were assessed by an expert panel of pulmonologists and interpretations were measured against gold standard guidelines from the European Respiratory Society and the American Thoracic Society. The expert panel considered patients' medical histories, results of all PFTs and any additional tests, before agreeing on the correct interpretation and diagnosis for each patient.""When training the AI algorithm, the use of good quality data is of utmost importance,"" explained Dr Topalovic. ""An expert panel examined all the results from the pulmonary function tests, and the other tests and medical information as well. They used these to reach agreement on final diagnoses that the experts were confident were correct. These were then used to develop an algorithm to train the AI, before validating it by incorporating it into real clinical practice at the University Hospital Leuven. The challenging part was making sure the algorithm recognised patterns of up to nine different diseases.""Then, 120 pulmonologists from 16 European hospitals (from Belgium, France, The Netherlands, Germany and Luxembourg) made 6000 interpretations of PFT data from 50 randomly selected patients. The AI also examined the same data. The results from both were measured against the gold standard guidelines in the same way as during development of the algorithm.The researchers found that the interpretation of the PFTs by the pulmonologists matched the guidelines in 74% of cases (with a range of 56-88%), but the AI-based software interpretations perfectly matched the guidelines (100%). The doctors were able to correctly diagnose the primary disease in 45% of cases (with a range of 24-62%), while the AI gave a correct diagnosis in 82% of cases.Dr Topalovic said: ""We found that the interpretation of pulmonary function tests and the diagnosis of respiratory disease by pulmonologists is not an easy task. It takes more information and further tests to reach a satisfactory level of accuracy. On the other hand, the AI-based software has superior performance and therefore can provide a powerful decision support tool to improve current clinical practice. Feedback from doctors is very positive, particularly as it helps them to identify difficult patterns of rare diseases.""Two large Belgian hospitals are already using the AI-based software to improve interpretations and diagnoses. ""We firmly believe that we can empower doctors to make their interpretations and diagnoses easier, faster and better. AI will not replace doctors, that is certain, because doctors are able to see a broader perspective than that presented by pulmonary function tests alone. This enables them to make decisions based on a combination of many different factors. However, it is evident that AI will augment our abilities to accomplish more and decrease chances for errors and redundant work. The AI-based software has superior performance and therefore may provide a powerful decision support tool to improve current clinical practice.""Nowadays, we trust computers to fly our planes, to drive our cars and to survey our security. We can also have confidence in computers to label medical conditions based on specific data. The beauty is that, independent of location or medical coverage, AI can provide the highest standards of PFT interpretation and patients can have the best and affordable diagnostic experience. Whether it will be widely used in future clinical applications is just a matter of time, but will be driven by the acceptance of the medical community,"" said Dr Topalovic.He said the next step would be to get more hospitals to use this technology and investigate transferring the AI technology to primary care, where the data would be captured by general practitioners (GPs) to help them make correct diagnoses and referrals.Professor Mina Gaga is President of the European Respiratory Society, and Medical Director and Head of the Respiratory Department of Athens Chest Hospital, Greece, and was not involved in the study. She said: ""This work shows the exciting possibilities that artificial intelligence offers to doctors to help them provide a better, quicker service to their patients. Over the past 20 to 30 years, the evolution in technology has led to better diagnosis and treatments: a revolution in imaging techniques, in molecular testing and in targeted treatments have make medicine easier and more effective. AI is the new addition! I think it will be invaluable in helping doctors and patients and will be an important aid to their decision-making.""",0.0644927,0.095174,0.152896,0.307935,0.118325,0.113435,0.179276,0,0
90,Tech streamlines computational science projects,"Since designing and launching a specialized workflow management system in 2010, a research team from the US Department of Energy's Oak Ridge National Laboratory has continuously updated the technology to help computational scientists develop software, visualize data and solve problems.Workflow management systems allow users to prepare, produce and analyze scientific processes to help simplify complex simulations. Known as the Eclipse Integrated Computational Environment, or ICE, this particular system incorporates a comprehensive suite of scientific computing tools designed to save time and effort expended during modeling and simulation experiments.Compiling these resources into a single platform both improves the overall user experience and expedites scientific breakthroughs. Using ICE, software developers, engineers, scientists and programmers can define problems, run simulations locally on personal computers or remotely on other systems -- even supercomputers -- and then analyze results and archive data. Recently, the team published an article in SoftwareX that both details the history of the system and previews the potential benefits of upcoming versions.""What I really love about this project is making complicated computational science automatic,"" said Jay Jay Billings, a researcher in ORNL's Computer Science and Mathematics Division who leads the ICE development team. ""Building workflow management systems and automation tools is a type of futurism, and it's challenging and rewarding to operate at the edge of what's possible.""Researchers use ICE to study topics in fields including nuclear energy, astrophysics, additive manufacturing, advanced materials, neutron science and quantum computing, answering questions such as how batteries behave and how some 3D-printed parts deform when exposed to heat.Several factors differentiate ICE from other workflow management systems. For example, because ICE exists on an open-source software framework called the Eclipse Rich Client Platform, anyone can access, download and use it. Users also can create custom combinations of reusable resources and deploy simulation environments tailored to tackle specific research challenges.""Eclipse ICE is an excellent example of how open-source software can be leveraged to accelerate science and discovery, especially in scientific computing,"" said Eclipse Foundation Executive Director Mike Milinkovich. ""The Eclipse Foundation, through its community-led Science Working Group, is fostering open-source solutions for advanced research in all areas of science.""Additionally, ICE circumvents the steep and time-consuming learning curve that usually accompanies any computational science project. Although other systems require expert knowledge of the code and computer in question, ICE enables users to immediately begin facilitating their experiments, thus helping them gather data and achieve results much faster.""We've produced a streamlined interface to computational workflows that differs from complicated systems that you have to be specifically qualified in to use properly,"" Billings said.Throughout this project, Billings has also emphasized the importance of accessibility and usability to ensure that users of all ages and experience levels, including nonscientists, can use the system without prior training.""The problem with a lot of workflow management systems and with modeling and simulation codes in general is that they are usually unusable to the lay person,"" Billings said. ""We designed ICE to be usable and accessible so anyone can pick up an existing code and use it to address pressing computational science problems.""ICE uses the programming language Java to define workflows, whereas other systems use more obscure languages. Thus, students in grade school, high school and college have successfully run codes using ICE.Finally, instead of relying on grid workflows -- collections of orchestrated computing processes -- ICE focuses on flexible modeling and simulation workflows that give users interactive control over their projects. Grid workflows are defined by strict parameters and executed without human intervention, but ICE allows users to input additional information during simulations to produce more complicated scenarios.""In ICE you can have humans in the loop, meaning the program can stop, ask questions and receive instructions before resuming activity,"" Billings said. ""This feature allows system users to complete more complex tasks like looping and conditional branching.""Next, the development team intends to combine the most practical aspects of ICE and other systems through workflow interoperability, a concept referring to the ability of two different systems to seamlessly communicate. Combining the best features of grid workflows with modeling and simulation workflows would allow scientists to address even greater challenges and solve scientific mysteries more efficiently.""If I'm using ICE and someone else is using a different system, we want to be able to address problems together with our combined resources,"" Billings said. ""With workflow interoperability, our systems would have a standard method of 'talking' to one another.""To further improve ICE's accessibility and usability, the team is also developing a cloud-based version to provide even more interactive computing services for simplifying scientific workflows.""That's what research is -- we keep figuring out the next step to understand the system better,"" Billings said.Story Source:Materials provided by DOE/Oak Ridge National Laboratory. ",0.0529344,0.0805364,0.116846,0.284148,0.0945007,0.110699,0.157649,0,0
91,"A reliable cryptocurrency needs good governance, say researchers Clear guidelines that advise stakeholders when software updates are beneficial could help stabilize cryptocurrencies","Participants in cryptocurrency networks like Bitcoin need to be better at preempting beneficial software changes. This will ensure the security and privacy of addresses and transactions, and help retain the value of cryptocurrencies, says Benjamin Trump (ORISE Fellow, United States Army Corps of Engineers). He is the lead author of a study in Springer's journal Environment Systems and Decisions, which analyzes the governance challenges of many cryptocurrencies and explains why such challenges threaten the long-term usefulness of such cryptocurrencies.Bitcoin and other cryptocurrencies could potentially revolutionize commerce and information exchange worldwide through their use of blockchain and distributed ledger technologies. However, cryptocurrencies are still highly volatile, and users lack longstanding trust in them as a reliable form of financial exchange.Although the underlying technologies driving cryptocurrencies are promising, Trump believes that the stability of cryptocurrencies is threatened by the current processes which enable software updates. These threats are known as ""hard forks"" or splits in the blockchain of a cryptocurrency. Disruption of a cryptocurrency's blockchain in this way might cause people to lose trust in it and its capacity to survive as a reliable vehicle of exchange, says Trump.In this study, Trump and his team reviewed the state of cryptocurrency ""forks"" by looking at more than 800 publicly acknowledged occurrences of so-called soft forks, source code forks or altcoins, as well as hard forks from Bitcoin. The researchers used sources such as the Map of Coins and the Bitcoin Exchange Guide.Their analysis showed a substantial growth in the number of separate blockchains stemming from initial Bitcoin software. Many of these Bitcoin forks and altcoins did not survive beyond a few months, while others such as Litecoin, Dogecoin and Vertcoin have lasted for years. Two hard forks executed in late-2017 to early-2018 opened the door to many other future forks. Experts predict that hard forks are expected to become more common, with some sources arguing that up to 50 are possible in 2018 alone.""Hard forks are a threat to maintaining a stable and predictable operating platform that is essential if cryptocurrencies are to be adopted for daily financial transactions,"" says Trump, who sees such increases in hard forks as a hurdle to mainstream adoption of selected cryptocurrencies.Trump says that if Bitcoin, in particular, is to become a more globally recognized, reliable, and a predictable medium of exchange on an international scale, cryptocurrency miners, wallet developers, exchange operators and other stakeholders within the Bitcoin network need to generate more stability through good governance.Suitable measures could include establishing metrics for key variables that can pre-emptively identify whether software changes are needed well before inflection points arise.Story Source:Materials provided by Springer. ",0.114122,0.148216,0.1892,0.28973,0.182636,0.183489,0.20347,0,0
92,"If military robot falls, it can get itself up ","Scientists at the U.S. Army Research Laboratory and the Johns Hopkins University Applied Physics Laboratory have developed software to ensure that if a robot falls, it can get itself back up, meaning future military robots will be less reliant on their Soldier handlers.Based on feedback from Soldiers at an Army training course, ARL researcher Dr. Chad Kessens began to develop software to analyze whether any given robot could get itself ""back on its feet"" from any overturned orientation.""One Soldier told me that he valued his robot so much, he got out of his vehicle to rescue the robot when he couldn't get it turned back over,"" Kessens said. ""That is a story I never want to hear again.""Researchers from Navy PMS-408 (Expeditionary Missions) and its technical arm, the Indian Head Explosive Ordnance Disposal Technology Division, agree. They teamed up with JHU/APL and the prime contractor, Northrop Grumman Remotec, to develop the Advanced Explosive Ordnance Disposal Robotic System, or AEODRS, a new family of EOD robotic systems featuring a modular opens systems architecture. A lightweight backpackable platform, which is increment one of the program, is expected to move into production later this year. One critical requirement of the program is that the robots must be capable of self-righting.""These robots exist to keep Soldiers out of harm's way,"" said Reed Young, Robotics and Autonomy Program Manager at JHU/APL. ""Self-righting is a critical capability that will only further that purpose.""To evaluate the AEODRS system's ability to self-right, JHU/APL teamed up with ARL to leverage the software Kessens developed. The team was able to extend its ability to robots with a greater number of joints (or degrees of freedom) due to JHU/APL researcher Galen Mullins' expertise in adaptive sampling techniques.""The analysis I've been working on looks at all possible geometries and orientations that the robot could find itself in,"" Kessens said. ""The problem is that each additional joint adds a dimension to the search space -- so it is important to look in the right places for stable states and transitions. Otherwise, the search could take too long.""Kessens said Mullins' work is what allowed the analysis to work efficiently for analyzing higher degree of freedom systems. While Kessens' work determines what to look for and how, Mullins figures out where to look.""""This analysis was made possible by our newly developed range adversarial planning tool, or RAPT, a software framework for testing autonomous and robotic systems,"" Mullins said. ""We originally developed the software for underwater vehicles, but when Chad explained his approach to the self-righting problem, I immediately saw how these technologies could work together.""He said the key to this software is an adaptive sampling algorithm that looks for transitions.""For this work, we were looking for states where the robot could transition from a stable configuration to an unstable one, thus causing the robot to tip over,"" Mullins explained. ""My techniques were able to effectively predict where those transitions might be so that we could search the space efficiently.""Ultimately, the team was able to evaluate the AEODRS systems' eight degrees of freedom and determined it can right itself on level ground no matter what initial state it finds itself in. The analysis also generates motion plans showing how the robot can reorient itself. The team's findings can be found in ""Evaluating Robot Self-Righting Capabilities using Adaptive Sampling,"" published in IEEE's Robotics and Automation Letters in August.Beyond the evaluation of any one specific robot, Kessens sees the analysis framework as important to the military's ability to compare robots from different vendors and select the best one for purchasing.""The Army and Navy want robots that can self-right, but we are still working to understand and evaluate what that means,"" Kessens said. ""Self-right under what conditions? We have developed a metric analysis for evaluating a robot's ability to self-right on sloped planar ground, and we could even use it as a tool for improving robot design. Our next step is to determine what a robot is capable of on uneven terrain.""Story Source:Materials provided by U.S. Army Research Laboratory. ",0.0767095,0.0758228,0.12362,0.20604,0.0964219,0.132712,0.168551,0,0
93,An avatar uses your gait to predict how many calories you will burn,"Humans instinctively adopt the gait that requires the least amount of energy given the walking conditions. Without realizing it, we are constantly tweaking our pace, stride length and foot lift. But could we consciously play with these parameters in order to influence our energy expenditure?Researchers at EPFL's Biorobotics Laboratory studied eight gait parameters in order to come up with a very sophisticated software program that uses an avatar to predict how much energy people use when they walk depending on their walking style. This research has been published in Scientific Reports. Salman Faraji, the co-lead author, devoted an entire section of his thesis to this topic.The avatar -- a torso equipped with two legs with feet -- can be freely configured. Users start by entering their height and weight and can then set the walking speed, distance between their feet (stride length and stride width), and foot lift, along with the incline of both the torso and the ground. They can also add mass and simulate the effect of being pushed or pulled at different parts of the body. The number of calories burned and the energy consumption are displayed in real time whenever the parameters are modified.Making custom exoskeletonsThis pioneering software drew on a number of experiments appearing in recent literature, and it offers a huge number of potential applications -- especially in the medical realm. ""The software could be used to select the best design for an exoskeleton or a custom prosthetic, in order to reduce the user's effort. With a wearable exoskeleton, for example, we could optimize the location of the battery and actuators, or determine the ideal walking pattern for the user's preferred speed,"" says Amy Wu, the study's other co-lead author. The software could even determine where a backpack should be worn in order to minimize energy expenditure. ""If, on the other hand, your goal is to burn calories, the software could be used to find a series of movements with a high metabolic cost.""Designed for humanoid robotsThe software was created in a robotics lab and was initially intended to study the mechanics of human gait for use in humanoid robots. ""The way humans walk is extremely complex. The level of control required is a huge challenge for humanoid robots, which often don't get it quite right,"" says Faraji. ""We have a long way to go before we really understand all the parameters that go into human, animal and robot locomotion.""An application can be downloaded in order to try out the simulator here: https://biorob.epfl.ch/research/humanoid/walkmanStory Source:Materials provided by Ecole Polytechnique Federale de Lausanne. Original written by Laure-Anne Pessina. ",0.0807092,0.0985148,0.114258,0.226011,0.110572,0.121779,0.198217,0,0
94,Better particle tracking software using artificial intelligence,"Scientists at the University of North Carolina at Chapel Hill have created a new method of particle tracking based on machine learning that is far more accurate and provides better automation than techniques currently in use.Single-particle tracking involves tracking the motion of individual particles, such as viruses, cells and drug-loaded nanoparticles, within fluids and biological samples. The technique is widely used in both physical and life sciences. The team at UNC-Chapel Hill that developed the new tracking method uses particle tracking to develop new ways to treat and prevent infectious diseases. They examine molecular interactions between antibodies and biopolymers and characterize and design nano-sized drug carriers. Their work is published in the Proceedings of the National Academy of Sciences.""In order to derive meaning from videos, you have to convert the videos into quantitative data,"" said Sam Lai, Ph.D., an associate professor in the UNC Eshelman School of Pharmacy and one of the creators of the new tracker. ""With current software, researchers must carefully supervise the video conversion to ensure accuracy. This often takes many weeks to months, and greatly limit both the throughput and accuracy.""We got tired of the bottleneck.""The root of the problem can be traced to the small number of parameters, such as particle size, brightness and shape, used by current software to identify the full range of particles present in any video. Things get missed because they don't quite fit the parameters, and the parameters vary as different operators set them, Alison Schaefer, a Ph.D. student in the Lai lab, said. This creates a tremendous challenge with data reproducibility, as two users analyzing the same video frequently obtain different results.""Self-driving cars work because they can see and keep track of many different objects around them in real time,"" said M. Gregory Forest, Ph.D., the Grant Dahlstrom Distinguished Professor in the UNC Departments of Mathematics and Applied Physical Sciences, and co-senior author on the project.""We wondered if we could create a version of that kind of artificial intelligence that could track thousands of nanoscale particles at one time and do it automatically.""As it turns out, they could and used their discovery to launch Chapel Hill-based AI Tracking Solutions, which is seeking to commercialize the new technology. The company has received a Small Business Technology Transfer award from the National Institutes of Health to commercialize the technology.Lai and his collaborators in the UNC Department of Mathematics designed an artificial neural network to go to work on their problem. Neural networks are loosely based on the human brain but learn by being fed a large number of examples. For example, if a neural network needs to recognize photos of dogs, it is shown lots of photos of dogs. It doesn't need to know what a dog looks like; it will figure that out from the common elements of the photographs. The better the examples, the better the neural network will be.The UNC team first taught the neural network tracker from a truth set of computer-generated data. They then further refined the tracker using high-quality data from past experiments conducted in Lai's lab. The result was a new tracker with thousands of well-tuned parameters that can process a highly diverse range of videos fully automatically, is at least 10 times more accurate than systems currently in use, is highly scalable, and possesses perfect reproducibility, Lai said. The team documented their achievement in the Proceedings of the National Academy of Sciences.The new system is ready just in time to support the increasing availability of powerful microscopes capable of collecting terabytes of high resolution 2D and 3D video in a single day, said Jay Newby, Ph.D., lead author of the study and an assistant professor at the University of Alberta.""Tracking the movement of nanometer-scale particles is critical for understanding how pathogens breach mucosal barriers and for the design of new drug therapies,"" Newby said. ""Our advancement provides, first and foremost, substantially improved automation. Additionally, our method greatly improves accuracy compared with current methods and reproducibility across users and laboratories.""Story Source:Materials provided by University of North Carolina at Chapel Hill - Office of Research Communication. ",0.0478526,0.0658918,0.0949212,0.196965,0.0777577,0.103606,0.12416,0,0
95,Engineers develop artificial intelligence system to detect often-missed cancer tumors,"Doctors may soon have help in the fight against cancer thanks to the University of Central Florida's Computer Vision Research Center.Engineers at the center have taught a computer how to detect tiny specks of lung cancer in CT scans, which radiologists often have a difficult time identifying. The artificial intelligence system is about 95 percent accurate, compared to 65 percent when done by human eyes, the team said.""We used the brain as a model to create our system,"" said Rodney LaLonde, a doctoral candidate and captain of UCF's hockey team. ""You know how connections between neurons in the brain strengthen during development and learn? We used that blueprint, if you will, to help our system understand how to look for patterns in the CT scans and teach itself how to find these tiny tumors.""The approach is similar to the algorithms that facial-recognition software uses. It scans thousands of faces looking for a particular pattern to find its match.Engineering Assistant Professor Ulas Bagci leads the group of researchers in the center that focuses on AI with potential medical applications.The group fed more than 1,000 CT scans -- provided by the National Institutes of Health through a collaboration with the Mayo Clinic -- into the software they developed to help the computer learn to look for the tumors.Graduate students working on the project had to teach the computer different things to help it learn properly. Naji Khosravan, who is pursuing his doctorate degree, created the backbone of the system of learning. His proficiency at novel machine learning and computer vision algorithms led to his summer as an intern at Netflix helping the company with various projects.LaLonde taught the computer how to ignore other tissue, nerves and other masses it encountered in the CT scans and analyze lung tissues. Sarfaraz Hussein who earned his doctorate degree this past summer, is fine-tuning the AI's ability to identify cancerous versus benign tumors, while graduate student Harish Ravi Parkash is taking lessons learned from this project and applying them see if another AI system can be developed to help identify or predict brain disorders.""I believe this will have a very big impact,"" Bagci said. ""Lung cancer is the number one cancer killer in the United States and if detected in late stages, the survival rate is only 17 percent. By finding ways to help identify earlier, I think we can help increase survival rates.""The team will present its finding in September at the largest premier conference for medical imaging research -- the MICCAI 2018 conference in Spain. The team's work has been published in advance of the conference.The next step is to move the research project into a hospital setting; Bagci is looking for partners to make that happen. After that, the technology could be a year or two away from the marketplace, Bagci said.""I think we all came here because we wanted to use our passion for engineering to make a difference and saving lives is a big impact,"" LaLonde said.Ravi Prakash agrees. He was studying engineering and its applications to agriculture before he heard about Bagci and his work at UCF. Bagci's research is in the area of biomedical imaging and machine learning and their applications in clinical imaging. Previously, Bagci was a staff scientist and the lab manager at the NIH's Center for Infectious Disease Imaging lab, in the department of Radiology and Imaging Sciences.Story Source:Materials provided by University of Central Florida. Original written by Zenaida Gonzalez Kotala. ",0.0744252,0.0980472,0.141593,0.302564,0.126236,0.13187,0.19335,0,0
96,Ciircuit-switching tech to help data centers recover from failures,"Anyone who has ever cursed a computer network as it slowed to a crawl will appreciate the remedy offered by scientists at Rice University.Rice computer scientist Eugene Ng and his team say their solution will keep data on the fast track when failures inevitably happen.Ng introduced ShareBackup, a strategy that would allow shared backup switches in data centers to take on network traffic within a fraction of a second after a software or hardware switch failure.He will present a peer-reviewed paper on the work this week at the SIGCOMM 2018 conference in Budapest, Hungary.Ng said the idea would solve a common annoyance among data professionals, scientists and everyone who relies on a network to deliver results day in and day out.""A data network consists of servers and network switches,"" said Ng, a professor of computer science and electrical and computer engineering. ""Switches move data packets to where they need to go. But things fail, especially in large-scale data centers with thousands of pieces of hardware.""The usual response to a failed switch is to shunt the flow of data to another line. ""Generally, the network has multiple paths for connecting servers so, just like if there's a closure on the highway, we'd drive around it. This is a conventional, natural approach that makes a lot of sense: You reroute around the failure to get where you need to go.""But sometimes that other road is congested and everything slows down. ""Data centers aren't the internet; they're not about people surfing websites,"" Ng said. ""They're about supporting data-intensive applications like data mining or machine learning. And a lot of these applications have stringent performance deadlines, so blindly rerouting traffic could be the wrong thing to do in a data center.""Rather than the expensive option of installing redundant switches throughout a network, the Ng lab's strategy would put fast switches and software in strategic locations that could pick up the traffic from a failed switch in a microsecond. When that problem is resolved, the team's software makes the backup switch available to handle another failure.The switch is fast enough -- the failure-recovery time is 0.73 milliseconds, including latency from hardware and control systems -- that most users would never know that part of the system had failed.""The reality is that the fraction of devices that fail at any given time is very small, and most of these failures can be addressed by things like rebooting the device,"" Ng said. ""Sometimes the software gets screwed up and a simple power cycle will bring it back. These failures may also not last long.""These are the characteristics we're trying to exploit,"" he said. ""Because of that, we can get away with having very few devices back up a large number of devices.""Ng said ShareBackup could save data centers time and money not only by maintaining full bandwidth but by also helping to analyze problems, including misconfigurations that commonly lead to network failure.""Part of our work is to help data centers figure out what went wrong in the network,"" he said. ""Once the backup is activated, you can take the failed device out of the production network and test it to identify which component caused the problem.""Now, if we take two devices out and can't figure out which went bad, both need to be replaced,"" he said. ""It's very likely only one of the devices is having the problem. Our software can diagnose these devices in a semiautomatic manner, and if one of the parts is good, it can be reinstated.""Story Source:Materials provided by Rice University. ",0.0809615,0.0885909,0.127542,0.272546,0.133433,0.143305,0.154151,0,0
97,Mapping the future direction for quantum research,"The way research in quantum technology will be taken forward has been laid out in a revised roadmap for the field.Published today in the New Journal of Physics, leading European quantum researchers summarise the field's current status, and examine its challenges and goals.In the roadmap:Introducing the collection of articles, Dr. Max Riedel and Professor Tommasso Calarco note: ""Within the last two decades, quantum technologies have made tremendous progress, moving from Nobel Prize-winning experiments on quantum physics into a cross-disciplinary field of applied research.""One success factor for the rapid advancement of quantum technology is a well-aligned global research community with a common understanding of the challenges and goals. In Europe, this community has profited from several EC funded coordination projects, which, among other things, have coordinated the creation of a quantum technology roadmap.""We hope this updated summary proves useful to our colleagues in the research community, as well as anyone with a broader interest in the progress of quantum technology.""Story Source:Materials provided by IOP Publishing. ",0.151767,0.175656,0.187178,0.231049,0.178715,0.213547,0.206933,0,0
98,Predicting landslide boundaries two weeks before they happen,"University of Melbourne researchers have developed a software tool that uses applied mathematics and big data analytics to predict the boundary of where a landslide will occur, two weeks in advance.Professor Antoinette Tordesillas from the School of Mathematics and Statistics said there are always warning signs in the lead up to a collapse or 'failure', the tricky part is identifying what they are.""These warnings can be subtle. Identifying them requires fundamental knowledge of failure at the microstructure level -- the movement of individual grains of earth,"" Professor Tordesillas said.""Of course, we cannot possibly see the movement of individual grains in a landslide or earthquake that stretches for kilometres, but if we can identify the properties that characterise failure in the small-scale, we can shed light on how failure evolves in time, no matter the size of the area we are observing.""These early clues include patterns of motion that change over time and become synchronised.""In the beginning, the movement is highly disordered,"" said Professor Tordesillas. ""But as we get closer to the point of failure -- the collapse of a sand castle, crack in the pavement or slip in an open pit mine -- motion becomes ordered as different locations suddenly move in similar ways.""Our model decodes this data on movement and turns it into a network, allowing us to extract the hidden patterns on motion and how they are changing in space and time. The trick is to detect the ordered motions in the network as early as possible, when movements are very subtle.""Professor Robin Batterham from the Department of Chemical and Biomolecular Engineering said the new software focuses on turning algorithms and big data into risk assessment and management actions that can save lives.""People have gone somewhat overboard on so-called data analytics, machine learning and so on,"" said Professor Batterham.""While we've been doing this sort of stuff for 40 years, this software harnesses the computer power and memory available to look not just at the surface movement, but extract the relevant data patterns. We're able to do things that were just unimaginable in a mathematical sense 30 years ago.""We can now predict when a rubbish landfill might break in a developing country, when a building will crack or the foundation will move, when a dam could break or a mudslide occur. This software could really make a difference.""Story Source:Materials provided by University of Melbourne. ",0.0757437,0.102865,0.182084,0.21188,0.116731,0.131826,0.179478,0,0
99,Password managers vulnerable to insider hacking A new study shows that communication channels between different parts and pieces of computer software are prone to security breaches.,"Researchers from Aalto University and the University of Helsinki have found over ten computer security-critical applications that are vulnerable to insider attacks. Most of the vulnerabilities were found in password managers used by millions of people to store their login credentials. Several other applications were found to be similarly susceptible to attacks and breaches across the Windows, macOS and Linux operating systems.Computer software often starts multiple processes to perform different tasks. For example, a password manager typically has two parts: a password vault and an extension to an internet browser, which both run as separate processes on the same computer.To exchange data, these processes use a mechanism called inter-process communication (IPC), which remains within the confines of the computer and does not send information to an outside network. For this reason, IPC has traditionally been considered secure. However, the software needs to protect its internal communication from other processes running on the same computer. Otherwise, malicious processes started by other users could access the data in the IPC communication channel.'Many security-critical applications, including several password managers, do not properly protect the IPC channel. This means that other users' processes running on a shared computer may access the communication channel and potentially steal users' credentials,' explains Thanh Bui, a doctoral candidate at Aalto University.While PCs are often thought to be personal, it is not uncommon that several people have access to the same machine. Large companies typically have a centralized identity and access management system that allows employees to log into any company computer. In these scenarios, it is possible for anyone in the company to launch attacks. An attacker can also log in to the computer as a guest or connect remotely, if these features are enabled.'The number of vulnerable applications shows that software developers often overlook the security problems related to inter-process communication. Developers may not understand the security properties of different IPC methods, or they place too much trust in software and applications that run locally. Both explanations are worrisome,' says Markku Antikainen, a post-doctoral researcher at the University of Helsinki.Following responsible disclosure, the researchers have reported the detected vulnerabilities to the respective vendors, which have taken steps to prevent the attacks. The research was done partly in co-operation with F-Secure, a Finnish cyber-security company.The research will be presented at the DEFCON security conference on August 12, 2018, and at the Usenix Security conference on August 17, 2018.Story Source:Materials provided by Aalto University. ",0.0718326,0.102533,0.137102,0.391181,0.151454,0.130906,0.201269,0,0
100,App that will extend your smartphone battery life,"New research out of the University of Waterloo has found a novel method to extend the battery life of smartphones for up to an hour each day.The researchers have developed an app which smartphone users can use to reduce the energy consumption of their devices without any significant impact on performance.""The built-in multi-window feature released with the new Android operating system allows users to have multiple windows and files open at the same time similar to a laptop, but this results in unnecessary energy drain,"" said Kshirasagar Naik, co-author of the study and a professor in Waterloo's faculty of Electrical and Computer Engineering.""We have developed an app which users can install on their devices and use to reduce the brightness of non-critical applications. So, when you're interacting with one application, the brightness of the other window goes down thereby reduces the energy consumption of the device.""The app was evaluated in an experiment involving 200 smartphone users, who downloaded the software on their devices and used it while they had multiple windows opened. It was found that when the energy saving technique was used it extended their battery life by 10 to 25 per cent.""What happens now is that you put the phone on a charger for the night and when you leave home the next day the battery is at 100 per cent, but there is a lot of behind the scenes computation and communication going on, and it drains the battery,"" said Naik. ""By midday charge is reduced to 30 per cent. So, you need to charge the battery many times in a day, and from the user's perspective that is a big pain.""Due to the excess energy consumption, the phone becomes warmer and warmer while the frequent charging reduces the life of the battery. So, batteries that are meant to last for three years may have to be replaced in two years.""The study, MultiDroid: An Energy Optimization Technique for Multi-Window Operations on OLED Smartphones, co-authored by Milind Kumar Rohit, Chiranjeev Kumar and Ginny all of the Indian Institute of Technology and Naik was recently published in IEEE Access, an open access journal of the Institute of Electrical and Electronics Engineers.Story Source:Materials provided by University of Waterloo. ",0.0984441,0.121698,0.140307,0.265578,0.156292,0.186597,0.187098,0,0
101,"Chips, light and coding moves the front line in beating bacteria Multidisciplinary study finds way to examine biofilms with high efficiency","The never-ending fight against bacteria has taken a turn in humanity's favor with the announcement of a tool that could give the upper hand in drug research.Bacterial resistance to antibiotics has produced alarming headlines in recent years, with the prospect of commonly prescribed treatments becoming obsolete setting off alarm bells in the medical establishment.More efficient ways of testing replacements are desperately needed, and a team from the Okinawa Institute of Science and Technology Graduate University (OIST) has just found one.In their paper, published in ACS Sensors, the scientists look at a microbial structure called biofilms -- bacterial cells that band together into a slimy matrix.These are advantageous for bacteria, even giving resistance to conventional antibiotics. With properties like these, biofilms can be hazardous when they contaminate environments and industries; everything from tainting food production to clogging sewage treatment pipes. Biofilms can also become lethal if they make their way into medical facilities.Understanding how biofilms are formed is key to finding ways to defeat them, and this study brought together OIST scientists from backgrounds in biotechnology, nanoengineering and software programming to tackle it.The team focused on biofilm assembly kinetics -- the biochemical reactions that allow bacteria to produce their linked matrix structure. Gathering intelligence on how these reactions function can tell a lot about what drugs and chemicals can be used to counteract them.No tools were available to the team that would allow them to monitor biofilm growth with the frequency they needed to have a clear understanding of it. So, they modified an existing tool to their own design.Dr. Nikhil Bhalla, working in OIST's Micro/Bio/Nanofluidics Unit led by Prof. Amy Shen took to the nanoscale to find a solution: ""We created little chips with tiny structures for E. coli to grow on,"" he said. ""They are covered in mushroom shaped nano-structures with a stem of silicon dioxide and a cap of gold.""Now all the team had to do was find some bacteria to work with. Reaching out to OIST's Structural Cellular Biology Unit, the team were helped by Dr Bill Sederstrem, who supplied stocks of E. coli on the surface of nanomushroom chips for the team to study.When these nanomushrooms are subject to a targeted beam of light, they absorb it by Localized Surface Plasmon Resonance (LSPR). By measuring the difference between light wavelengths entering and exiting the chip, the scientists could make observations of the bacteria growing around the mushroom structures without disturbing their test subjects and affecting their results.""This is the first time we have used this sensing technique to study bacterial cells,"" said Dr. Riccardo Funari, the team's resident biotechnologist, ""but the problem we found was we couldn't monitor it in real time.""Getting a constant stream of data from their LSPR setup was possible, but required a whole new set of software to make it functional. Fortunately, research technician Kang-yu Chu was on hand to lend his programming expertise to the problem.""We made an automatic measuring program with instant analysis based on existing software, which let us process the data with one click. It greatly reduced the manual work involved and let us correct any problems with the experiment as they happen,"" said Kang-yu.Now these three disciplines have combined to make a benchtop tool that can be used in virtually any laboratory, and there are plans to miniaturize the technology into a portable device that could be used in a huge array of biosensing applications.""Studies on clinically relevant microorganisms are coming next,"" said Dr. Funari, ""and we're really excited about the applications. This could be a great tool for testing future drugs on lots of different kinds of bacteria."" For now at least, humans are taking the lead in the bacterial battle.Story Source:Materials provided by Okinawa Institute of Science and Technology (OIST) Graduate University. ",0.049008,0.0853242,0.112365,0.156542,0.0813185,0.0961743,0.11904,0,0
102,Popular encryption software: Researchers help close security hole,"Cybersecurity researchers at the Georgia Institute of Technology have helped close a security vulnerability that could have allowed hackers to steal encryption keys from a popular security package by briefly listening in on unintended ""side channel"" signals from smartphones.The attack, which was reported to software developers before it was publicized, took advantage of programming that was, ironically, designed to provide better security. The attack used intercepted electromagnetic signals from the phones that could have been analyzed using a small portable device costing less than a thousand dollars. Unlike earlier intercept attempts that required analyzing many logins, the ""One & Done"" attack was carried out by eavesdropping on just one decryption cycle.""This is something that could be done at an airport to steal people's information without arousing suspicion and makes the so-called 'coffee shop attack' much more realistic,"" said Milos Prvulovic, associate chair of Georgia Tech's School of Computer Science. ""The designers of encryption software now have another issue that they need to take into account because continuous snooping over long periods of time would no longer be required to steal this information.""The side channel attack is believed to be the first to retrieve the secret exponent of an encryption key in a modern version of OpenSSL without relying on the cache organization and/or timing. OpenSSL is a popular encryption program used for secure interactions on websites and for signature authentication. The attack showed that a single recording of a cryptography key trace was sufficient to break 2048 bits of a private RSA key.Results of the research, which was supported in part by the National Science Foundation, the Defense Advanced Research Projects Agency (DARPA), and the Air Force Research Laboratory (AFRL) will be presented at the 27th USENIX Security Symposium August 16th in Baltimore.After successfully attacking the phones and an embedded system board -- which all used ARM processors -- the researchers proposed a fix for the vulnerability, which was adopted in versions of the software made available in May.Side channel attacks extract sensitive information from signals created by electronic activity within computing devices during normal operation. The signals include electromagnetic emanations created by current flows within the devices computational and power-delivery circuitry, variation in power consumption, and also sound, temperature and chassis potential variation. These emanations are very different from communications signals the devices are designed to produce.In their demonstration, Prvulovic and collaborator Alenka Zajic listened in on two different Android phones using probes located near, but not touching the devices. In a real attack, signals could be received from phones or other mobile devices by antennas located beneath tables or hidden in nearby furniture.The ""One & Done"" attack analyzed signals in a relatively narrow (40 MHz wide) band around the phones' processor clock frequencies, which are close to 1 GHz (1,000 MHz). The researchers took advantage of a uniformity in programming that had been designed to overcome earlier vulnerabilities involving variations in how the programs operate.""Any variation is essentially leaking information about what the program is doing, but the constancy allowed us to pinpoint where we needed to look,"" said Prvulovic. ""Once we got the attack to work, we were able to suggest a fix for it fairly quickly. Programmers need to understand that portions of the code that are working on secret bits need to be written in a very particular way to avoid having them leak.""The researchers are now looking at other software that may have similar vulnerabilities, and expect to develop a program that would allow automated analysis of security vulnerabilities.""Our goal is to automate this process so it can be used on any code,"" said Zajic, an associate professor in Georgia Tech's School of Electrical and Computer Engineering. ""We'd like to be able to identify portions of code that could be leaky and require a fix. Right now, finding these portions requires considerable expertise and manual examination.""Side channel attacks are still relatively rare, but Prvulovic says the success of ""One & Done"" demonstrates an unexpected vulnerability. The availability of low-cost signal processing devices small enough to use in coffee shops or airports could make the attacks more practical.""We now have relatively cheap and compact devices -- smaller than a USB drive -- that are capable of analyzing these signals,"" said Prvulovic. ""Ten years ago, the analysis of this signal would have taken days. Now it takes just seconds, and can be done anywhere -- not just in a lab setting.""Producers of mobile devices are becoming more aware of the need to protect electromagnetic signals of phones, tablets and laptops from interception by shielding their side channel emissions. Improving the software running on the devices is also important, but Prvulovic suggests that users of mobile devices must also play a security role.""This is something that needs to be addressed at all levels,"" he said. ""A combination of factors -- better hardware, better software and cautious computer hygiene -- make you safer. You should not be paranoid about using your devices in public locations, but you should be cautious about accessing banking systems or plugging your device into unprotected USB chargers.""In addition to those already mentioned, the research involved Monjur M. Alam, Haider A. Khan, Moutmita Dey, Nishith Sinha and Robert Callen, all of Georgia Tech.This work has been supported, in part, by the National Science Foundation under grant 1563991 and by the Air Force Research Laboratory and DARPA LADS under contract FA8650-16-C-7620. The views and findings in this paper are those of the authors and do not necessarily reflect the official views of NSF, DARPA or the AFRL.Story Source:Materials provided by Georgia Institute of Technology. Original written by John Toon. ",0.0453912,0.0510102,0.082823,0.191236,0.103851,0.0773698,0.1017,0,0
103,Mathematics meets biology to uncover unexpected biorhythms,"A novel mathematical approach has uncovered that some animal cells have robust 12-hour cycles of genetic activity, in addition to circadian or 24-hour cycles. The method, published in the journal PLOS ONE, assessed the periodicity of gene expression data and compared the results with those obtained with other computational methods. As opposed to the other methods, this novel approach showed not only the existence of unsuspected biological cycles, but also that the 12-hour cycles work independently from the 24-hour cycles, which has been confirmed by laboratory experiments. These findings open a new area of study of how gene functions over time influence health and disease.""Circadian rhythms are physical, mental and behavioral changes that follow a 24-hour cycle driven by environmental light and darkness. One of the best known circadian cycles is sleeping at night and being awake during the day,"" said corresponding author Dr. Clifford C. Dacso, professor of molecular and cellular biology and member of the Dan L Duncan Comprehensive Cancer Center at Baylor College of Medicine. ""These biological rhythms reflect complex interactions at the molecular level that occur among the paths mediating the expression of genes into active proteins carrying functions in the cell.""There is evidence, however, that other biological cycles exist in addition to 24-hour rhythms. Blood pressure, body temperature, cognitive performance, some circulating hormones, reaction to stress and responses to drug therapy, for instance, appear to follow a 12-hour rhythm, but little is known about the biological basis of it. Dacso and his colleagues set out to find out answers.""We decided that we needed to apply an unbiased mathematical approach different from those other groups were using,"" Dacso said. He contacted colleagues at the Department of Electrical and Computer Engineering at Rice University and they referred him to Dr. Athanasios Antoulas, a professor in that department who also is a fellow at the Max-Planck Society in Germany.Antoulas is a mathematician and an engineer with expertise in processing digital signals such as electronic waves. He had developed a new mathematical tool called the matrix pencil method that allowed him to uncover frequencies from very noisy digital data.""Other mathematical methods approached this type of problem by asking, does a specific wave form exist in the data? They were already biased to find a particular type of wave,"" said Antoulas, who is the first author of the paper as well as adjunct professor of molecular and cellular biology at Baylor. ""On the other hand, the method that we proposed asked an unbiased question; what type of wave is present in the data, if any?""When math meets biologyThe researchers applied the mathematical method Antoulas used to analyze electronic digital signals to analyze a biological phenomenon, specifically gene expression data that had been collected every hour for 36 hours. The researchers analyzed more than 18,000 mouse liver genes involved in a variety of cellular processes, including metabolism, cell stress, cell cycle and cellular respiration.""Our method revealed the fundamental cycles present in each data set collected for each gene,"" Antoulas said. ""We confirmed the 24-hour circadian cycles and uncovered genes whose expression over time followed a 12-hour cycle that was not evident when using other computational methods.""""We took a closer look at the 12- and 24-hour cycles,"" Dacso said. ""The matrix pencil method revealed that these cycles were independent, and this has been confirmed by laboratory experiments showing that knocking down genes that follow a 24-hour cycle does not affect the expression pattern of the 12-hour genes.""""By looking at the function of genes over time, as opposed to looking at a single moment, we have uncovered that fundamental cell functions, such as inflammation, stress response, protein quality control and energy supply, follow certain cycles,"" Dacso said. ""This finding has enormous implications for redefining aspects of human health as controlled by genes.""""We and others have shown that disturbing the 24-hour clocks may lead to diseases of metabolism,"" said co-author Dr. Bert O'Malley, chancellor and professor of molecular and cellular biology and Thomas C. Thompson Chair in Cell Biology at Baylor College of Medicine. ""For instance, experimental evidence shows that night-shift workers who periodically change their night and day shifts or people who travel overseas often alter their sleep cycles, and this seems to make them prone to gain weight and develop diabetes and other alterations of metabolism that may lead to disease. It's not a good idea to disturb the circadian rhythm on a regular basis. We anticipate that disturbing the other cycles may also affect health and disease.""Story Source:Materials provided by Baylor College of Medicine. ",0.0775672,0.127329,0.130152,0.204436,0.121372,0.152269,0.165559,0,0
104,Mathematicians solve age-old spaghetti mystery,"If you happen to have a box of spaghetti in your pantry, try this experiment: Pull out a single spaghetti stick and hold it at both ends. Now bend it until it breaks. How many fragments did you make? If the answer is three or more, pull out another stick and try again. Can you break the noodle in two? If not, you're in very good company.The spaghetti challenge has flummoxed even the likes of famed physicist Richard Feynman '39, who once spent a good portion of an evening breaking pasta and looking for a theoretical explanation for why the sticks refused to snap in two.Feynman's kitchen experiment remained unresolved until 2005, when physicists from France pieced together a theory to describe the forces at work when spaghetti -- and any long, thin rod -- is bent. They found that when a stick is bent evenly from both ends, it will break near the center, where it is most curved. This initial break triggers a ""snap-back"" effect and a bending wave, or vibration, that further fractures the stick. Their theory, which won the 2006 Ig Nobel Prize, seemed to solve Feynman's puzzle. But a question remained: Could spaghetti ever be coerced to break in two?The answer, according to a new MIT study, is yes -- with a twist. In a paper published this week in the Proceedings of the National Academy of Sciences, researchers report that they have found a way to break spaghetti in two, by both bending and twisting the dry noodles. They carried out experiments with hundreds of spaghetti sticks, bending and twisting them with an apparatus they built specifically for the task. The team found that if a stick is twisted past a certain critical degree, then slowly bent in half, it will, against all odds, break in two.The researchers say the results may have applications beyond culinary curiosities, such as enhancing the understanding of crack formation and how to control fractures in other rod-like materials such as multifiber structures, engineered nanotubes, or even microtubules in cells.""It will be interesting to see whether and how twist could similarly be used to control the fracture dynamics of two-dimensional and three-dimensional materials,"" says co-author Jern Dunkel, associate professor of physical applied mathematics at MIT. ""In any case, this has been a fun interdisciplinary project started and carried out by two brilliant and persistent students -- who probably don't want to see, break, or eat spaghetti for a while.""The two students are Ronald Heisser '16, now a graduate student at Cornell University, and Vishal Patil, a mathematics graduate student in Dunkel's group at MIT. Their co-authors are Norbert Stoop, instructor of mathematics at MIT, and Emmanuel Villermaux of Universite Aix Marseille.A deep dish diveHeisser, together with project partner Edgar Gridello, originally took up the challenge of breaking spaghetti in the spring of 2015, as a final project for 18.354 (Nonlinear Dynamics: Continuum Systems), a course taught by Dunkel. They had read about Feynman's kitchen experiment, and wondered whether spaghetti could somehow be broken in two and whether this split could be controlled.""They did some manual tests, tried various things, and came up with an idea that when he twisted the spaghetti really hard and brought the ends together, it seemed to work and it broke into two pieces,"" Dunkel says. ""But you have to twist really strongly. And Ronald wanted to investigate more deeply.""So Heisser built a mechanical fracture device to controllably twist and bend sticks of spaghetti. Two clamps on either end of the device hold a stick of spaghetti in place. A clamp at one end can be rotated to twist the dry noodle by various degrees, while the other clamp slides toward the twisting clamp to bring the two ends of the spaghetti together, bending the stick.Heisser and Patil used the device to bend and twist hundreds of spaghetti sticks, and recorded the entire fragmentation process with a camera, at up to a million frames per second. In the end, they found that by first twisting the spaghetti at almost 360 degrees, then slowly bringing the two clamps together to bend it, the stick snapped exactly in two. The findings were consistent across two types of spaghetti: Barilla No. 5 and Barilla No. 7, which have slightly different diameters.Noodle twistIn parallel, Patil began to develop a mathematical model to explain how twisting can snap a stick in two. To do this, he generalized previous work by the French scientists Basile Audoly and Sebastien Neukirch, who developed the original theory to describe the ""snap-back effect,"" in which a secondary wave caused by a stick's initial break creates additional fractures, causing spaghetti to mostly snap in three or more fragments.Patil adapted this theory by adding the element of twisting, and looked at how twist should affect any forces and waves propagating through a stick as it is bent. From his model, he found that, if a 10-inch-long spaghetti stick is first twisted by about 270 degrees and then bent, it will snap in two, mainly due to two effects. The snap-back, in which the stick will spring back in the opposite direction from which it was bent, is weakened in the presence of twist. And, the twist-back, where the stick will essentially unwind to its original straightened configuration, releases energy from the rod, preventing additional fractures.""Once it breaks, you still have a snap-back because the rod wants to be straight,"" Dunkel explains. ""But it also doesn't want to be twisted.""Just as the snap-back will create a bending wave, in which the stick will wobble back and forth, the unwinding generates a ""twist wave,"" where the stick essentially corkscrews back and forth until it comes to rest. The twist wave travels faster than the bending wave, dissipating energy so that additional critical stress accumulations, which might cause subsequent fractures, do not occur.""That's why you never get this second break when you twist hard enough,"" Dunkel says.The team found that the theoretical predictions of when a thin stick would snap in two pieces, versus three or four, matched with their experimental observations.""Taken together, our experiments and theoretical results advance the general understanding of how twist affects fracture cascades,"" Dunkel says.For now, he says the model is successful at predicting how twisting and bending will break long, thin, cylindrical rods such as spaghetti. As for other pasta types?""Linguini is different because it's more like a ribbon,"" Dunkel says. ""The way the model is constructed it applies to perfectly cylindrical rods. Although spaghetti isn't perfect, the theory captures its fracture behavior pretty well,""Story Source:Materials provided by Massachusetts Institute of Technology. Original written by Jennifer Chu. ",0.130547,0.129027,0.184609,0.207995,0.152518,0.186076,0.182673,0,0
105,New geometric shape used by nature to pack cells efficiently,"As an embryo develops, tissues bend into complex three-dimensional shapes that lead to organs. Epithelial cells are the building blocks of this process forming, for example, the outer layer of skin. They also line the blood vessels and organs of all animals.These cells pack together tightly. To accommodate the curving that occurs during embryonic development, it has been assumed that epithelial cells adopt either columnar or bottle-like shapes.However, a group of scientists dug deeper into this phenomenon and discovered a new geometric shape in the process.They uncovered that, during tissue bending, epithelial cells adopt a previously undescribed shape that enables the cells to minimize energy use and maximize packing stability. The team's results will be published in Nature Communications in a paper called ""Scutoids are a geometrical solution to three-dimensional packing of epithelia.""The study is the result of a United States-European Union collaboration between the teams of Luis M. Escudero (Seville University, Spain) and that of Javier Buceta (Lehigh University, USA). Pedro Gomez-Galvez and Pablo Vicente-Munuera are the first authors of this work that also includes scientists from the Andalucian Center of Developmental Biology, and the Severo Ochoa Center of Molecular Biology, among others.Buceta and colleagues first made the discovery through computational modeling that utilized Voronoi diagramming, a tool used in a number of fields to understand geometrical organization.""During the modeling process, the results we saw were weird,"" says Buceta. ""Our model predicted that as the curvature of the tissue increases, columns and bottle-shapes were not the only shapes that cells may developed. To our surprise the additional shape didn't even have a name in math! One does not normally have the opportunity to name a new shape.""The group has named the new shape the ""scutoid,"" for its resemblance to the scutellum -- the posterior part of an insect thorax or midsection.To verify the model's predictions, the group investigated the three-dimensional packing of different tissues in different animals . The experimental data confirmed that epithelial cells adopted shapes and three-dimensional packing motifs similar to the ones predicted by the computational model.Using biophysical approaches, the team argues that the scutoids stabilize the three-dimensional packing and make it energetically efficient. As Buceta puts it: ""We have unlocked nature's solution to achieving efficient epithelial bending.""Their findings could pave the way to understanding the three-dimensional organization of epithelial organs and lead to advancements in tissue engineering.""In addition to this fundamental aspect of morphogenesis,"" they write, ""the ability to engineer tissues and organs in the future critically relies on the ability to understand, and then control, the 3D organization of cells.""Adds Buceta: ""For example, if you are looking to grow artificial organs, this discovery could help you build a scaffold to encourage this kind of cell packing, accurately mimicking nature's way to efficiently develop tissues.""Story Source:Materials provided by Lehigh University. ",0.0982711,0.133249,0.149139,0.183117,0.135204,0.14514,0.171882,0,0
106,New method automatically computes realistic movement with friction from 3D design,"Simulating any 3D surface or structure -- from tree leaves and garments to pages of a book -- is a computationally challenging, time-consuming task. While various geometric tools are available to mimic the shape modeling of these surfaces, a new method is making it possible to also compute and enable the physics -- movement and distortion -- of the surface and does so intuitively and with realistic results.Researchers from Inria, the French National Institute for computer science and applied mathematics, have developed a novel algorithm that computes the shape of the surface at rest, that is, without any external force, and when this shape is deformed under gravity, contact and friction, it precisely matches the shape the user has designed.""Imagine you want to create a fancy garment on a 3D character. With our method, you can freely design this garment directly in 3D, around the character. You don't have to care about physics, but just about the shape, including the folds and wrinkles, that you would like to see at the final stage,"" explains Florence Bertails-Descoubes, scientific supervisor of the work and a researcher at INRIA. ""Once you've completed the modeling, our algorithm automatically converts the geometric cloth into a physical one.""Users draw or design any 3D surface utilizing their preferred geometric tools and can then turn to the new computational method to convert the surface into a physical object, and one that may or may not make contact with other surfaces. Bertails-Descoubes collaborated on the work with her PhD students Mickael Ly and Romain Casati and Inria colleagues Melina Skouras and Laurence Boissieux, and the team will present at SIGGRAPH Asia 2018 in Tokyo 4 December to 7 December. The annual conference features the most respected technical and creative members in the field of computer graphics and interactive techniques, and showcases leading edge research in science, art, gaming and animation, among other sectors.Many geometric tools exist to perform accurate modeling of shapes with flexibility given to the user. Given the example of modeling clothing around a 3D character, the researchers' method provides a simpler way for the clothing to mimic movement on the animated character, automatically computing for gravity and frictional contact with an external body.""For instance, if a user draws a 3D skirt on an animated character, our method will automatically shrink the rest shape and tighten it at the waist, to compensate for gravity which 'wants' to pull the item downwards,"" notes Bertails-Descoubes. The team's method also enables users to change the physical properties of the garment designed, i.e. making it out of linen instead of cotton. In turn, the cloth will behave differently, appearing softer, for instance, for lightweight cotton and will appear to have less friction with the body.The scientists note that ""the major difficulty in this kind of inverse problem stems from the fact that it is highly nonlinear. This complexity is particularly exacerbated by the presence of contact and dry friction, which was never explicitly accounted for in previous studies. It is thus challenging to design a robust algorithm able to find a valid rest shape for a large variety of different scenarios.""The researchers provided several examples in the paper, showcasing their algorithm's performance on 3D animated designs. Included in the paper are two hat examples -- noted as 'floppy hat' and 'beret' -- which are posed on a human head through contact and friction. Without the researchers' inversion method, the floppy hat sags, completely losing its original style and partly covering the face of the animated character. In contrast, after running the new algorithm, the hat preserves its original style and realistically flip-flops with the movement of the character. The beret example produced similar realistic results after applying the team's method -- the beret correctly remained inflated and posed on the head. When 'wind' is applied to the design, the beret slides with the movement but does not fall completely off the head, exemplifying the algorithm's ability to realistically simulate the physics involved.In future work, the team will focus on making their algorithm work faster and be adapted to the creation of real garment patterns.Story Source:Materials provided by Association for Computing Machinery. ",0.0612475,0.0708471,0.129494,0.208718,0.0869576,0.117714,0.142133,0,0
107,Single-cell asymmetries control how groups of cells form 3D shapes together,"Scientists have developed a mathematical model showing that two types of cellular asymmetry, or 'polarity', govern the shaping of cells into sheets and tubes, according to an article in eLife.The research is a major advance in understanding the processes that allow a single cell to develop into an entire organism, and could help understand what happens when cells gain or lose their polarity in diseases such as cancer.Multicellular organisms can develop highly complex structures that make up their tissues and organs and are able to regenerate perfect reproductions of these structures after injury. This requires the unfolding of sheets formed by groups of dividing and interacting cells. Although much is understood about some of the intermediate steps that occur during development and repair, we still do not know how thousands of cells together work out what shapes they need to form.There are two types of polarity known to influence how cells organise themselves into tissues and they are oriented at right angles to one another. One is apical-basal polarity, which marks the inside-outside part of our skin, and the other is planar cell polarity, which is responsible for the direction of the hairs on our skin.""In this study, we wanted to see how cells organise into folded sheets and tubes, and how this process can be so precisely reproduced,"" says lead author Silas Boye Nissen, PhD student at the Center for Stem Cell Decision Making (StemPhys), University of Copenhagen, Denmark. ""To answer this question, we built a mathematical tool that can model these two types of cell polarities and simulated how many cells organise themselves into folded sheets and organs.""They found that by altering just one of the two polarities in the model, they were able to simulate a rich diversity of shapes. The differences in the shapes were dictated by two factors: the initial arrangement of the cells, and external boundaries -- such as the shape of an egg influencing the development of the embryo inside.By exploring a multitude of theoretical scenarios in which the polarities were altered, the model was able to narrow down theories to test experimentally. For example, in pancreatic organoids -- miniaturised versions of organs grown in the lab -- the team could predict that rapid, off-balance growth of cells will cause the growing organoid to develop lots of shallow folds, but that the deeper external pressure caused by the medium the organoids grow in will cause fewer, deeper and longer folds.""Our findings advance our understanding of how properties of individual cells lead to differences in shapes formed by thousands of cells,"" says co-senior author Professor Kim Sneppen, Director of the Center for Models of Life (CMOL), University of Copenhagen.Co-senior author Ala Trusina, Associate Professor at StemPhys and CMOL, concludes: ""This work suggests that body parts may not need detailed instructions to form, but can instead emerge as cells follow a few simple rules. We can now explore what happens if cells gain or lose their polarities at the wrong time and place, as often happens in cancer.""Story Source:Materials provided by eLife. ",0.0896218,0.164764,0.155338,0.192479,0.143981,0.13417,0.174532,0,0
108,New definition returns meaning to information,"A fish on the Great Barrier Reef continually acquires new information from its environment -- the location of food, the murkiness of the water, and the sounds of distant ships, to name a few examples. But only some of that information is meaningful, in that it actually helps the fish survive. In various disciplines, from biology to artificial intelligence, identifying such meaningful, or ""semantic,"" information is a key challenge. Yet a broadly applicable, fully formal definition of this kind of information has never been developed.A new paper by the Santa Fe Institute's Artemy Kolchinsky, a postdoctoral fellow specializing in information theory, and professor David Wolpert, a mathematician and physicist, proposes one. Taking cues from statistical physics and information theory, they've come up with a definition that emphasizes how a particular piece of information contributes to the ability of a physical system to perpetuate itself -- which in the context of common biological organisms means its ability to survive. Semantic information, they write, is ""the information that a physical system has about its environment that is causally necessary for the system to maintain its own existence over time.""For example, the location of food is semantic information to the Great Barrier Reef fish because it's essential for the fish's survival. But the sound of a distant ship does not contribute to the fish's viability, so it does not qualify as semantic information.Kolchinsky and Wolpert hope that this new, formal definition of semantic information can help researchers sort the wheat from the chaff when trying to make sense of the information a physical system has about its environment.""Some information can be extraordinarily meaningful to an organism, while other information can have no meaning,"" Wolpert says. ""While it seems obvious that this distinction is crucial for analyzing biological organisms, it has never been formalized. Moreover, to avoid the fraught issue of defining a 'biological organism,' we wanted our definition of meaningful information to be applicable to both living and non-living physical systems, such as rocks and hurricanes.""The researchers' definition fills a hole in information theory left by Claude Shannon, who intentionally omitted the issue of the ""meaning"" of information in his iconic paper that created the field, ""A Mathematical Theory of Communication,"" in 1948.In the realm of biology, understanding the role of semantic information could help answer some of the discipline's most intriguing questions, such as how the earliest life forms evolved, or how existing ones adapt, says Kolchinsky. ""When we talk about fitness and adaptation, does semantic information increase over evolutionary time? Do organisms get better at picking up information that's meaningful to them?""Story Source:Materials provided by Santa Fe Institute. ",0.121214,0.162048,0.208688,0.282908,0.178458,0.188108,0.222385,0,0
109,"How does brain structure influence performance on language tasks? Computational modeling shows promise as a tool for probing this question, a study finds","The architecture of each person's brain is unique, and differences may influence how quickly people can complete various cognitive tasks.But how neuroanatomy impacts performance is largely an open question.To learn more, scientists are developing a new tool -- computational models of the brain -- to simulate how the structure of the brain may impact brain activity and, ultimately, human behavior. The research focuses on interconnectivity within the brain, looking at how different regions are linked to and interact with one another (traits that vary between individuals).In an initial proof-of-concept study, a team led by University at Buffalo mathematician Sarah Muldoon finds that this approach shows promise for understanding the interplay between brain structure and performance on language-related tasks. The research was published in PLOS Computational Biology on Oct. 17.""We are creating these personalized brain network models to understand what the brain is doing, based on how connected different regions of a person's brain are to one another,"" says first author Kanika Bansal, a postdoctoral researcher jointly working at UB, the U.S. Army Research Laboratory (ARL) and Columbia University.""Models like this are powerful tools because they allow us to conduct 'in silico' experiments to understand brain function on a personal basis,"" adds Muldoon, PhD, assistant professor of mathematics in the UB College of Arts and Sciences and a faculty member in UB's Computational and Data-Enabled Science and Engineering and neuroscience programs.The power of a personalized brain modelIn the new study, researchers created data-driven mathematical models of the individual brains of 10 people based on diffusion spectrum images which capture the structural wiring of the subjects' actual brains.Scientists then used the models to learn about each person's brain, including:The research identified some relationships between these characteristics of the brain and how quickly people were able to carry out three language-demanding tasks: saying the first verb that came to mind when presented with a noun; filling in a missing word in a sentence; and reading a large number. (Each participant completed each activity multiple times before and after receiving transcranial magnetic stimulation to the left inferior frontal gyrus.)While small, the study demonstrates the potential of data-driven modeling for learning about the link between brain structure and task performance.Possible applications in treating disease, enhancing performanceDeveloping personalized models of brain activity could not only improve neuroscience research, but also spur advancements in using brain stimulation to treat disease or enhancing human performance on various tasks.""It's important to create biologically inspired ways to predict individual responses to brain stimulation,"" says co-author John Medaglia, PhD, assistant professor of psychology at Drexel University and adjunct assistant professor of neurology at the University of Pennsylvania. ""The attractive idea here is that we can examine complex activity in each person's brain networks. Then, we can define relatively simple measures that are strongly related to real-world performance. This balance between simulating complex processes and making simple predictions is necessary to drive brain stimulation research forward.""""The Army has a challenge of creating Soldier systems at large scale for the thousands of military who protect our civilians around the world,"" says co-author Jean Vettel, PhD, a senior science lead at ARL who is also affiliated with the University of Pennsylvania and University of California, Santa Barbara, ""and this means that systems have to be designed for the most general skills across Soldiers. As exemplified in this work, ARL research strives to find new approaches to robustly quantify differences among Soldiers in a way that would allow development of individualized systems to capitalize on the unique expertise of each Soldier.""In both medical treatment and task performance, understanding individuals' brains -- as opposed to the human brain in general -- could have benefits, the authors say. This is because variations in the architecture and function of the brain may influence how the organ responds to neurostimulation, leading to different results for different people.Danielle Bassett, Eduardo D. Glandt Faculty Fellow and associate professor of bioengineering and of electrical and systems engineering at the University of Pennsylvania, contributed to the study. She is also affiliated with the University of Pennsylvania departments of neurology, and physics and astronomy.The research was funded by mission funding to ARL, with certain authors supported by the MacArthur Foundation, Alfred P. Sloan Foundation, and National Institute of Mental Health.Story Source:Materials provided by University at Buffalo. Original written by Charlotte Hsu. ",0.0620897,0.0946177,0.156698,0.311211,0.111674,0.123124,0.179412,0,0
110,Why don't we understand statistics? Fixed mindsets may be to blame,"Unfavorable methods of teaching statistics in schools and universities may be to blame for people ignoring simple solutions to statistical problems, making them hard to solve. This can have serious consequences when applied to professional settings like court cases. Published in Frontiers in Psychology, the study shows for the first time that fixed mindsets -- potentially triggered by suboptimal education curricula -- lead to difficulties finding the simple solution to statistical problems.We are faced with probabilities and statistics on a daily basis. These are most commonly presented as percentages (i.e. 10% of the population), but a more intuitive way of understanding this information -- called natural frequencies -- is to present it as two whole numbers (i.e. 1 in 10 people).Does this remind you of math problems you had to try solving in school? You're not alone.""Even though natural frequencies are much easier to understand, people are more familiar with probabilities represented by percentages because of their education,"" says Patrick Weber of the University of Regensburg, Germany, who led the study with colleagues Karin Binder and Stefan Krauss.However, although people are more familiar with probabilities, it does not mean they are any better at understanding them.""A recent meta-analysis showed the vast majority of people have difficulties solving a task presented in probability format,"" says Weber. ""This can result in severe misjudgments when applied in professional settings.""Weber refers to a famous example of the misuse of statistics in court when the prosecution relied heavily on flawed statistical evidence presented by a medical professional. An insufficient understanding of statistical probability led to Sally Clark being wrongly convicted of the murder of her two sons, based on the misjudgment of the probability that they could have died from natural causes.The researchers believe that people are 'blind' to probabilities -- yet have a fear of changing them into simpler natural frequencies which would make them easier to understand.""The same meta-analysis showed that when the task was presented in natural frequency format instead of probabilities, performance rates increased from 4% to 24%,"" says Weber. (See below for an example task.)But while the success rate was much higher when the data was presented as two whole numbers rather than a percentage, around three-quarters of participants still could not solve the task at all. Weber and his colleagues were keen to find out why.They gave groups of university students different reasoning tasks, one presented in probability format and the other in natural frequency. Participants were asked to show their working so the researchers could understand their cognitive processes behind answering the questions.They found that, when the questions were presented in natural frequencies, half the participants did not use natural frequencies to solve the problems, but instead 'translated' them into the more difficult probability format.Weber and his team believe that a fixed mindset -- known as the Einstellung effect -- may explain participants' preference to change the data.""Students are a lot more familiar with probabilities than with natural frequencies due to their education. In high school and university contexts, natural frequencies are not considered as equally mathematically valid as probabilities,"" says Weber.""This means that working with probabilities is a well-established strategy when it comes to solving statistical problems,"" Weber continues. ""While in many situations students profit from such an established strategy, the mental sets developed over a long period of time during school and university can make them 'blind' to simpler solutions -- or unable to find a solution at all.""Weber and his team believe this is a widespread problem deeply rooted in school and university curricula all over the world. They do, however, recognize their study only consisted of university students which may produce different results from the general population.""We assume that while overall solution rates might vary, the tendency to avoid using natural frequencies is widespread across the whole population,"" says Weber.The researchers hope their new insights -- published in a research collection on judgment and decision making under uncertainty -- will encourage global change to statistical teaching strategies in schools and universities.""We want our findings to encourage curriculum designers to incorporate natural frequencies systematically into school mathematics and statistics. This would give students a helpful tool to understand the concept of uncertainty -- in addition to the 'standard' probabilities.""Example of a problem posed in probability and natural frequency formatProbability format: The probability of being addicted to heroin is 0.01% for a person randomly picked from a population (base rate). If a randomly picked person from this population is addicted to heroin, the probability is 100% that he or she will have fresh needle pricks (sensitivity). If a randomly picked person from this population is not addicted to heroin, the probability is 0.19% that he or she will still have fresh needle pricks (false alarm rate). What is the probability that a randomly picked person with fresh needle pricks is addicted to heroin (posterior probability)?Solution: With the help of Bayes' theorem, the corresponding posterior probability P(H|N), with H denoting ""person is addicted to heroin"" and N denoting ""person has fresh needle pricks,"" can be calculated:P(H|N) = (P(N|H) x P(H)) / (P(N|H) x P(H) + P(N|eH) x P(eH)) = (100% x 0.01%) / (100% x 0.01% + 0.19% x 99.99%) = 5%Natural frequencies format: 10 out of 100,000 people from a given population are addicted to heroin. 10 out of 10 people who are addicted to heroin will have fresh needle pricks. 190 out of 99,990 people who are not addicted to heroin will nevertheless have fresh needle pricks. What percentage of the people with fresh needle pricks is addicted to heroin?Solution:Number of heroin addicts: 10Number of people with needle pricks: All the heroin addicts + 190 non-addicts = 200Percentage of people with needle pricks who are addicts = 10/200 = 5%Story Source:Materials provided by Frontiers. ",0.0790433,0.113848,0.142155,0.261967,0.137888,0.134848,0.179628,0,0
111,Songbird data yields new theory for learning sensorimotor skills Mathematical models describes distribution of sensory errors,"Songbirds learn to sing in a way similar to how humans learn to speak -- by listening to their fathers and trying to duplicate the sounds. The bird's brain sends commands to the vocal muscles to sing what it hears, and then the brain keeps trying to adjust the command until the sound echoes the one made by the parent.During such trial-and-error processes of sensorimotor learning, a bird remembers not just the best possible command, but a whole suite of possibilities, suggests a study by scientists at Emory University. The Proceedings of the National Academy of the Sciences (PNAS) published the study results, which include a new mathematical model for the distribution of sensory errors in learning.""Our findings suggest that an animal knows that even the perfect neural command is not going to result in the right outcome every time,"" says Ilya Nemenman, an Emory professor of biophysics and senior author of the paper. ""Animals, including humans, want to explore and keep track of a range of possibilities when learning something in order to compensate for variabilities.""Nemenman uses the example of learning to swing a tennis racket. ""You're only rarely going to hit the ball in the racket's exact sweet spot,"" he says. ""And every day when you pick up the racket to play your swing is going to be a little bit different, because your body is different, the racket and the ball are different, and the environmental conditions are different. So your body needs to remember a whole range of commands, in order to adapt to these different situations and get the ball to go where you want.""First author of the study is Baohua Zhou, a graduate student of physics. Co-authors include David Hofmann and Itai Pinkoviezky (post-doctoral fellows in physics) and Samuel Sober, an associate professor of biology.Traditional theories of learning propose that animals use sensory error signals to zero in on the optimal motor command, based on a normal distribution of possible errors around it -- what is known as a bell curve. Those theories, however, cannot explain the behavioral observations that small sensory errors are more readily corrected, while the larger ones may be ignored by the animal altogether.For the PNAS paper, the researchers analyzed experimental data on Bengalese finches collected in previous work with the Sober lab. The lab uses finches as a model system for understanding how the brain controls complex vocal behavior and motor behavior in general.Miniature headphones were custom-fitted to adult birds and used to provide auditory feedback in which the pitch that the bird perceives it vocalizes at could be manipulated, replacing what the bird hears -- its natural auditory feedback -- with the manipulated version. The birds would try to correct the pitch they were hearing to match the sound they were trying to make. Experiments allowed the researchers to record and measure the relationship between the size of a vocal error the bird perceives, and the probability of the brain making a correction of a specific size.The researchers analyzed the data and found that the variability of errors in correction did not have the normal distribution of a bell curve, as previously proposed. Instead, the distribution had long tails of variability, indicating that the animal believed that even large fluctuations in the motor commands could sometimes produce a correct pitch. The researchers also found that the birds combined their hypotheses about the relationship between the motor command and the pitch with the new information that their brains received from their ears while singing. In fact, they did this surprisingly accurately.""The birds are not just trying to sing in the best possible way, but appear to be exploring and trying wide variations,"" Nemenman says. ""In this way, they learn to correct small errors, but they don't even try to correct large errors, unless the large error is broken down and built up gradually.""The researchers created a mathematical model for this process, revealing the pattern of how small errors are corrected quickly and large errors take much longer to correct, and might be neglected altogether, when they contradict the animal's ""beliefs"" about the errors that its sensorimotor system can produce.""Our model provides a new theory for how an animal learns, one that allows us to make predictions for learning that we have tested experimentally,"" Nemenman says.The researchers are now exploring if this model can be used to predict learning in other animals, as well as predicting better rehabilitative protocols for people dealing with major disruptions to their learned behaviors, such as when recovering from a stroke.The work was funded by the National Institutes of Health BRAIN Initiative, the James S. McDonnell Foundation, and the National Science Foundation. The NVIDIA corporation donated high-performance computing hardware that supported the work.Story Source:Materials provided by Emory Health Sciences. Original written by Carol Clark. ",0.0642289,0.0802613,0.0981274,0.228893,0.0944582,0.115225,0.151451,0,0
112,Science learns from its mistakes too,"Scientific studies should always be published irrespective of their result. That is one of the conclusions of a research project conducted by the German Centre for the Protection of Laboratory Animals at the German Federal Institute for Risk Assessment (BfR), the results of which have now been published in the journal PLOS ONE.Using a mathematical model, the scientists examined the influence that individual benchmarks have on further research when preparing the studies. ""The research community should do everything possible to maintain social trust in science,"" says BfR President, Professor Dr. Dr. Andreas Hensel. ""This also means that results have to be understandable and reproducible so that false conclusions can be refuted easily. Our study shows that we achieve better results when seemingly inconclusive studies are published.""Investigations show that scientific studies have a better chance of getting published if they have a desired ""positive"" result, such as measuring an expected effect or detecting a substance or validating a hypothesis. ""Negative"" or ""null"" results, which do not have any of these effects, have a lesser chance of publication.It goes without saying that scientists also have a great interest in achieving meaningful results that are worthy of publication, thus advancing research. The great significance the publishing of a study in journals has on reputation and future sponsorship further intensifies this interest. The result of this can be, however, that studies are published the results of which are not reproducible and which therefore only appear to be ""positive.""These seemingly positive results then lead to further studies which build on the supposedly proven effect. The well-established practice among publishers of mainly publishing studies with positive results thus favours studies which do not stand up to scrutiny and therefore entail further unnecessary studies.The mathematical model presented in the publication shows how the mechanism of ""false positive"" results can be broken through. If all studies -- irrespective of their results -- were to be published after complying with good scientific practice, a false result could be disproven more quickly.This means that a seemingly negative result is not a drawback but rather a gain in knowledge, too. An animal test, for example, which cannot prove the efficacy of a new drug, would then not be a failure in the eyes of science but rather a valuable result which prevents unnecessary follow-on studies (and further animal tests) and speeds up the development of new therapies.As it turned out, an additional criterion helps to facilitate the knowledge gain when preparing studies: in biomedical studies, a scientifically argued, sufficiently high number of test animals for a single experiment increases the likelihood of achieving correct and reproducible results at the first attempt. In the long run, unnecessary follow on tests with animals based on false assumptions can be avoided in this way. Ultimately therefore, the use of more test animals in a single experiment can reduce the total number of animals used.The calculations of the BfR research team are based on biomedical research with laboratory animals, but the results can in general be applied to the life sciences.The background of the study is the reproducibility crisis bemoaned in the life sciences and psychological research. Depending on meta-research, between 51 and 89 percent of the results published in bioscientific studies cannot be reproduced by other researchers. Neuroscientific studies show that shortcomings in the statistical evaluation of experiments are often a reason why studies cannot be reproduced.Story Source:Materials provided by BfR Federal Institute for Risk Assessment. ",0.105578,0.118651,0.16588,0.188889,0.128602,0.157975,0.160172,0,0
113,"Mitigating stress, PTSD risk in warfighters ","A U.S. Army Research Laboratory scientist has collaborated with a team of researchers from the University of North Texas to develop a new data processing technique that uses electroencephalogram, or EEG, time series variability as a measure of the state of the brain.The researchers say such a technique has the potential to provide measures that facilitate the development of procedures to mitigate stress and the onset of conditions such as Post-Traumatic Stress Disorder in warfighters.""The human brain is considered by many to be the most complex organ in existence, with over a billion neurons and having in excess of a trillion interconnections,"" said Dr. Bruce West, senior scientist of mathematics and information science at the U.S. Army Research Office and ARL Fellow.According to West, it is the operation of this extraordinary complex network of neurons that hosts human thinking, and through the central nervous system, enables the functioning of most, if not all, of the physiologic networks, such as the respiratory, motor control and cardiovascular.However, according to the researchers, even with this central role the brain plays in enabling our existence, remarkably little is known about how it does what it does.Consequently, measures for how well the brain carries out its various functions are critical surrogates for understanding, particularly for maintaining the health and wellbeing of military personnel.A small but measureable electrical signal generated by the mammalian brain was captured in the electrocardiogram of small animals by Caton in 1875 and in human brains by Berger in 1925.Norbert Wiener, a half century later, provided the mathematical tools believed necessary to penetrate the mysterious relations between the brain waves in EEG time series and the functioning of the brain.According to West, progress along this path has been slow, and after over a century of data collection and analysis, there is no taxonomy of EEG patterns that delineates the correspondence between those patterns and brain activity....until now!The technique developed by West and his academic partners generalizes Evolutionary Game Theory, a mathematical technique historically used in the formulation of decision making in war gaming.Their findings are reported in a paper published in the August edition of Frontiers in Physiology.In the paper, titled ""Bridging Waves and Crucial Events in the Dynamics of the Brain,"" West, along with Gyanendra Bohara and Paolo Grigolini of the University of North Texas, propose and successfully test a new model for the collective behavior within the brain, which bridges the gap between waves and random fluctuations in EEG data.""The work horse of decision making within the military has historically been Game Theory, in which players cooperate or defect, and with pairwise interactions receive various payoffs so that under given conditions certain strategies always win,"" West said. ""When the game is extended to groups in which individual strategy choices are made sequentially and can change over time, the situation evolves offering a richer variety of outcomes including the formation of collective states in which everyone is a cooperator or a defector, resulting in a collective state.""It turns out, West said, that the technique developed to process EEG data, the self-organized time criticality method, or SOTC method, incorporates a strategy that is an extension of Evolutionary Game Theory directly into the modeling of the brain's dynamics.""The collective, or critical, state of the neural network is reached spontaneously by the internal dynamics of the brain and as with all critical phenomena its emergent properties are determined by the macroscale independently of the microscale dynamics,"" West said.This macroscale can be directly accessed by the EEG spectrum.The EEG spectrum, obtained by the SOTC method, decays like Brownian motion at high frequencies, has a peak at an intermediate frequency (alpha wave) and at low frequencies has an inverse power law.In the case of the brain, the inverse power law has revealed that there is a broad range of time scales over which the brain is able to respond to the demands placed on it.This spectrum suggests a flexibility in response, reflecting a potential range from concentrating on a single task for hours to rapidly countering a physical assault.""This means that in the foreseeable future the physical training of warriors, along with the necessary monitoring of progress associated with that training, will be expanded to include the brain,"" West said. ""The reliable processing of brain activity, along with the interpretation of the processed EEG signal, will guide the development of reliable techniques to reduce stress, enhance situational awareness and increase the ability to deal with uncertainty, both on and off the battlefield.""West said that the research team even speculates that such understanding of brain dynamics may provide the insight necessary to mitigate the onset of PTSD by early detection and intervention, as is routinely done for more obvious maladies.According to West, going forward with this research can proceed in at least two ways.""One way is to apply these promising results to data sets of interest to the Army,"" West said. ""For example, quantify how the EEG records of warriors with PTSD differ from a control group of warriors and how this measure changes under different therapy and medication protocols. The other way is to refine the technique, for example, locate where on the scalp it is the most robust, while retaining sensitivity.""However this research proceeds, these Army scientists are focused on bringing the technology to fruition to help the Soldier of the future succeed in an ever-changing world and battlefield.Earlier this year, the research team published on work that look at the processing heart rate data and how heart rate was indirectly influenced by meditation through the dynamics of the brain. That work examined how the brain influences the operation of the body by directly measuring how the physiologic system (cardiovascular in this case) responds to changes in the brain (by means of meditation).This current work focuses on processing EEG data and directly interpreting the dynamics of the brain; it examines how the rhythmic behavior of brain waves (alpha, beta, gamma, etc. waves) can be understood to be compatible with the fluctuations in brain wave data.Both papers are part of an ongoing ARL-University of North Texas study to determine if the fluctuations in all the physiological systems are produced by a previously unidentified mechanism that we call crucial events.Story Source:Materials provided by U.S. Army Research Laboratory. ",0.073966,0.0860727,0.134496,0.232746,0.114114,0.130672,0.153117,0,0
114,Patterns in STEM grades of girls versus boys,"A new study, led by UNSW Sydney PhD student Rose O'Dea, has explored patterns in academic grades of 1.6 million students, showing that girls and boys perform very similarly in STEM -- including at the top of the class.The analysis, published today in the journal Nature Communications, casts doubt on the view that there are fewer women in STEM-related jobs because they aren't as capable in those subjects as men -- a notion that has been supported by the concept that gender differences in variability lead to gender gaps in associated fields.In their meta-analysis, the UNSW researchers compared gender differences in variation of academic grades from over 1.6 million students aged six through to university from all over the world, across 268 different schools and classrooms.""We combined data from hundreds of studies, and used a method developed by my supervisor to comprehensively test for greater male variability in academic performance,"" lead author Rose O'Dea says.A classroom with more variable grades indicates a bigger gap between high and low performing students, and greater male variability could result in boys outnumbering girls at the top and bottom of the class.""Greater male variability is an old idea that people have used to claim that there will always be more male geniuses -- and fools -- in society,"" O'Dea says.The team found that on average, girls' grades were higher than boys', and girls' grades were less variable than boys'.""We already knew that girls routinely outperform boys at school, and we also expected female grades to be less variable than those of males, so that wasn't surprising. In fact, our study suggests that these two factors haven't changed in 80 years,"" O'Dea says.""However, what was most surprising was that both of these gender differences were far larger in non-STEM subjects, like English. In STEM subjects girls and boys received surprisingly similar grades, in both average and variability.""In other words, the researchers demonstrated that academic STEM achievements of boys and girls are very similar -- in fact, the analysis suggests that the top 10% of a class contained equal numbers of girls and boys.O'Dea says that there are multiple reasons that these figures don't translate into equivalent participation in STEM jobs later in life.""Even if men and women have equal abilities, STEM isn't an equal playing field for women -- and so women often go down paths with less male competition. ""For example, we found that the ability overlap between girls and boys is much greater in STEM, and smaller in non-STEM subjects, meaning that there are fewer boys competing with girls in non-STEM subjects.""So say you're a girl in a class and you're a straight A student. In your math class, you're surrounded by top-achieving boys, and then in English there's fewer boys that you're competing with, so it can look like non-STEM is an easier option or a safer path.""Stereotypical societal beliefs about what fields girls are seen to be successful in also play a role. ""Girls are susceptible to conforming to stereotypes in the traditionally male-dominated fields of STEM. Girls who try to succeed in these fields are often hindered by backlash effects,"" O'Dea says.""For example, the stereotype that girls aren't good at maths actually makes it harder for girls to be good at maths, both because of the way we perceive ourselves and the way other people perceive us. We all have subconscious biases, and there's a strange phenomenon called stereotype threat, where being reminded of the stereotype connected to your identity can make it harder to defy that stereotype.""O'Dea says that there's no simple fix to work on the underrepresentation of women in STEM.""Science and academia have a lot of structural issues that will take time to fix. However, there's a lot we can do to encourage girls to perform better at maths -- for example, girls tend to do better when they're taught by a woman with a strong maths background, so they can see they can do maths, too.""Professor Emma Johnston, Dean of Science at UNSW, says a lot needs to be done to encourage girls to choose a STEM path. ""This powerful, evidence-based research has revealed that girls and boys are equally good at STEM subjects. Differential participation in STEM training and STEM careers must therefore be explained by other factors.""Australia really needs more women to enter, stay, and succeed in STEM areas. We absolutely need to change the structural barriers to gender equality in science, but we must also change the strong negative stereotypes and unconscious biases as well. We must give our girls and women more successful science role models -- something grand to aspire to.""We all need to actively work to close this gap -- for example, UNSW's Women in Maths and Science Champions Program is a unique opportunity to support women who are completing their PhD in UNSW Science. The program focuses on strengthening the cohort's communication and leadership skills to support their professional careers and their lifelong role of advocacy to inspire women to pursue a career in maths and science.""The author of this landmark study is a great example -- Rose is an incredible role model and her leadership in traditionally male-dominated fields like science and the AFL is inspiring to many girls,"" Professor Johnston concludes.Story Source:Materials provided by University of New South Wales. Original written by Isabelle Dubach. ",0.135399,0.195823,0.201616,0.267622,0.175134,0.222423,0.235701,0,0
115,What your cell phone camera tells you about your brain Research unites cognitive science and information theory,"Driving down a dark country road at night, you see a shape ahead on the roadside. Is it a deer or a mailbox? Your brain is structured to make the best possible decision given its limited resources, according to new research that unites cognitive science and information theory -- the branch of mathematics that underlies modern communications technology.The finding, which was published in the journal Science, is an outgrowth of National Science Foundation-supported research into improving pedagogy in STEM fields, which often rely heavily on perceptual abilities and perceptual expertise, said Chris Sims, a Rensselaer Polytechnic Institute assistant professor of cognitive science.""Take for example geology, where students have to learn to differentiate between rocks or geologic formations that may have a highly similar appearance,"" said Sims. ""The way it's typically done is through repetition -- you look at lots of rocks until it sinks in. But understanding how the brain works can help us build better classroom training exercises that teach those abilities more efficiently.""A canonical law of cognitive science -- the Universal Law of Generalization, introduced in a 1987 article also published in Science -- tells us that your brain makes perceptual decisions based on how similar the new stimulus is to previous experience. Specifically, the law states that the probability you will extend a past experience to new stimulus depends on the similarity between the two experiences, with an exponential decay in probability as similarity decreases. This empirical pattern has proven correct in hundreds of experiments across species including humans, pigeons, and even honeybees.""This is a fundamental equation that is universal in nature, and it's held up very well,"" said Sims. ""But while the law describes the empirical pattern, it doesn't adequately explain why this pattern should appear in nature. And that's what I set out to do.""In his research, Sims turned to information theory, a branch of mathematics founded at Bell Labs in the 1940s that makes it possible to predict the best possible performance of a communication system given the limits of the system. For example, information theory makes it possible to predict the best possible voice accuracy that a telephone wire could transmit given a specific level of noise in the signal.Building on the evident parallels between noisy telephone wires and noisy neurons, Sims has been using information theory to understand the biological communication systems of perception and memory.""The idea is that visual perception is a communication channel: there is information in the world, and you need to transmit that information from your eyes to your brain,"" said Sims.Just as there are limits on a mechanical system like a telephone line, there are limits on a biological system, and Sims looked to information theory to describe and predict the optimal performance that could be achieved from the human visual system.And that led to a serendipitous connection between the Universal Law of Generalization long studied in cognitive science, and the mathematical framework of information theory.When Sims described the visual system using the information theory framework, he found that a well-known aspect of information theory known as efficient coding predicted the same exponential generalization gradient as that predicted by the Universal Law of Generalization. His work connected the dots between two foundational laws within disparate fields, and suggests that evolution has given us a perceptual system that approaches the optimum predicted by the mathematical laws of information theory.""I set out to explain why this pattern appears in nature, and the answer according to information theory is that nature has given us perceptual systems that are as efficient as possible, given the constraints and limits they have to work with. It's a simple explanation for why this pattern exists everywhere and that's promising.""The finding might be used to help develop more accurate measurement of perceptual expertise and progression, but Sims said at heart, he's pleased to have advanced the foundational science.""I'm excited that now we have mathematical laws we can use to better describe and understand information processing in the brain, and the nature of intelligence in general,"" said Sims.Story Source:Materials provided by Rensselaer Polytechnic Institute. Original written by Mary L. Martialay. ",0.0529552,0.0688041,0.100569,0.231734,0.0880142,0.0997825,0.131237,0,0
116,Researchers resolve a major mystery in 2D material electronics,"Schottky diode is composed of a metal in contact with a semiconductor. Despite its simple construction, Schottky diode is a tremendously useful component and is omnipresent in modern electronics. Schottky diode fabricated using two-dimensional (2D) materials have attracted major research spotlight in recent years due to their great promises in practical applications such as transistors, rectifiers, radio frequency generators, logic gates, solar cells, chemical sensors, photodetectors, flexible electronics and so on.The understanding of 2D material-based Schottky diode is, however, plagued by multiple mysteries. Several theoretical models have co-existed in the literatures and a model is often selected a priori without rigorous justifications. It is not uncommon to see a model, whose underlying physics fundamentally contradicts with the physical properties of 2D materials, being deployed to analyse a 2D material Schottky diode.Reporting in Physical Review Letters, researchers from the Singapore University of Technology and Design (SUTD) have made a major step forward in resolving the mysteries surrounding 2D material Schottky diode. By employing a rigorous theoretical analysis, they developed a new theory to describe different variants of 2D-material-based Schottky diodes under a unifying framework. The new theory lays down a foundation that helps to unite prior contrasting models, thus resolving a major confusion in 2D material electronics.""A particularly remarkable finding is that the electrical current flowing across a 2D material Schottky diode follows a one-size-fits-all universal scaling law for many types of 2D materials,"" said first-author Dr. Yee Sin Ang from SUTD.Universal scaling law is highly valuable in physics since it provides a practical ""Swiss knife"" for uncovering the inner workings of a physical system. Universal scaling law has appeared in many branches of physics, such as semiconductor, superconductor, fluid dynamics, mechanical fractures, and even in complex systems such as animal life span, election results, transportation and city growth.The universal scaling law discovered by SUTD researchers dictates how electrical current varies with temperature and is widely applicable to broad classes of 2D systems including semiconductor quantum well, graphene, silicene, germanene, stanene, transition metal dichalcogenides and the thin-films of topological solids.""The simple mathematical form of the scaling law is particularly useful for applied scientists and engineers in developing novel 2D material electronics,"" said co-author Prof. Hui Ying Yang from SUTD.The scaling laws discovered by SUTD researchers provide a simple tool for the extraction of Schottky barrier height -- a physical quantity critically important for performance optimisation of 2D material electronics.""The new theory has far reaching impact in solid state physics,"" said co-author and principal investigator of this research, Prof. Lay Kee Ang from SUTD, ""It signals the breakdown of classic diode equation widely used for traditional materials over the past 60 years, and shall improve our understanding on how to design better 2D material electronics.""Story Source:Materials provided by Singapore University of Technology and Design. ",0.0865326,0.128489,0.123512,0.166238,0.177874,0.122026,0.145191,0,0
117,Mechanism of biological noise cancellation revealed,"Noise is usually considered a nuisance, for example when preventing us from clearly hearing what someone is saying. The general concept of noise applies more broadly. In biology, the physical differences between organisms of the same type, for example two humans, originate from so-called developmental noise -- ultimately stemming from probabilistic collisions between reacting molecules and environmental conditions in cells during the early stages of the organism's growth. Generally, mechanisms are in place to prevent biological noise from resulting in incorrect organismal developments.Makoto Sato from Kanazawa University and colleagues have now discovered that a particular biochemical signaling pathway contributes to noise cancelling in the differentiation process of neural stem cells -- the self-renewing cells that play an important part in the development of the nervous system of animals during the embryonic stage.As a model system for understanding cancellation mechanisms of biological noise in a multicellular organism, the researchers looked at a particular development stage of the visual system in fruit flies: the propagation of the so-called 'proneural wave', during which cells of a type called neuroepithelial cell differentiate into neuroblasts (cells in the process of dividing into neurons).Inspired by earlier research, Sato and colleagues hypothesized that a signaling pathway (a particular chain of reactions between proteins in cells) known as JAK/STAT is involved in noise cancellation. The scientists combined mathematical modeling and genetic experiments. For the former, they designed a set of mathematical equations that quantify the variations in the system and reproduce the progression of the proneural wave. When including JAK/STAT signaling, they found that spontaneous, noise-induced neuroblast differentiation is cancelled.To confirm the effect of JAK/STAT signaling in vivo, the researchers performed experiments wherein the signaling pathway was reduced in a controlled way. Upon reduction, stochastic differentiation of neuroblasts was observed, consistent with the conclusion that JAK/STAT has a noise-canceling function. Further genetic experiments provided more insights into the details of how noise canceling by JAK/STAT happens, and that this may be a function that has been conserved throughout evolution.Not only does the work of Sato and colleagues contribute to a better understanding of noise-canceling mechanisms in developmental biology, it also underlines the power of combining 'in silico' with 'in vivo' studies. Quoting the scientists, ""by applying a combination of mathematical modeling and molecular genetics, we can solve biological questions that have previously been difficult to address.""Story Source:Materials provided by Kanazawa University. ",0.105777,0.179519,0.15691,0.224633,0.172943,0.158086,0.194891,0,0
118,Using physics to predict crowd behavior,"Electrons whizzing around each other and humans crammed together at a political rally don't seem to have much in common, but researchers at Cornell are connecting the dots.They've developed a highly accurate mathematical approach to predict the behavior of crowds of living creatures, using Nobel Prize-winning methods originally developed to study large collections of quantum mechanically interacting electrons. The implications for the study of human behavior are profound, according to the researchers.For example, by using publicly available video data of crowds in public spaces, their approach could predict how people would distribute themselves under extreme crowding. By measuring density ?uctuations using a smartphone app, the approach could describe the current behavioral state or mood of a crowd, providing an early warning system for crowds shifting toward dangerous behavior.Tomas Arias, professor of physics, is lead author of ""Density-Functional Fluctuation Theory of Crowds,"" which published Aug. 30 in Nature Communications. Co-authors include Itai Cohen, professor of physics; and Yunus A. Kinkhabwala, a doctoral student in the field of engineering.Interactions among individuals in a crowd can be complex and difficult to quantify mathematically; the large number of actors in a crowd results in a complex mathematical problem. The researchers sought to predict the behavior of crowds by using simple measurements of density to infer underlying interactions and to use those interactions to predict new behaviors.To achieve this, they applied mathematical concepts and approaches from density-functional theory (DFT), a branch of many-body physics developed for quantum mechanical systems, to the behavior of crowds.""This is one of the all-too-rare cases -- particularly where living systems are involved -- where the theory preceded the experiments, and the experiments, in precise mathematical detail, completely confirmed the theory,"" said Arias.To test their theory, the researchers created a model system using walking fruit flies (Drosophila melanogaster). They first demonstrated a mathematical way to extract functions that quantify how much the flies like different locations in their environment -- the ""vexation"" function -- and how much they mind crowding together -- the ""frustration"" function based on the details of how the population densities change as the flies more around.They then showed that by mixing and matching this information with observations of a single fly in an entirely new environment, they could accurately predict, before any observations, how a large crowd of flies would distribute themselves in that new environment. They also tracked changes in the overall behavior of the crowd -- i.e., its ""mood"" -- by tracking evolution of the social preference ""frustration"" function.While fruit flies were ""a convenient, and ethical, first test system,"" Arias said, the behavior of a crowd at a political rally would provide a human example of DFT theory. Individuals will try to find the best location to stand -- typically closest to the stage -- while avoiding overcrowded areas. When new and better locations become available, individuals are likely to move toward them.To develop a mathematically predictive theory, the researchers associated a number -- the vexation function -- with the intrinsic desirability of each location; the lowest value would be at the ideal location, closest to the stage. The frustration function accounts for the undesirability of crowding effects, and a behavioral rule accounts for the tendency of individuals to look for better locations.""The remarkable mathematical discovery,"" Arias said, ""is that precise values for vexation and frustration can be obtained instantly and automatically, simply by observing changes in crowding as the crowd mills around, without the need for any kind of survey to ask people in the crowd how they feel about different locations or crowding together.""By varying the social circumstances in their fly experiments -- such as changing the ratio of male and female, or inducing hunger and thirst -- and monitoring the frustration values of the crowd, the researchers showed they can detect changes in the ""mood"" of the crowd. The DFT approach, therefore, not only predicts crowd behaviors under new circumstances, but also can be used to quickly and automatically detect changes in social behaviors.Another application, using cell-phone and census data, could analyze political or economic drivers and population pressures to describe and predict large-scale population flows, such as mass migrations. ""The resulting predictions of migration during acute events would enable better planning by all levels of government o?cials, from local municipalities to international bodies, with the potential to save millions of human lives,"" note the researchers.Other contributors included J. Felipe Mendez-Valderrama, professor of physics, University of Los Andes, Bogota, Colombia; and Jeffrey Silver, senior analyst at Metron Inc.Story Source:Materials provided by Cornell University. Original written by Linda B. Glaser. ",0.0711478,0.0887608,0.11526,0.193936,0.0952181,0.126096,0.146944,0,0
119,Mathematics can assist cities in addressing unstructured neighborhoods,"New mathematical models developed by the Department of Energy's Oak Ridge National Laboratory with collaborators at Sam Houston State University and the University of Chicago can help guide changes to the layout of poor urban neighborhoods to improve access to resources with minimum disruption and cost.The researchers established a novel way to mathematically analyze poor and informally developed urban communities, revealing obstacles between unplanned areas and the infrastructure that provides resources for basic human necessities. In their paper published in Science Advances, they used satellite imagery and municipal data to develop mathematical algorithms that reveal slums and planned neighborhood are fundamentally different.Their models clearly identify distinctions between the informal arrangement of underserviced urban areas and the formal structure of city neighborhoods. In two case studies, the researchers used real-world data to show that the physical layout of some unplanned neighborhoods does not allow space for sewer lines, roads or water pipes.Of the estimated four billion people currently living in urban areas worldwide, approximately one billion reside in slums. With inadequate infrastructure for health, sanitation and access to emergency services, these areas are becoming humanitarian and sustainability issues in the wake of rapid urbanization. Christa Brelsford, an ORNL Liane Russell Fellow and lead author, believes this research can transform the future of slums.""For me, this research was an opportunity to look at cities from a new and exciting mathematical perspective,"" Brelsford said. ""By putting these tools in the hands of local community organizations and residents, efforts for accessible infrastructure empower residents to make decisions about their neighborhoods and communities.""""Anything we can do to improve lives from a human rights perspective is both good for the world and also supports U.S. national security, because the more people have their basic needs met, the more secure we all are,"" she added.Although underserved communities are the first application, the researchers' algorithm provides a mathematical way of describing all cities. The team used a novel topological technique, based on connections between places, to characterize the first-time slums rather than a traditional geometric approach.""By understanding the fundamental topology -- the relationship between places of residence and work to urban infrastructure networks -- we can determine parts of cities remain only incipiently connected,"" said coauthor Lues Bettencourt, director of the Mansueto Institute for Urban Innovation at the University of Chicago.""It is a situation that can be mapped, measured and resolved with minimal interference to create new people-centered urban planning solutions and cities that have history and character,"" he added.Story Source:Materials provided by DOE/Oak Ridge National Laboratory. ",0.0726629,0.0987432,0.208319,0.213956,0.107755,0.141492,0.15705,0,0
120,A better way to count boreal birds,"Knowing approximately how many individuals of a certain species are out there is important for bird conservation efforts, but raw data from bird surveys tends to underestimate bird abundance. The researchers behind a new paper from The Condor: Ornithological Applications tested a new statistical method to adjust for this and confirmed several mathematical tweaks that can produce better population estimates for species of conservation concern.The University of Alberta's Peter Selymos and his colleagues tested a type of mathematical model called a ""removal model"" using bird count data for 152 species from the Boreal Avian Modelling Project, or BAM, which covers a vast area of Canada, Alaska, and the northeastern U.S. They found that incorporating variation in birds' singing behavior improved models' accuracy -- how likely birds are to sing changes over the course of the day and the year, affecting how easy they are to detect. Extending the length of individual bird counts from three or five minutes to ten minutes was also beneficial.""The chances of spotting something -- a coin on the pavement, a bear in the mountains, or the apricot jam in the freezer -- increases with effort,"" explains Selymos. ""The more we walk, travel, look, or listen, the more things we find, but there is also a tradeoff between the number of places one can do a search and the length of the searches. Such decisions drive how long field biologists conduct bird counting at a given place. In our study, we looked at how the duration of counting influenced finding different bird species at different times of the day and the year. We also wanted to find the best way of how to standardize for different count durations and how to use these findings to provide better estimates of bird population sizes.""This is more than just a math problem, however -- accurate estimates of population size can be crucial for effective bird conservation, and many of the boreal bird species this study looked at are experiencing severe declines. This new approach offers a way to combine data from surveys that weren't standardized in their design. ""Population size of different species is one of the key metrics that affects their conservation importance, but estimating population size is a very challenging task that involves numerous assumptions,"" says Selymos. ""Besides the ability to hopefully provide more accurate population size estimates, our modeling approach and findings can also help in timing of bird surveys to maximize the number of species detected.""""While the authors of this study present the results of a rigorous comparison of modeling techniques to achieve better estimates of bird abundance from point counts, they also provide clear and simple recommendations on how and when to apply their findings to any study that expects to use time-interval point counts,"" adds Jeff Wells, Science and Policy Director of the Boreal Songbird Initiative, who was not involved in the research. ""This is a rich contribution not only to avian research methodology, but in the long run, also to bird conservation.""Story Source:Materials provided by American Ornithological Society Publications Office. ",0.113713,0.143117,0.191202,0.235612,0.145189,0.159797,0.183934,0,0
121,"More efficient security for cloud-based machine learning Novel combination of two encryption techniques protects private data, while keeping neural networks running quickly","A novel encryption method devised by MIT researchers secures data used in online neural networks, without dramatically slowing their runtimes. This approach holds promise for using cloud-based neural networks for medical-image analysis and other applications that use sensitive data.Outsourcing machine learning is a rising trend in industry. Major tech firms have launched cloud platforms that conduct computation-heavy tasks, such as, say, running data through a convolutional neural network (CNN) for image classification. Resource-strapped small businesses and other users can upload data to those services for a fee and get back results in several hours.But what if there are leaks of private data? In recent years, researchers have explored various secure-computation techniques to protect such sensitive data. But those methods have performance drawbacks that make neural network evaluation (testing and validating) sluggish -- sometimes as much as million times slower -- limiting their wider adoption.In a paper presented at this week's USENIX Security Conference, MIT researchers describe a system that blends two conventional techniques -- homomorphic encryption and garbled circuits -- in a way that helps the networks run orders of magnitude faster than they do with conventional approaches.The researchers tested the system, called GAZELLE, on two-party image-classification tasks. A user sends encrypted image data to an online server evaluating a CNN running on GAZELLE. After this, both parties share encrypted information back and forth in order to classify the user's image. Throughout the process, the system ensures that the server never learns any uploaded data, while the user never learns anything about the network parameters. Compared to traditional systems, however, GAZELLE ran 20 to 30 times faster than state-of-the-art models, while reducing the required network bandwidth by an order of magnitude.One promising application for the system is training CNNs to diagnose diseases. Hospitals could, for instance, train a CNN to learn characteristics of certain medical conditions from magnetic resonance images (MRI) and identify those characteristics in uploaded MRIs. The hospital could make the model available in the cloud for other hospitals. But the model is trained on, and further relies on, private patient data. Because there are no efficient encryption models, this application isn't quite ready for prime time.""In this work, we show how to efficiently do this kind of secure two-party communication by combining these two techniques in a clever way,"" says first author Chiraag Juvekar, a PhD student in the Department of Electrical Engineering and Computer Science (EECS). ""The next step is to take real medical data and show that, even when we scale it for applications real users care about, it still provides acceptable performance.""Co-authors on the paper are Vinod Vaikuntanathan, an associate professor in EECS and a member of the Computer Science and Artificial Intelligence Laboratory, and Anantha Chandrakasan, dean of the School of Engineering and the Vannevar Bush Professor of Electrical Engineering and Computer Science.Maximizing performanceCNNs process image data through multiple linear and nonlinear layers of computation. Linear layers do the complex math, called linear algebra, and assign some values to the data. At a certain threshold, the data is outputted to nonlinear layers that do some simpler computation, make decisions (such as identifying image features), and send the data to the next linear layer. The end result is an image with an assigned class, such as vehicle, animal, person, or anatomical feature.Recent approaches to securing CNNs have involved applying homomorphic encryption or garbled circuits to process data throughout an entire network. These techniques are effective at securing data. ""On paper, this looks like it solves the problem,"" Juvekar says. But they render complex neural networks inefficient, ""so you wouldn't use them for any real-world application.""Homomorphic encryption, used in cloud computing, receives and executes computation all in encrypted data, called ciphertext, and generates an encrypted result that can then be decrypted by a user. When applied to neural networks, this technique is particularly fast and efficient at computing linear algebra. However, it must introduce a little noise into the data at each layer. Over multiple layers, noise accumulates, and the computation needed to filter that noise grows increasingly complex, slowing computation speeds.Garbled circuits are a form of secure two-party computation. The technique takes an input from both parties, does some computation, and sends two separate inputs to each party. In that way, the parties send data to one another, but they never see the other party's data, only the relevant output on their side. The bandwidth needed to communicate data between parties, however, scales with computation complexity, not with the size of the input. In an online neural network, this technique works well in the nonlinear layers, where computation is minimal, but the bandwidth becomes unwieldy in math-heavy linear layers.The MIT researchers, instead, combined the two techniques in a way that gets around their inefficiencies.In their system, a user will upload ciphertext to a cloud-based CNN. The user must have garbled circuits technique running on their own computer. The CNN does all the computation in the linear layer, then sends the data to the nonlinear layer. At that point, the CNN and user share the data. The user does some computation on garbled circuits, and sends the data back to the CNN. By splitting and sharing the workload, the system restricts the homomorphic encryption to doing complex math one layer at a time, so data doesn't become too noisy. It also limits the communication of the garbled circuits to just the nonlinear layers, where it performs optimally.""We're only using the techniques for where they're most efficient,"" Juvekar says.Secret sharingThe final step was ensuring both homomorphic and garbled circuit layers maintained a common randomization scheme, called ""secret sharing."" In this scheme, data is divided into separate parts that are given to separate parties. All parties synch their parts to reconstruct the full data.In GAZELLE, when a user sends encrypted data to the cloud-based service, it's split between both parties. Added to each share is a secret key (random numbers) that only the owning party knows. Throughout computation, each party will always have some portion of the data, plus random numbers, so it appears fully random. At the end of computation, the two parties synch their data. Only then does the user ask the cloud-based service for its secret key. The user can then subtract the secret key from all the data to get the result.""At the end of the computation, we want the first party to get the classification results and the second party to get absolutely nothing,"" Juvekar says. Additionally, ""the first party learns nothing about the parameters of the model.""Story Source:Materials provided by Massachusetts Institute of Technology. Original written by Rob Matheson. ",0.0449911,0.0601609,0.0922687,0.385729,0.108025,0.0985702,0.161347,0,0
122,Math shows how human behavior spreads infectious diseases,"Mathematics can help public health workers better understand and influence human behaviours that lead to the spread of infectious disease, according to a study from the University of Waterloo.Current models used to predict the emergence and evolution of pathogens within host populations do not include social behaviour.""We tend to treat disease systems in isolation from social systems, and we don't often think about how they connect to each other, or influence each other,"" said Chris Bauch, co-author and a professor in the Department of Applied Mathematics at Waterloo. ""This gives us a better appreciation of how social reactions to infectious diseases can influence which strains become prominent in the population.""By adding dynamic social interactions to the models already used for disease outbreaks and evolution, researchers could better anticipate how a virulent pathogen strain may emerge based on how humans attempt to control the spread of the disease. This new addition to disease modelling could allow scientists to better prevent undesirable outcomes, such as more dangerous mutant strains from evolving and spreading.The social modelling could impact public health responses to emerging infectious diseases like Ebola and Severe Acute Respiratory Syndrome (SARS). Human behaviour during these outbreaks often changes dramatically during the outbreak. People may start using face masks, or stop using them prematurely. Also, public fear of the pathogens may end up driving the wrong type of behaviour if the public's information is incorrect. The modelling could help public health responses navigate and better channel these kinds of population responses,Bauch and his co-author Joe Pharaon formulated the new mathematical model to study the influence of social behaviour on the competition between pathogen strains with different virulence. Using computer simulations, they analyzed how the model behaved under various possible scenarios that might occur to populations to explore the logic of the hypothesis that social behaviour plays a role in the evolution of the strain.""Human behaviour plays a big role in the spread and evolution of an infectious disease,"" said Pharaon,a PhD candidate at Waterloo's Faculty of Mathematics. ""The model we formulated was a general model, but it could be adapted with more biological detail and structure for more specific pathogens.""Story Source:Materials provided by University of Waterloo. ",0.0996754,0.15428,0.213291,0.253936,0.164254,0.164512,0.219939,0,0
123,Why zebrafish (almost) always have stripes Mathematical model helps explain key role of one pigment cell,"One of the most remarkable things about the iconic yellow and blue stripes of zebrafish is that they reliably appear at all.Zebrafish begin life as transparent embryos, with three types of pigment cells on their skin. As they develop, the pigment cells somehow manage to organize themselves almost without fail into the stripes we all know.Now researchers have developed a mathematical model that may explain the key role that one of those pigment cells plays in making sure each stripe ends up exactly where it belongs on the fish.""It's amazing that you have these individual cells that can sort themselves into these reliable patterns,"" said Alexandria Volkening, lead author of the study and a postdoctoral fellow at Ohio State University's Mathematical Biosciences Institute.""The cells move around on the skin to create stripes. It's like individual birds that know how to flock together and fly in formation.""This new model suggests that one of the pigment cell types -- called iridophores -- leads the process of cell organization. These cells provide redundancies in the cell interaction process that ensures that if one interaction fails, another one can take over.The result is that zebrafish get their stripes, even when some of the cellular processes go wrong, she said.Volkening conducted the research with Bjorn Sandstede, professor of applied mathematics at Brown University. Their study was published today in the journal Nature Communications.Until recently, almost all research on zebrafish stripes focused on the other two pigment cells: the black cells (called melanophores) and the yellow cells (called xanthophores). It wasn't until 2013 that biologists discovered that iridophores also played a role.The role of iridophores is definitely vital: Some mutations of zebrafish that don't have iridophores develop spots instead of stripes.""In our mathematical model, we use what we know about the interactions of the other two cell types to explain what drives iridophore behavior. We found that we could predict what iridophores would do in a way that matches up well with what biologists have observed in zebrafish,"" she said.Researchers have known that iridophores change their shape in carefully orchestrated patterns on the skin of zebrafish, and the changes in shape instruct the other two types of cells on where to go in ways that result in stripes.The model explains what drives these shape changes.The model showed, not surprisingly, that the process is extremely complex, Volkening said. But the complexity is necessary to build redundancy into the process. The researchers found that they could reduce the complexity in the model in some ways, and the zebrafish would still develop stripes.""We think that is because iridophores are getting their signals from multiple places. If one interaction fails, there is another that can take its place,"" she said.""Because of the redundancies, you can remove some interactions, and still get stripes. They're not perfect stripes, but they are similar.""However, there are cases when the process breaks down so much that stripes no longer form. That occurs in some mutations of zebrafish. Volkening said the model can account for these mutations, which adds to the likelihood that the model is correct.""The biological discovery of the role of iridophores was a real paradigm shift in how we thought zebrafish stripes were created. This led to a lot of open questions, and our mathematical model provides an explanation for how iridophores behave on the fish skin,"" she said.""We show that the complex interactions of these cells may be important for reliable stripe formation, but also key to why zebrafish have stripes but related fish have different patterns.""Story Source:Materials provided by Ohio State University. Original written by Jeff Grabmeier. ",0.0655966,0.126616,0.116019,0.177822,0.105803,0.108234,0.144794,0,0
124,Toward a universal quantum computer Researchers demonstrate fault-tolerant universal holonomic quantum gates,"Researchers have demonstrated holonomic quantum gates under zero-magnetic field at room temperature, which will enable the realization of fast and fault-tolerant universal quantum computers.A quantum computer is a powerful machine with the potential to solve complex problems much faster than today's conventional computer can. Researchers are currently working on the next step in quantum computing: building a universal quantum computer.The paper, published in the journal Nature Communications, reports experimental demonstration of non-adiabatic and non-abelian holonomic quantum gates over a geometric spin qubit on an electron or nitrogen nucleus, which paves the way to realizing a universal quantum computer.The geometric phase is currently a key issue in quantum physics. A holonomic quantum gate manipulating purely the geometric phase in the degenerate ground state system is believed to be an ideal way to build a fault-tolerant universal quantum computer. The geometric phase gate or holonomic quantum gate has been experimentally demonstrated in several quantum systems including nitrogen-vacancy (NV) centers in diamond. However, previous experiments required microwaves or light waves to manipulate the non-degenerate subspace, leading to the degradation of gate fidelity due to unwanted interference of the dynamic phase.""To avoid unwanted interference, we used a degenerate subspace of the triplet spin qutrit to form an ideal logical qubit, which we call a geometric spin qubit, in an NV center. This method facilitated fast and precise geometric gates at a temperature below 10 K, and the gate fidelity was limited by radiative relaxation,"" says the corresponding author Hideo Kosaka, Professor, Yokohama National University. ""Based on this method, in combination with polarized microwaves, we succeeded in manipulation of the geometric phase in an NV center in diamond under a zero-magnetic field at room temperature.""The group also demonstrated a two-qubit holonomic gate to show universality by manipulating the electron-nucleus entanglement. The scheme renders a purely holonomic gate without requiring an energy gap, which would have induced dynamic phase interference to degrade the gate fidelity, and thus enables precise and fast control over long-lived quantum memories, for realizing quantum repeaters interfacing between universal quantum computers and secure communication networks.Story Source:Materials provided by Yokohama National University. ",0.142083,0.165624,0.161886,0.257768,0.215489,0.203818,0.207536,0,0
125,New study views cancer treatment as a game to find strategies that improve patient outcomes,"Game theory can be utilized to identify potential flaws in current cancer treatment approaches and suggest new strategies to improve outcomes in patients with metastatic cancer, according to a new article published online today by JAMA Oncology. The study, which is authored by a mathematician, an evolutionary biologist and clinical physicians from Moffitt Cancer Center and Maastricht University, challenges the decades old standard of treatment for metastatic cancers in which drugs are typically administered continuously at the maximum-tolerated dose (MTD) until the tumor progresses.The study shows that, by viewing cancer therapy as a game between the treating physician and the cancer cells, continuous administration of the same drug or drugs at MTD fails to exploit critical advantages possessed by the physician. Instead, the authors encourage oncologists to develop flexible strategic treatment plans. By exploiting his/her knowledge of the cancer's evolutionary dynamics, the oncologist can continuously adjust drugs and doses to delay or prevent cancer progression caused by the evolution of resistance. With each adjustment, the oncologist updates information on the cancer's response.The Moffitt research team, led by Robert A. Gatenby, M.D., used mathematical modeling to investigate cancer treatment as a game played by the physician and cancer cells. In their study, the researchers demonstrate that the physician has two big advantages over his/her tumor opponents. First, the physician is rational while the cancer cells are not. This means that the physician, by understanding the principles of evolution, can plan ahead and anticipate the tumor cells' response. Cancers, like all evolving organisms, can never anticipate the future and because of this are particularly vulnerable to changes in treatment by the physician. In addition, the physician has the advantage of always ""playing"" first -- the cancer cells cannot begin to evolve resistance until the physician administers a therapy. This sequence of moves means that cancer treatment has a distinctive game theoretic form termed a ""leader-follower"" or ""Stackelberg"" game. Von Stackelberg was a German mathematician who extensively investigated the game dynamics in the mid-20th century. His work and that of several other game theorists have demonstrated that the leader in a Stackelberg gains a substantial advantage by using the first move to limit subsequent tumor responses. Furthermore, the leader can use fore-knowledge of the cancer to anticipate and steer its evolution and vulnerabilities.""Current treatments for metastatic cancers, by giving the same drug repeatedly at the maximum tolerated dose, can inadvertently increase the speed with which cancer cells can evolve effective counter measures and then regrow,"" said Gatenby, co-director of Moffitt's Center of Excellence in Evolutionary Therapy. ""Today, therapy is usually changed only when the tumor progresses. By using this strategy the physician cedes control to the cancer. Although standard practice for decades, administering drugs at maximum-tolerated dose until progression is rarely the optimal game theoretic strategy for metastatic cancers.""""The current maximum-tolerated dose approach will only be successful if the cancer cell population is made up of similar cells that are unable to adapt and evolve quickly,"" said Joel Brown, Ph.D., an evolutionary biologist at Moffitt. ""That is rarely what we see for cancers that have widely metastasized. We can and must anticipate, steer and exploit the cancer cells' evolutionary responses to our therapies.""The study proposes changes in the standard treatment paradigms by encouraging physicians to better use their advantages as the sentient leader in their high-stakes game with the cancer. They can do this by continuously adjusting treatment and forcing the cancer cells to constantly change their response to unpredictable attacks from new drugs or combinations of drugs. The authors suggest that the physicians begin by precisely defining the goal of treatment. Is to cure the patient or is it to prolong life? This allows the physician to better balance the benefit of therapy against the potential toxicity and its effects on the patient's quality of life. In a related suggestion, the authors suggest that the treating physician develops a strategy to deal with cancer cells that are resistant to therapy. If the goal is to cure, then resistant cells must be killed or prevented. If the goal is control, then physicians can use evolutionary principles to minimize the proliferation of resistant cells while limiting the toxicity of treatment. Finally, the physician, as the sentient player in the game, can continuously analyze the intratumoral evolutionary dynamics based on the tumor response during each treatment cycle. This information, often with the aid of a mathematical model, can provide information to improve the outcomes in subsequent cycles -- a well-recognized approach in recursive games termed Bellman's Principle.To operationalize their theoretical study, the research team suggests precision medicine for metastatic cancers. In addition to using molecular techniques to identify treatment targets, precision medicine should integrate strategies to deal with the evolution of resistance which almost invariably leads to failure of even highly successful targeted therapies. The health team should design an explicit Resistance Management Plan (RMP) for each patient -- an approach commonly used in pest management. They also recommend that the physician analyzes the outcomes of every patient, similar to After Action Reports (AAR) used in the military and disaster response teams. By learning from each individual patient, personalized oncology can itself evolve and improve over time.While this approach is designed for incurable metastatic cancers, Katerina Stankova, Ph.D., a mathematician at Maastricht University and an expert on game theory, notes that the full dynamics of Stackelberg games have not yet been rigorously explored. ""As we develop the mathematics in conjunction with cancer therapies, we expect that our analyses will uncover novel game-theoretic, evolutionary strategies that may increase the probability of curing even aggressive and heterogeneous cancers,"" Stankova added.The study was supported by the European Union's Horizon 2020 research and innovation program, the James S. McDonnell Foundation grant, a V Foundation grant and the National Institutes of Health/National Cancer Institute.Story Source:Materials provided by H. Lee Moffitt Cancer Center & Research Institute. ",0.101961,0.148761,0.186744,0.281116,0.146677,0.172095,0.215333,0,0
126,"Math with good posture can mean better scores, study suggests Sitting up straight aids performance, researchers find","If you've ever felt like a deer in the headlights before taking a math test or speaking before a large group of people, you could benefit from a simple change in posture. As part of a new study by researchers at San Francisco State University, 125 college students were tested to see how well they could perform simple math -- subtracting 7 from 843 sequentially for 15 seconds -- while either slumped over or sitting up straight with shoulders back and relaxed. Fifty-six percent of the students reported finding it easier to perform the math in the upright position.""For people who are anxious about math, posture makes a giant difference,"" said Professor of Health Education Erik Peper. ""The slumped-over position shuts them down and their brains do not work as well. They cannot think as clearly."" Before the study began, students filled out an anonymous questionnaire asking them to rate their anxiety levels while taking exams and performing math; they also described any physical symptoms of stress they experienced during test taking.According to co-author Associate Professor of Health Education Richard Harvey, slumping over is a defensive posture that can trigger old negative memories in the body and brain. While the students without math anxiety did not report as great a benefit from better posture, they did find that doing math while slumped over was somewhat more difficult.Peper and Harvey say these findings about body position can help people prepare for many different types of performance under stress, not just math tests. Athletes, musicians and public speakers can all benefit from better posture prior to and during their performance. ""You have a choice,"" said Peper. ""It's about using an empowered position to optimize your focus.""That empowerment could be particularly helpful to students facing the challenge called ""stereotype threat,"" said Lauren Mason, one of the paper's authors and a recent SF State graduate. A first-generation college student, Mason can identify with such students, who experience fear and insecurity because of a belief by others -- which can become internalized -- that they won't do as well at math. Mason said she has benefitted personally from using a more empowered posture before taking difficult tests, including math. She believes that adopting a more confident posture could help other first-generation students as well as women entering science and math, who often battle stereotype threat, too.""I always felt insecure about my math abilities even though I excelled at other subjects,"" said Mason, who helped design the experiment in the study. ""You build a relationship with [math] so early -- as early as elementary school. You can carry that negative self-talk throughout your life, impacting your perception of yourself.""Mason said the study results demonstrate a simple way to improve many aspects of life, especially when stress is involved: ""The way we carry ourselves and interact in space influences not only how others perceive us but also how we perceive ourselves.""Story Source:Materials provided by San Francisco State University. ",0.119651,0.146532,0.175288,0.279371,0.163114,0.183983,0.205486,0,0
127,First proof of quantum computer advantage,"For many years, quantum computers were not much more than an idea. Today, companies, governments and intelligence agencies are investing in the development of quantum technology. Robert Kenig, professor for the theory of complex quantum systems at the TUM, in collaboration with David Gosset from the Institute for Quantum Computing at the University of Waterloo and Sergey Bravyi from IBM, has now placed a cornerstone in this promising field.Why should quantum computers be faster?Conventional computers obey the laws of classical physics. They rely on the binary numbers 0 and 1. These numbers are stored and used for mathematical operations. In conventional memory units, each bit -- the smallest unit of information -- is represented by a microscopic dot on a microchip. Each of these dots can hold a charge that determines whether the bit is set to 1 or 0.In a quantum computer, however, a bit can be both 0 and 1 at the same time. This is because the laws of quantum physics allow electrons to be in multiple places at one time. Quantum bits, or qubits, thus exist in multiple overlapping states. This so-called superposition allows quantum computers to perform operations on many values in one fell swoop whereas a single conventional computer typically must execute these operations sequentially. The promise of quantum computing lies in the ability to solve certain problems significantly faster.From conjecture to proofKenig and his colleagues have now conclusively demonstrated the advantage of quantum computers. To this end, they developed a quantum circuit that can solve a specific ""difficult"" algebraic problem. The new circuit has a simple structure: it only performs a fixed number of operations on each qubit. Such a circuit is referred to as having a constant depth. In their work, the researchers prove that the problem at hand cannot be solved using classical constant-depth circuits. They furthermore answer the question of why the quantum algorithm beats any comparable classical circuit: The quantum algorithm exploits the non-locality of quantum physics.Prior to this work, the advantage of quantum computers had neither been proven nor experimentally demonstrated -- notwithstanding that evidence pointed in this direction. One example is Shor's quantum algorithm, which efficiently solves the problem of prime factorization. However, it is merely a complexity-theoretic conjecture that this problem cannot be efficiently solved without quantum computers. It is also conceivable that the right approach has simply not yet been found for classical computers.A step on the road to quantum computingRobert Kenig considers the new results primarily as a contribution to complexity theory. ""Our result shows that quantum information processing really does provide benefits -- without having to rely on unproven complexity-theoretic conjectures,"" he says. Beyond this, the work provides new milestones on the road to quantum computers. Because of its simple structure, the new quantum circuit is a candidate for a near-term experimental realization of quantum algorithms.Further informationThe results have fallen on fertile ground in Munich: A globally acclaimed quantum technology research focus has been established here in recent years, with a new research building for quantum research under construction at the TUM in Garching. In September the TUM, together with the Ludwig-Maximilians-Universitet Menchen (LMU), was awarded the contract for the Cluster of Excellence Munich Center for Quantum Science and Technology (MCQST).Story Source:Materials provided by Technical University of Munich (TUM). ",0.0986886,0.134564,0.137756,0.28417,0.14736,0.171617,0.204954,0,0
128,Researchers develop 3D printed objects that can track and store how they are used,"Cheap and easily customizable, 3-D printed devices are perfect for assistive technology, like prosthetics or ""smart"" pill bottles that can help patients remember to take their daily medications.But these plastic parts don't have electronics, which means they can't monitor how patients are using them.Now engineers at the University of Washington have developed 3-D printed devices that can track and store their own use -- without using batteries or electronics. Instead, this system uses a method called backscatter, through which a device can share information by reflecting signals that have been transmitted to it with an antenna.""We're interested in making accessible assistive technology with 3-D printing, but we have no easy way to know how people are using it,"" said co-author Jennifer Mankoff, a professor in the UW's Paul G. Allen School of Computer Science & Engineering. ""Could we come up with a circuitless solution that could be printed on consumer-grade, off-the-shelf printers and allow the device itself to collect information? That's what we showed was possible in this paper.""The UW team will present its findings Oct. 15 at the ACM Symposium on User Interface Software and Technology in Berlin.Previously the team developed the first 3-D printed objects that connect to Wi-Fi without electronics. These purely plastic devices can measure if a detergent bottle is running low and then automatically order more online.""Using plastic for these applications means you don't have to worry about batteries running out or your device getting wet. That can transform the way we think of computing,"" said senior author Shyam Gollakota, an associate professor in the Allen School. ""But if we really want to transform 3-D printed objects into smart objects, we need mechanisms to monitor and store data.""The researchers tackled the monitoring problem first. In their previous study, their system tracks movement in one direction, which works well for monitoring laundry detergent levels or measuring wind or water speed. But now they needed to make objects that could monitor bidirectional motion like the opening and closing of a pill bottle.""Last time, we had a gear that turned in one direction. As liquid flowed through the gear, it would push a switch down to contact the antenna,"" said lead author Vikram Iyer, a doctoral student in the UW Department of Electrical & Computer Engineering. ""This time we have two antennas, one on top and one on bottom, that can be contacted by a switch attached to a gear. So opening a pill bottle cap moves the gear in one direction, which pushes the switch to contact one of the two antennas. And then closing the pill bottle cap turns the gear in the opposite direction, and the switch hits the other antenna.""Both of the antennas are identical, so the team had to devise a way to decode which direction the cap was moving.""The gear's teeth have a specific sequencing that encodes a message. It's like Morse code,"" said co-author Justin Chan, a doctoral student in the Allen School. ""So when you turn the cap in one direction, you see the message going forward. But when you turn the cap in the other direction, you get a reverse message.""In addition to tracking, for example, pill bottle cap movement, this same method can be used to monitor how people use prosthetics, such as 3-D printed e-NABLE arms. These mechanical hands, which attach at the wrist, are designed to help children with hand abnormalities grasp objects. When children flex their wrists, cables on the hand tighten to make the fingers close. So the team 3-D printed an e-NABLE arm with a prototype of their bidirectional sensor that monitors the hand opening and closing by determining the angle of the wrist.The researchers also wanted to create a 3-D printed object that could store its usage information while out of Wi-Fi range. For this application, they chose an insulin pen that could monitor its use and then signal when it was getting low.""You can still take insulin even if you don't have a Wi-Fi connection,"" Gollakota said. ""So we needed a mechanism that stores how many times you used it. Once you're back in the range, you can upload that stored data into the cloud.""This method requires a mechanical motion, like the pressing of a button, and stores that information by rolling up a spring inside a ratchet that can only move in one direction. Each time someone pushes the button, the spring gets tighter. It can't unwind until the user releases the ratchet, hopefully when in range of the backscatter sensor. Then, as the spring unwinds, it moves a gear that triggers a switch to contact an antenna repeatedly as the gear turns. Each contact is counted to determine how many times the user pressed the button.These devices are only prototypes to show that it is possible for 3-D printed materials to sense bidirectional movement and store data. The next challenge will be to take these concepts and shrink them so that they can be embedded in real pill bottles, prosthetics or insulin pens, Mankoff said.""This system will give us a higher-fidelity picture of what is going on,"" she said. ""For example, right now we don't have a way of tracking if and how people are using e-NABLE hands. Ultimately what I'd like to do with these data is predict whether or not people are going to abandon a device based on how they're using it.""Story Source:Materials provided by University of Washington. Original written by Sarah McQuate. ",0.0734952,0.0709358,0.0971408,0.209295,0.120877,0.135491,0.143408,0,0
129,A new light on significantly faster computer memory devices,"In an article published online today in Science Advances, a team of scientists from Arizona State University's School of Molecular Sciences and Germany offer an explanation of how a particular phase-change memory (PCM) material can work a thousand times faster than current flash computer memory, while being significantly more durable with respect to the number of daily read-writes.PCMs are a form of computer random-access memory (RAM) that store data by altering the state of the matter of the ""bits"" (millions of which make up the device) between liquid, glass and crystal states. PCM technology has the potential to provide inexpensive, high-speed, high-density, high-volume, nonvolatile storage on an unprecedented scale.The basic idea and material were invented by Stanford Ovshinsky in 1975, but applications have lingered due to lack of clarity about how the material can execute the phase changes on such short time scales and technical problems related to controlling the changes with necessary precision. Now high tech companies like Samsung, IBM and Intel are racing to perfect it.The semimetallic material under current study is an alloy of germanium, antimony and tellurium in the ratio of 1:2:4. In this work, the team probes the microscopic dynamics in the liquid state of this PCM using quasi-elastic neutron scattering (QENS) for clues as to what might make the phase changes so sharp and reproducible.On command, the structure of each microscopic bit of this PCM material can be made to change from glass to crystal or from crystal back to glass (through the liquid intermediate) on the time scale of a thousandth of a millionth of a second just by a controlled heat or light pulse, the former now being preferred. In the amorphous or disordered phase, the material has high electrical resistance, the ""off"" state; in the crystalline or ordered phase, its resistance is reduced 1,000 times or more to give the ""on"" state.These elements are arranged in two-dimensional layers between activating electrodes, which can be stacked to give a three-dimensional array with particularly high active site density making it possible for the PCM device to function many times faster than conventional flash memory, while using less power.""The amorphous phases of this kind of material can be regarded as 'semimetallic glasses,'"" explained Shuai Wei, who at the time was conducting postdoctoral research in Regents' Professor Austen Angell's lab as a Humboldt Foundation Fellowship recipient.""Contrary to the strategy in the research field of 'metallic glasses,' where people have made efforts for decades to slow down the crystallization in order to obtain the bulk glass, here we want those semimetallic glasses to crystallize as fast as possible in the liquid, but to stay as stable as possible when in the glass state. I think now we have a promising new understanding of how this is achieved in the PCMs under study.""A deviation from the expectedOver a century ago, Einstein wrote in his PhD thesis that the diffusion of particles undergoing Brownian motion could be understood if the frictional force retarding the motion of a particle was that derived by Stokes for a round ball falling through a jar of honey. The simple equation:D (diffusivity) = kBT/6??rwhere T is the temperature, ? is the viscosity and r is the particle radius, implies that the product D?/T should be constant as T changes, and the surprising thing is that this seems to be true not only for Brownian motion, but also for simple molecular liquids whose molecular motion is known to be anything but that of a ball falling through honey.""We don't have any good explanation of why it works so well, even in the highly viscous supercooled state of molecular liquids until approaching the glass transition temperature, but we do know that there are a few interesting liquids in which it fails badly even above the melting point,"" said Angell.""One of them is liquid tellurium, a key element of the PCM materials. Another is water which is famous for its anomalies, and a third is germanium, a second of the three elements of the GST type of PCM. Now we are adding a fourth, the GST liquid itself. Thanks to the neutron scattering studies proposed and executed by Shuai Wei and his German colleagues, Zach Evenson of the Technical University of Munich and Moritz Stolpe of Saarland University on samples prepared by Shuai with the help of Pierre Lucas from University of Arizona.""Another feature in common for this small group of liquids is the existence of a maximum in-liquid density which is famous for the case of water. A density maximum closely followed during cooling by a metal-to-semiconductor transition is also seen in the stable liquid state of arsenic telluride, (As2Te3), which is first cousin to the antimony telluride (Sb2Te3 ) component of the PCMs. These all lie on the ""Ovshinsky"" line connecting antimony telluride (Sb2Te3) to germanium telluride (GeTe) in the three-component phase diagram.It is the suggestion of Wei and coauthors that when germanium, antimony and tellurium are mixed together in the ratio of 1:2:4 -- or others along Ovshinsky's ""magic"" line -- both the density maxima and the associated metal-to-nonmetal transitions are pushed below the melting point and, concomitantly, the transition becomes much sharper than in other chalcogenide mixtures.Then, as in the much-studied case of supercooled water, the fluctuations associated with the response function extrema should give rise to extremely rapid crystallization kinetics. In all cases, the high temperature state, now the metallic state, is the denser.""This would explain a lot,"" said Angell. ""Above the transition the liquid is very fluid and crystallization is extremely rapid, while below the transition the liquid stiffens up quickly and retains the amorphous, low-conductivity state down to room temperature."" In nanoscopic 'bits,' it then remains indefinitely stable until instructed by a computer-programmed heat pulse to rise instantly to a temperature where, on a nanosecond time scale, it flash crystallizes to the conducting state, the ""on"" state.Lindsay Greer at Cambridge University has made the same argument, couched in terms of a ""fragile-to-strong"" liquid transition.A second, slightly larger heat pulse can take the ""bit"" instantaneously above its melting point and then, with no further heat input and close contact with a cold substrate, it quenches at a rate sufficient to avoid crystallization and is trapped in the semiconducting state, the ""off"" state.""The high resolution of the neutron time-of-flight spectrometer from the Technical University of Munich was necessary to see the details of the atomic movements. Neutron scattering at the Heinz Maier-Leibnitz Zentrum in Garching is the ideal method to make these movements visible,"" said Evenson.Story Source:Materials provided by Arizona State University. ",0.0474184,0.0885938,0.0742572,0.115354,0.0915183,0.0833012,0.111446,0,0
130,Searching an artificial bee colony for real-world results,"Honey bees are not only vitally important pollinators of food crops, their hunt for rich food sources has also proved to be an excellent model for optimizing numerical problems. Now, researchers from Kanazawa University and the University of Toyama have used the intelligent behavior of bees to improve optimization performance in real-world problems.Inspired by the foraging behavior of a honey bee colony, the artificial bee colony (ABC) algorithm is a relatively new population-based algorithm for solving complex optimization problems. In the algorithm, employed bees look for food sources and share the information with onlooker bees, who then choose a food source to exploit. Scout bees conduct random searches to discover new food. The positions of food sources represent possible solutions to an optimization problem. The ABC algorithm has fewer control parameters, a simpler structure, and more convincing performance than other methods, so it has been widely applied in fields such as data mining and vehicle routing. ""However, it takes many iterations to obtain a solution,"" explains Yuki Todo from the Faculty of Electrical and Computer Engineering at Kanazawa University, ""thus, it performs well in exploration but poorly in exploitation.""Therefore, the researchers developed a novel scale-free mechanism to guide the search of the ABC algorithm and verified its performance using real-world problems. They analyzed how the network properties of a scale-free network, namely, the power law distribution and low degree-degree correlation coefficient, influence the optimization process. In each iteration, employed bees (or onlooker bees) were placed onto the nodes of the scale-free network according to the quality of their food sources. Employed bees with high-quality food sources were relocated to high-degree nodes and employed bees with relatively low-quality food sources were relocated to low-degree nodes. Using these rules, the power law distribution property made low-quality employed bees more likely to connect with high-quality employed bees.The scale-free mechanism enables each employed bee to learn more effective information from its neighbors, which improves the exploitation ability of the ABC algorithm. Because the low degree-degree correlation coefficient property can control the information exchange among high-quality employed bees, it stops the information of these bees from taking over the whole population quickly. This helps to maintain population diversity and avoids the problem of the population converging too early. ""During the optimization process, the scale-free ABC algorithm obtains a better balance between exploration and exploitation,"" says Junkai Ji from the Faculty of Engineering at the University of Toyama, ""and also enhances the search ability of other iterative approaches, such as the flower pollination algorithm.""Story Source:Materials provided by Kanazawa University. ",0.110615,0.123047,0.167525,0.274206,0.145383,0.179193,0.182572,0,0
131,Scientists find a way to enhance the performance of quantum computers The method has the potential to solve some of society's biggest challenges,"USC scientists have demonstrated a theoretical method to enhance the performance of quantum computers, an important step to scale a technology with potential to solve some of society's biggest challenges.The method addresses a weakness that bedevils performance of the next-generation computers by suppressing erroneous calculations while increasing fidelity of results, a critical step before the machines can outperform classic computers as intended. Called ""dynamical decoupling,"" it worked on two quantum computers, proved easier and more reliable than other remedies and could be accessed via the cloud, which is a first for dynamical decoupling.The technique administers staccato bursts of tiny, focused energy pulses to offset ambient disturbances that muck sensitive computations. The researchers report they were able to sustain a quantum state up to three times longer than would otherwise occur in an uncontrolled state.""This is a step forward,"" said Daniel Lidar, professor of electrical engineering, chemistry and physics at USC and director of the USC Center for Quantum Information Science and Technology (CQIST). ""Without error suppression, there's no way quantum computing can overtake classical computing.""The results were published today in the journal Physical Review Letters. Lidar is the Viterbi Professor of Engineering at USC and corresponding author of the study; he led a team of researchers at CQIST, which is a collaboration between the USC Viterbi School of Engineering and the USC Dornsife School of Letters, Arts and Sciences. IBM and Bay Area startup Rigetti Computing provided cloud access to their quantum computers.Quantum computers are fast, but fragileQuantum computers have the potential to render obsolete today's super computers and propel breakthroughs in medicine, finance and defense capabilities. They harness the speed and behavior of atoms, which function radically different than silicon computer chips, to perform seemingly impossible calculations.Quantum computing has the potential to optimize new drug therapies, models for climate change and designs for new machines. They can achieve faster delivery of products, lower costs for manufactured goods and more efficient transportation. They are powered by qubits, the subatomic workhorses and building blocks of quantum computing.But qubits are as temperamental as high-performance race cars. They are fast and hi-tech, but prone to error and need stability to sustain computations. When they don't operate correctly, they produce poor results, which limits their capabilities relative to traditional computers. Scientists worldwide have yet to achieve a ""quantum advantage"" -- the point where a quantum computer outperforms a conventional computer on any task.The problem is ""noise,"" a catch-all descriptor for perturbations such as sound, temperature and vibration. It can destabilize qubits, which creates ""decoherence,"" an upset that disrupts the duration of the quantum state, which reduces time a quantum computer can perform a task while achieving accurate results.""Noise and decoherence have a large impact and ruin computations, and a quantum computer with too much noise is useless,"" Lidar explained. ""But if you can knock down the problems associated with noise, then you start to approach the point where quantum computers become more useful than classic computers.""USC research spans multiple quantum computing platformsUSC is the only university in the world with a quantum computer; its 1098-qubit D-Wave quantum annealer specializes in solving optimization problems. Part of the USC-Lockheed Martin Center for Quantum Computing, it's located at USC's Information Sciences Institute. However, the latest research findings were achieved not on the D-Wave machine, but on smaller scale, general-purpose quantum computers: IBM's 16-qubit QX5 and Rigetti's 19-qubit Acorn.To achieve dynamical decoupling (DD), the researchers bathed the superconducting qubits with tightly focused, timed pulses of minute electromagnetic energy. By manipulating the pulses, scientists were able to envelop the qubits in a microenvironment, sequestered -- or decoupled -- from surrounding ambient noise, thus perpetuating a quantum state.""We tried a simple mechanism to reduce error in the machines that turned out to be effective,"" said Bibek Pokharel, an electrical engineering doctoral student at USC Viterbi and first author of the study.The time sequences for the experiments were exceedingly small with up to 200 pulses spanning up to 600 nanoseconds. One-billionth of a second, or a nanosecond, is how long it takes for light to travel one foot.For the IBM quantum computers, final fidelity improved threefold, from 28.9 percent to 88.4 percent. For the Rigetti quantum computer, final fidelity improvement was a more modest 17 percent, from 59.8 to 77.1, according to the study. The scientists tested how long fidelity improvement could be sustained and found that more pulses always improved matters for the Rigetti computer, while there was a limit of about 100 pulses for the IBM computer.Overall, the findings show the DD method works better than other quantum error correction methods that have been attempted so far, Lidar said.""To the best of our knowledge,"" the researchers wrote, ""this amounts to the first unequivocal demonstration of successful decoherence mitigation in cloud-based superconducting qubit platforms ... we expect that the lessons drawn will have wide applicability.""High stakes in the race for quantum supremacyThe quest for quantum computing supremacy is a geopolitical priority for Europe, China, Canada, Australia and the United States. Advantage gained by acquiring the first computer that renders all other computers obsolete would be enormous and bestow economic, military and public health advantages to the winner.Congress is considering two new bills to establish the United States as a leader in quantum computing. In September, the House of Representatives passed the National Quantum Initiative Act to allocate $1.3 billion in five years to spur research and development. It would create a National Quantum Coordination Office in the White House to supervise research nationwide. A separate bill, the Quantum Computing Research Act by Sen. Kamala Harris, D-Calif., directs the Department of Defense to lead a quantum computing effort.""Quantum computing is the next technological frontier that will change the world and we cannot afford to fall behind,"" Harris said in prepared remarks. ""It could create jobs for the next generation, cure diseases and above all else make our nation stronger and safer. ... Without adequate research and coordination in quantum computing, we risk falling behind our global competition in the cyberspace race, which leaves us vulnerable to attacks from our adversaries,"" she said.Story Source:Materials provided by University of Southern California. Original written by Gary Polakovic. ",0.0675354,0.0916234,0.112134,0.208717,0.106041,0.130436,0.148333,0,0
132,Study unlocks full potential of 'supermaterial' graphene Researchers remove silicon contamination from graphene to double its performance,"New research reveals why the ""supermaterial"" graphene has not transformed electronics as promised, and shows how to double its performance and finally harness its extraordinary potential.Graphene is the strongest material ever tested. It's also flexible, transparent and conducts heat and electricity 10 times better than copper.After graphene research won the Nobel Prize for Physics in 2010 it was hailed as a transformative material for flexible electronics, more powerful computer chips and solar panels, water filters and bio-sensors. But performance has been mixed and industry adoption slow.Now a study published in Nature Communications identifies silicon contamination as the root cause of disappointing results and details how to produce higher performing, pure graphene.The RMIT University team led by Dr Dorna Esrafilzadeh and Dr Rouhollah Ali Jalili inspected commercially-available graphene samples, atom by atom, with a state-of-art scanning transition electron microscope.""We found high levels of silicon contamination in commercially available graphene, with massive impacts on the material's performance,"" Esrafilzadeh said.Testing showed that silicon present in natural graphite, the raw material used to make graphene, was not being fully removed when processed.""We believe this contamination is at the heart of many seemingly inconsistent reports on the properties of graphene and perhaps many other atomically thin two-dimensional (2D) materials ,"" Esrafilzadeh said.""Graphene was billed as being transformative, but has so far failed to make a significant commercial impact, as have some similar 2D nanomaterials. Now we know why it has not been performing as promised, and what needs to be done to harness its full potential.""The testing not only identified these impurities but also demonstrated the major influence they have on performance, with contaminated material performing up to 50% worse when tested as electrodes.""This level of inconsistency may have stymied the emergence of major industry applications for graphene-based systems. But it's also preventing the development of regulatory frameworks governing the implementation of such layered nanomaterials, which are destined to become the backbone of next-generation devices,"" she said.The two-dimensional property of graphene sheeting, which is only one atom thick, makes it ideal for electricity storage and new sensor technologies that rely on high surface area.This study reveals how that 2D property is also graphene's Achilles' heel, by making it so vulnerable to surface contamination, and underscores how important high purity graphite is for the production of more pure graphene.Using pure graphene, researchers demonstrated how the material performed extraordinarily well when used to build a supercapacitator, a kind of super battery.When tested, the device's capacity to hold electrical charge was massive. In fact, it was the biggest capacity so far recorded for graphene and within sight of the material's predicted theoretical capacity.In collaboration with RMIT's Centre for Advanced Materials and Industrial Chemistry, the team then used pure graphene to build a versatile humidity sensor with the highest sensitivity and the lowest limit of detection ever reported.These findings constitute a vital milestone for the complete understanding of atomically thin two-dimensional materials and their successful integration within high performance commercial devices.""We hope this research will help to unlock the exciting potential of these materials.""Story Source:Materials provided by RMIT University. ",0.12374,0.20153,0.169379,0.20344,0.18028,0.177352,0.187073,0,0
133,Computer hackers could be thwarted by new 'deception consistency' method Researchers working to make it harder for attackers to know when a system begins to detect and deceive a bad actor,"Can you deceive a deceiver? That's the question that computer scientists at Binghamton University, State University of New York have recently been exploring.Assistant Professor of Computer Science Guanhua Yan and PhD student Zhan Shu are looking at how to make cyber deception a more effective tool against malicious hackers.Their study was inspired by the 2013 Target data breach that affected 41 million consumers and cost Target $18.5 million, and the 2017 Equifax hack which exposed the personal information of 147.7 million Americans. Both of these were what can be classified as Advanced Persistent Threats (APTs).Yan and Shu wanted to improve the ways in which hackers are countered when attempting APTs, so they focused on refining existing cyber deception tools.Cyber deception is a responsive technique that puts malicious hackers into a fake environment once the system detects a hack in progress.In the abstract of the study, the researchers wrote that ""the main objective of our work is to ensure deception consistency: when the attackers are trapped, they can only make observations that are consistent with what they have seen already so that they cannot recognize the deceptive environment.""They found that this focus on only showing attackers what has been seen before increases the efficiency of the deception.""The issue is that sometimes cyber deception uses what are called 'bad lies' that are easily recognizable by the attacker. Once the deception is realized, the attacker can adjust and work around this form of protection,"" said Yan.The deception consistency method that Yan and Shu created was tested on college students who had recently completed a cybersecurity course. The students were asked to act like malicious hackers, with some ending up in the deceptive environment.The researchers found that because the deceptive environment was consistent with what students had previously seen, most did not realize they had entered into the deception.""It was clear that most students were simply guessing whether they had entered into the deceptive environment or not. They couldn't quite tell the difference when we used our consistent model,"" said Yan.Although the deception consistency may make it more difficult for APT attackers to recognize the deception, the researchers were clear that their proposed method is not a cure-all for things like what happened to Target and Equifax.""It may not hold up against more advanced attacks, but we will continue to improve the effectiveness of the deception-based methods against a variety of attack scenarios,"" said Yan.Yan and Shu published ""Ensuring Deception Consistency for FTP Services Hardened against Advanced Persistent Threats"" as part of the recent Proceedings of the 5th ACM Workshop on Moving Target Defense.Story Source:Materials provided by Binghamton University. ",0.145967,0.163301,0.207301,0.350816,0.177836,0.215285,0.23557,0,0
134,Artificial intelligence may help reduce gadolinium dose in MRI,"Researchers are using artificial intelligence to reduce the dose of a contrast agent that may be left behind in the body after MRI exams, according to a study being presented today at the annual meeting of the Radiological Society of North America (RSNA).Gadolinium is a heavy metal used in contrast material that enhances images on MRI. Recent studies have found that trace amounts of the metal remain in the bodies of people who have undergone exams with certain types of gadolinium. The effects of this deposition are not known, but radiologists are working proactively to optimize patient safety while preserving the important information that gadolinium-enhanced MRI scans provide.""There is concrete evidence that gadolinium deposits in the brain and body,"" said study lead author Enhao Gong, Ph.D., researcher at Stanford University in Stanford, Calif. ""While the implications of this are unclear, mitigating potential patient risks while maximizing the clinical value of the MRI exams is imperative.""Dr. Gong and colleagues at Stanford have been studying deep learning as a way to achieve this goal. Deep learning is a sophisticated artificial intelligence technique that teaches computers by examples. Through use of models called convolutional neural networks, the computer can not only recognize images but also find subtle distinctions among the imaging data that a human observer might not be capable of discerning.To train the deep learning algorithm, the researchers used MR images from 200 patients who had received contrast-enhanced MRI exams for a variety of indications. They collected three sets of images for each patient: pre-contrast scans, done prior to contrast administration and referred to as the zero-dose scans; low-dose scans, acquired after 10 percent of the standard gadolinium dose administration; and full-dose scans, acquired after 100 percent dose administration.The algorithm learned to approximate the full-dose scans from the zero-dose and low-dose images. Neuroradiologists then evaluated the images for contrast enhancement and overall quality.Results showed that the image quality was not significantly different between the low-dose, algorithm-enhanced MR images and the full-dose, contrast-enhanced MR images. The initial results also demonstrated the potential for creating the equivalent of full-dose, contrast-enhanced MR images without any contrast agent use.These findings suggest the method's potential for dramatically reducing gadolinium dose without sacrificing diagnostic quality, according to Dr. Gong.""Low-dose gadolinium images yield significant untapped clinically useful information that is accessible now by using deep learning and AI,"" he said.Now that the researchers have shown that the method is technically possible, they want to study it further in the clinical setting, where Dr. Gong believes it will ultimately find a home.Future research will include evaluation of the algorithm across a broader range of MRI scanners and with different types of contrast agents.""We're not trying to replace existing imaging technology,"" Dr. Gong said. ""We're trying to improve it and generate more value from the existing information while looking out for the safety of our patients.""Story Source:Materials provided by Radiological Society of North America. ",0.13008,0.154687,0.203122,0.35463,0.191461,0.196321,0.234785,0,0
135,Putting a face on a cell surface,"On the cell surface, anchored in the cell membrane, a wide array of proteins perform functions, which are vital for the cell. These proteins, collectively known as the surfaceome, are a cell's antennae to the outside world, sending and receiving signals that enable it to communicate with other cells. They also serve as gate keepers for molecules, transporting materials into and out of the cell, and enable cells to attach themselves to other cells or structures.The medical field has a keen interest in the surfaceome and uses it to treat diseases. Some two-thirds of known medications achieve their effect by slotting precisely into a specific surface protein and triggering a cellular signalling cascade.Revamping the paradigmYet as practical and simple as that sounds, the current ""one drug-one target"" approach has a serious drawback: a given target surface protein can be found on many different cell types. Many drugs therefore attack not only the target cell type, but other cell types too. This is one reason why many drugs have unwanted side effects.To find an alternative, researchers in Professor Bernd Wollscheid's group at the ETH Institute of Molecular Systems Biology and the Department of Health Sciences and Technology (D-HEST) are investigating the distribution and organisation of proteins on cell surfaces. Their goal is to take a fresh approach hoping to identify more suitable targets for drug intervention.To date, the variety of the surfaceome in human cells has hardly been researched. In an initial step to remedy this, doctoral student Damaris Bausch-Fluck and bioinformatician Ulrich Goldmann, both members of Wollscheid's group, worked together to create an in silico inventory of these molecules. Their study was recently published in the journal PNAS.The researchers made use of the benefits of machine learning in their work: First, they taught the computer to compile properties and features of surface proteins by feeding it with protein data collected in previous experiments. The computer turned presence of cell surface specific features into a score and then calculated a surfaceome score of the 20,000 or so proteins found in humans. Finally he predicted above which score a protein is likely to appear at the cell surface.Predictions largely correctIn the end, the computer-generated inventory encompassed about 2,900 different proteins. In other words, out of all the proteins in a human cell, one in seven could appear on the cell surface. The newly developed algorithm achieved a high degree of accuracy in its predictions: a subsequent review of the experiment revealed that the computer was correct in more than 93 percent of cases.In addition, the researchers were able to show that the number of surface proteins varies widely by cell type. Using publicly accessible data on cell lines, they were able to show that immune cells have only about 500 different surface proteins, whereas lung and brain cells have more than 1,000. But the cells that showed the greatest variety in surface proteins were primary stem cells, with about 1,800 different kinds. ""Cell lines have a less complex surface proteome than cells that have just been removed from body tissue, since the interactions that cell lines undergo are less diverse,"" Wollscheid says. The ETH researchers have stored their findings in a public database.Organisation and distribution are keyIdentifying the incidence and diversity of proteins in the surfaceome does not automatically lead to new drugs, however. ""Surface proteins are not evenly distributed, but are instead grouped like 'islands' in functional units. These units have varying combinations of proteins and thus varying functions,"" says Damaris Bausch-Fluck, who is lead author not only of the study mentioned above, but also of a review article for the journal Current Opinion in Chemical Biology. In the latter, she and her colleagues present different options for how the surfaceome might be organised at the nanoscale and how the ""protein islands"" can influence cell function and signalling.The following analogy may help explain how surface proteins are arranged and their functions: if a protein were as large as a person, then the surface area of a cell would be three times the area of New York City's Central Park. A baseball field in the park would be one of these functional islands upon which proteins -- the players -- group themselves for a particular function -- in this case, the baseball game. If the players leave the field, they lose their ability to function in this way.Subsequent alterations change functionsFurthermore, each cell type creates surface proteins in different concentrations. Once they have been made, surface proteins can be altered, for example being ""decorated"" with sugar molecules or phosphates. As a result, they take on other functions that they would not otherwise perform without this molecular ""garnish."" This is why Wollscheid doesn't talk about proteins, but rather about proteoforms. The underlying structure may always be the same, but subsequent alterations to the original protein produce multiple different forms with various functions.Islands as therapeutic targets""In developing new drugs, it's crucial to know about functional units with multiple proteins and bear them in mind as potential targets,"" Wollscheid says. For example, instead of seeking out just one specific protein, antibody drugs could be designed to dock with several different targets simultaneously in order to neutralise a functional island. Since islands are much more specific to a given cell type, such as cancer cells, it is conceivable that this approach means diseases can be treated in a more targeted way and with fewer side effects.""At present we're working to explore the many blank areas that remain on our maps of the surface of various cell types -- almost like the pioneers of old who set off for unknown continents,"" Wollscheid explains.Story Source:Materials provided by ETH Zurich. ",0.0767097,0.161888,0.141354,0.191356,0.125511,0.124626,0.159295,0,0
136,"Putting food-safety detection in the hands of consumers Simple, scalable wireless system uses the RFID tags on billions of products to sense contamination","MIT Media Lab researchers have developed a wireless system that leverages the cheap RFID tags already on hundreds of billions of products to sense potential food contamination -- with no hardware modifications needed. With the simple, scalable system, the researchers hope to bring food-safety detection to the general public.Food safety incidents have made headlines around the globe for causing illness and death nearly every year for the past two decades. Back in 2008, for instance, 50,000 babies in China were hospitalized after eating infant formula adulterated with melamine, an organic compound used to make plastics, which is toxic in high concentrations. And this April, more than 100 people in Indonesia died from drinking alcohol contaminated, in part, with methanol, a toxic alcohol commonly used to dilute liquor for sale in black markets around the world.The researchers' system, called RFIQ, includes a reader that senses minute changes in wireless signals emitted from RFID tags when the signals interact with food. For this study they focused on baby formula and alcohol, but in the future, consumers might have their own reader and software to conduct food-safety sensing before buying virtually any product. Systems could also be implemented in supermarket back rooms or in smart fridges to continuously ping an RFID tag to automatically detect food spoilage, the researchers say.The technology hinges on the fact that certain changes in the signals emitted from an RFID tag correspond to levels of certain contaminants within that product. A machine-learning model ""learns"" those correlations and, given a new material, can predict if the material is pure or tainted, and at what concentration. In experiments, the system detected baby formula laced with melamine with 96 percent accuracy, and alcohol diluted with methanol with 97 percent accuracy.""In recent years, there have been so many hazards related to food and drinks we could have avoided if we all had tools to sense food quality and safety ourselves,"" says Fadel Adib, an assistant professor at the Media Lab who is co-author on a paper describing the system, which is being presented at the ACM Workshop on Hot Topics in Networks. ""We want to democratize food quality and safety, and bring it to the hands of everyone.""The paper's co-authors include: postdoc and first author Unsoo Ha, postdoc Yunfei Ma, visiting researcher Zexuan Zhong, and electrical engineering and computer science graduate student Tzu-Ming Hsu.The power of ""weak coupling""Other sensors have also been developed for detecting chemicals or spoilage in food. But those are highly specialized systems, where the sensor is coated with chemicals and trained to detect specific contaminations. The Media Lab researchers instead aim for broader sensing. ""We've moved this detection purely to the computation side, where you're going to use the same very cheap sensor for products as varied as alcohol and baby formula,"" Adib says.RFID tags are stickers with tiny, ultra-high-frequency antennas. They come on food products and other items, and each costs around three to five cents. Traditionally, a wireless device called a reader pings the tag, which powers up and emits a unique signal containing information about the product it's stuck to.The researchers' system leverages the fact that, when RFID tags power up, the small electromagnetic waves they emit travel into and are distorted by the molecules and ions of the contents in the container. This process is known as ""weak coupling."" Essentially, if the material's property changes, so do the signal properties.A simple example of feature distortion is with a container of air versus water. If a container is empty, the RFID will always respond at around 950 megahertz. If it's filled with water, the water absorbs some of the frequency, and its main response is around only 720 megahertz. Feature distortions get far more fine-grained with different materials and different contaminants. ""That kind of information can be used to classify materials ... [and] show different characteristics between impure and pure materials,"" Ha says.In the researchers' system, a reader emits a wireless signal that powers the RFID tag on a food container. Electromagnetic waves penetrate the material inside the container and return to the reader with distorted amplitude (strength of signal) and phase (angle).When the reader extracts the signal features, it sends those data to a machine-learning model on a separate computer. In training, the researchers tell the model which feature changes correspond to pure or impure materials. For this study, they used pure alcohol and alcohol tainted with 25, 50, 75, and 100 percent methanol; baby formula was adulterated with a varied percentage of melamine, from 0 to 30 percent.""Then, the model will automatically learn which frequencies are most impacted by this type of impurity at this level of percentage,"" Adib says. ""Once we get a new sample, say, 20 percent methanol, the model extracts [the features] and weights them, and tells you, 'I think with high accuracy that this is alcohol with 20 percent methanol.'""Broadening the frequenciesThe system's concept derives from a technique called radio frequency spectroscopy, which excites a material with electromagnetic waves over a wide frequency and measures the various interactions to determine the material's makeup.But there was one major challenge in adapting this technique for the system: RFID tags only power up at a very tight bandwidth wavering around 950 megahertz. Extracting signals in that limited bandwidth wouldn't net any useful information.The researchers built on a sensing technique they developed earlier, called two-frequency excitation, which sends two frequencies -- one for activation, and one for sensing -- to measure hundreds more frequencies. The reader sends a signal at around 950 megahertz to power the RFID tag. When it activates, the reader sends another frequency that sweeps a range of frequencies from around 400 to 800 megahertz. It detects the feature changes across all these frequencies and feeds them to the reader.""Given this response, it's almost as if we have transformed cheap RFIDs into tiny radio frequency spectroscopes,"" Adib says.Because the shape of the container and other environmental aspects can affect the signal, the researchers are currently working on ensuring the system can account for those variables. They are also seeking to expand the system's capabilities to detect many different contaminants in many different materials.""We want to generalize to any environment,"" Adib says. ""That requires us to be very robust, because you want to learn to extract the right signals and to eliminate the impact of the environment from what's inside the material.""Story Source:Materials provided by Massachusetts Institute of Technology. Original written by Rob Matheson. ",0.0650972,0.111182,0.117643,0.18023,0.136854,0.106383,0.140755,0,0
137,Study opens route to ultra-low-power microchips Innovative approach to controlling magnetism could lead to next-generation memory and logic devices,"A new approach to controlling magnetism in a microchip could open the doors to memory, computing, and sensing devices that consume drastically less power than existing versions. The approach could also overcome some of the inherent physical limitations that have been slowing progress in this area until now.Researchers at MIT and at Brookhaven National Laboratory have demonstrated that they can control the magnetic properties of a thin-film material simply by applying a small voltage. Changes in magnetic orientation made in this way remain in their new state without the need for any ongoing power, unlike today's standard memory chips, the team has found.The new finding is being reported today in the journal Nature Materials, in a paper by Geoffrey Beach, a professor of materials science and engineering and co-director of the MIT Materials Research Laboratory; graduate student Aik Jun Tan; and eight others at MIT and Brookhaven.Spin doctorsAs silicon microchips draw closer to fundamental physical limits that could cap their ability to continue increasing their capabilities while decreasing their power consumption, researchers have been exploring a variety of new technologies that might get around these limits. One of the promising alternatives is an approach called spintronics, which makes use of a property of electrons called spin, instead of their electrical charge.Because spintronic devices can retain their magnetic properties without the need for constant power, which silicon memory chips require, they need far less power to operate. They also generate far less heat -- another major limiting factor for today's devices.But spintronic technology suffers from its own limitations. One of the biggest missing ingredients has been a way to easily and rapidly control the magnetic properties of a material electrically, by applying a voltage. Many research groups around the world have been pursuing that challenge.Previous attempts have relied on electron accumulation at the interface between a metallic magnet and an insulator, using a device structure similar to a capacitor. The electrical charge can change the magnetic properties of the material, but only by a very small amount, making it impractical for use in real devices. There have also been attempts at using ions instead of electrons to change magnetic properties. For instance, oxygen ions have been used to oxidize a thin layer of magnetic material, causing a extremely large changes in magnetic properties. However, the insertion and removal of oxygen ions causes the material to swell and shrink, causing mechanical damage that limits the process to just a few repetitions -- rendering it essentially useless for computational devices.The new finding demonstrates a way around that, by using hydrogen ions instead of the much larger oxygen ions used in previous attempts. Since the hydrogen ions can zip in and out very easily, the new system is much faster and provides other significant advantages, the researchers say.Because the hydrogen ions are so much smaller, they can enter and exit from the crystalline structure of the spintronic device, changing its magnetic orientation each time, without damaging the material. In fact, the team has now demonstrated that the process produces no degradation of the material after more than 2,000 cycles. And, unlike oxygen ions, hydrogen can easily pass through metal layers, which allows the team to control properties of layers deep in a device that couldn't be controlled in any other way.""When you pump hydrogen toward the magnet, the magnetization rotates,"" Tan says. ""You can actually toggle the direction of the magnetization by 90 degrees by applying a voltage -- and it's fully reversible."" Since the orientation of the poles of the magnet is what is used to store information, this means it is possible to easily write and erase data ""bits"" in spintronic devices using this effect.Beach, whose lab discovered the original process for controlling magnetism through oxygen ions several years ago, says that initial finding unleashed widespread research on a new area dubbed ""magnetic ionics,"" and now this newest finding has ""turned on its end this whole field.""Essentially, Beach explains, he and his team are ""trying to make a magnetic analog of a transistor,"" which can be turned on and off repeatedly without degrading its physical properties.Just add waterThe discovery came about, in part, through serendipity. While experimenting with layered magnetic materials in search of ways of changing their magnetic behavior, Tan found that the results of his experiments varied greatly from day to day for reasons that were not apparent. Eventually, by examining all the conditions during the different tests, he realized that the key difference was the humidity in the air: The experiment worked better on humid days compared to dry ones. The reason, he eventually realized, was that water molecules from the air were being split up into oxygen and hydrogen on the charged surface of the material, and while the oxygen escaped to the air, the hydrogen became ionized and was penetrating into the magnetic device -- and changing its magnetism.The device the team has produced consists of a sandwich of several thin layers, including a layer of cobalt where the magnetic changes take place, sandwiched between layers of a metal such as palladium or platinum, and with an overlay of gadolinium oxide, and then a gold layer to connect to the driving electrical voltage.The magnetism gets switched with just a brief application of voltage and then stays put. Reversing it requires no power at all, just short-circuiting the device to connect its two sides electrically, whereas a conventional memory chip requires constant power to maintain its state. ""Since you're just applying a pulse, the power consumption can go way down,"" Beach says.The new devices, with their low power consumption and high switching speed, could eventually be especially useful for devices such mobile computing, Beach says, but the work is still at an early stage and will require further development.""I can see lab-based prototypes within a few years or less,"" he says. Making a full working memory cell is ""quite complex"" and might take longer, he says.Story Source:Materials provided by Massachusetts Institute of Technology. ",0.0372991,0.0715561,0.0650877,0.106419,0.0690578,0.09827,0.0803315,0,0
138,Quantum step forward in protecting communications from hackers,"Researchers at the University of York have shown that a new quantum-based procedure for distributing secure information along communication lines could be successful in preventing serious security breaches.Securing highly sensitive information, such as hospital records and bank details, is a major challenge faced by companies and organisation throughout the world.Standard communication systems are vulnerable to hacks, where encrypted information can be intercepted and copied. It is currently possible for hackers to make a copy of transmitted information, but it would not be possible to read it without a method of breaking the encryption that protects it.This means that information might be secure for a period of time, but there is no guarantee that it would be secure forever, as supercomputers in development could potentially decipher particular encryptions in the future.Researchers at York investigated a prototype, based on the principles of quantum mechanics, that has the potential to side-step the vulnerabilities of current communications, but also allow information to be secure in the future.Dr Cosmo Lupo, from the University of York's Department of Computer Science, said: ""Quantum mechanics has come a long way, but we are still faced with significant problems that have to be overcome with further experimentation.""One such problem is that a hacker can attack the electronic devices used for information transmission by jamming the detectors that are used to collect and measure the photons that carries information.""Such an attack is powerful because we assume that a given device works according to its technical specifications and will therefore perform its job. If a hacker is able to attack a detector and change the way it works, then the security is unavoidably compromised.""""The principles of quantum mechanics, however, allows for communication security even without making assumptions on how the electronic devices will work. By removing these assumptions we pay the price of lowering the communication rate, but gain in improving the security standard.""Instead of relying on possibly compromised electronic components at the point at which information needs to be detected and read, the researchers found that if the untrusted detectors existed at a separate point in the communications -- somewhere between the sender and receiver -- the communication was far more secure.The detector would receive a combination of two signals, one from the sender and one from the receiver. The detector would only be able to read the result of this combined signal, but not its component parts.Dr Lupo said: ""In our work, not only have we provided a first rigorous mathematical proof that this 'detector- independent' design works, but we have also considered a scheme that is compatible with existing optical fibre communication networks.""In principle our proposal can allow for the exchange of unbreakable codes across the internet without major changes in the actual infrastructure.""We are still at prototype stage, but by finding ways to reduce the cost of these systems, we are that much closer to making quantum communications a reality.""The research is funded by the EPSRC Quantum Communications hub and by Quantum Innovation Center Qubiz, and published in the journal Physical Review Letters.Story Source:Materials provided by University of York. ",0.0633598,0.074271,0.109926,0.213713,0.120106,0.106764,0.133857,0,0
139,Quantum transfer at the push of a button,"Data transmission is the backbone of the modern information society, on both the large and small scale. On the internet, data are exchanged between computers all over the world, most often using fibre optic cables. Inside a computer, on the other hand, information has to be shuttled back and forth between different processors. A reliable exchange of data is also of great importance for the new quantum information technologies that are currently being developed -- but at the same time it is also fiendishly difficult. At the ETH in Zurich, a team of physicists led by Andreas Wallraff of the Laboratory for Solid State Physics has now succeeded in transmitting quantum information, at the push of button and with high fidelity, between two quantum bits roughly a metre apart. Their results are published in the scientific journal Nature this week.Flying quantum bitsThe main peculiarity of quantum information technologies, such as quantum computers and quantum cryptography, is the use of quantum bits or ""qubits"" as the elementary unit of information. Differently from classical bits, qubits cannot just have the value 0 or 1, but also take on so-called superposition states. On the one hand, this results in the possibility to build extremely powerful computers that make use of those superposition states to perform calculations much more efficiently and faster than classical computers. On the other hand, those states are also very sensitive and cannot be transmitted simply using conventional techniques. The problem is that the state of a stationary qubit first has to be transformed into a so-called ""flying"" qubit, for instance a photon, and then back into another stationary qubit. A few years ago researchers were able to transmit the quantum state of an atom in this way. Wallraff and his co-workers have now succeeded in realizing such a transmission also from one superconducting solid-state qubit to another one some distance away.To do so, the physicists connected two superconducting qubits using a coaxial cable of the kind that is also used to connect to antenna terminals. The quantum state of the first qubit, which is defined by the number of superconducting electron pairs (also known as Cooper pairs) contained in it, was first transferred to a microwave photon of a resonator using very precisely controlled microwave pulses. From that resonator the photon could then fly through the coaxial cable to a second resonator, inside of which microwave pulses, once more, transferred its quantum state onto the the second qubit. Similar experiments were recently carried out at Yale University.Deterministic rather than probabilistic""The important point of our method is that the transmission of the quantum state is deterministic, which means that it works at the push of a button,"" Philipp Kurpiers, a PhD student in Wallraff's lab, emphasizes. In some earlier experiments a transfer of quantum states could already be realized, but that transmission was probabilistic: sometimes it worked, but most of the time it didn't. A successful transmission could, for instance, be signalled by a ""heralding photon."" Whenever the transmission hadn't worked, one simply tried again. In that way, the effective quantum transmission rate was, of course, strongly reduced. For practical applications, therefore, deterministic methods such as the one now demonstrated at ETH are clearly advantageous.""Our transmission rate for quantum states is among the highest ever realized, and at 80% our transmission fidelity is very good in the first realization of the protocol,"" says Andreas Wallraff. Using their technique, the researchers were also able to create a quantum mechanical entanglement between the qubits as many as 50,000 times per second. The transmission procedure itself took less than a millionth of a second, which means that there is quite a bit of room for improvement in the transmission rate. Quantum mechanical entanglement creates an intimate link between two quantum objects even across large distances, a feature that is used for cryptography or quantum teleportation.Quantum transfer for quantum computersAs a next step, the researchers want to try to use two qubits each as transmitter and receiver, which makes entanglement swapping between the qubit pairs possible. Such a process is useful for larger quantum computers, which are supposed to be built in the next few years. So far, they only consist of a handful of qubits, but when trying to build larger computers, already for a few hundred qubits one will have to worry about how to connect them most effectively in order to exploit the advantages of a quantum computer in the best possible way.Much like clusters of single computers used today, quantum computer modules could then be connected together using Wallraff's technique. The transmission distance, which is currently about a metre, could certainly be increased. Wallraff and his colleagues recently demonstrated that an extremely cold, and thus superconducting, cable could transmit photons over distances of several tens of metres with very little loss. Wiring together a quantum computing centre, therefore, seems to be quite feasible.Story Source:Materials provided by ETH Zurich. ",0.0775797,0.0994871,0.0956929,0.198487,0.134234,0.13697,0.142284,0,0
140,"Trust is good, quantum trickery is better ","In quantum cryptography, the laws of quantum mechanics are exploited to send messages with higher security than is possible in conventional cryptographic schemes based on classical physical phenomena. In principle, quantum communication enables absolute security -- that is, no adversary can intercept messages or tinker with them. But in practice such unconditional security is not realizable. One main route for an unauthorized person to 'listen in' is to manipulate in advance the communication devices that will be used later. Writing in Nature Communications, Rotem Arnon-Friedman and colleagues prove that there exist quantum-cryptographic protocols that ensure nearly optimal security even if the devices are manipulated, and that such device-independent quantum cryptography should be possible with current quantum technology.Device-independent quantum cryptography is the 'gold standard' of quantum communication, as the advantages of quantum cryptography over its classical counterpart can be realized without having to worry whether the device used can be trusted or not. This is an appealing prospect, but so far device-independent quantum cryptography has been mostly a theoretical construct, with experimental requirements that are not achievable under realistic conditions.Therefore the appeal of the new work of Arnon-Friedman, a PhD student in the group of Prof. Renato Renner in the Institute of Theoretical Physics at ETH Zurich, and co-workers in the US, France and the Czech Republic. The team developed a new theoretical concept, dubbed 'entropy accumulation', and applied it to quantum cryptography. They find that any attack strategy, no matter how complex, can be decomposed into a sequence of simple steps. This is very helpful for security proofs, which are notoriously hard because every possible attack strategy that an adversary may conceive has to be taken into account. With their new approach, Arnon-Friedman and her colleagues now prove, for the first time, the security of device-independent quantum cryptography in a regime that is attainable with state-of-the-art quantum technology, thus paving the way to practical realization of such schemes.Story Source:Materials provided by ETH Zurich Department of Physics. ",0.121179,0.146641,0.159388,0.336436,0.192713,0.180878,0.230208,0,0
141,Quantum computing with molecules for a quicker search of unsorted databases Grover's quantum algorithm successfully implemented -- superposition manipulated and read out electrically,"Scrapbooks or social networks are collections of mostly unsorted data. The search for single elements in very large data volumes, i.e. for the needle in the data haystack, is extremely complex for classical computers. Scientists of Karlsruhe Institute of Technology (KIT) have now quantum mechanically implemented and successfully executed Grover's algorithm, a process for the quick finding of a search element in unsorted databases.A universal quantum computer still is a vision. Special quantum systems that promise to solve certain tasks more quickly than a classical computer, however, are already playing an important role in science. To reliably find a certain element in unsorted data, a conventional computer has to run through all search elements successively in the most unfavorable case. A quantum system with an implemented Grover's search algorithm quadratically accelerates search.Research teams headed by Professors Wolfgang Wernsdorfer and Mario Ruben of KIT, together with scientists of the Institut Neel (Grenoble), have succeeded in doing this: The scientists applied Grover's algorithm to a molecular magnet and, thus, created a quantum system, whose task is the rapid finding of search elements in unsorted data.In their latest research project, they demonstrated feasibility of a quick search for a small database of four elements. ""But this method can be implemented in any quantum system with many, non-equidistant energy levels, which opens up the way towards a universal quantum search algorithm,"" Professor Ruben says.The scientists implemented Grover's algorithm in a molecular magnet that was subjected to superposition with specially designed microwaves. Superposition is a quantum effect, in which a particle assumes different states at the same time. Upon execution of the quantum operations, a single-molecule transistor read out the search results. An animation illustrates this process.Wolfgang Wernsdorfer, Professor of KIT's Physikalisches Institut and Institute of Nanotechnology (INT), emphasizes that the quantum states were manipulated at very low temperatures using electric fields exclusively. ""That is why we hope that this technology can be integrated into current electronic devices,"" Wernsdorfer adds.The customized molecule transistor was synthesized by Mario Ruben's team at INT and KIT's Institute for Inorganic Chemistry. In its center, a terbium atom with a pronounced magnetic moment, a spin, is located. The terbium is surrounded by organic molecules that shield it against external impacts.Story Source:Materials provided by Karlsruher Institut fer Technologie (KIT). ",0.111363,0.153195,0.145149,0.313307,0.16172,0.182408,0.215352,0,0
142,"New attacks on graphics processors endanger user privacy Hackers can use the graphics processing unit to spy on web activity, steal passwords, and break into cloud-based applications","Computer scientists at the University of California, Riverside have revealed for the first time how easily attackers can use a computer's graphics processing unit, or GPU, to spy on web activity, steal passwords, and break into cloud-based applications.Marlan and Rosemary Bourns College of Engineering computer science doctoral student Hoda Naghibijouybari and post-doctoral researcher Ajaya Neupane, along with Associate Professor Zhiyun Qian and Professor Nael Abu-Ghazaleh, reverse engineered a Nvidia GPU to demonstrate three attacks on both graphics and computational stacks, as well as across them. The group believes these are the first reported general side channel attacks on GPUs.All three attacks require the victim to first acquire a malicious program embedded in a downloaded app. The program is designed to spy on the victim's computer.Web browsers use GPUs to render graphics on desktops, laptops, and smart phones. GPUs are also used to accelerate applications on the cloud and data centers. Web graphics can expose user information and activity. Computational workloads enhanced by the GPU include applications with sensitive data or algorithms that might be exposed by the new attacks.GPUs are usually programmed using application programming interfaces, or APIs, such as OpenGL. OpenGL is accessible by any application on a desktop with user-level privileges, making all attacks practical on a desktop. Since desktop or laptop machines by default come with the graphics libraries and drivers installed, the attack can be implemented easily using graphics APIs.The first attack tracks user activity on the web. When the victim opens the malicious app, it uses OpenGL to create a spy to infer the behavior of the browser as it uses the GPU. Every website has a unique trace in terms of GPU memory utilization due to the different number of objects and different sizes of objects being rendered. This signal is consistent across loading the same website several times and is unaffected by caching.The researchers monitored either GPU memory allocations over time or GPU performance counters and fed these features to a machine learning based classifier, achieving website fingerprinting with high accuracy. The spy can reliably obtain all allocation events to see what the user has been doing on the web.In the second attack, the authors extracted user passwords. Each time the user types a character, the whole password textbox is uploaded to GPU as a texture to be rendered. Monitoring the interval time of consecutive memory allocation events leaked the number of password characters and inter-keystroke timing, well-established techniques for learning passwords.The third attack targets a computational application in the cloud. The attacker launches a malicious computational workload on the GPU which operates alongside the victim's application. Depending on neural network parameters, the intensity and pattern of contention on the cache, memory and functional units differ over time, creating measurable leakage. The attacker uses machine learning-based classification on performance counter traces to extract the victim's secret neural network structure, such as number of neurons in a specific layer of a deep neural network.The researchers reported their findings to Nvidia, who responded that they intend to publish a patch that offers system administrators the option to disable access to performance counters from user-level processes. They also shared a draft of the paper with the AMD and Intel security teams to enable them to evaluate their GPUs with respect to such vulnerabilities.In the future the group plans to test the feasibility of GPU side channel attacks on Android phones.The paper,""Rendered Insecure: GPU Side Channel Attacks are Practical,"" was presented at the ACM SIGSAC Conference on Computer and Communications Security October 15-19, 2018, in Toronto, Canada. The research was supported by National Science Foundation Grant CNS-1619450.Story Source:Materials provided by University of California - Riverside. Original written by Holly Ober. ",0.0713071,0.0909677,0.135045,0.430364,0.149254,0.125065,0.199144,0,0
143,One step closer to complex quantum teleportation Novel complex quantum entanglement generated in the laboratory for the first time,"For future technologies such as quantum computers and quantum encryption, the experimental mastery of complex quantum systems is inevitable. Scientists from the University of Vienna and the Austrian Academy of Sciences have succeeded in making another leap. While physicists around the world are trying to increase the number of two-dimensional systems, so-called qubits, researchers around Anton Zeilinger are breaking new ground. They pursue the idea to use more complex quantum systems as qubits and thus can increase the information capacity with the same number of particles. The developed methods and technologies could in the future enable the teleportation of complex quantum systems. The results of their work ""Experimental Greenberger-Horne-Zeilinger Entanglement Beyond QuBits"" is published recently in the journal Nature Photonics.Similar to bits in conventional computers, QuBits are the smallest unit of information in quantum systems. Big companies like Google and IBM are competing with research institutes around the world to produce an increasing number of entangled QuBits. The clear motivation is to develop a functioning quantum computer. A research group at the University of Vienna and the Austrian Academy of Sciences, however, is pursuing a new path to increase the information capacity of complex quantum systems.The idea behind it is simple: instead of just increasing the number of particles involved, the complexity of each system is increased. ""The special thing about our experiment is that for the first time it entangles three photons beyond the conventional two-dimensional nature,"" explains Manuel Erhard, first author of the study. For this purpose, the Viennese physicists use quantum systems which have more than two possible states -- in this particular case, the angular momentum of individual light particles. These individual photons now have a higher information capacity than QuBits. However, the entanglement of these light particles turned out to be difficult on a conceptual level. The researchers overcame this challenge with a ground-breaking idea: a computer algorithm that autonomously searches for an experimental implementation.With the help of the computer algorithm Melvin an experimental setup to produce this type of entanglement has been uncovered. At first this was still very complex, but at least it worked in principle. After some simplifications, physicists still faced major technological challenges. The team was able to solve these with state-of-the-art laser technology and a specially developed multi-port. ""This multi-port is the heart of our experiment and combines the three photons so that they are entangled in three dimensions,"" explains Manuel Erhard.The peculiar property of the three-photon entanglement in three dimensions allows for experimental investigation of new fundamental questions about the behaviour of quantum systems. In addition, the results of this work could also have a significant impact on future technologies, such as quantum teleportation. ""I think the methods and technologies that we developed in this publication allow us to teleport a higher proportion of the total quantum information of a single photon, which could be important for quantum communication networks,"" Anton Zeilinger points out into the future of possible applications.Story Source:Materials provided by University of Vienna. ",0.0792898,0.106302,0.127338,0.199487,0.119453,0.137452,0.149569,0,0
144,Computer Security: Preventing attacks made possible by Meltdown/Spectre 'DAWG' system breaks up cache memory more efficiently to defend against 'timing attacks',"In January the technology world was rattled by the discovery of Meltdown and Spectre, two major security vulnerabilities in the processors that can be found in virtually every computer on the planet.Perhaps the most alarming thing about these vulnerabilities is that they didn't stem from normal software bugs or physical CPU problems. Instead, they arose from the architecture of the processors themselves -- that is, the millions of transistors that work together to execute operations.""These attacks fundamentally changed our understanding of what's trustworthy in a system, and force us to re-examine where we devote security resources,"" says Ilia Lebedev, a PhD student at MIT's Computer Science and Artificial Intelligence Laboratory (CSAIL). ""They've shown that we need to be paying much more attention to the microarchitecture of systems.""Lebedev and his colleagues believe that they've made an important new breakthrough in this field, with an approach that makes it much harder for hackers to cash in on such vulnerabilities. Their method could have immediate applications in cloud computing, especially for fields like medicine and finance that currently limit their cloud-based features because of security concerns.With Meltdown and Spectre, hackers exploited the fact that operations all take slightly different amounts of time to execute. To use a simplified example, someone who's guessing a PIN might first try combinations ""1111"" through ""9111."" If the first eight guesses take the same amount of time, and ""9111"" takes a nanosecond longer, then that one most likely has at least the ""9"" right, and the attacker can then start guessing ""9111"" through ""9911,"" and so on and so forth.An operation that's especially vulnerable to these so-called ""timing attacks"" is accessing memory. If systems always had to wait for memory before doing the next step of an action, they'd spend much of their time sitting idle.To keep performance up, engineers employ a trick: they give the processor the power to execute multiple instructions while it waits for memory -- and then, once memory is ready, discards the ones that weren't needed. This is called ""speculative execution.""While it pays off in performance speed, it also creates new security issues. Specifically, the attacker could make the processor speculatively execute some code to read a part of memory it shouldn't be able to. Even if the code fails, it could still leak data that the attacker can then access.A common way to try to prevent such attacks is to split up memory so that it's not all stored in one area. Imagine an industrial kitchen shared by chefs who all want to keep their recipes secret. One approach would be to have the chefs set up their work on different sides -- that's essentially what happens with the ""Cache Allocation Technology"" (CAT) that Intel started using in 2016. But such a system is still quite insecure, since one chef can get a pretty good idea of others' recipes by seeing which pots and pans they take from the common area.In contrast, the MIT CSAIL team's approach is the equivalent of building walls to split the kitchen into separate spaces, and ensuring that everyone only knows their own ingredients and appliances. (This approach is a form of so-called ""secure way partitioning""; the ""chefs,"" in the case of cache memory, are referred to as ""protection domains."")As a playful counterpoint to Intel's CAT system, the researchers dubbed their method ""DAWG,"" which stands for ""Dynamically Allocated Way Guard."" (The ""dynamic"" part means that DAWG can split the cache into multiple buckets whose size can vary over time.)Lebedev co-wrote a new paper about the project with lead author Vladimir Kiriansky and MIT professors Saman Amarasinghe, Srini Devadas and Joel Emer. They will present their findings next week at the annual IEEE/ACM International Symposium on Microarchitecture (MICRO) in Fukuoka City, Japan.""This paper dives into how to fully isolate one program's side-effects from percolating through to another program through the cache,"" says Mohit Tiwari, an assistant professor at the University of Texas at Austin who was not involved in the project. ""This work secures a channel that's one of the most popular to use for attacks.""In tests, the team also found that the system was comparable with CAT on performance. They say that DAWG requires very minimal modifications to modern operating systems.""We think this is an important step forward in giving computer architects, cloud providers and other IT professionals a better way to efficiently and dynamically allocate resources,"" says Kiriansky, a PhD student at CSAIL. ""It establishes clear boundaries for where sharing should and should not happen, so that programs with sensitive information can keep that data reasonably secure.""The team is quick to caution that DAWG can't yet defend against all speculative attacks. However, they have experimentally demonstrated that it is a foolproof solution to a broad range of non-speculative attacks against cryptographic software.Lebedev says that the growing prevalence of these types of attacks demonstrates that, contrary to popular tech-CEO wisdom, more information-sharing isn't always a good thing.""There's a tension between performance and security that's come to a head for a community of architecture designers that have always tried to share as much as possible in as many places as possible,"" he says. ""On the other hand, if security was the only priority, we'd have separate computers for every program we want to run so that no information could ever leak, which obviously isn't practical. DAWG is part of a growing body of work trying to reconcile these two opposing forces.""It's worth recognizing that the sudden attention on timing attacks reflects the paradoxical fact that computer security has actually gotten a lot better in the last 20 years.""A decade ago software wasn't written as well as it is today, which means that other attacks were a lot easier to perform,"" says Kiriansky. ""As other aspects of security have become harder to carry out, these microarchitectural attacks have become more appealing, though they're still fortunately just a small piece in an arsenal of actions that an attacker would have to take to actually do damage.""The team is now working to improve DAWG so that it can stop all currently known speculative-execution attacks. In the meantime, they're hopeful that companies such as Intel will be interested in adopting their idea -- or others like it -- to minimize the chance of future data breaches.""These kinds of attacks have become a lot easier thanks to these vulnerabilities,"" says Kiriansky. ""With all the negative PR that's come up, companies like Intel have the incentives to get this right. The stars are aligned to make an approach like this happen.""Story Source:Materials provided by Massachusetts Institute of Technology, CSAIL. ",0.0365604,0.0491135,0.0808009,0.299006,0.0778062,0.0865355,0.128753,0,0
145,A quantum gate between atoms and photons may help in scaling up quantum computers,"The quantum computers of the future will be able to perform computations that cannot be done on today's computers. These may likely include the ability to crack the encryption that is currently used for secure electronic transactions, as well as the means to efficiently solve unwieldy problems in which the number of possible solutions increases exponentially. Research in the quantum optics lab of Prof. Barak Dayan in the Weizmann Institute of Science may be bringing the development of such computers one step closer by providing the ""quantum gates"" that are required for communication within and between such quantum computers.In contrast with today's electronic bits that can only exist in one of two states -- zero or one -- quantum bits known as qubits can also be in states that correspond to both zero and one at the same time. This is called quantum superposition, and it gives qubits an edge as a computer made of them could perform numerous computations in parallel.There is just one catch: The state of quantum superposition state can exist only as long as it is not observed or measured in any way by the outside world; otherwise all the possible states collapse into a single one. This leads to contradicting requirements: For the qubits to exist in several states at once they need to be well isolated, yet at the same time they need to interact and communicate with many other qubits. That is why, although several labs and companies around the world have already demonstrated small-scale quantum computers with a few dozen qubits, the challenge of scaling up these to the desired scale of millions of qubits remains a major scientific and technological hurdle.One promising solution is using isolated modules with small, manageable numbers of qubits, which can communicate between them when needed with optical links. The information stored in a material qubit (e.g. a single atom or ion) would then be transferred to a ""flying qubit"" -- a single particle of light called a photon. This photon can be sent through optical fibers to a distant material qubit and transfer its information without letting the environment sense the nature of that information. The challenge in creating such a system is that single photons carry extremely small amounts of energy, and the minuscule systems comprising material qubits generally do not interact strongly with such weak light.Dayan's quantum optics lab in the Weizmann Institute of Science is one of the few groups worldwide that are focused entirely on attacking this scientific challenge. Their experimental setup has single atoms coupled to unique micron-scale silica resonators on chips; and photons are sent directly to these through special optical fibers. In previous experiments Dayan and his group had demonstrated the ability of their system to function as a single-photon activated switch, and also a way to ""pluck"" a single photon from a flash of light. In the present study, reported in Nature Physics, Dayan and his team succeeded -- for the first time -- to create a logic gate in which a photon and an atom automatically exchange the information they carry.""The photon carries one qubit, and the atom is a second qubit,"" says Dayan. ""Each time the photon and the atom meet they exchange the qubits between them automatically and simultaneously, and the photon then continues on its way with the new bit of information. In quantum mechanics, in which information cannot be copied or erased, this swapping of information is in fact the basic unit of reading and writing -- the ""native"" gate of quantum communication.""This type of logic gate -- a SWAP gate -- can be used to exchange qubits both within and between quantum computers. As this gate needs no external control fields or management system, it can enable the construction of the quantum equivalent of very large-scale integration (VLSI) networks. ""The SWAP gate we demonstrated is applicable to photonic communication between all types of matter-based qubits -- not only atoms,"" says Dayan. ""We therefore believe that it will become an essential building-block in the next generation of quantum computing systems.""Story Source:Materials provided by Weizmann Institute of Science. ",0.0678199,0.111766,0.106092,0.208039,0.145352,0.109779,0.146068,0,0
146,How unsecured medical record systems and medical devices put patient lives at risk,"A team of physicians and computer scientists at the University of California has shown that it is easy to modify medical test results remotely by attacking the connection between hospital laboratory devices and medical record systems.These types of attacks might be more likely used against high-profile targets, such as heads of state and celebrities, than against the general public. But they could also be used by a nation-state to cripple the United States' medical infrastructure.The researchers from UC San Diego and UC Davis detailed their findings Aug. 9 at the Black Hat 2018 conference in Las Vegas, where they staged a demonstration of the attack. Dubbed Pestilence, the attack is solely proof-of-concept and will not be released to the general public. While the vulnerabilities the researchers exploited are not new, this is the first time that a research team has shown how they could be exploited to compromise patient health.These vulnerabilities arise from the standards used to transfer patient data within hospital networks, known as the Health Level Seven standards, or HL7. Essentially the language that allows all devices and systems in a medical facility to communicate, HL7 was developed in the 1970s and has remained untouched by many of the cybersecurity advances made in the last four decades.Implementation of the standards on aging medical equipment by personnel with little or no cybersecurity training has led to untold amounts of patient data circulating in an unsecure fashion. Specifically, the data are transmitted as unencrypted plain text on networks that do not require any passwords or other forms of authentication.Data hacking in hospitals has been in the news in recent years. But researchers want to draw attention to how that data, once compromised, could be manipulated. ""Healthcare is distinct from other sectors in that the manipulation of critical infrastructure has the potential to directly impact human life, whether through direct manipulation of devices themselves or through the networks which connect them,"" the researchers write in a white paper released in conjunction with their Black Hat demonstration.The vulnerabilities and methodologies used to create the Pestilence tool have been previously published. The innovation here is combining computer science know-how and clinicians' knowledge to exploit weaknesses in the HL7 standard to negatively impact the patient care process.The team includes Dr. Christian Dameff, an emergency physician and clinical informatics fellow, and Maxwell Bland, a master's student in computer science, both at UC San Diego, and Dr. Jeffrey Tully, an anesthesiology resident at the UC Davis Medical Center. Physicians need to be able to trust that their data are accurate, Tully said. ""As a physician, I aim to educate my colleagues that the implicit trust we place in the technologies and infrastructure we use to care for our patients may be misplaced, and that an awareness of and vigilance for these threat models is critical for the practice of medicine in the 21st century,"" he said.Securing data against manipulation is imperative. ""We are talking about this because we are trying to secure healthcare devices and infrastructure before medical systems experience a major failure,"" Dameff said. ""We need to fix this now.""Researchers outline countermeasures medical systems can take to protect themselves against these types of attack.The Pestilence toolResearchers used what's called a ""man in the middle attack,"" where a computer inserts itself between the laboratory machine and the medical records system. Bland, the UC San Diego computer science graduate student, automated the attack so it could tackle large amounts of data remotely. Researchers did not infiltrate an existing hospital system, of course. Instead, they built a testbed comprising medical laboratory testing devices, computers and servers. This allowed the team to run tests, such as blood and urine analysis, intercept the test results, change them and then send the modified information to a medical records system.Researchers took normal blood test results and modified them to make it look like the patient was suffering from diabetic ketoacidosis, or DKA, a severe complication of diabetes. This diagnosis would cause physicians to prescribe an insulin drip, which in a healthy patient could lead to a coma, or even death.Researchers also modified normal blood test results to look like the patient had extremely low potassium. Treatment with a potassium IV on a healthy patient would cause a heart attack, which would likely be fatal.Countermeasures The researchers detail a number of steps that hospitals and government agencies can take to protect medical infrastructure in their Black Hat white paper.Hospitals need to improve their security practices. Specifically, medical record systems and medical devices need to be password-protected and secured behind a firewall. Each device and system on the network needs to be restricted to communicating with only one server, to limit hackers' opportunities to penetrate inside hospital networks. This is called ""network segmenting"" and is the best way to protect medical infrastructure, said Bland, the computer science graduate student at the Jacobs School of Engineering at UC San Diego.Researchers also would like to raise awareness of a new standard that could replace HL7: the Fast Healthcare Interoperability Resource, or FHIR, would allow for encrypted communications inside hospital networks.Hospital IT staff need to be made aware of cybersecurity issues and trained to put in place defenses against potential attacks, researchers said. For example, IT personnel need to know how to configure networks and servers to support encryption. Researchers point a 2017 report from a Health and Human Services task force stating that 80 percent of hospital IT personnel are not trained in cybersecurity.In addition, cybersecurity needs to become part of the FDA approval process for healthcare devices, the researchers said. Manufacturers should be encouraged to adopt the newest and most secure operating systems. For example, today, many medical devices still run on Windows XP, an operating system that was released in 2001 and is no longer supported by Microsoft -- meaning that vulnerabilities are not fixed. These devices can't be easily upgraded as they would need to be taken offline, which would compromise patient care. In addition, some devices are too told to be upgraded.""Working together, we are able to raise awareness of security vulnerabilities that have the potential to impact patient care and then develop solutions to remediate them,"" UC Davis' Tully said.Story Source:Materials provided by University of California - San Diego. ",0.0483798,0.0736919,0.12873,0.27174,0.0921274,0.101427,0.149356,0,0
147,These new techniques expose your browsing history to attackers,"Security researchers at UC San Diego and Stanford have discovered four new ways to expose Internet users' browsing histories. These techniques could be used by hackers to learn which websites users have visited as they surf the web.The techniques fall into the category of ""history sniffing"" attacks, a concept dating back to the early 2000s. But the attacks demonstrated by the researchers at the 2018 USENIX Workshop on Offensive Technologies (WOOT) in Baltimore can profile or 'fingerprint' a user's online activity in a matter of seconds, and work across recent versions of major web browsers.All of the attacks the researchers developed in their WOOT 2018 paper worked on Google Chrome. Two of the attacks also worked on a range of other browsers, from Mozilla Firefox to Microsoft Edge, as well various security-focused research browsers. The only browser which proved immune to all of the attacks is the Tor Browser, which doesn't keep a record of browsing history in the first place.""My hope is that the severity of some of our published attacks will push browser vendors to revisit how they handle history data, and I'm happy to see folks from Mozilla, Google, and the broader World Wide Web Consortium (W3C) community already engage in this,"" said Deian Stefan, an assistant professor in computer science at the Jacobs School of Engineering at UC San Diego and the paper's senior author.""History sniffing"": smelling out your trail across the webMost Internet users are by now familiar with ""phishing;"" cyber-criminals build fake websites which mimic, say, banks, to trick them into entering their login details. The more the phisher can learn about their potential victim, the more likely the con is to succeed. For example, a Chase customer is much more likely to be fooled when presented with a fake Chase login page than if the phisher pretends to be Bank of America.After conducting an effective history sniffing attack, a criminal could carry out a smart phishing scheme, which automatically matches each victim to a faked page corresponding to their actual bank. The phisher preloads the attack code with their list of target banking websites, and conceals it in, for example, an ordinary-looking advertisement. When a victim navigates to a page containing the attack, the code runs through this list, testing or 'sniffing' the victim's browser for signs that it's been used to visit each target site. When one of these sites tests positive, the phisher could then redirect their victim to the corresponding faked version.The faster the attack, the longer the list of target sites an attacker can 'sniff' in a reasonable amount of time. The fastest history sniffing attacks have reached rates of thousands of URLs tested per second, allowing attackers to quickly put together detailed profiles of web surfers' online activity. Criminals could put this sensitive data to work in a number of ways besides phishing: for example, by blackmailing users with embarrassing or compromising details of their browsing histories.History sniffing can also be deployed by legitimate, yet unscrupulous, companies, for purposes like marketing and advertising. A 2010 study from UC San Diego documented widespread commercial abuse of previously known history sniffing attack techniques, before these were subsequently fixed by browser vendors.""You had internet marketing firms popping up, hawking pre-packaged, commercial history sniffing 'solutions', positioned as analytics tools,"" said Michael Smith, a computer science Ph.D. student at UC San Diego and the paper's lead author. The tools purported to offer insights into the activity of their clients' customers on competitors' websites, as well as detailed profiling information for ad targeting -- but at the expense of those customers' privacy.""Though we don't believe this is happening now, similar spying tools could be built today by abusing the flaws we discovered,"" said Smith.New attacksThe attacks the researchers developed, in the form of JavaScript code, cause web browsers to behave differently based on whether a website had been visited or not. The code can observe these differences -- for example, the time an operation takes to execute or the way a certain graphic element is handled -- to collect the computer's browsing history. To design the attacks, researchers exploited features that allow programmers to customize the appearance of their web page -- controlling fonts, colors, backgrounds, and so forth -- using Cascading Style Sheets (CSS), as well as a cache meant to improve to performance of web code.The researchers' four attacks target flaws in relatively new browser features. For example, one attack takes advantage of a feature added to Chrome in 2017, dubbed the ""CSS Paint API,"" which lets web pages provide custom code for drawing parts of their visual appearance. Using this feature, the attack measures when Chrome re-renders a picture linked to a particular target website URL, in a way invisible to the user. When a re-render is detected, it indicates that the user has previously visited the target URL. ""This attack would let an attacker check around 6,000 URLs a second and develop a profile of a user's browsing habits at an alarming rate,"" said Fraser Brown, a Ph.D. student at Stanford, who worked closely with Smith.Though Google immediately patched this flaw -- the most egregious of the attacks that the researchers developed -- the computer scientists describe three other attacks in their WOOT 2018 paper that, put together, work not only on Chrome but Firefox, Edge, Internet Explorer, but on Brave as well. The Tor Browser is the only browser known to be totally immune to all the attacks, as it intentionally avoids storing any information about a user's browsing history.As new browsers add new features, these kinds of attacks on privacy are bound to resurface.A proposed defenseThe researchers propose a bold fix to these issues: they believe browsers should set explicit boundaries controlling how users' browsing histories are used to display web pages from different sites. One major source of information leakage was the mechanism which colors links either blue or purple depending on whether the user has visited their destination pages, so that, for example, someone clicking down a Google search results page can keep their place. Under the researchers' model, clicking links on one website (e.g., Google) wouldn't affect the color of links appearing on another website (e.g., Facebook). Users could potentially grant exceptions to certain websites of their choosing. The researchers are prototyping this fix and evaluating the trade-offs of such a privacy-conscious browser.Story Source:Materials provided by University of California - San Diego. ",0.0564078,0.0722207,0.120738,0.322748,0.116881,0.0989208,0.153376,0,0
148,"Publicizing a firm's security levels may strengthen security over time, study finds ","Cyberattacks grow in prominence each and every day; in fact, 2017 was the worst year to-date for data breaches, with the number of cyber incidents targeting businesses nearly doubling from 2016 to 2017.Now, new research from the UBC Sauder School of Business has quantified the security levels of more than 1,200 Pan-Asian companies in order to determine whether increased awareness of one's security levels leads to improved defense levels against cybercrime.The study found that when cyberattacks were less likely to directly harm a company, organizations were unlikely to prioritize security improvements. Firms were more likely to fix issues related to spam emails originating from their compromised computers, but failed to act when they were found to host phishing websites on their servers. Most of the firms with phishing websites are actually hosting service providers.The researchers conducted a randomized field experiment on organizations in Hong Kong, China, Singapore, Macau, Malaysia and Taiwan -- which were chosen for their significant economic development as well as rapid adoption of technologies. The experiment evaluated each organization's preparedness against two distinct security issues: spam emissions and phishing website hosting. Spam usually consists of unsolicited bulk messages sent out by compromised ""zombie"" computers controlled by cyber attackers, while phishing refers to fraudulently obtaining sensitive information, such as passwords and credit card details for malicious reasons.""For companies hosting phishing websites, there were fewer incentives to crack down on the sites since they were operated by paying customers and the sites failed to negatively impact the company itself,"" explains Gene Moo Lee, study co-author and assistant professor of Accounting and Information Systems at the UBC Sauder School of Business.The researchers developed and assigned an information security score, similar to the idea of Moody's and Standard and Poor's credit ratings, to each organization. The score can be used as an indicator of each organization's security vulnerabilities.The security results from each company were then published online. According to Lee, publicizing firms' security levels not only leads to greater transparency, but it could also be used to strengthen their security over time. In addition, organizations with poor performance could face greater pressure from their customers and a loss of reputation.""The ever-increasing number of cyberattacks motivated my co-authors and I to explore a more effective way to enhance the security awareness of organizations and the general public,"" explains Lee. ""By establishing a ranking scheme of firms against online scams, we hope this will heighten firms' awareness to address suboptimal security issues.""For Lee, cybersecurity is an international concern that needs to be managed more effectively. ""Many organizations don't understand the threats posed by emerging, sophisticated cyberattacks and usually adopt a wait-and-see approach in security investments until a huge security incident affects them significantly,"" he said. ""Our hope with this research is that companies improve their security levels to prevent the potential of cyberattacks from happening in the first place. And, ultimately, the goal of our research is to provide insights for cybersecurity policy makers.""""Information Disclosure and Security Policy Design: A Large-Scale Randomization Experiment in Pan-Asia"" was recently presented at the Workshop on Economics of Information Security. It was co-authored by Yun-Sik Choi and Andrew B. Whinston from the University at Texas in Austin, Shu He from the University of Connecticut, and Yunhui Zhuang and Alvin Chung Man Leung from the City University of Hong Kong.Story Source:Materials provided by University of British Columbia - Sauder School of Business. ",0.0820405,0.0961703,0.149983,0.302768,0.131459,0.141018,0.175746,0,0
149,Computer security: Processor vulnerability can be exploited to access memory,"Two international teams of security researchers have uncovered Foreshadow, a new variant of the hardware vulnerability Meltdown announced earlier in the year, that can be exploited to bypass Intel Processors' secure regions to access memory and data.The vulnerability affects Intel's Software Guard Extension (SGX) technology, a new feature in modern Intel CPUs which allows computers to protect users' data in a secure 'fortress' even if the entire system falls under an attacker's control.The two teams that independently and concurrently discovered Foreshadow have published a report on the vulnerability, which causes the complete collapse of the SGX ecosystem and compromises users' data.""SGX can be used by developers to enable secure browsing to protect fingerprints used in biometric authentication, or to prevent content being downloaded from video streaming services,"" Dr Yuval Yarom from CSIRO's Data61 and the University of Adelaide's School of Computer Science said.""Foreshadow compromises the confidentiality of the 'fortresses', where this sensitive information is stored and once a single fortress is breached, the whole system becomes vulnerable.""The researchers reported these findings to Intel earlier this year, and the company's own analysis into the causes of the vulnerability led to the discovery of a new variant of Foreshadow, called Foreshadow-NG which affects nearly all Intel servers used in cloud computing.Foreshadow-NG is theoretically capable of bypassing the earlier fixes introduced to mitigate against Meltdown and Spectre, potentially re-exposing millions of computers globally to attacks.""The SGX feature is widely used by developers and businesses globally, and this opens them up to a data breach that can potentially affect their customers as well,"" Dr Yarom said.""Intel will need to revoke the encryption keys used for authentication in millions of computers worldwide to mitigate the impact of Foreshadow.""Intel's discovery of the Foreshadow-NG variant is even more severe but will require further research to gauge the full impact of the vulnerability.""Intel has since released patches, updates and guidelines to resolve both Foreshadow and Foreshadow-NG.Researchers have not yet tested if similar flaws exist in processors of other manufacturers.Adrian Turner, CEO of CSIRO's Data61 said this is a significant discovery that shows the far-reaching impact of Meltdown and Spectre and reinforces the role of research for discovering and preventing flaws.""Experts like Dr Yarom play a vital role in finding vulnerabilities, responsibly disclosing them and developing trustworthy systems to keep critical infrastructure secure,"" Mr Turner said.""Data61 has also joined the RISC-V Foundation's security task group which aims to prevent the likes of Meltdown and Spectre from occurring again.""The two teams that discovered Foreshadow include:For more information, visit: https://foreshadowattack.comStory Source:Materials provided by University of Adelaide. ",0.0852772,0.109197,0.182109,0.354002,0.154868,0.158453,0.201188,0,0
150,Security gaps identified in Internet protocol 'IPsec',"In collaboration with colleagues from Opole University in Poland, researchers at Horst Gertz Institute for IT Security (HGI) at Ruhr-Universitet Bochum (RUB) have demonstrated that the Internet protocol ""IPsec"" is vulnerable to attacks. The Internet Key Exchange protocol ""IKEv1,"" which is part of the protocol family, has vulnerabilities that enable potential attackers to interfere with the communication process and intercept specific information.The research results are published by Dennis Felsch, Martin Grothe and Prof Dr Jerg Schwenk from the Chair for Network and Data Security at RUB as well as Adam Czubak and Marcin Szymanek from Opole University on 16 August 2018 at the Usenix Security Symposium.Secure and encrypted communicationAs an enhancement of Internet protocol (IP), ""IPsec"" has been developed to ensure cryptographically secure communication via publicly accessible resp. insecure networks, such as the Internet, by using encryption and authentication mechanisms. This type of communication is often relevant for enterprises whose employees operate from decentralised workplaces -- for example as sales reps or from home office -- and have to access company resources. The protocol can, moreover, be utilised to set up virtual private networks, or VPNs.In order to enable an encrypted connection with ""IPsec,"" both parties must authenticate and define shared keys that are necessary for communication. Automated key management and authentication, for example via passwords or digital signatures, can be conducted via the Internet Key Exchange protocol ""IKEv1.""""Even though the protocol is considered obsolete and a newer version, namely IKEv2, has been long available in the market, we see in real-life applications that it is still being implemented in operating systems and still enjoys great popularity, even on newer devices,"" explains Dennis Felsch. But it is precisely this protocol that has vulnerabilities, as the researchers found out during their analysis.Bleichenbacher's attack successfulIn the course of their project, the researchers attacked the encryption-based logon mode of ""IPsec"" by deploying the so-called Bleichenbacher's attack, which had been invented in 1998. Its principle is: errors are deliberately incorporated into an encoded message, which is then repeatedly sent to a server. Based on the server's replies to the corrupted message, an attacker can gradually draw better and better conclusions about the encrypted contents.""Thus, the attacker approaches the target step by step until he reaches his goal,"" says Martin Grothe and adds: ""It is like a tunnel with two ends. It's enough if one of the two parties is vulnerable. Eventually, the vulnerability permits the attacker to interfere with the communication process, to assume the identity of one of the communication partners, and to actively commit data theft.""Bleichenbacher's attack proved effective against the hardware of four network equipment providers. The affected parties were Clavister, Zyxel, Cisco, and Huawei. All four manufacturers have been notified and have now eliminated the security gaps.Passwords under scrutinyIn addition to the encryption-base logon mode, the researchers have also been looking into password-based login. ""Authentication via passwords is carried out with hash values, which are similar to a fingerprint. During our attack, we demonstrated that both IKEv1 and the current IKEv2 present vulnerabilities and may be easily attacked -- especially if the password is weak. Accordingly, a highly complex password provides the best protection if IPsec is deployed in this mode,"" concludes Martin Grothe. The vulnerability was also communicated to the Computer Emergency Response Team (CERT), as it coordinates the response to actual IT security incidents and provided assistance to the researchers as they notified the industry about the vulnerability.All-clear for users and network equipment providersThe identified Bleichenbacher vulnerability is not a bug in the standard but rather an implementation error that can be avoided -- it all depends on how manufacturers integrate the protocol in their devices. Moreover, the attacker has to enter the network first, before he can do anything. Nevertheless, the researchers' successful attack has demonstrated that established protocols such as ""IPsec"" still include the Bleichenbacher gap that makes them potentially vulnerable to attack.Story Source:Materials provided by Ruhr-University Bochum. ",0.0610733,0.076321,0.11476,0.358232,0.130656,0.114498,0.160631,0,0
151,Holding law enforcement accountable for electronic surveillance System encourages government transparency using cryptography on a public log of wiretap requests,"When the FBI filed a court order in 2016 commanding Apple to unlock the San Bernandino shooter's iPhone, the news made headlines across the globe. Yet every day there are tens tens of thousands of of other court orders asking tech companies to turn over Americans' private data. Many of these orders never see the light of day, leaving a whole privacy-sensitive aspect of government power immune to judicial oversight and lacking in public accountability.To protect the integrity of ongoing investigations, these data requests require some secrecy: companies usually aren't allowed to inform individual users that they're being investigated, and the court orders themselves are also temporarily hidden from the public.In many cases, though, charges never actually materialize, and the sealed orders usually end up forgotten by the courts that issue them, resulting in a severe accountability deficit.To address this issue, researchers from MIT's Computer Science and Artificial Intelligence Laboratory (CSAIL) and Internet Policy Research Initiative (IPRI) have proposed a new cryptographic system to improve the accountability of government surveillance while still maintaining enough confidentiality for the police to do their jobs.""While certain information may need to stay secret for an investigation to be done properly, some details have to be revealed for accountability to even be possible,"" says CSAIL graduate student Jonathan Frankle, one of the lead authors of a new paper about the system, which they've dubbed ""AUDIT"" (""Accountability of Unreleased Data for Improved Transparency""). ""This work is about using modern cryptography to develop creative ways to balance these conflicting issues.""Many of AUDIT's technical methods were developed by one of its co-authors, MIT professor Shafi Goldwasser. AUDIT is designed around a public ledger on which government officials share information about data requests. When a judge issues a secret court order or a law enforcement agency secretly requests data from a company, they have to make an iron-clad promise to make the data request public later in the form of what's known as a ""cryptographic commitment."" If the courts ultimately decide to release the data, the public can rest assured that the correct documents were released in full. If the courts decide not to, then that refusal itself will be made known.AUDIT can also be used to demonstrate that actions by law-enforcement agencies are consistent with what a court order actually allows. For example, if a court order leads to the FBI going to Amazon to get records about a specific customer, AUDIT can prove that the FBI's request is above board using a cryptographic method called ""zero-knowledge proofs."" First developed in the 1980s by Goldwasser and other researchers, these proofs counterintuitively make it possible to prove that surveillance is being conducted properly without revealing any specific information about the surveillance.AUDIT's approach builds on privacy research in accountable systems led by paper co-author Daniel J. Weitzner, director of IPRI.""As the volume of personal information expands, better accountability for how that information is used is essential for maintaining public trust,"" says Weitzner. ""We know that the public is worried about losing control over their personal data, so building technology that can improve actual accountability will help increase trust in the Internet environment overall.""As a further effort to improve accountability, statistical information from the data can also be aggregated so that that the extent of surveillance can be studied at a larger scale. This enables the public to ask all sorts of tough questions about how their data is being shared. What kinds of cases are most likely to prompt court orders? How many judges issued more than 100 orders in the past year, or more than 10 requests to Facebook this month? Frankle says the team's goal is to establish a set of reliable, court-issued transparency reports, to supplement the voluntary reports that companies put out.""We know that the legal system struggles to keep up with the complexity of increasing sophisticated uses of personal data,"" says Weitzner. ""Systems like AUDIT can help courts keep track of how the police conduct surveillance and assure that they are acting within the scope of the law, without impeding legitimate investigative activity.""Importantly, the team developed its aggregation system using an approach called multi-party computation (MPC), which allows courts to disclose relevant information without actually revealing their internal workings or data to one another. The current state-of-the-art MPC would normally be too slow to run on the data of hundreds of federal judges across the entire court system, so the team took advantage of the court system's natural hierarchy of lower and higher courts to design a particular variant of MPC that would scale efficiently for the federal judiciary.""[AUDIT] represents a plausible way, both legally and technologically, for increasing public accountability through modern cryptographic proofs of integrity,"" says Eli Ben-Sasson, a professor in the computer science department at the Technion Israel Institute of Technology.According to Frankle, AUDIT could be applied to any process in which data must be both kept secret but also subject to public scrutiny. For example, clinical trials of new drugs often involve private information, but also require enough transparency to assure regulators and the public that proper testing protocols are being observed.""It's completely reasonable for government officials to want some level of secrecy, so that they can perform their duties without fear of interference from those who are under investigation,"" Frankle says. ""But that secrecy can't be permanent. People have a right to know if their personal data has been accessed, and at a higher level, we as a public have the right to know how much surveillance is going on.""Next the team plans to explore what could be done to AUDIT so that it can handle even more complex data requests -- specifically, by looking at tweaking the design via software engineering. They also are exploring the possibility of partnering with specific federal judges to develop a prototype for real-world use.""My hope is that, once this proof of concept becomes reality, court administrators will embrace the possibility of enhancing public oversight while preserving necessary secrecy,"" says Stephen William Smith, a federal magistrate judge who has written extensively about government accountability. ""Lessons learned here will undoubtedly smooth the way towards greater accountability for a broader class of secret information processes, which are a hallmark of our digital age.""Frankle co-wrote the paper with Goldwasser, Weitzner, CSAIL PhD graduate Sunoo Park and undergraduate Daniel Shaar, The paper will be presented at the USENIX Security conference in Baltimore August 15 to 17.IPRI team members will also discuss related surveillance issues in more detail at upcoming workshops for both USENIX and this month's International Cryptography Conference (Crypto 2018) in Santa Barbara.The research was supported by IPRI, National Science Foundation, the Defense Advanced Research Projects Agency and the Simons Foundation.Story Source:Materials provided by Massachusetts Institute of Technology, CSAIL. Original written by Adam Conner-Simons. ",0.0516811,0.0610972,0.115104,0.281823,0.0929693,0.0958313,0.138526,0,0
152,Generation of random numbers by measuring phase fluctuations from a laser diode with a silicon-on-in Millimeter-scale chip uses less power to create quantum random numbers at high speeds,"Researchers have shown that a chip-based device measuring a millimeter square could be used to generate quantum-based random numbers at gigabit per second speeds. The tiny device requires little power and could enable stand-alone random number generators or be incorporated into laptops and smart phones to offer real-time encryption.""While part of the control electronics is not integrated yet, the device we designed integrates all the required optical components on one chip,"" said first author Francesco Raffaelli, University of Bristol, United Kingdom. ""Using this device by itself or integrating it into other portable devices would be very useful in the future to make our information more secure and to better protect our privacy.""Random number generators are used to encrypt data transmitted during digital transactions such as buying products online or sending a secure e-mail. Today's random number generators are based on computer algorithms, which can leave data vulnerable if hackers figure out the algorithm used.In The Optical Society (OSA) journal Optics Express, the researchers report a quantum random number generator based on randomly emitted photons from a diode laser. Because the photon emission is inherently random, it is impossible to predict the numbers that will be generated.""Compared to other integrated quantum random number generators demonstrated recently, ours can accomplish very high generation rates with relatively low optical powers,"" said Raffaelli. ""Using less power to produce random numbers helps avoid problems such as excess heat on the chip.""Silicon photonicsThe new chip was enabled by developments in silicon photonics technology, which uses the same semiconductor fabrication techniques used to make computer chips to fabricate optical components in silicon. It is now possible to fabricate waveguides into silicon that can guide light through the chip without losing the light energy along the way. These waveguides can be integrated onto a chip with electronics and integrated detectors that operate at very high speeds to convert the light signals into information.The new chip-based random number generator takes advantage of the fact that under certain conditions a laser will emit photons randomly. The device converts these photons into optical power using a tiny device called an interferometer. Very small photodetectors integrated into the same chip then detect the optical power and convert it into a voltage that can be turned into random numbers.""Despite the advancements in silicon photonics, there is still light lost inside the chip, which leads to very little light reaching the detectors,"" said Raffaelli. ""This required us to optimize all the parameters very precisely and design low noise electronics to detect the optical signal inside the chip.""The new chip-based device not only brings portability advantages but is also more stable than the same device made using bulk optics. This is because interferometers are very sensitive to environmental conditions such as temperature and it is easier to control the temperature of a small chip. It is also far easier to precisely reproduce thousands of identical chips using semiconductor fabrication, whereas reproducing the necessary precision with bulk optics is more difficult.Testing the chipTo experimentally test their design, the researchers had a foundry fabricate the random number generator chip. After characterizing the optical and electronic performance, they used it for random number generation. They estimate a potential randomness generation rate of nearly 2.8 gigabits per second for their device, which would be fast enough to enable real-time encryption.""We demonstrated random number generation using about a tenth of the power used in other chip-based quantum random number generator devices,"" said Raffaelli. ""Our work shows the feasibility of this type of integrated platform.""Although the chip containing the optical components is only one millimeter square, the researchers used an external laser which provides the source of randomness and electronics and measurement tools that required an optical table. They are now working to create a portable device about the size of a mobile phone that contains both the chip and the necessary electronics.Story Source:Materials provided by The Optical Society. ",0.0818211,0.102516,0.103149,0.212278,0.207859,0.118687,0.142708,0,0
153,Decade of research shows little improvement in websites' password guidance,"Leading internet brands including Amazon and Wikipedia are failing to support users with advice on how to securely protect their data, a study shows.More than a decade after first examining the issue, research by the University of Plymouth has shown most of the top 10 English-speaking websites offer little or no advice guidance on creating passwords that are less likely to be hacked.Some still allow people to use the word 'password', while others will allow single-character passwords and basic words including a person's surname or a repeat of their user identity.Professor of Information Security Steve Furnell conducted the research, having carried out similar assessments in 2007, 2011 and 2014.He said it was concerning that more than a decade after the issue was first highlighted companies were not doing more to aid consumers amid the increased threat of global cyber-attacks.""We keep hearing that passwords are a thing of the past,"" said Professor Furnell, Director of the University's Centre for Security, Communications and Network Research (CSCAN). ""But despite the prospect of new technologies coming into force, they are still the predominant protection people can use when setting up online accounts. With personal data now being guarded more closely than ever, providing clear and upfront guidance would seem a basic means through which to ensure users can be confident that the information they are providing is both safe and secure.""The study, published in Computer Fraud and Security, examined the password practices of Google, Facebook, Wikipedia, Reddit, Yahoo, Amazon, Twitter, Instagram, Microsoft Live and Netflix.It looked at whether users were provided with guidance when creating an account, changing their password or resetting a password, and how rigorously any guidelines were enforced.The best provisions, taking into account permitted password length and other restrictions, were offered by Google, Microsoft Live and Yahoo were also the top three sites when the last assessment was carried out in 2014.The three least favourable sets of results were from Amazon, Reddit and Wikipedia, with Amazon's password requirements remaining the most liberal, in line with the previous assessments.In fact, the one area where there has been a notable improvement over the whole 11 years is the proportion of sites that prevent the word 'password' being used, but even now several still allow it.The only other improvement has been in the number of sites offering some form of additional authentication (from three in 2011 to eight in 2018), but it is not something any of the websites assessed flag during the account sign-up process.Professor Furnell added: ""With over ten years between the studies, it is somewhat disappointing to find that the overall story in 2018 remains largely similar to that of 2007. In the intervening years, much has continued to be written about the failings of passwords and the ways in which we use them, but little is being done to encourage or oblige us to follow the right path.""The increased availability of two-step verification and two-factor authentication options is positive. But users arguably require more encouragement or obligation to use them otherwise, like passwords themselves, they will offer the potential for protection while falling short of doing so in practice.""Story Source:Materials provided by University of Plymouth. ",0.0709275,0.0812558,0.148644,0.286321,0.122555,0.126039,0.159364,0,0
154,Faster photons could enable total data security,"Researchers at the University of Sheffield have solved a key puzzle in quantum physics that could help to make data transfer totally secure.The team have developed a way of generating very rapid single-photon light pulses. Each photon, or particle of light, represents a bit of binary code -- the fundamental language of computing. These photons cannot be intercepted without disturbing them in a way that would alert the sender that something was amiss.Transferring data using light passed along fibre optic cables has become increasingly common over the past decades, but each pulse currently contains millions of photons. That means that, in principle, a portion of these could be intercepted without detection.Secure data is already encrypted, but if an 'eavesdropper' was able to intercept the signals containing details of the code then -- in theory -- they could access and decode the rest of the message.Single photon pulses offer total security, because any eavesdropping is immediately detected, but scientists have struggled to produce them rapidly enough to carry data at sufficient speeds to transfer high volumes of data.In a new study, published in Nature Nanotechnology, the Sheffield team have employed a phenomenon called the Purcell Effect to produce the photons very rapidly. A nanocrystal called a quantum dot is placed inside a cavity within a larger crystal -- the semiconductor chip. The dot is then bombarded with light from a laser which makes it absorb energy. This energy is then emitted in the form of a photon.Placing the nanocrystal inside a very small cavity makes the laser light bounce around inside the walls. This speeds up the photon production by the Purcell Effect. One problem is that the photons carrying data information can easily become confused with the laser light. The Sheffield researchers have overcome this by funnelling the photons away from the cavity and inside the chip to separate the two different types of pulse.In this way, the team have succeeded in making the photon emission rate about 50 times faster than would be possible without using the Purcell Effect. Although this isn't the fastest photon light pulse yet developed, it has a crucial advantage because the photons produced are all identical -- an essential quality for many quantum computing applications.Mark Fox, Professor of Optical Physics at the University of Sheffield, explains: ""Using photons to transmit data enables us to use the fundamental laws of physics to guarantee security. It's impossible to measure or 'read' the particle in any way without changing its properties. Interfering with it would therefore spoil the data and sound an alarm.""He added: ""Our method also solves a problem that has puzzled scientists for about 20 years -- how to use this Purcell Effect to speed up photon production in an efficient way.""This technology could be used within secure fibre optic telecoms systems, although it would be most useful initially in environments where security is paramount, including governments and national security headquarters.""Story Source:Materials provided by University of Sheffield. ",0.0949035,0.114668,0.127183,0.22643,0.169645,0.148186,0.16346,0,0
155,High-fidelity quantum secret sharing prevents eavesdropping Quantum secret-sharing scheme for noisy environments,"To protect the confidentiality of a message during its transmission, people encrypt it. However, noise in the transmission channels can be a source of concern regarding how faithful the message transmission may be after it has been decrypted. This is particularly important for secrets shared using quantum scale messengers. For example, a classical secret takes the shape of a string of zeros and ones, whereas a quantum secret is akin to an unknown quantum state of two entangled particles carrying the secret. This is because no two quantum particles can be in the same state at any given time. In a new study published in EPJ D, Chen-Ming Bai from Shaanxi Normal University, Xi'an, China, and colleagues calculate the degree of fidelity of the quantum secret once transmitted and explore how to avoid eavesdropping.What is exciting about quantum secrets is that they make it possible to share a secret among a number of participants. Yet, only certain participants can reconstruct the secret by collaborating. Creating a permission system to decide who can access the secret requires the development of a specific procedure. In this study, the authors provide a concrete example of how such an approach could work with three participants.Since noise in the transmission channel has a great influence on the quantum secret shared, the authors analyse the impacts of two kinds of noisy channels on sharing quantum secrets. Indeed, the quantum system inevitably interacts with the external environment to produce quantum noise, which leads to entanglement between the quantum state and the environment.In particular, the authors evaluate the consequences of quantum noise and the resulting degree of damping of the encrypted signal by examining its physical characteristics, like its amplitude. This helps determine how faithfully the secret has been transmitted to the receiver. Bai and colleagues find that the fidelity of the encrypted message's transmission improves depending on the quantum state in which the particles carrying the secret find themselves. The authors subsequently provide an optimised strategy to enhance fidelity in secret transmission.Story Source:Materials provided by Springer. ",0.137171,0.178703,0.195911,0.294411,0.22635,0.192127,0.23394,0,0
156,Scientists pump up chances for quantum computing,"University of Adelaide-led research has moved the world one step closer to reliable, high-performance quantum computing.An international team has developed a ground-breaking single-electron ""pump."" The electron pump device developed by the researchers can produce one billion electrons per second and uses quantum mechanics to control them one by one. And it's so precise they have been able to use this device to measure the limitations of current electronics equipment.This paves the way for future quantum information processing applications, including in defence, cybersecurity and encryption, and big data analysis.""This research puts us one step closer to the holy grail -- reliable, high-performance quantum computing,"" says project leader Dr Giuseppe C. Tettamanzi, Senior Research Fellow, at the University of Adelaide's Institute for Photonics and Advanced Sensing.Published in the journal Nano Letters, the researchers also report observations of electron behaviour that's never been seen before -- a key finding for those around the world working on quantum computing.""Quantum computing, or more broadly quantum information processing, will allow us to solve problems that just won't be possible under classical computing systems,"" says Dr Tettamanzi.""It operates at a scale that's close to an atom and, at this scale, normal physics goes out the window and quantum mechanics comes into play.""To indicate its potential computational power, conventional computing works on instructions and data written in a series of 1s and 0s -- think about it as a series of on and off switches; in quantum computing every possible value between 0 and 1 is available. We can then increase exponentially the number of calculations that can be done simultaneously.""This University of Adelaide team, in collaboration with the University of Cambridge, Aalto University in Finland, University of New South Wales, and the University of Latvia, is working in an emerging field called electron quantum optics. This involves controlled preparation, manipulation and measurement of single electrons. Although a considerable amount of work has been devoted world-wide to understand electronic quantum transport, there is much still to be understood and achieved.""Achieving full control of electrons in these nano-systems will be highly beneficial for realistic implementation of a scalable quantum computer. We, of course, have been controlling electrons for the past 150 years, ever since electricity was discovered. But, at this small scale, the old physics rules can be thrown out,"" says Dr Tettamanzi.""Our final goal is to provide a flow of electrons that's reliable, continuous and consistent -- and in this research, we've managed to move a big step towards realistic quantum computing.""And, maybe equally exciting, along the way we have discovered new quantum effects never observed before, where, at specific frequencies, there is competition between different states for the capture of the same electrons. This observation will help advances in this game-changing field.""Story Source:Materials provided by University of Adelaide. ",0.0771125,0.110953,0.103455,0.20596,0.117842,0.133946,0.147219,0,0
157,System allows surveillance cameras to 'talk' to the public through individual smartphones Technology sends personalized messages without compromising privacy,"Purdue University researchers have created a technology that allows public cameras to send personalized messages to people without compromising their privacy.The team developed a real-time end-to-end system called PHADE to allow this process, known as private human addressing. While traditional data transmission protocols need to first learn the destination's IP or MAC address, this system uses motion patterns as the address code for communication. The smartphones then locally make their own decisions on whether to accept a message.The PHADE system works using a server to receive video streams from cameras to track people. The camera builds a packet by linking a message to the address code and broadcasts the packet. Upon receiving the packet, a mobile device of each of the targets uses sensors to extract its owner's behavior and follow the same transformation to derive a second address code. If the second address code matches with the address code in the message, the mobile device automatically delivers the message to its owner.""Our technology enables public cameras to send customized messages to targets without any prior registration,"" said He Wang, an assistant professor in the Purdue Department of Computer Science, who created the technology along with his PhD student, Siyuan Cao. ""Our system serves as a bridge to connect surveillance cameras and people and protects targets' privacy.""PHADE protects privacy in two key ways -- it keeps the users' personal sensing data within their smartphones and it transforms the raw features of the data to blur partial details. The creators named the system PHADE because the blurring process ""fades"" people's motion details out.PHADE can be used in places such as at a museum, where visitors can receive messages with information about the artifacts or exhibits they are viewing. The technology also could be implemented in shopping malls to provide consumers with digital product information or coupons. In a similar way, PHADE could be valuable for new store prototypes such as Amazon Go, which uses phone technology instead of traditional checkout registers.""PHADE may also be used by government agencies to enhance public safety,"" Cao said. ""For example, the government can deploy cameras in high-crime or high-accident areas and warn specific users about potential threats, such as suspicious followers.""Wang said surveillance camera and security companies would also be able to integrate the technology into their products directly as a key feature. He also said this technology has advantages over Bluetooth-based beacons, which have difficulties in adjusting for ranges of transmission and do not allow for context-aware messaging.A video explaining more about the camera technology is available at https://www.youtube.com/watch?v=eUvslFNlLnMStory Source:Materials provided by Purdue University. ",0.0713753,0.0851656,0.12498,0.302746,0.157918,0.10118,0.16968,0,0
158,"Engineers fly first-ever plane with no moving parts The silent, lightweight aircraft doesn't depend on propellers or turbines","Since the first airplane took flight over 100 years ago, virtually every aircraft in the sky has flown with the help of moving parts such as propellers, turbine blades, and fans, which are powered by the combustion of fossil fuels or by battery packs that produce a persistent, whining buzz.Now MIT engineers have built and flown the first-ever plane with no moving parts. Instead of propellers or turbines, the light aircraft is powered by an ""ionic wind"" -- a silent but mighty flow of ions that is produced aboard the plane, and that generates enough thrust to propel the plane over a sustained, steady flight.Unlike turbine-powered planes, the aircraft does not depend on fossil fuels to fly. And unlike propeller-driven drones, the new design is completely silent.""This is the first-ever sustained flight of a plane with no moving parts in the propulsion system,"" says Steven Barrett, associate professor of aeronautics and astronautics at MIT. ""This has potentially opened new and unexplored possibilities for aircraft which are quieter, mechanically simpler, and do not emit combustion emissions.""He expects that in the near-term, such ion wind propulsion systems could be used to fly less noisy drones. Further out, he envisions ion propulsion paired with more conventional combustion systems to create more fuel-efficient, hybrid passenger planes and other large aircraft.Barrett and his team at MIT have published their results in the journal Nature.Hobby craftsBarrett says the inspiration for the team's ion plane comes partly from the movie and television series, ""Star Trek,"" which he watched avidly as a kid. He was particularly drawn to the futuristic shuttlecrafts that effortlessly skimmed through the air, with seemingly no moving parts and hardly any noise or exhaust.""This made me think, in the long-term future, planes shouldn't have propellers and turbines,"" Barrett says. ""They should be more like the shuttles in 'Star Trek,' that have just a blue glow and silently glide.""About nine years ago, Barrett started looking for ways to design a propulsion system for planes with no moving parts. He eventually came upon ""ionic wind,"" also known as electroaerodynamic thrust -- a physical principle that was first identified in the 1920s and describes a wind, or thrust, that can be produced when a current is passed between a thin and a thick electrode. If enough voltage is applied, the air in between the electrodes can produce enough thrust to propel a small aircraft.For years, electroaerodynamic thrust has mostly been a hobbyist's project, and designs have for the most part been limited to small, desktop ""lifters"" tethered to large voltage supplies that create just enough wind for a small craft to hover briefly in the air. It was largely assumed that it would be impossible to produce enough ionic wind to propel a larger aircraft over a sustained flight.""It was a sleepless night in a hotel when I was jet-lagged, and I was thinking about this and started searching for ways it could be done,"" he recalls. ""I did some back-of-the-envelope calculations and found that, yes, it might become a viable propulsion system,"" Barrett says. ""And it turned out it needed many years of work to get from that to a first test flight.""Ions take flightThe team's final design resembles a large, lightweight glider. The aircraft, which weighs about 5 pounds and has a 5-meter wingspan, carries an array of thin wires, which are strung like horizontal fencing along and beneath the front end of the plane's wing. The wires act as positively charged electrodes, while similarly arranged thicker wires, running along the back end of the plane's wing, serve as negative electrodes.The fuselage of the plane holds a stack of lithium-polymer batteries. Barrett's ion plane team included members of Professor David Perreault's Power Electronics Research Group in the Research Laboratory of Electronics, who designed a power supply that would convert the batteries' output to a sufficiently high voltage to propel the plane. In this way, the batteries supply electricity at 40,000 volts to positively charge the wires via a lightweight power converter.Once the wires are energized, they act to attract and strip away negatively charged electrons from the surrounding air molecules, like a giant magnet attracting iron filings. The air molecules that are left behind are newly ionized, and are in turn attracted to the negatively charged electrodes at the back of the plane.As the newly formed cloud of ions flows toward the negatively charged wires, each ion collides millions of times with other air molecules, creating a thrust that propels the aircraft forward.The team, which also included Lincoln Laboratory staff Thomas Sebastian and Mark Woolston, flew the plane in multiple test flights across the gymnasium in MIT's duPont Athletic Center -- the largest indoor space they could find to perform their experiments. The team flew the plane a distance of 60 meters (the maximum distance within the gym) and found the plane produced enough ionic thrust to sustain flight the entire time. They repeated the flight 10 times, with similar performance.""This was the simplest possible plane we could design that could prove the concept that an ion plane could fly,"" Barrett says. ""It's still some way away from an aircraft that could perform a useful mission. It needs to be more efficient, fly for longer, and fly outside.""Barrett's team is working on increasing the efficiency of their design, to produce more ionic wind with less voltage. The researchers are also hoping to increase the design's thrust density -- the amount of thrust generated per unit area. Currently, flying the team's lightweight plane requires a large area of electrodes, which essentially makes up the plane's propulsion system. Ideally, Barrett would like to design an aircraft with no visible propulsion system or separate controls surfaces such as rudders and elevators.""It took a long time to get here,"" Barrett says. ""Going from the basic principle to something that actually flies was a long journey of characterizing the physics, then coming up with the design and making it work. Now the possibilities for this kind of propulsion system are viable.""This research was supported, in part, by MIT Lincoln Laboratory Autonomous Systems Line, the Professor Amar G. Bose Research Grant, and the Singapore-MIT Alliance for Research and Technology (SMART). The work was also funded through the Charles Stark Draper and Leonardo career development chairs at MIT.Video: https://www.youtube.com/watch?v=boB6qu5dcCwStory Source:Materials provided by Massachusetts Institute of Technology. Original written by Jennifer Chu. ",0.130365,0.0812883,0.0863018,0.0933504,0.0903228,0.154104,0.09095,0,0
159,"Autonomous glider can fly like an albatross, cruise like a sailboat ","MIT engineers have designed a robotic glider that can skim along the water's surface, riding the wind like an albatross while also surfing the waves like a sailboat.In regions of high wind, the robot is designed to stay aloft, much like its avian counterpart. Where there are calmer winds, the robot can dip a keel into the water to ride like a highly efficient sailboat instead.The robotic system, which borrows from both nautical and biological designs, can cover a given distance using one-third as much wind as an albatross and traveling 10 times faster than a typical sailboat. The glider is also relatively lightweight, weighing about 6 pounds. The researchers hope that in the near future, such compact, speedy robotic water-skimmers may be deployed in teams to survey large swaths of the ocean.""The oceans remain vastly undermonitored,"" says Gabriel Bousquet, a former postdoc in MIT's Department of Aeronautics and Astronautics, who led the design of the robot as part of his graduate thesis. ""In particular, it's very important to understand the Southern Ocean and how it is interacting with climate change. But it's very hard to get there. We can now use the energy from the environment in an efficient way to do this long-distance travel, with a system that remains small-scale.""Bousquet will present details of the robotic system this week at IEEE's International Conference on Robotics and Automation, in Brisbane, Australia. His collaborators on the project are Jean-Jacques Slotine, professor of mechanical engineering and information sciences and of brain sciences; and Michael Triantafyllou, the Henry L. and Grace Doherty Professor in Ocean Science and Engineering.The physics of speedLast year, Bousquet, Slotine, and Triantafyllou published a study on the dynamics of albatross flight, in which they identified the mechanics that enable the tireless traveler to cover vast distances while expending minimal energy. The key to the bird's marathon voyages is its ability to ride in and out of high- and low-speed layers of air.Specifically, the researchers found the bird is able to perform a mechanical process called a ""transfer of momentum,"" in which it takes momentum from higher, faster layers of air, and by diving down transfers that momentum to lower, slower layers, propelling itself without having to continuously flap its wings.Interestingly, Bousquet observed that the physics of albatross flight is very similar to that of sailboat travel. Both the albatross and the sailboat transfer momentum in order to keep moving. But in the case of the sailboat, that transfer occurs not between layers of air, but between the air and water.""Sailboats take momentum from the wind with their sail, and inject it into the water by pushing back with their keel,"" Bousquet explains. ""That's how energy is extracted for sailboats.""Bousquet also realized that the speed at which both an albatross and a sailboat can travel depends upon the same general equation, related to the transfer of momentum. Essentially, both the bird and the boat can travel faster if they can either stay aloft easily or interact with two layers, or mediums, of very different speeds.The albatross does well with the former, as its wings provide natural lift, though it flies between air layers with a relatively small difference in windspeeds. Meanwhile, the sailboat excels at the latter, traveling between two mediums of very different speeds -- air versus water -- though its hull creates a lot of friction and prevents it from getting much speed. Bousquet wondered: What if a vehicle could be designed to perform well in both metrics, marrying the high-speed qualities of both the albatross and the sailboat?""We thought, how could we take the best from both worlds?"" Bousquet says.Out on the waterThe team drafted a design for such a hybrid vehicle, which ultimately resembled an autonomous glider with a 3-meter wingspan, similar to that of a typical albatross. They added a tall, triangular sail, as well as a slender, wing-like keel. They then performed some mathematical modeling to predict how such a design would travel.According to their calculations, the wind-powered vehicle would only need relatively calm winds of about 5 knots to zip across waters at a velocity of about 20 knots, or 23 miles per hour.""We found that in light winds you can travel about three to 10 times faster than a traditional sailboat, and you need about half as much wind as an albatross, to reach 20 knots,"" Bousquet says. ""It's very efficient, and you can travel very fast, even if there is not too much wind.""The team built a prototype of their design, using a glider airframe designed by Mark Drela, professor of aeronautics and astronautics at MIT. To the bottom of the glider they added a keel, along with various instruments, such as GPS, inertial measurement sensors, auto-pilot instrumentation, and ultrasound, to track the height of the glider above the water.""The goal here was to show we can control very precisely how high we are above the water, and that we can have the robot fly above the water, then down to where the keel can go under the water to generate a force, and the plane can still fly,"" Bousquet says.The researchers decided to test this ""critical maneuver"" -- the act of transitioning between flying in the air and dipping the keel down to sail in the water. Accomplishing this move doesn't necessarily require a sail, so Bousquet and his colleagues decided not to include one in order to simplify preliminary experiments.In the fall of 2016, the team put its design to the test, launching the robot from the MIT Sailing Pavilion out onto the Charles River. As the robot lacked a sail and any mechanism to get it started, the team hung it from a fishing rod attached to a whaler boat. With this setup, the boat towed the robot along the river until it reached about 20 miles per hour, at which point the robot autonomously ""took off,"" riding the wind on its own.Once it was flying autonomously, Bousquet used a remote control to give the robot a ""down"" command, prompting it to dip low enough to submerge its keel in the river. Next, he adjusted the direction of the keel, and observed that the robot was able to steer away from the boat as expected. He then gave a command for the robot to fly back up, lifting the keel out of the water.""We were flying very close to the surface, and there was very little margin for error -- everything had to be in place,"" Bousquet says. ""So it was very high stress, but very exciting.""The experiments, he says, prove that the team's conceptual device can travel successfully, powered by the wind and the water. Eventually, he envisions fleets of such vehicles autonomously and efficiently monitoring large expanses of the ocean.""Imagine you could fly like an albatross when it's really windy, and then when there's not enough wind, the keel allows you to sail like a sailboat,"" Bousquet says. ""This dramatically expands the kinds of regions where you can go.""Story Source:Materials provided by Massachusetts Institute of Technology. Original written by Jennifer Chu. ",0.139766,0.091861,0.116788,0.137025,0.0987829,0.160509,0.145553,0,0
160,Flights worldwide face increased risk of severe turbulence due to climate change,"Flights all around the world could be encountering lots more turbulence in the future, according to the first ever global projections of in-flight bumpiness.A new study published online in Geophysical Research Letters, a journal of the American Geophysical Union, has calculated that climate change will significantly increase the amount of severe turbulence worldwide by 2050-2080. Severe turbulence involves forces stronger than gravity, and is strong enough to throw people and luggage around an aircraft cabin.Flights to the most popular international destinations are projected to experience the largest increases, with severe turbulence at a typical cruising altitude of 39,000 feet becoming up to two or three times as common throughout the year over the North Atlantic (180 percent more common), Europe (160 percent more common), North America (110 percent more common), the North Pacific (90 percent more common), and Asia (60 percent more common).""Air turbulence is increasing across the globe, in all seasons, and at multiple cruising altitudes. This problem is only going to worsen as the climate continues to change,"" said Paul Williams, Professor of Atmospheric Science at the University of Reading in the United Kingdom and lead author of the new study. ""Our study highlights the need to develop improved turbulence forecasts, which could reduce the risk of injuries to passengers and lower the cost of turbulence to airlines.""The study also makes the first ever turbulence projections for the Southern Hemisphere and the tropical regions of the planet. The amount of airspace containing severe turbulence is calculated to increase over South America (60 percent increase), Australia (50 percent increase), and Africa (50 percent increase).""While turbulence does not usually pose a major danger to flights, it is responsible for hundreds of passenger injuries every year,"" said Luke Storer, a researcher at the University of Reading and co-author of the new study. ""It is also by far the most common cause of serious injuries to flight attendants. Turbulence is thought to cost United States air carriers up to $200 million annually.""The new research analyzes supercomputer simulations of the future atmosphere with a focus on clear-air turbulence, which is particularly hazardous because it is invisible. The expected turbulence increases are a consequence of global temperature changes, which are strengthening wind instabilities at high altitudes in the jet streams and making pockets of rough air stronger and more frequent.""The study is another example of how the impacts of climate change can be felt through the circulation of the atmosphere, not just through increases in surface temperature itself,"" said Manoj Joshi, a Senior Lecturer in Climate Dynamics at the University of East Anglia in the United Kingdom and co-author of the new study.Story Source:Materials provided by American Geophysical Union. ",0.185783,0.131217,0.190058,0.14104,0.132311,0.217982,0.144762,0,0
161,Surging heat may limit aircraft takeoffs globally Study sees widespread effects on aviation,"Rising temperatures due to global warming will make it harder for many aircraft around the world to take off in coming decades, says a new study. During the hottest parts of the day, 10 to 30 percent of fully loaded planes may have to remove some fuel, cargo or passengers, or else wait for cooler hours to fly, the study concludes. The study, which is the first such global analysis, appears today in the journal Climatic Change.""Our results suggest that weight restriction may impose a non-trivial cost on airline and impact aviation operations around the world,"" said lead author Ethan Coffel, a Columbia University PhD. student.As air warms, it spreads out, and its density declines. In thinner air, wings generate less lift as a plane races along a runway. Thus, depending on aircraft model, runway length and other factors, at some point a packed plane may be unable to take off safely if the temperature gets too high. Weight must be dumped, or else the flight delayed or canceled.Average global temperatures have gone up nearly 1 degree Centigrade (1.8 Fahrenheit) since about 1980, and this may already be having an effect. In late June, American Airlines canceled more than 40 flights out of Phoenix, Ariz., when daytime highs of nearly 120 degrees made it too hot for smaller regional jets to take off. Worldwide, average temperatures are expected to go up as much as another 3 degrees C (5.4 degrees F) by 2100. But that is only part of the story; heat waves will probably become more prevalent, with annual maximum daily temperatures at airports worldwide projected to go up 4 to 8 degrees C (7.2 to 14.4 F) by 2080, according to the study. It is these heat waves that may produce the most problems.""This points to the unexplored risks of changing climate on aviation,"" said coauthor Radley Horton, a climatologist at Columbia University's Lamont-Doherty Earth Observatory. ""As the world gets more connected and aviation grows, there may be substantial potential for cascading effects, economic and otherwise."" Most studies so far have focused on how aviation may affect global warming (aircraft comprise about 2 percent of global greenhouse-gas emissions), not vice versa. But a handful of studies have warned that warming climate may increase dangerous turbulence along major air routes, and head winds that could lengthen travel times. Rising sea levels are already threatening to swamp some major airports. Coffel and Horton may be the only ones so far to look at takeoffs.In 2015, they published a smaller-scale paper, predicting up to four times more future temperature-related takeoff problems for the common Boeing 737-800 at Phoenix, as well as Denver, New York's LaGuardia and Washington's Ronald Reagan. The new study projects effects on a wide range of jets at these, plus 15 of the other busiest airports in the United States, Europe, the Mideast, China and south Asia.The authors estimate that if globe-warming emission continue unabated, fuel capacities and payload weights will have to be reduced by as much as 4 percent on the hottest days for some aircraft. If the world somehow manages to sharply reduce carbon emissions soon, such reductions may amount to as little as 0.5 percent, they say. Either figure is significant in an industry that operates on thin profit margins. For an average aircraft operating today, a 4 percent weight reduction would mean roughly 12 or 13 fewer passengers on an average 160-seat craft. This does not count the major logistical and economic effects of delays and cancellations that can instantly ripple from one air hub to another, said Horton.Some aircraft with lower temperature tolerances will far worse than others, and certain airports -- those with shorter runways, in hotter parts of the world or at higher elevations, where the air is already thinner -- will suffer more. For instance, facing LaGuardia's short runways, a Boeing 737-800 may have to offload weight half the time during the hottest days. Dubai, in the United Arab Emirates, might be worse; its runways are long, but its temperatures are already very high. Airports probably less affected because they are in temperate regions and have long runways include New York's JFK, London Heathrow and Paris's Charles de Gaulle.Horton said that some effects could be mitigated with new engine or body designs, or expanded runways. But modifications would come at a cost, as aircraft are already highly engineered for efficiency; and expanded runways in densely packed cities such as New York are not an option. ""The sooner climate can be incorporated into mid- and long-range plans, the more effective adaptation efforts can be,"" said Coffel.Story Source:Materials provided by Lamont-Doherty Earth Observatory, Columbia University. Original written by Kevin Krajick. ",0.109459,0.084421,0.120543,0.0949243,0.0858608,0.133476,0.093022,0,0
162,Putting hybrid-electric aircraft performance to the test,"Although hybrid-electric cars are becoming commonplace, similar technology applied to airplanes comes with significantly different challenges. University of Illinois aerospace engineers are addressing some of them toward the development of a more sustainable alternative to fossil fuels to power airplanes.""Jet fuel and aviation gasoline are easy to store on an airplane. They are compact and lightweight when compared to the amount of energy they provide. Unfortunately, the actual combustion process is very inefficient. We're harnessing only a small fraction of that energy but we currently don't have electrical storage systems that can compete with that,"" said Phillip Ansell, assistant professor in the Department of Aerospace Engineering in the College of Engineering at the University of Illinois.Ansell said adding more batteries to fly farther may seem logical, but it works against the goal to make an aircraft as lightweight as possible. ""That's one of the big barriers we run into when designing battery-powered electrified aircraft. The current technology has very significant range disadvantages. But strong fuel-burn advantages.""He, along with former aerospace undergraduate student, Tyler Dean, and current doctoral student Gabrielle Wroblewski, utilized a series of simulations to model the performance of hybrid-electric aircraft.""We started with an existing twin-engine aircraft and looked at how we might create a hybrid-electric drivetrain for it using existing off-the-shelf hardware,"" Ansell said. ""We wanted to know how well it would perform. If I used a certain set of drivetrain components, I want to know how far the aircraft could fly, how much fuel does it burn, how fast can if climb -- all of the overall flight performance changes.""A flight-performance simulator was created to accurately represent the true flight performance of a Tecnam P2006T on a general mission to include take off, climb, cruise, descent, and landing, along with sufficient reserves to meet FAA regulations. Transition segments were incorporated into the simulation during climb and descent where the throttle setting, flap deployment, propeller rotation rate, and all other flight control variables were either set to mimic input from a typical pilot or prescribed in accordance with the aircraft flight manual.After configuring the simulator to collect baseline performance data, a parallel hybrid drivetrain was integrated into the simulation. The researchers compared the sensitivity of range and fuel economy to the level of electrification, battery specific energy density, and electric motor power density. The same sensitivities were studied with a series hybrid-electric drivetrain.Ansell said that, overall, a hybrid-electric drivetrain can lead to substantial improvements in fuel efficiency of a given aircraft configuration, though these gains depend strongly on the coupled variations in the degree of drivetrain electrification and the required mission range. Both of these factors influence the weight allocation of battery and fuel systems, as well as the weight scaling imposed by internal combustion engine and electrical motor components. In general, to obtain the greatest fuel efficiency a hybrid architecture should be used with as much electrification in the drivetrain as is permissible within a given range requirement.The fuel efficiency improvements were shown to particularly shine for short-range missions, which is a good thing since range limitations serve as one of the key bottlenecks in hybrid aircraft feasibility. Though, through this study the changes in the range capabilities of the aircraft were also able to be forecast with advancements in hybrid component technologies. ""For example,"" Ansell said, ""the propulsion system today could be configured to have 25 percent of its propulsive power come from an electric motor. However, it would only be able to fly about 80 nautical miles. Fast forward to projections for lighter battery technologies for roughly the year 2030 and the same aircraft could fly two and a half to three times as far. The range increase is nonlinear, so the largest improvements can be seen for the most immediate improvements with battery specific energy density, with gradually diminishing returns for that same proportional increase in specific energy.""""One interesting and unexpected result we observed, however, came about when comparing the parallel and series hybrid architectures. Since the parallel architecture mechanically couples the shaft power of the engine and motor together, only one electrical machine is needed. For the series architecture, a generator is also needed to convert the engine power to electrical power, along with a larger motor than the parallel hybrid configuration to drive the propulsor. Unexpectedly, this aspect made the parallel architecture more beneficial for improved range and fuel burn almost across the board due to its lighter weight. However, we did observe that if significant improvements are made in maturing electrical motor components in the very long term, we may actually someday see better efficiency out of series-hybrid architectures, as they permit a greater flexibility in the placement and distribution of propulsors.""The team chose to model the Tecnam P2006T using a series of performance variables found in published articles by the aircraft manufacturer. They selected that particular aircraft, in part, because NASA has been working on their X-57 aircraft, which has leading-edge propellers for high lift. ""This study was being conducted for NASA, and use of this aircraft also allowed our results to be better applicable to the X-57 concept vehicle,"" Ansell said. ""Using our data, they will be able to have at least a ballpark idea about how the hybrid system will perform without the other distributed propulsion modifications.""Ansell said propulsion electrification is still very much an unknown in terms of how a vehicle should be built, engineered, flown. ""Our study helps inform those discussions. We looked only at battery storage systems though there are many more that can be implemented, each with their own advantages and disadvantages. This study allowed us to look at what types of advancements need to be made in motor technology, in battery technology, etc.""The study, ""Mission Analysis and Component-Level Sensitivity Study of Hybrid-Electric General Aviation Propulsion Systems,"" was conducted by Tyler Dean, Gabrielle Wroblewski, and Phillip Ansell. It appears in the Journal of Aircraft.This project was supported by NASA Neil A. Armstrong Flight Research Center under Small Business Technology Transfer in collaboration with Rolling Hills Research Corporation.Story Source:Materials provided by University of Illinois College of Engineering. ",0.100292,0.0547078,0.0659736,0.114603,0.0769142,0.149133,0.0854863,0,0
163,Could an anti-global warming atmospheric spraying program really work?,"A program to reduce Earth's heat capture by injecting aerosols into the atmosphere from high-altitude aircraft is possible, but unreasonably costly with current technology, and would be unlikely to remain secret.Those are the key findings of new research published today in Environmental Research Letters, which looked at the capabilities and costs of various methods of delivering sulphates into the lower stratosphere, known as stratospheric aerosol injection (SAI).The researchers examined the costs and practicalities of a large scale, hypothetical 'solar geoengineering' project beginning 15 years from now. Its aim would be to halve the increase in anthropogenic radiative forcing, by deploying material to altitudes of around 20 kilometres.They also discussed whether such an idealized program could be kept secret.Dr Gernot Wagner, from Harvard University's John A. Paulson School of Engineering and Applied Sciences, is a co-author of the study. He said: ""Solar geoengineering is often described as 'fast, cheap, and imperfect'.""While we don't make any judgement about the desirability of SAI, we do show that a hypothetical deployment program starting 15 years from now, while both highly uncertain and ambitious, would be technically possible strictly from an engineering perspective. It would also be remarkably inexpensive, at an average of around $2 to 2.5 billion per year over the first 15 years.""The researchers confirm earlier studies that discuss the low direct costs of potential stratospheric aerosol geoengineering intervention, but they arrive at those numbers with the help of direct input from aerospace engineering companies in specifying what the paper dubs the 'SAI Lofter (SAIL)'.Wake Smith, a co-author of the study, is a lecturer at Yale College and held former positions as CEO of Pemco World Air Services (a leading aircraft modification company), COO of Atlas Air Worldwide Holdings (a global cargo airline), and President of the flight training division of Boeing. He said: ""I became intrigued by the engineering questions around SAI and the many studies that purport to show that modified existing planes could do the job. Turns out that is not so. It would indeed take an entirely new plane design to do SAI under reasonable albeit entirely hypothetical parameters. No existing aircraft has the combination of altitude and payload capabilities required.""Mr. Smith said: ""We developed the specifications for SAIL with direct input from several aerospace and engine companies. It's equivalent in weight to a large narrow body passenger aircraft. But to sustain level flight at 20 kms, it needs roughly double the wing area of an equivalently sized airliner, and double the thrust, with four engines instead of two.""At the same time, its fuselage would be stubby and narrow, sized to accommodate a heavy but dense mass of molten sulphur rather than the large volume of space and air required for passengers.""The team estimated the total development costs at less than $2 billion for the airframe, and a further $350 million for modifying existing low-bypass engines.The new planes would comprise a fleet of eight in the first year, rising to a fleet of just under 100 within 15 years. The fleet would fly just over 4,000 missions a year in year one, rising to just over 60,000 per year by year 15.Dr Wagner said: ""Given the potential benefits of halving average projected increases in radiative forcing from a particular date onward, these numbers invoke the 'incredible economics' of solar geoengineering. Dozens of countries could fund such a program, and the required technology is not particularly exotic.""However, in the authors' view, this should not reinforce the often-invoked fear that a rogue country or operator might launch a clandestine SAI program upon an unsuspecting world.Mr Smith said: ""No global SAI program of the scale and nature discussed here could reasonably expect to maintain secrecy. Even our hypothesized Year one deployment program entails 4000 flights at unusually high altitudes by airliner-sized aircraft in multiple flight corridors in both hemispheres. This is far too much aviation activity to remain undetected, and once detected, such a program could be deterred.""Story Source:Materials provided by IOP Publishing. ",0.0971607,0.0769231,0.0955445,0.137428,0.0886617,0.127055,0.11093,0,0
164,"Optimizing winglets for minimum drag, more efficient flight ","If you've ever taken a photo out the window of a commercial airplane, you most likely have a great shot of a winglet -- that part of the wing at the tip that angles upward. That little change in the wingtip's shape does a lot. It reduces drag, which can translate to higher speed or to allow a pilot to throttle back and save fuel. It also helps to reduce wingtip vortices that can be problematic for airplanes flying in their wake.Although, winglets have been around since the mid-1970s, there is still a wide variety of shapes, sizes, and angles. Analyzing winglets to find the optimal characteristics to result in the lowest net drag for an aircraft was the goal of University of Illinois researchers Phillip Ansell, Kai James, and graduate student Prateek Ranjan.""Many academic studies on non-planar wing designs idealize winglets installed with a sharp 90-degree turn at the tips, though there are a lot of things potentially wrong with having these sharp junctures. Because individual aircraft have a unique set of constraints and requirements, it's difficult to make generalizations about how an aircraft should be designed,"" said Ansell, assistant professor in the Department of Aerospace Engineering in the College of Engineering at the University of Illinois. ""However, when looking at non-planar wing systems, we distilled the problem to something very specific and canonical. We used a method of multi-fidelity optimization, beginning with very simple mathematical algorithms to better understand the design space within plus or minus 10 percent accuracy, then ran more advanced simulations to understand how the winglet influences the flow field and performance of the wing.""In their research, the team focused on a -- non-linear wing design, known as Hyper Elliptic Cambered Span (HECS) wing configurations, where the vertical projection of the wing can be described mathematically using the equation of a hyper-ellipse.""We distilled the wing geometry down to something very simple,"" Ansell said. ""We expressed the non-planarity of the wing -- how curved it is, how high the wingtips are, etc. -- using equations for a hyper-ellipse. Now we can easily change the values in the equation to find the best-performing wing while trading off sharper or smoother curvature as the tip is approached, as well as larger or smaller winglet heights.""Ansell said the algorithm began with a fixed lift, a fixed projected span, a fixed bending moment of the wing, and a fixed weight, to generate a wing that has the minimum drag -- and ultimately be more efficient.""While others have studied non-planar wings with blended winglet designs, most have only looked at the so-called 'inviscid' aspect of the wing drag, ignoring the complex sources of drag introduced by the viscosity of the air,"" Ansell said. ""But that's only about half of the picture. In our formulation, we included these viscous drag sources because it has a substantial influence on the net efficiency of the wing. For example, it is easy to reduce the inviscid drag of the wing by adding very tall winglets at the tips with very sharp junctures. However, there is a distinct viscous drag penalty by doing so that reduces the effectiveness of such a design in practice.""""By performing a rigorous numerical optimization procedure, we were able to systematically explore the space of possible designs, and ultimately obtain designs that may seem unusual, and that we could never have predicted by relying on mere intuition,"" said Kai James, also an assistant professor in the Department of Aerospace Engineering.Ansell said this integrated optimization framework will assist the current state of low-speed wing design but may also result in an improvement over the current conventional wing designs, operating in the subsonic flight regime.Story Source:Materials provided by University of Illinois College of Engineering. ",0.184146,0.0993246,0.153873,0.165835,0.119385,0.196334,0.140005,0,0
165,Opening communication lines between propulsion and airflow poses new questions,"On the runway to more fuel-efficient aircraft, one alternative propulsion scheme being explored is an array of electrically powered ducted fans. The fans are distributed across the wing span or integrated into the wing. Researchers at the University of Illinois gained new understanding in how the fans and especially their precise placement on the aircraft can affect the cross-conversation between propulsion and the airflow around the wing.In most commercial aircraft, the engines are isolated from the rest of the wing system. Instead of being embedded in the wing or mounted more closely to that surface, they hang out from underneath the wings. This is done, in part, to try to reduce the influence in cross coupling -- the cross-communication between the engine's RPM and the airflow characteristics about the airplane wing.""If we allow those two systems to talk to each other, there is a lot of increased complexity in the flow field over the wing and into the propulsor -- which also substantially alters the performance,"" said Phillip Ansell, assistant professor in the Department of Aerospace Engineering in the College of Engineering at the University of Illinois. ""We've taken two subsystems -- propulsion and aerodynamics -- and we've said that these are not isolated subsystems. These are now one thing.""Ansell, along with his graduate student Aaron Perry at U of I and Michael Kerho from the Rolling Hills Research Corporation conducted the study to understand on a basic level what those interactions are and how that coupling between ducted fan systems and wing sections will modify the aerodynamic behavior and the overall lift, drag, and pitching moment characteristics.""If we integrate the propulsors, which in this case are fans, into the wing, we can improve the aircraft's propulsive efficiency by ingesting the low-speed air across wing surface into the propulsor. But it's challenging to figure out how to do it in a smart way.""This research project was conducted experimentally using a 3D printed model of an airfoil, which is a cross-section of a wing, mounted inside a subsonic wind tunnel. ""We had a model with ducted fans mounted over the trailing edge of the airfoil. The flow goes across the upper surface and then into the fan,"" Ansell said.He said that the manipulation of the throttle of the ducted fan mounted on top of the wing provided large changes in the aerodynamic behavior of the airfoil.""We can adjust the throttle to make the fan spin faster or slower, so that I now have a high-speed jet that's coming out the back end and acts to substantially lift the aircraft through a phenomenon known as supercirculation. It also changes the flow across the surface,"" he said. ""I have little regions of the flow on the surface called boundary layers. Whenever I ramp up the throttle and start pulling air into that propulsor, it thins out the boundary layer. It modifies the distribution of the pressure across the airfoil itself. There are some complex things happening. That fan RPM talking to the aerodynamics of the larger airfoil is substantial.""Ansell said the study provides a new way to understand the dialogue between a full aircraft system and a propulsion system. It's not just about increasing the throttle to create a larger thrust and produce a force that goes through the axis of the orientation of the fan.""It's not that simple because it also changes the air flow over the wing,"" Ansell said. ""The different orientations of the end of the fan changes the performance of the wing section as well as the pressure distribution because it changes the local flow quality characteristics. We have now quantified that and can understand some aspects of what that looks like.""We were able to take measurements to better understand what those variations in coupling characteristics are. Previously we knew that if we ramp up the throttle on this fan, the result is a thrust vector pointed in a certain direction. Now we know that it will also modify my local wing aerodynamics.""Story Source:Materials provided by University of Illinois College of Engineering. ",0.257108,0.103055,0.132855,0.13864,0.135386,0.238878,0.139056,0,0
166,"New smart materials could open new research field Serendipitous discovery could increase efficiency in jet engines, reduce plane noise, more","A group of new smart materials discovered by researchers at Texas A&M University and their colleagues has the potential to significantly improve the efficiency of fuel burn in jet engines, cutting the cost of flying. The materials, which could also reduce airplane noise over residential areas, have additional applications in a variety of other industries.""What excites me is that we have just scratched the surface of something new that could not only open a completely new field of scientific research, but also enable new technologies,"" said Dr. Ibrahim Karaman, Chevron Professor I and head of the university's Department of Materials Science and Engineering.The work was published in Scripta Materialia. Karaman's co-authors are Demircan Canadinc, William Trehern, and Ji Ma of Texas A&M, and Fanping Sun and Zaffir Chaudhry, Technical Fellow of the United Technologies Research Center (UTRC).The discovery is based on bringing together two relatively new areas of materials science involving metal alloys, or metals composed of two or more elements. The first area involves shape-memory alloys, ""smart"" materials that can switch from one shape to another with specific triggers, in this case temperature. Picture a straight metal rod that is bent into a corkscrew. By changing the temperature, the corkscrew turns back into a rod and vice versa.Many applicationsMany potential applications for shape-memory alloys involve extremely hot environments like a working jet engine. Until now, however, economical high-temperature shape memory alloys, (HTSMAs), have only worked at temperatures up to about 400 degrees Celsius. Adding elements like gold or platinum can significantly increase that temperature, but the resulting materials are much too expensive, among other limitations.Karaman, while working on a NASA project with UTRC and colleagues, began this research to address a specific problem: controlling the clearance, or space, between turbine blades and the turbine case in a jet engine. A jet engine is most fuel-efficient when the gap between the turbine blades and the case is minimized. However, this clearance has to have a fair margin to deal with peculiar operating conditions. HTSMAs incorporated into the turbine case could allow the maintenance of the minimum clearance across all flight regimes, thereby improving thrust specific fuel consumption.Another important potential application of HTSMAs is the reduction of noise from airplanes as they come in to an airport. Planes with larger exhaust nozzles are quieter, but less efficient in the air. HTSMAs could automatically change the size of the core exhaust nozzle depending on whether the plane is in flight or is landing. Such a change, triggered by the temperatures associated with these modes of operation, could allow both more efficient operation while in the air and quieter conditions at touchdown.Karaman and his colleagues decided to try increasing the operating temperatures of HTSMAs by applying principles from another new class of materials, high-entropy alloys, which are composed of four or more elements mixed together in roughly equal amounts. The team created materials composed of four or more elements known to form shape-memory alloys (nickel, titanium, hafnium, zirconium and palladium), but purposefully omitted gold or platinum.""When we mixed these elements in equal proportions we found that the resulting materials could work at temperatures well over 500 degrees C -- one worked at 700 degrees C -- without gold or platinum. That's a discovery,"" said Karaman. ""It was also unexpected because the literature suggested otherwise.""How do the new materials work? Karaman said they have ideas on how they operate at such high temperatures, but do not have solid theories yet. To that end, future work includes trying to understand what is happening at the atomic scale by conducting computer simulations. The researchers also aim to explore ways to improve the materials' properties even further. Karaman notes, however, that many other questions remain.""That's why I believe this could open a completely new area of research,"" he said. ""While we will continue our own efforts, we are excited that others will now join us so that together we can push the boundaries of science.""This joint project between UTRC and Texas A&M was funded by the NASA Leading Edge Aeronautics Research initiative.Story Source:Materials provided by Texas A&M University. Original written by Elizabeth Thomson. ",0.0636719,0.0953516,0.0738452,0.109998,0.0716239,0.1046,0.109795,0,0
167,Robotic herding of a flock of birds using drones,"Researchers made a new algorithm for enabling a single robotic unmanned aerial vehicle to herd a flock of birds away from a designated airspace. This novel approach allows a single autonomous quadrotor drone to herd an entire flock of birds away without breaking their formation.Professor David Hyunchul Shim at KAIST in collaboration with Professor Soon-Jo Chung of Caltech and Professor Aditya Paranjape of Imperial College London investigated the problem of diverting a flock of birds away from a prescribed area, such as an airport, using a robotic UVA. A novel boundary control strategy called the m-waypoint algorithm was introduced for enabling a single pursuer UAV to safely herd the flock without fragmenting it.The team developed the herding algorithm on the basis of macroscopic properties of the flocking model and the response of the flock. They tested their robotic autonomous drone by successfully shepherding an entire flock of birds out of a designated airspace near KAIST's campus in Daejeon, South Korea. This study is published in IEEE Transactions on Robotics.""It is quite interesting, and even awe-inspiring, to monitor how birds react to threats and collectively behave against threatening objects through the flock. We made careful observations of flock dynamics and interactions between flocks and the pursuer. This allowed us to create a new herding algorithm for ideal flight paths for incoming drones to move the flock away from a protected airspace,"" said Professor Shim, who leads the Unmanned Systems Research Group at KAIST.Bird strikes can threaten the safety of airplanes and their passengers. Korean civil aircraft suffered more than 1,000 bird strikes between 2011 and 2016. In the US, 142,000 bird strikes destroyed 62 civilian airplanes, injured 279 people, and killed 25 between 1990 and 2013. In the UK in 2016, there were 1,835 confirmed bird strikes, about eight for every 10,000 flights. Bird and other wildlife collisions with aircraft cause well over 1.2 billion USD in damages to the aviation industry worldwide annually. In the worst case, Canadian geese knocked out both engines of a US Airway jet in January 2009. The flight had to make an emergency landing on the Hudson River.Airports and researchers have continued to reduce the risk of bird strikes through a variety of methods. They scare birds away using predators such as falcons or loud noises from small cannons or guns. Some airports try to prevent birds from coming by ridding the surrounding areas of crops that birds eat and hide in.However, birds are smart. ""I was amazed with the birds' capability to interact with flying objects. We thought that only birds of prey have a strong sense of maneuvering with the prey. But our observation of hundreds of migratory birds such as egrets and loons led us to reach the hypothesis that they all have similar levels of maneuvering with the flying objects. It will be very interesting to collaborate with ornithologists to study further with birds' behaviors with aerial objects,"" said Professor Shim. ""Airports are trying to transform into smart airports. This algorithm will help improve safety for the aviation industry. In addition, this will also help control avian influenza that plagues farms nationwide every year,"" he stressed.For this study, two drones were deployed. One drone performed various types of maneuvers around the flocks as a pursuer of herding drone, while a surveillance drone hovered at a high altitude with a camera pointing down for recording the trajectories of the pursuer drone and the birds.During the experiments on egrets, the birds made frequent visits to a hunting area nearby and a large number of egrets were found to return to their nests at sunset. During the time, the team attempted to fly the herding drone in various directions with respect to the flock.The drone approached the flock from the side. When the birds noticed the drone, they diverted from their original paths and flew at a 45? angle to their right. When the birds noticed the drone while it was still far away, they adjusted their paths horizontally and made smaller changes in the vertical direction. In the second round of the experiment on loons, the drone flew almost parallel to the flight path of a flock of birds, starting from an initial position located just off the nominal flight path. The birds had a nominal flight speed that was considerably higher than that of the drone so the interaction took place over a relatively short period of time.Professor Shim said, ""I think we just completed the first step of the research. For the next step, more systems will be developed and integrated for bird detection, ranging, and automatic deployment of drones."" ""Professor Chung at Caltech is a KAIST graduate. And his first student was Professor Paranjape who now teaches at Imperial. It is pretty interesting that this research was made by a KAIST faculty member, an alumnus, and his student on three different continents,"" he said.Story Source:Materials provided by The Korea Advanced Institute of Science and Technology (KAIST). ",0.207539,0.139591,0.148581,0.212022,0.166381,0.207184,0.19328,0,0
168,Optimizing airport flight patterns take a toll on human health Case Study on La Guardia Airport shows increased noise can pose health threats including cardiovascular disease to communities underneath,"Health costs associated with noise from changing flight patterns over populated urban landscapes far outweigh the benefits of reduced flight times, according to a new study conducted at Columbia University's Mailman School of Public Health and Queens Quiet Skies. The researchers used flights from LaGuardia airport that have historically flown over Flushing Meadows and the U.S. Tennis Center in Queens -- known as the TNNIS route -- as a case study to explore the trade-offs between more efficient flight routes and suffering on the ground. The findings appear in the International Journal of Environmental Research and Public Health.""Airports in the U.S. have gradually been transitioning to automated flight systems,"" said Peter Muenning, MD, professor of Health Policy and Management at the Mailman School. ""These systems generate new flight paths over populated areas. While they can improve flight efficiency, the increased noise associated with these novel flight patterns potentially pose serious health threats to nearby communities -- including cardiovascular disease and anxiety disorder as consequences of noise.""The year-round use of 'TNNIS Climb' at La Guardia implemented in 2012 was a result of flight automation in New York City. No environmental assessment or environmental impact statement was ever performed on the route.""Flights from LaGuardia airport have historically flown over Flushing Meadows in Queens,"" noted Muennig, who also leads Global Research Analytics for Population Health at Columbia.""During U.S. Open tennis matches, the residents of certain neighborhoods in Queens had to endure heavy airplane traffic over their homes, but it only lasted a few weeks. Now, they have to contend with it year-round.""The researchers compared the costs and quality-adjusted life years (QALYs) gained associated with reverting to pre-2012 flight patterns seen prior to the year-round use of TNNIS. The TNNIS climb increased airplane noise to above 60dB over some of the most densely populated areas of the city.""Our study focuses on health and economic impacts of a single flight route as a result of flight automation, however, our analysis uses inputs that may be generalizable to other settings,"" observed Muenning. ""The results point to the strong need for careful study of public health impacts of such changes before they are implemented.""Story Source:Materials provided by Columbia University's Mailman School of Public Health. ",0.190743,0.147246,0.217924,0.166725,0.161729,0.229845,0.166883,0,0
169,New aircraft-scheduling models may ease air travel frustrations,"Flight schedules that allow for a little carefully designed wiggle room could prevent the frustration of cascading airport delays and cancellations. By focusing on the early phases of flight schedule planning and delays at various scales, researchers have developed models to help create schedules that are less susceptible to delays and easier to fix once disrupted.Passenger and resource reaccommodation in the wake of a delay costs the airlines -- an industry that operates on less than two percent profit margins -- billions of dollars each year. A reduction in delays, cancellations and their cascading downstream impacts would greatly benefit the U.S. airline industry and travelers, the researchers said.The study, published in the journal Computers and Operations Research, looks at early phase flight schedule and aircraft route planning. It takes a proactive approach in designing cascade-resistant schedules, rather than the reactive approach of trying to manage delays after they occur.""There is an overwhelming amount of data generated from airline on-time performance records,"" said Lavanya Marla, an industrial and enterprise systems engineering professor and lead author. ""It is challenging to find the best model to quantify the uncertainty in the aviation system in order to improve on-time performance and cost savings. Our research shows that the existing models are unable to distinguish the cascading downstream impact of one solution over another, which is critical to the airlines for decision-making.""The team, which also includes Vikrant Vaze, a professor of engineering at Dartmouth College, and Cynthia Barnhart, chancellor and a professor of engineering at the Massachusetts Institute of Technology, used historical data from U.S. airlines to tweak aircraft routing to help prevent and minimize delays from the extent to which they exist today.Aircraft routing refers to the route that a single aircraft takes between Federal Aviation Administration-mandated maintenance checks that typically occur on a 72-hour cycle and multiple flights. The team hones in on this layer of the system because it hits a sort of sweet spot where they feel scheduling adjustments can make the most impact. ""Of the tens of billions of dollars of delay costs that occur annually, almost a third are a result of delay-cascading effects through aircraft routings,"" Vaze said. ""So this is a very important layer of the system to focus on.""The researchers constructed different models to help determine what kinds of solutions offer the most flexibility in reducing delay cascades. One set of models focuses on purely avoiding the outcomes of the worst-case delays only, and a second considers all kinds of delays that occur -- extreme to common day-to-day.The researchers evaluated the various models by observing the percentage of flight delays and passenger disruptions that result from the different kinds of delays. Marla and her team found that having the ability to control the less severe day-to-day types of delays results in the most benefit for passengers.""It is not that one or the other model by itself gives a better solution,"" Marla said. ""They give different solutions. The best choice depends on the patterns of evolving delays, the most relevant evaluation metrics, and on which solution aligns best with the priorities of the Department of Transportation, the passengers and the airline industry.""""The key to enhancing the performance of complex and uncertain systems like those in aviation often lies in intelligently combining sophisticated mathematical models, careful attention to real-world data, and detailed simulation tools for system evaluation,"" Barnhart said.Story Source:Materials provided by University of Illinois at Urbana-Champaign. ",0.144427,0.09739,0.131986,0.203648,0.124028,0.164728,0.139668,0,0
170,Flight: Research examines wing shapes to reduce vortex and wake,"It's common to see line-shaped clouds in the sky, known as contrails, trailing behind the engines of a jet airplane.What's not always visible is a vortex coming off of the tip of each wing -- like two tiny horizontal tornadoes -- leaving behind a turbulent wake behind the vehicle. The wake poses a destabilizing flight hazard, particularly for smaller aircraft that share the same flight path.Recent research at the University of Illinois demonstrated that, although most wing shapes used today create these turbulent wake vortices, wing geometrics can be designed to reduce or eliminate wingtip vortices almost entirely. In the study, the vortex and wake characteristics were computed for three classic wing designs: the elliptic wing, and wing designs developed in classic studies by R.T. Jones and Ludwig Prandt.""The elliptic wing configuration has been used as the gold standard of aerodynamic efficiency for the better part of a century. We teach our students that it has the optimal loading characteristics and that it's often used when looking at wing efficiency for say, minimizing drag,"" said Phillip Ansell, assistant professor in the Department of Aerospace Engineering at U of I.In a previous experimental study on optimizing wing configurations, Ansell learned you can gain efficiency of the wing system with a non-elliptic wing profile. ""Previous academic studies have shown that, theoretically, there are other designs that actually provide lower drag of a planar wing for a fixed amount of lift generation. But what has been missing is an actual apples-to-apples experiment to prove it.""In this new research, Ansell, and his graduate student, Prateek Ranjan, used the real data from the previous study to analyze the three wing configurations.""We chased this down because we saw something curious in our measurements in the earlier experiment. Consequently, in this new study, we simulated the flow about these three wings and saw significant differences in how the vortices and wakes developed from each of the three wing types. The Jones and the Prandtl wing configurations didn't have wing-tip vortices like the elliptic wing. They had a much more gradual bulk deformation of the whole wake structure, rather than an immediate coherent roll-up. We now know that we can delay the formation of wake vortex structures, and increase the distance it takes a trailing wake vortex to roll up by about 12 times, making it weaker and less of a hazard to the aircraft entering its wake.""Ansell said this information can be used to re-tailor how formation flight is viewed between aircraft, or to develop a new an ideal configuration for the lift loading for takeoffs and landings, and subsequently reduce the length of separation between aircraft in the same flight path.""Trailing wingtip vortices tend to take a long time to go away once they form in the atmosphere. So the time it takes for the vortex to dissipate has to be figured into the takeoff time of the next aircraft going in that same path. The motion of the air produced by these vortices can create a hazard for trailing aircraft, as it can be unpredictable and make for dangerous flight regimes. So using the Jones or Prandtl wings would result in much less turbulent air behind a plane,"" Ansell said.You'd think that Ansell's conclusion is to use only the Jones or Prandtl wing configurations, but it's not.""One of the things that first drew me to the topic of aerodynamics is that the right answer always depends on what your constraints are. If you're building a tiny unmanned vehicle that will fly at a low speed, you'll get a different solution for design needs than if you're building an aircraft that will carry people at high altitudes and high speeds. So technically, you could argue that all three wing types are the best solution. The question is, what are your driving constraints, such as wing span and weight, behind selecting one of them?""Ansell added that this is a basic research study and not intended to advise a specific aircraft designer or company.""We are looking at how the wing flow behaves and the information can be used to understand how the roll-up process of vortices is produced. This study allows us to be aware of how the wing configuration affects the trailing vortex formation and wake by studying the extreme bounds of immediate and delayed vortex roll-up processes,"" Ansell said.""Interestingly we identified that one of the worst offenders of creating vortices is indeed the elliptic lift distribution, which is also among the most conventional wing design. It has definitely changed the way I talk about the issue in my classes. Instead of simply referring to the flow patterns produced behind the wing as a pair of 'wingtip vortices,' I've taken to describe the full wake produced as the trailing vortex system.""Story Source:Materials provided by University of Illinois College of Engineering. ",0.26464,0.130094,0.166856,0.176844,0.149113,0.234084,0.15805,0,0
171,"General aviation pilots struggle to interpret weather forecast and observation displays Improved weather displays could help advance aviation safety, researcher says","When tested on their knowledge of 23 types of weather information, from icing forecasts and turbulence reports to radar, 204 general aviation (GA) pilots surveyed by Embry-Riddle Aeronautical University researchers were stumped by about 42% of the questions.The findings, published in the April 2018 edition of the International Journal of Aerospace Psychology, are worrisome because GA pilots flying smaller planes at lower altitudes, usually with minimal ground-based support, have higher weather-related accident and fatality rates, said Embry-Riddle's Elizabeth Blickensderfer, a professor in the Department of Human Factors and Behavioral Neurobiology.Four categories of GA pilots who completed the 95-question exam scored as follows:Overall, the mean score across all 204 pilots was 57.89%, based on assessments conducted on the university's Daytona Beach, Fla., campus and at an air show in the midwestern United States.Improved testing of GA pilots is needed, Blickensderfer said, noting that in 2014, the National Transportation Safety Board named ""identifying and communicating hazardous weather"" a top priority for improving safety. Currently, however, the U.S. Federal Aviation Administration's Knowledge Exam allows pilots to pass even if they fail the weather portion of the test.Weather Displays are Part of the ProblemShe emphasized, however, that her research should not be interpreted solely as a symptom of faulty pilot training. ""I don't want to blame the pilots for deficiencies in understanding weather information,"" she said. ""We have got to improve how weather information is displayed so that pilots can easily and quickly interpret it. At the same time, of course, we can fine-tune pilot assessments to promote learning and inform training.""What kinds of questions were asked on the survey? As an example, respondents might be prompted to interpret cryptic METAR (Meteorological Terminal Aviation Routine Weather Report) information, which helps pilots prepare for safe flights: ""You notice the comment, `CB DSNT N MOV N.' Based on this information, which of the following is true?"" Pilots should understand the METAR comment to mean, ""Cumulonimbus clouds are more than 10 statute [land-measured] miles north of the airport and moving away from the airport.""As another example, pilots might be asked to interpret a ground-based radar cockpit display, which would only show recent thunderstorm activity -- not current conditions. Or, the survey might ask pilots to look at an infrared (color) satellite image and determine where the clouds with the highest-altitude clouds would most likely be found.Thomas A. Guinn, an associate professor of meteorology at Embry-Riddle and a co-author on the study, noted that it's critical for pilots to assess big-picture weather issues before takeoff. In addition, they need to understand, for instance, that radar displayed inside a cockpit shows what happened up to 15 minutes earlier.""If you're flying 120 miles per hour and you don't understand that there's a lag time in ground-based radar data reaching your cockpit,"" Guinn noted, ""that can be deadly.""All test questions were designed to push pilots beyond whatever facts they had memorized, so that ""they had to think about it and answer the question using the same thought processes as if they were performing a pre-flight check,"" said Robert Thomas, another co-author of the study who is a Gold Seal Certified Flight Instructor and an assistant professor of aeronautical science at Embry-Riddle.The research, supported by $491,000 in funding from the U.S. Federal Aviation Administration, could help guide pilot training and assessments. That's important because pilots can actually pass the FAA's existing Knowledge Exam even if they fail the weather portion of the test.Story Source:Materials provided by Embry-Riddle Aeronautical University. ",0.210683,0.109437,0.160597,0.189224,0.141382,0.190919,0.149857,0,0
172,"New scheduling system could help reduce flight delays Study shows when airports factor in airline and passenger costs, delays decrease","Scheduling and coordinating air traffic can be difficult, but taking the airlines' and passengers' delay costs into account can actually save airlines money and result in fewer delays, according to a new study from Binghamton University, State University of New York.With New York Gov. Andrew Cuomo's recent proposal to transform New York City's John F. Kennedy International Airport, researchers at Binghamton University decided to look into how to make airports more efficient. They did so by incorporating the actual cost of a delayed passenger into the equation.Currently, decision support systems (DSS) are used to help direct flights and air traffic in hopes of preventing delays. Most of these systems use parameters such as time or distance to manage and determine the optimal schedule.Associate Professor of Systems Science and Industrial Engineering Sang Won Yoon, along with Binghamton University PhD student Duaa Serhan and NASA aerospace engineer Hanbong Lee, developed two models that consider the actual cost incurred when passengers and flights are delayed.Delay costs can include numerous things such as fuel consumption, maintenance, crew costs, taxiing and additional travel time -- all of which are included in the models.""Airports mainly focus on increasing the runway throughput and, in general, most other papers focus on delay as a time, but they don't consider the actual cost of the delay,"" said Serhan, a graduate student in industrial and systems engineering who developed the models for the paper.The researchers explained that there are normally two perspectives used when scheduling aircraft and that these perspectives focus on different parameters to either maximize or minimize. The first perspective is the airport itself, which is typically owned by the government and regulated by the Federal Aviation Administration (FAA). The second is that of the airlines or the companies operating the actual aircraft.""From the airport's perspective, the runway throughput is the most important, so they want to quickly send out as many aircraft as possible,"" said Yoon. ""From the airline company's perspective, they just want to minimize the delay."" The airport tries to choose a schedule based on which one allows the most aircraft to leave in a given period, while the airlines want to minimize the amount of time the aircraft spends at an airport.The researchers decided to take into account a third and often-overlooked perspective: the passengers.Passenger delay costs can take many forms. The delay costs can represent things like missed connections or the cost associated with rebooking or compensation.""Time is a cost, but airline companies also have to reroute passengers. If they miss the flight, then they have to schedule a new flight or provide a voucher. There are also immeasurable costs, like missing a call with your family, or a business meeting,"" said Yoon.It's these immeasurable human costs that can sometimes be hard to quantify and are difficult to include in calculations, but nonetheless are something almost everyone has experienced.""You can have the passenger missing something important. For example, I almost missed a conference last year. So, you can miss opportunities as well,"" said Serhan.In the researchers' models, these inconveniences were captured by their classification of three types of delay costs: passenger delays, passenger missed connections and additional operating costs.Two models were developed and tested under various air traffic scenarios and compared against three other commonly used scheduling methods.One of the models that took the passengers' perspective into account displayed the overall lowest airline and passenger delay cost, reducing the costs by a minimum of 6.4 percent compared to the other methods. It was also able to reduce the number of flight delays by 8.6-65.4 percent when compared to two of the other reference models, and performed similarly to the third reference model.As for what comes next, the researchers said they are looking at the effect of weather.""What we are investigating now is changing air terminal configurations based on the weather uncertainty. We are working on routing and scheduling aircraft when there are uncertain weather conditions, and trying to minimize delays,"" said Serhan.Story Source:Materials provided by Binghamton University. ",0.165013,0.100106,0.124561,0.182415,0.134311,0.175965,0.133422,0,0
173,"Aircraft microbiome much like that of homes and offices, study finds ","What does flying in a commercial airliner have in common with working at the office or relaxing at home?According to a new study, the answer is the microbiome -- the community of bacteria found in homes, offices and aircraft cabins. Believed to be the first to comprehensively assess the microbiome of aircraft, the study found that the bacterial communities accompanying airline passengers at 30,000 feet have much in common with the bacterial communities surrounding people in their homes and offices.Using advanced sequencing technology, researchers from the Georgia Institute of Technology and Emory University studied the bacteria found on three components of an airliner cabin that are commonly touched by passengers: tray tables, seat belt buckles and the handles of lavatory doors. They swabbed those items before and after 10 transcontinental flights and also sampled air in the rear of the cabin during flight.What they found was surprisingly unexciting.""Airline passengers should not be frightened by sensational stories about germs on a plane,"" said Vicki Stover Hertzberg, a professor in Emory University's Nell Hodgson Woodruff School of Nursing and a co-author of the study. ""They should recognize that microbes are everywhere and that an airplane is no better and no worse than an office building, a subway car, home or a classroom. These environments all have microbiomes that look like places occupied by people.""The results of the FlyHealthye study are reported June 6, 2018, in the journal Microbial Ecology. In March, the researchers reported on a separate part of the study that examined potential routes for transmitting certain respiratory viruses -- such as the flu -- on commercial flights.Given the unusual nature of an aircraft cabin, the researchers hadn't known what to expect from their microbiome study. On transcontinental flights, passengers spend four or five hours in close proximity breathing a very dry mix of outdoor air and recycled cabin air that has been passed through special filters, similar to those found in operating rooms.""There were reasons to believe that the communities of bacteria in an aircraft cabin might be different from those in other parts of the built environment, so it surprised me that what we found was very similar to what other researchers have found in homes and offices,"" said Howard Weiss, a professor in Georgia Tech's School of Mathematics and the study's corresponding author. ""What we found was bacterial communities that were mostly derived from human skin, the human mouth -- and some environmental bacteria.""The sampling found significant variations from flight to flight, which is consistent with the differences other researchers have found among the cars of passenger trains, Weiss noted. Each aircraft seemed to have its own microbiome, but the researchers did not detect statistically significant differences between preflight and post-flight conditions on the flights studied.""We identified a core airplane microbiome -- the genera that were present in every sample we studied,"" Weiss added. The core microbiome included genera Propionibacterium, Burkholderia, Staphylococcus, and Strepococcus (oralis).Though the study revealed bacteria common to other parts of the built environment, Weiss still suggests travelers exercise reasonable caution. ""I carry a bottle of hand sanitizer in my computer bag whenever I travel,"" said Weiss. ""It's a good practice to wash or sanitize your hands, avoid touching your face, and get a flu shot ever year.""This new information on the aircraft microbiome provides a baseline for further study, and could lead to improved techniques for maintaining healthy aircraft.""The finding that airplanes have their own unique microbiome should not be totally surprising since we have been exploring the unique microbiome of everything from humans to spacecraft to salt ponds in Australia. The study does have important implications for industrial cleaning and sterilization standards for airplanes,"" said Christopher Dupont, another co-author and an associate professor in the Microbial and Environmental Genomics Department at the J. Craig Venter Institute, which provided bioinformatics analysis of the study's data.The 229 samples obtained from the aircraft cabin testing were subjected to 16S rRNA sequencing, which was done at the HudsonAlpha Institute for Biotechnology in Huntsville, Alabama. The small amount of genetic material captured on the swabs and air sampling limited the level of detail the testing could provide to identifying genera of bacteria, Weiss said. The extensive bioinformatics, or sequence analysis, was carried out at the J. Craig Venter Institute in La Jolla, Calif.In the March 19 issue of the journal Proceedings of the National Academy of Sciences, the researchers reported on the results of another component of the FlyHealthye study that looked at potential transmission of respiratory viruses on aircraft. They found that an infectious passenger with influenza or other droplet-transmitted respiratory infection will most likely not transmit infection to passengers seated farther away than two seats laterally and one row in front or back on an aircraft.That portion of the study was designed to assess rates and routes of possible infectious disease transmission during flights, using a model that combines estimated infectivity and patterns of contact among aircraft passengers and crew members to determine likelihood of infection. FlyHealthye team members were assigned to monitor specific areas of the passenger cabin, developing information about contacts between passengers as they moved around.Among next steps, the researchers would like to study the microbiome of airport areas, especially the departure lounges where passengers congregate before boarding. They would also like to study long-haul international flights in which passengers spend more time together -- and are more likely to move about the cabin.Story Source:Materials provided by Georgia Institute of Technology. Original written by John Toon. ",0.128793,0.094905,0.138804,0.16114,0.107129,0.17054,0.131446,0,0
174,Airlines and passengers save billions through crew planning,"Airlines avoid up to 80 percent of crew-related delays through advance planning, according to the authors of a Dartmouth College study on the commercial airline industry. The research explains the complex reality of crew scheduling and provides an inside look at the techniques used by airlines to absorb system delays.While individual airlines use proprietary systems for crew planning, this is the first study that publicly describes how crew itineraries are developed. It is also the first open study of how airlines weigh planned costs and disruption costs when deciding crew schedules.Understanding the rules and regulations that define how airlines work and that often compound frustrating delays for passengers is difficult. The analysis explains how positioning of flight crews attempts to ease travel disruptions while saving airlines and consumers billions of dollars.""Airlines are among the world's most complex businesses,"" said Vikrant Vaze, an assistant professor of engineering at Dartmouth's Thayer School of Engineering. ""Understanding the sophisticated planning systems used by airlines to schedule crews -- as well as understanding how delays impact the air travel system -- are both massive tasks, but such information can result in great savings for the airlines and the flying public.""Annual revenue and annual costs in the U.S. airline industry are each estimated to be around $150 billion. On the expense side, crew costs are counted as the second-largest outlay. On average, it is estimated that crew expenses -- including pilots and flight attendants -- account for about one-third of total airline costs, or about $45 billion, according to the U.S. Department of Transportation's Bureau of Transportation Statistics (BTS).Calculations in the Dartmouth study demonstrate that sophisticated crew scheduling practices allow airlines to avoid 60-80 percent of crew-related delays. While a dollar figure is not provided in the paper, rough estimates suggest that this amounts to savings of up to $13 billion system-wide each year in terms of reduction in delay costs to airlines and consumers.Researchers focused on four categories of operational factors related directly to pilot scheduling that affect the extent of the spread of delays: aircraft change, connection buffers, crew legality and crew swaps. These categories relate to the complex constellation of operational decisions, regulations and labor guidelines that guide real-time decision-making in the airline industry.Among the regulations that airlines need to factor when determining crew schedules are: the amount of total flying time, sit time, rest time and the total time away from the crew base.""While the industry's problems are easy to pick out, finding solutions to keep countless moving parts working efficiently is much more challenging,"" said Vaze, the senior researcher of the study.According to this research, published in the journal Transportation Science, airline delays are strongly impacted by decisions that are made to pair crews to a series of flights during the course of a work period. In order to better understand how crew delays amplify throughout the airline system, the researchers developed a model to estimate the proprietary crew itinerary generation algorithms used by airlines.Once the research team could accurately estimate how airlines make crew pairing decisions, they were able to model how delays propagate throughout the airline system as a result of technical problems, poor weather, security issues and other disruptions.The team analyzed a series of confidential crew scheduling datasets from multiple major US airlines in order to confirm how well mathematical models used in the research estimate how the industry accounts for the cascading delays.""This study answers often-asked questions about crew pairing and airline delays,"" said Keji Wei, a doctoral candidate at Dartmouth and lead author of the paper. ""Understanding the extent, causes and impacts of the propagation of delays and disruptions is essential for developing methods to improve the overall aviation system performance.""Just under 80 percent of U.S. flights were delayed by 15 minutes or less in the first half of the current decade. According to the paper, public data does not include specific information on the number of flight delays caused by crew delays and disruptions.Researchers expect the study will help airports, airlines and government agencies improve delay-mitigation techniques used throughout the industry.""Accurate estimation of crew delays is critical for an overall understanding of aviation system performance and to inform government policy and air carrier decisions,"" said Vaze.The study -- that focuses on pilot schedules only -- comes after a difficult period for some airlines, including a high-profile incident in which a passenger was forcefully removed from a plane to provide room for transporting a crew member.Story Source:Materials provided by Dartmouth College. ",0.213093,0.140493,0.174695,0.213509,0.173245,0.21491,0.168393,0,0
175,Reading the minds of pilots on the fly Research paves the way to improve the safety and efficiency of aircraft-pilot interactions,"After a plane engine blew apart at 32,000 feet in the air last month, the pilot flying Southwest Flight 1380 safely brought the Boeing 737 to an emergency landing in Philadelphia. Captain Tammie Jo Shults was heralded a hero, but a different flier may not have been able to respond as adeptly.Consider what was racing through her mind: where to land, the speed and altitude to maintain, how to aid an injured passenger and comfort a bewildered crew. It's likely Shults was experiencing ""cognitive overload,"" said Frederic Dehais, PhD, a professor at ISAE-SUPAERO in Toulouse, France, and an expert in flight safety.The efficiency and safety of human-machine systems depend on the cognitive workload and situational awareness of human operators, according to Hasan Ayaz, PhD, an associate research professor in the School of Biomedical Engineering, Science and Health Systems at Drexel University.""Unfortunately, many human-machine interfaces expose users to workload extremes, diminishing the operator's attention and potentially leading to catastrophic consequences,"" Ayaz said.An ideal human-machine system would actually be able to read its operator's mind in real-time, to know how well he or she was paying attention or able to process new information. Such a system may sound like the makings of a sci-fi movie. But Ayaz and Dehais, along with a team of researchers at Drexel and ISAE-SUPAERO, have now successfully measured the brain activity of pilots in real-time using functional near-infrared spectroscopy, or fNIRS. Their results were published this week in Frontiers in Human Neuroscience.Unlike traditional functional magnetic resonance imaging (fMRI), in which a person is resigned to a large machine while lying down, Drexel's portable fNIRS system is worn like a headband, making it ideal for measuring brain activity while study subjects move around freely in their natural environments.The fNIRS system measures ""the brain at work"" by monitoring blood oxygenation changes in the prefrontal cortex -- the area underneath the forehead that is involved in cognitive functions such as problem solving, memory, judgement and impulse control. When first learning a new task, for instance, this area of the brain is highly activated. However, as you become more proficient, these tasks move to other brain areas, clearing up important resources in the prefrontal cortex for skills like split-second decision making.""The exciting thing is we can now quantify this,"" Ayaz said. In a flying scenario, Ayaz envisions that the airplane itself might one day be able to assess the cognitive and emotional state of the pilot, and then make adjustments accordingly.To find out whether this could be possible, the researchers split 28 pilots into two categories: The first group flew a real aircraft, while the second group operated a flight simulator. In both cases, the researchers monitored the pilots' brain activity as they completed a series of memorization tasks from pre-recorded air traffic control instructions, which varied in levels of difficulty.The results showed that the pilots in the real flight condition committed more errors and had higher anterior prefrontal cortex activation than pilots in the simulator when completing cognitively demanding tasks.The implications are twofold, says Ayaz. First, the researchers successfully demonstrated the feasibility of monitoring cognitive workload in a realistic flight situation. Secondly, the differences between the two groups underscores the need for ""ecologically valid"" research studies. In other words, solely measuring brain activity in a laboratory likely won't lead to the most accurate results.In the future, understanding the underlying neurocognitive process of pilot-plane interactions could help to make simulators more realistic, as well as to improve the safety and efficiency of aircraft-pilot interactions.""We believe that this type of approach will open a whole new direction of research for studying parameters in an aviation setting and eventually designing better machines,"" Dehais said.Story Source:Materials provided by Drexel University. ",0.0872065,0.0655255,0.102158,0.201364,0.0827999,0.132819,0.130493,0,0
176,Going beyond 'human error',"Failures in highly technological environments, such as military aircraft, can be investigated using known tools like HFACS, the U.S. Department of Defense's Human Factors Analysis and Classification System. However, because of some limitations, HFACS does not always highlight the deeper causal factors that contribute to such failures. In what might be the first application of the Bayes' theorem probability formula to an HFACS dataset, Andrew Miranda examined data from 95 severe incidents to pinpoint external influences behind so-called human error.""Understanding Human Error in Naval Aviation Mishaps"" discusses the three potential influences on performance-based errors that Miranda found: sensory misperception (for instance, spatial disorientation), mental awareness (cognition, attention), and the technological environment (e.g., design of cockpit displays and controls).In addition, factors that likely contributed to judgment/decision-making errors included supervisory or organizational influences that may have placed aviators in situations of increased risk that taxed, if not their skills, then their decision-making abilities.Digging deeper into external influences in the 95 mishaps, Miranda, an aerospace experimental psychologist at the Naval Safety Center, used content analysis. Themes drawn from the mishap reports helped to explain how and why the failures occurred. These themes could be classified as involving teamwork and organizational/supervisory influences. For example, there was evidence that crewmembers were unexpectedly put in a position of shared expectations that someone else was responsible for a particular task. When this occurred during circumstances with slowly increasing risk, individual crewmembers did not speak up or intervene because the social and technical conditions unintentionally encouraged it. Slowly but surely, an unsafe situation would emerge.Miranda notes, ""This project was essentially the extension of human factors work spanning 70 years: examine beyond the label 'human error' in favor of more careful considerations about the general conditions of aviation accidents. There were 95 severe mishaps in our dataset. To those of us on the outside, it's easy to look back with hindsight at each one of those accidents and wonder why the people involved did (or didn't) do what they did (or didn't). But we won't learn much with that approach. Instead, we made the effort to take an insider perspective. Each of these mishaps is an intricate story of people and technology under changing, dynamic circumstances that ultimately lead to an aircraft being destroyed or even lives being lost. The people involved made decisions and actions that made sense to them at the time. Human factors principles and methods are uniquely capable at both uncovering how conditions foster pilot error, as well as suggesting how to improve those conditions for future aviators.""Miranda's work has the potential to reveal ways in which HFACS or similar incident analysis tools can be used in other complex systems, such as health care, oil and gas, transportation, and maritime operations.Story Source:Materials provided by Human Factors and Ergonomics Society. ",0.0973398,0.094993,0.165129,0.191358,0.104707,0.148415,0.142134,0,0
177,Evading in-flight lightning strikes Electrically charging planes would reduce their risk of being struck by lightning.,"Aviation experts estimate that every commercial airplane in the world is struck by lightning at least once per year. Around 90 percent of these strikes are likely triggered by the aircraft itself: In thunderstorm environments, a plane's electrically conductive exterior can act as a lightning rod, sparking a strike that could potentially damage the plane's outer structures and compromise its onboard electronics.To avoid lightning strikes, flights are typically rerouted around stormy regions of the sky. Now, MIT engineers are proposing a new way to reduce a plane's lightning risk, with an onboard system that would protect a plane by electrically charging it. The proposal may seem counterintuitive, but the team found that if a plane were charged to just the right level, its likelihood of being struck by lightning would be significantly reduced.The idea stems from the fact that, when a plane flies through an ambient electric field, its external electrical state, normally in balance, shifts. As an external electric field polarizes the aircraft, one end of the plane becomes more positively charged, while the other end swings towards a more negative charge. As the plane becomes increasingly polarized, it can set off a highly conductive flow of plasma, called a positive leader -- the preceding stage to a lightning strike.In such a precarious scenario, the researchers propose temporarily charging a plane to a negative level to dampen the more highly charged positive end, thus preventing that end from reaching a critical level and initiating a lightning strike.The researchers have shown through modeling that such a method would work, at least conceptually. They report their results in the American Institute of Aeronautics and Astronautics Journal.The team, which includes Emeritus Professor Manuel Martinez-Sanchez and Assistant Professor Carmen Guerra-Garcia, envisions outfitting a plane with an automated control system consisting of sensors and actuators fitted with small power supplies. The sensors would monitor the surrounding electric field for signs of possible leader formation, in response to which the actuators would emit a current to charge the aircraft in the appropriate direction. The researchers say such charging would require power levels lower than that for a standard lightbulb.""We're trying to make the aircraft as invisible to lightning as possible,"" says co-author Jaime Peraire, head of MIT's Department of Aeronautics and Astronautics and the H.N. Slater Professor of Aeronautics and Astronautics. ""Aside from this technological solution, we are working on modeling the physics behind the process. This is a field where there was little understanding, and this is really an attempt at creating some understanding of aircraft-triggered lightning strikes, from the ground up.""The paper's other co-author is Ngoc Cuong Nguyen, a research scientist in the aeronautics and astronautics department.Lightning flourishingTo be clear, lightning itself poses very little danger to passengers inside an aircraft, as a plane's cabin is well-insulated against any external electrical activity. In most cases, passengers may only see a bright flash or hear a loud bang. Nevertheless, an aircraft that has been hit by lightning often requires follow-up inspections and safety checks that may delay its next flight. If there is physical damage to the plane, it may be taken out of service -- something the airlines would rather avoid.What's more, newer aircraft made partly from nonmetallic composite structures such as carbon fiber may be more vulnerable to lightning-related damage, compared with their older, all-metal counterparts. That's because charge may accumulate on poorly conducting panels and create potential diffences from panel to panel, which may cause certain regions of a panel to spark. A standard protective measure is to cover the outside of the aircraft with a light metallic mesh.""Modern aircraft are about 50 percent composites, which changes the picture very significantly,"" Guerra-Garcia says. ""Lightning-related damage is very different, and repairs are much more costly for composite versus metallic aircraft. This is why research on lightning strikes is flourishing now.""Following the leaderGuerra-Garcia and her colleagues looked at whether electrically charging an airplane would bring down its risk of lightning strikes -- an idea that was initially suggested to them by collaborators at Boeing, the research sponsor.""They are very eager to reduce the incidence of these things, partly because there are large cost expenses related to lightning protection,"" Martinez-Sanchez says.To see whether the charging idea held up, the MIT team first developed a simple model of an aircraft-triggered lightning strike. As a plane flies through a thunderstorm or other electrically charged environment, the outside of the plane begins to be polarized, forming ""leaders,"" or channels of highly conductive plasma, flowing from opposite ends of the plane and eventually out toward oppositely charged regions of the atmosphere.""Imagine two channels of plasma propagating very quickly, and when they reach the cloud and the ground, they form a circuit, and current flows through,"" Guerra-Garcia says.""These leaders carry current, but not very much,"" Martinez-Sanchez adds. ""But in the worst cases, once they establish a circuit, you can get 100,000 amps, and that is when damage happens.""The researchers developed a mathematical model to describe the electric field conditions under which leaders would develop, and how they would evolve to trigger a lightning strike. They applied this model to a representative aircraft geometry and looked to see whether changing the aircraft's potential (charging it negatively) would prevent the leaders from forming and triggering a lightning strike.Their results show that, averaging over field directions and intensities, the charged scenario required a 50 percent higher ambient electric field to initiate a leader, compared with an uncharged scenario. In other words, by charging a plane to an optimal level, its risk of being struck by lightning would be significantly reduced.""Numerically, one can see that if you could implement this charge strategy, you would have a significant reduction in the incidents of lightning strikes,"" Martinez-Sanchez says. ""There's a big if: Can you implement it? And that's where we're working now.""Graduate student Theodore Mouratidis is performing preliminary experiments in MIT's Wright Brothers Wind Tunnel, testing the feasibility of charging on a simple, metallic sphere. The researchers also hope to carry out experiments in more realistic environments, for instance by flying drones through a thunderstorm.To make the charging system practical, Martinez-Sanchez says researchers will have to work to speed up its response time. Based on their modeling, he and his colleagues have found that such a system could charge and protect a plane within fractions of a second, but this will not be enough to protect against some forms of triggered lightning.""The scenario we can take care of is flying into an area where there are storm clouds, and the storm clouds produce an intensification of the electric field in the atmosphere,"" Martinez-Sanchez says. ""That can be sensed and measured on board, and we can claim that for such relatively slow-developing events, you can charge a plane and adapt in real time. That is quite feasible.""Story Source:Materials provided by Massachusetts Institute of Technology. ",0.116456,0.0989633,0.139467,0.141944,0.112395,0.197849,0.12554,0,0
178,Robotic spiders and bees: The rise of bioinspired microrobots,"Jumping robot spiders and swarms of robotic bees sounds like the stuff of science fiction, but researchers at The University of Manchester are already working on such projects and aiming to lead the world in micro robotics.But what will these kinds of robots be used for and is it something we should be worried? Dr Mostafa Nabawy is the Microsystems Research Theme Leader at The University of Manchester's School of Mechanical, Aerospace and Civil Engineering. He is presenting some of his research, ""Spiders Attack: The rise of bioinspired microrobots"" at Manchester's Industry 4.0 Summit on Thursday 1 March.Here Dr Nabawy explains why micro robots really aren't anything to worry about and, instead, could be the revolution in robotics that spearheads the next generation in manufacturing technology:'For our robotic spiders research we are looking at a specific species of jumping spider called Phidippus regius. We have trained it to jump different distances and heights, recording the spider's every movement in extreme detail through high resolution cameras which can be slowed down.'We are now using this bio-mechanical data to model robots that can perform with the same abilities. With this extensive dataset we have already started developing prototype robots that can mimic these biomechanical movements and jump several centimetres.'Why jumping spiders you ask? Unlike humans, our spiders can jump up to six-times longer than their own body length from a standing start. In comparison, the maximum a human can jump is just one and half times. Dr Nabawy says if we can perfect the way spiders jump in robots they can be used for a variety of different purposes in complex engineering and manufacturing and can be deployed in unknown environments to execute different missions.Dr Nabawy's research and background is in aerodynamics, aircraft design, and the modelling of engineering systems. But he is now combining this expertise with bio-inspired flying and jumping technologies, including flying robot bees.He added: 'The ultimate aim is to create a robot bee that can fly independently and we're quite a long way into that project. But there are also many different opportunities for brilliant science and engineering outcomes along the way so it is a very exciting process.'We're aiming to create the world's first robot bee that can fly unaided and unaccompanied. These technologies can also be used for many different applications, including improving the current aerodynamic performances of aircraft.'Or, imagine if the current trend of a declining bee population continues, swarms of robot bees pollinating crops and flowers could become a reality. Whilst this may sound like something out of a transformers film this is our ultimate aim. But don't worry we are someway off swarms of flying mechanical bees and armies of mechanical spider robots.'The Industry 4.0 summit is looking at the future of the manufacturing industry, but will also look at topics ranging from Brexit and the Northern Powerhouse to skills shortages, cyber-security and blue skies technology.Story Source:Materials provided by University of Manchester. ",0.0986876,0.104671,0.134893,0.227059,0.114931,0.166967,0.203828,0,0
179,"Drones more damaging than bird strikes to planes, study finds ","As part of a multi-institution Federal Aviation Administration (FAA) study focused on unmanned aerial systems, researchers at The Ohio State University are helping quantify the dangers associated with drones sharing airspace with planes.Last week, a research team from the Alliance for System Safety of UAS through Research Excellence (ASSURE) released a report concluding that drone collisions with large manned aircraft can cause more structural damage than birds of the same weight for a given impact speed.The FAA will use the research results to help develop operational and collision risk mitigation requirements for drones. ASSURE conducted its research with two different types of drones on two types of aircraft through computer modeling and physical validation testing.Kiran D'Souza, assistant professor of mechanical and aerospace engineering at Ohio State, led the engine ingestion portion of the first-of-its-kind study.""Even small unmanned aircraft systems can do significant damage to engines,"" D'Souza said.Reports of close calls between drones and airliners have surged. The FAA gets more than 100 sightings a month of drones posing potential risks to planes, such as operating too close to airports. The FAA estimates that 2.3 million drones will be bought for recreational use this year, and the number is expected to rise in coming years.Unlike the soft mass and tissue of birds, drones typically are made of more rigid materials. The testing showed that the stiffest components of the drone -- such as the motor, battery and payload -- can cause the most damage to the aircraft body and engine.Led by Gerardo Olivares, director of Wichita State University's National Institute for Aviation Research, the team evaluated the potential impacts of drones weighing 2.7 to 8 pounds on a single-aisle commercial transport jet and a business jet.They examined collisions with the wing leading edge, the windshield, and the vertical and horizontal stabilizers. The windshields generally sustained the least damage and the horizontal stabilizers suffered the most serious damage. The severity levels ranged from no damage to failure of the primary structure and penetration of the drone into the airframe.An expert in gas turbine dynamics, Ohio State's D'Souza conducted computer simulations to evaluate the potential damage of a drone entering a generic mid-sized business jet engine, including damage to fan blades, the nacelle and the nosecone.The simulations revealed that the greatest damage and risk occurs during takeoff, since the fan is operating at the highest speed at this phase of flight. The location of the drone's contact on the fan is a key parameter, with the most damage occurring when the impact is near the blade tip.According to D'Souza, the next step is the development of a representative commercial jet engine model for ingestion simulations, as well as full-scale testing to verify and validate the simulations. The team is planning additional research on engine ingestion in collaboration with engine manufacturers, as well as additional airborne collision studies with helicopters and general aviation aircraft.The researchers concluded that drone manufacturers should adopt ""detect and avoid"" or ""geo-fencing"" capabilities to reduce the probability of collisions with other aircraft.Story Source:Materials provided by Ohio State University. Original written by Matt Schutte. ",0.18194,0.113245,0.147478,0.166771,0.132315,0.18178,0.15241,0,0
180,Research aims to help renewable jet fuel take flight,"Airplanes zoom overhead, wispy-white contrails streaming behind them. The Federal Aviation Administration (FAA) handled 43,684 flights, on average, every day last year, and U.S. military and commercial flights together used over 20 billion gallons of jet fuel.All those emissions add up. World air travel contributed 815 million tons of CO2 emissions in 2016 -- two percent of the global humanmade total, according to the International Air Transport Association. And global air traffic is not slowing down. IATA predicts that 7.2 billion passengers will travel by air in 2035, nearly doubling the 3.8 billion that flew in 2016.So how do we make air travel easier on the environment? University of Delaware researchers are working to develop an alternative jet fuel. Instead of petroleum, UD researchers want to power planes with corncobs and wood chips -- stuff you generally don't care much about unless you're a groundhog or a beaver looking for leftovers.In UD's Harker Interdisciplinary Science and Engineering Laboratory, researchers are transforming such plant material, known scientifically as lignocellulosic biomass, into green products, including new fuels and chemicals. The scientists are affiliated with the Catalysis Center for Energy Innovation (CCEI), an Energy Frontier Research Center supported by the U.S. Department of Energy. Based at UD, the center brings together scientists from nine institutions to work on clean energy challenges.One of the biggest hurdles to making renewable jet fuel, according to CCEI Associate Director Basudeb Saha, is increasing the speed and efficiency of two critical chemical processes -- coupling and deoxygenation. Since the plant material the center works with has a low carbon content once it's broken down from a solid into a liquid, the carbon molecules must be chemically stitched together or ""coupled"" to create high-carbon molecules in the jet fuel range. Then the oxygen must be removed from these molecules to form branched hydrocarbons. This branching is essential to improving the flow of fuel at the freezing temperatures of commercial flight.""International planes may fly at an altitude of 35,000 feet, where the outside temperature could be as low as -14e Centigrade,"" says Saha, who is leading a renewable jet fuel project at the center. ""That's the temperature at which a plane has to run, and the fuel can't be frozen.""Accelerating renewable jet fuel productionThe demand persists for non-petroleum-based fuel for aviation. More than a decade ago, the FAA had set a target of using 1 billion gallons of renewable jet fuel by 2018. According to IATA, sustainable aviation fuels are integral to its pursuit of carbon neutral growth from 2020 on, and to a 50 percent reduction in net carbon emissions by 2050 (relative to 2005 levels). But not enough quantities of this alternative fuel are being produced, nor at a competitive cost.Currently, several U.S. companies make renewable jet fuel from materials such as triglycerides extracted from used oil and grease, or from a combination of carbon monoxide and hydrogen called syngas. One company uses algae as its source material and even has an underground pipeline to the Los Angeles Airport (LAX), where a percentage gets mixed with conventional jet fuel, Saha says.However, processing this non-conventional material requires high temperatures -- 350eC (662eF) -- and high pressure as well.Not so with those wood chips and corn cobs at UD, where Saha and his colleagues have developed new catalysts -- so called ""chemical goats"" -- that kickstart the chemical reactions that can transform this plant material into fuel. One of these catalysts, made from inexpensive graphene, looks like a honeycomb of carbon molecules. Its unique surface properties increase the speed of the coupling reaction. It also operates at low temperature (60eC). Another catalyst removes oxygen in an energy-efficient way and produces high yields of branched molecules, up to 99 percent, suitable for jet fuel. Both catalysts are recyclable, and the processes are scalable.""The low temperature and high selectivity of our process can enable cost-competitive and sustainable production of bio-based aviation fuels from lignocellulosic biomass,"" Saha says.Story Source:Materials provided by University of Delaware. Original written by Tracey Bryant. ",0.103051,0.156698,0.112146,0.109807,0.0939802,0.140323,0.125052,0,0
181,Airline industry could fly thousands of miles on biofuel from a new promising feedstock,"A Boeing 747 burns one gallon of jet fuel each second. A recent analysis from researchers at the University of Illinois estimate that this aircraft could fly for 10 hours on bio-jet fuel produced on 54 acres of specially engineered sugarcane.Plants Engineered to Replace Oil in Sugarcane and Sweet Sorghum (PETROSS), funded by the Advanced Research Projects Agency -- Energy (ARPA-E), has developed sugarcane that produces oil, called lipidcane, that can be converted into biodiesel or jet fuel in place of sugar that is currently used for ethanol production. With 20% oil -- the theoretical limit -- all of the sugar in the plant would be replaced by oil.""Oil-to-Jet is one of the direct and efficient routes to convert bio-based feedstocks to jet fuel,"" said Vijay Singh, Director of the Integrated Bioprocessing Research Laboratory. ""Reducing the feedstock cost is critical to improving process economics of producing bio-jet fuel. Lipidcane allows us to reduce feedstock cost.""This research analyzed the economic viability of crops with different levels of oil. Lipidcane with 5% oil produces four times more jet fuel (1,577 liters, or 416 gallons) per hectare than soybeans. Sugarcane with 20% oil produces more than 15 times more jet fuel (6,307 liters, or 1,666 gallons) per hectare than soybeans.""PETROSS sugarcane is also being engineered to be more cold tolerant, potentially enabling it to be grown on an estimated 23 million acres of marginal land in the Southeastern U.S.,"" said PETROSS Director Stephen Long, Gutgsell Endowed Professor of Plant Biology and Crop Sciences at the Carl R. Woese Institute for Genomic Biology at the University of Illinois. ""If all of this acreage was used to produce renewable jet fuel from lipid-cane, it could replace about 65% of national jet fuel consumption.""""We estimate that this biofuel would cost the airline industry $5.31/gallon, which is less than most of the reported prices of renewable jet fuel produced from other oil crops or algae,"" said Deepak Kumar, a postdoctoral researcher at Illinois, who led the analysis.This crop also produces profitable co-products: A hydrocarbon fuel is produced along with bio-jet fuel or biodiesel that can be used to produce various bioproducts. The remaining sugar (for plants with less than 20% oil) could be sold or used to produce ethanol. In addition, biorefineries could use lipidcane bagasse to produce steam and electricity to become self-sustainable for their energy needs and provide surplus electricity, providing environmental benefits by displacing electricity produced with fossil fuels.PETROSS (Plants Engineered to Replace Oil in Sugarcane and Sorghum) is a research project transforming sugarcane and sweet sorghum to naturally produce large amounts of oil, a sustainable source of biofuel. PETROSS is supported by the Advanced Research Projects Agency-Energy (ARPA-E), which funds initial research for high-impact energy technologies to show proof of concept before private-sector investment.Story Source:Materials provided by Carl R. Woese Institute for Genomic Biology, University of Illinois at Urbana-Champaign. ",0.139969,0.194209,0.174266,0.178094,0.143181,0.202102,0.194689,0,0
182,Do video game players make the best unmanned pilots?,"New research from the University of Liverpool highlights the usefulness of Video Game Players (VGPs) as unmanned aircraft operators.The move to significant automation has been a feature of aviation over the last 40 years. Unmanned Aerial Systems (UAS) operations, commonly known as a aircraft which are unmanned, have outpaced current training regimes resulting in a shortage of qualified UAS pilots.In an effort to address this problem researchers from the University's Institute of Psychology, Health and Society, led by Dr Jacqueline Wheatcroft, and the Department of Mechanical, Materials and Aerospace Engineering (Dr Mike Jump), explored the suitability of three potential UAS groups; VGPs, private pilots and professional pilots.The participants, 60 in total, all took part in a simulated civilian cargo flight to enable the researchers to assess their levels of accuracy, confidence and confidence-accuracy judgements (W-S C-A).The participants made 21 decision tasks, which varied across three levels of danger/ risk.As danger increased levels of confidence, accuracy and the relationship between how accurate the decision was and the level of confidence applied to those decisions decreased.The dangerousness of the decision also affected how confident participants were when choosing to intervene or rely on the automation; confidence was lower when the operator chose to intervene.Professional pilots and VGPs exhibited the highest level of decision confidence, with VGPs maintaining a constant and positive W-S C-A relationship across decision danger/risk.All groups showed higher levels of decision confidence in decisions controlled by the UAS in comparison to decisions where the operator manually intervened.Dr Jacqueline Wheatcroft, said: ""Understanding which potential supervisory group has the best skills to make the best decisions can help to improve UAS supervision. Overall, video game players were less overconfident in their decision judgements.""The outcome supports the idea that this group could be a useful resource in UAS operation.""Story Source:Materials provided by University of Liverpool. ",0.165993,0.15401,0.20418,0.271676,0.183428,0.210775,0.225699,0,0
183,"Long term exposure to aircraft noise linked to high blood pressure Night-time noise may be particularly influential, findings suggest","Long term exposure to aircraft noise, particularly during the night, is linked to an increased risk of developing high blood pressure and possibly heart flutter and stroke as well, suggests research published online in Occupational & Environmental Medicine.The research team drew on data from 420 people living near Athens International Airport in Greece, where up to 600 planes take off and land every day.They formed one of six groups of people living near six large European airports who had taken part in the HYENA study, which assessed the potential health impacts of aircraft noise in 2004-6.The aircraft and road traffic noise exposure levels estimated for their postcodes at that time -- less than 50 decibels to more than 60 dB -- were used for the current study in 2013.Daytime aircraft noise was defined as that occurring between 0700 and 2300 hours, and that occurring between 2300 and 0700 hours was defined as night-time aircraft noise.Around half of the participants (just under 49%) were exposed to more than 55 dB of daytime aircraft noise, while around one in four (just over 27%) were exposed to more than 45 dB of night-time aircraft noise. Only around one in 10 (11%) were exposed to significant road traffic noise of more than 55 dB.Between 2004-6 and 2013, 71 people were newly diagnosed with high blood pressure and 44 were diagnosed with heart flutter (cardiac arrhythmia). A further 18 had a heart attack.Exposure to aircraft noise, particularly at night, was associated with all cases of high blood pressure, and with new cases.When all cases of high blood pressure were included, every additional 10 dB of night-time aircraft noise was associated with a 69% heightened risk of the condition. When only new cases were included, every additional 10 dB was associated with a more than doubling in risk.Exposure to night-time aircraft noise was also associated with a doubling in risk of heart flutter diagnosed by a doctor, but this only reached statistical significance when all cases, not just new ones, were included in the calculations.A heightened risk of stroke was similarly linked to increasing aircraft noise exposure, but this was not statistically significant, possibly because of the small number of cases involved, suggest the researchers.The associations between road traffic noise and ill health were much weaker and less consistent, the findings showed.This is one of the first long term follow-up studies of aircraft noise so it's not possible to draw conclusions about cause and effect at this stage until more evidence/studies become available, say the researchers.They point out that they were unable to look at specific causes of death among the 78 people who died between 2004-6 and 2013. The numbers studied were also relatively small, and it wasn't possible to account for the potential effects of air pollution.Nevertheless, a growing body of evidence links noise exposure to ill health, they emphasize.Story Source:Materials provided by BMJ. ",0.187641,0.152847,0.176396,0.178057,0.156738,0.217047,0.177197,0,0
184,Autonomous 'soaring with solar' concept,"Researchers at the U.S. Naval Research Laboratory (NRL), Vehicle Research Section and Photovoltaic Section are building on the proven concept of autonomous cooperative soaring of unmanned aerial vehicles (UAVs). Their research investigates the presence of solar photovoltaics (PV) to the cooperative autonomous soaring techniques, which enables long endurance flights of unmanned sailplanes that use the power of the sun.The Solar Photovoltaic and Autonomous Soaring Base Program and the U.S. Marine Corps' Expeditionary Energy Office (E2O) want to improve the ability of unmanned platforms to support a 24-7 information, surveillance, and reconnaissance (ISR) mission. By doing so, the warfighter will greatly benefit because it will reduce the amount of batteries or fuel they must carry into battle, and improve the availability of continuous coverage of ISR assets.""NRL has twice flown our solar UAV [based on the SBXC sailplane] over 10 hours using a combination of solar photovoltaics and autonomous soaring as part of the 'solar-soaring' research program,"" said Dr. Dan Edwards, aerospace engineer. ""This research is investigating the value of combining autonomous soaring algorithms and solar photovoltaics for capturing energy from the environment to extend flight endurance and mission operations of an aircraft.""A photovoltaic array, custom built in NRL's Vehicle Research Section and Photovoltaic Section, is integrated into the center wing panel of the PV-SBXC aircraft as a drop-in replacement to the original wing. A power management and distribution system converts the power from the solar arrays into direct current (DC) voltage, which the electric motor can use for propulsion, or recharge a 'smart battery.'Additionally, an autonomous soaring software algorithm -- that would typically monitor the local vertical winds around the aircraft -- commands the aircraft to orbit in any nearby updrafts, very similar to soaring birds. However, the algorithm was disabled for the two solar flights in order to assess the solar-only performance. Passive soaring -- meaning no specific maneuvers are attempted to catch thermals -- was still allowed, to let the aircraft turn the motor off if altitude increased because of an updraft along the aircraft's pre-defined flight path. The autonomous soaring software was tested extensively in previous flight demonstrations in late October 2015.The UAV with solar arrays built at NRL using SunPower Inc. solar cells, flew for 10 hours, 50 minutes on October 14, 2016. Takeoff occurred at 7:20 a.m. at 95 percent battery state of charge and landing occurred at 6:10 p.m. with the battery at 10 percent state of charge. Thermal activity was very good in the middle of the day and 40 percent of the flight was spent with the motor off, and the solar array partly recharged the battery while the motor was off.The UAV equipped with solar wings incorporated PV arrays from Alta Devices, Inc. It flew for 11 hours, 2 minutes on April 19, 2017. Takeoff occurred at 7:46 a.m., approximately an hour after sunrise, with the battery's state of charge at 90 percent. Landing occurred at 6:48 p.m., approximately an hour before sunset, with the battery's state of charge at 26 percent. Thermal activity was very weak and almost all of the flight was spent running the motor. Near solar noon, the solar array provided sufficient power to cruise on solar power alone.The power management system for both flights was provided by Packet Digital, Inc., as part of a grant from the North Dakota Renewable Energy Council.""The experiments confirm significant endurance gains are possible by leveraging thermal updrafts and incident solar radiation, rather than ignoring these free sources of energy,"" Edwards said. ""Future testing will focus on quantifying the trade space between improvements in solar cell efficiency and combining with autonomous soaring for improved solar-recharging.""The Vehicle Research Section at NRL conducts research to develop technologies for autonomous, affordably expendable, unmanned systems that carry a wide variety of payloads for numerous mission scenarios. The Section is composed of aeronautical, aerospace, electrical, and mechanical engineers, scientists, and technicians dedicated to advancing the state-of-the-art in unmanned systems technology.The Photovoltaics Section at NRL conducts research to develop photovoltaic (solar cell) technologies to enable logistics free, renewable, portable, power sources for the warfighter. The Section is composed of physicists, electrical engineers, and chemists dedicated to advancing the state-of-the-art in PV power sources and systems.Story Source:Materials provided by Naval Research Laboratory. ",0.191196,0.137843,0.14009,0.170307,0.161885,0.207589,0.158901,0,0
185,"In-flight, on-demand hydrogen production could mean 'greener' aircraft ","Aerospace engineers at the Technion-Israel Institute of Technology have developed and patented a process that can be used onboard aircraft while in flight to produce hydrogen from water and aluminum particles safely and cheaply. The hydrogen can then be converted into electrical energy for inflight use. The breakthrough could pave the way for non-polluting, more-electric aircraft that replace current hydraulic and pneumatic systems typically powered by the main engine.The groundbreaking work was reported in a recent paper published in the International Journal of Hydrogen Energy.""Hydrogen produced onboard the aircraft during flight can be channeled to a fuel cell for electrical energy generation,"" said lead researcher Dr. Shani Elitzur of the Technion Faculty of Aerospace Engineering. ""This technology offers a good solution to several challenges, such as hydrogen storage, without the problems associated with storing hydrogen in a liquid or gas state.""While the use of hydrogen fuels has been a potential greener energy solution for some time, storing hydrogen has always been a problem. The engineers were able to work around the hydrogen storage problem by using non-polluting Proton Exchange Membrane (PEM) fuel cells and a process of aluminum activation patented by the paper's co-authors, Prof. Alon Gany and Dr. Valery Rosenband.Dr. Elitzur's research was focused on the reaction between the activated aluminum powder and water (from different types) to produce hydrogen. The foundation for the technology is in the chemical reaction between aluminum powder and water to produce hydrogen. Either fresh water or waste water, already onboard the aircraft, can be used for activation, which means the aircraft does not need to carry any additional water.The spontaneous and sustained reaction between powdered aluminum and water is enabled by a special thermo-chemical process of aluminum activation the researchers developed. The protective properties of the oxide or hydroxide film covering the aluminum particle surface are modified by a small fraction of lithium-based activator diffused into aluminum bulk, allowing water at room temperature to react spontaneously with the aluminum.The process does generate heat, which the researchers say can be used for a number of tasks, including heating water and food in the galley, de-icing operations, or heating aircraft fuel prior to starting the engines.According to the researchers, their technology would provide:""The possibility of using available, onboard wastewater boosts both the efficiency and safety of the system,"" explained Dr. Rosenband. ""Also, the PEM fuel cells exhibit high efficiency in electric energy generation.""Aircraft manufacturers, including Boeing and Airbus, have already investigated using onboard fuel cells. Boeing has experimented with them in smaller aircraft, in anticipation of using them on its 787-8, the current state-of-the-art electric airplane. According to the Technion researchers, fuel cells can even play an energy saving role in airline and airport ground support operations when they are on used for systems such as de-icing and runway light towers.""Efficient hydrogen production and storage represents the future for efficient and safe aircraft inflight energy needs."" summarized Prof. Gany.Story Source:Materials provided by American Technion Society. ",0.127727,0.207135,0.127828,0.137058,0.129985,0.17273,0.152754,0,0
186,"Drones collect measurements from a volcanic plume at Volcen de Fuego, Guatemala ","A team of volcanologists and engineers from the Universities of Bristol and Cambridge have collected measurements from directly within volcanic clouds, together with visual and thermal images of inaccessible volcano peaks.During a ten-day research trip the team carried out many proof-of-concept flights at the summits of both Volcen de Fuego and Volcen de Pacaya in Guatemala. Using lightweight modern sensors they measured temperature, humidity and thermal data within the volcanic clouds and took images of multiple eruptions in real-time.This is one of the first times that bespoke fixed-wing unmanned aerial vehicles (UAVs) have been used at a volcano such as Fuego, where the lack of close access to the summit vent has prevented robust gas measurements. Funding from the Cabot Institute has helped the team to develop technologies to enable this capability. The UAVs were successfully flown beyond-visual-line-of-sight (BVLOS) at distances of up to 8 kilometres away, and 10,000 feet above the launch site.The group plan to return to Guatemala later in the year with a wider range of sensors including a multiGAS gas analyser (CO2, SO2, H2S); a four-stage filter pack; carbon stubs for ash sampling; thermal and visual cameras, and atmospheric sensors.Dr Emma Liu, Volcanologist from the Department of Earth Sciences at Cambridge, said: ""Volcanoes are prodigious sources of volatiles and trace metals and have a key role in the geochemical cycling of these elements through the Earth system. Drones offer an invaluable solution to the challenges of in-situ sampling and routine monitoring of volcanic emissions, particularly those where the near-vent region is prohibitively hazardous or inaccessible. These sensors not only help to understand emissions from volcanoes, they could also be used in the future to help alert local communities of impending eruptions -- particularly if the flights can be automated.""Dr Tom Richardson, Senior Lecturer in Flight Dynamics in the Department of Aerospace Engineering at Bristol, explained: ""Building on our award winning work on Ascension Island, the team carried out multiple beyond-visual-line-of-sight (BVLOS) flights from the observatory flying up to 10,000 feet above the launch site to reach the summit of Volcen de Fuego. Our success has resulted in direct invitations from the Direccien General de Aeroneutica Civil and Instituto Nacional de Sismologea, Vulcanologea, Meteorologea e Hidrologea to return and continue this ground-breaking work.""Dr Kieran Wood, Senior Research Associate in the Department of Aerospace Engineering at Bristol, added: ""Even during this initial campaign we were able to meet significant science and engineering targets. For example, multiple imaging flights over several days captured the rapidly changing topography of Fuego's summit. These showed that the volcano was erupting from not just one, but two active summit vents.""Ben Schellenberg, a first year Aerospace Engineering PhD student at Bristol, expressed: ""Being involved in a field trip of this type so early on in my PhD has been incredibly exciting. Initial analysis of the sensor and flight data tell us that we will be able to automatically identify when we are in volcanic emissions. I can't wait to return to test out this hypothesis.""Taking time out from their sample flights, the research group also used their aircraft to map the topology of a barranca and the volcanic deposits within it. These deposits were formed by a recent pyroclastic flow, a fast-moving cloud of superheated ash and gas, which travelled down the barranca from Fuego. The data captured will assist in modelling flow pathways and the potential impact of future volcanic eruptions on nearby settlements.Dr Matt Watson, Reader in Natural Hazards in the School of Earth Sciences at Bristol, said: ""This is exciting initial research for future investigations, and would not be possible without a very close collaboration between volcanology and engineering.""The team of engineers and volcanologists involved in the research trip were: Dr Colin Greatwood, Dr Tom Richardson, Ben Schellenberg, Dr Helen Thomas, Dr Matt Watson, Dr Kieran Wood from the University of Bristol and Dr Emma Liu from the University of Cambridge.Story Source:Materials provided by University of Bristol. ",0.133406,0.108814,0.1669,0.174632,0.122702,0.172724,0.15445,0,0
187,Computer linguists are developing an intelligent system aid for air traffic controllers,"Together with the German Aerospace Center (DLR), computer scientists from Saarland University have now developed a new system that listens in to these conversations and engages with the controllers. The scientists are presenting their prototype at the Cebit computer fair in Hannover, Germany.Air traffic controllers are responsible for keeping aircraft at a safe distance to one other in the air and on runways and airstrips. Their most important tool is the radar, which uses radio waves to pinpoint the positions of the airplanes and measure their relative distances. The so-called system aid that air traffic controllers use for planning proposes an optimal order for the airplanes in that particular airspace. These automated suggestions are based on radar data. The controller then radios in with the individual pilots to communicate the correct order. So far, the system aid has been excluded from these short and often terse dialogues between controller and pilot. This lowers the quality of the system aid's automated suggestions, which is particularly dangerous in critical situations. ""The more stressful the situation is, the less you can rely on the system aid,"" Youssef Oualil points out. Oualil is a researcher in the Department of Language Science and Technology at Saarland University. Together with his colleague Marc Schulder, the professor of Spoken Language Systems at Saarland University, Dietrich Klakow, as well as Hartmut Helmke from the German Aerospace Center DLR, Oualil developed a software system named ""AcListant,"" which listens in to air controllers' radio conversations and makes more informed suggestions for their current situation.The researchers relied entirely on automatic speech recognition, so that controllers do not have to enter any new commands themselves by keyboard or mouse. As the speech recognition system is supposed to filter nonsensical or unsuitable commands out immediately, the computer scientists incorporated additional information from the system aid, so that the controllers' display will only include commands that actually match the current situation. The software system only filters such basic information that is actually relevant to controllers. ""This means that verbal padding like Hello or Good morning will be edited out, but identification numbers, altitudes and commands stay in,"" Marc Schulder explains. The system also performs a kind of reality check in which it incorporates current information from the radar. Data from the radar is used to generate probable word sequences, and then only such pieces of information that are most similar to the generated phrases are subsequently forwarded to the system aid. The flight controller is then shown these filtered items as suggested instructions for the pilot.The researchers have already tested their prototype in various simulations for major airports at the DLR Research Airport in Braunschweig. ""With AcListant, we have not just reduced the number of incorrect commands that are processed by a factor of four, compared to less sophisticated systems. The flight controllers are also able to communicate a lot better with pilots who talk very fast or with an accent,"" says Dietrich Klakow. The German Aerospace Center is now trying to promote the commercialization of the system.AcListant is not the only research project that Klakow and his colleagues are working on. One focus of their work is teaching computers to understand ambiguous statements, and learning to recognize the dominant sentiment -- in future even in ambiguous expressions like ""damn good"" or ""pretty bad."" Users will be able to ask their computer intuitive questions, and receive answers like from a human respondent. The Saarbruecken scientists also developed a special software for the computer game ""Sonar Silence,"" which automatically understands in-game questions of players of different nationalities, and responds with the according information in the appropriate language. The core elements of this technology can be used both in medicine, for instance for the early diagnosis and detection of depressive episodes, and in online retail, for instance for automatic responses to customer requests.Further InformationProject website: http://www.aclistant.deStory Source:Materials provided by Saarland University. ",0.107006,0.0711332,0.09713,0.226862,0.120027,0.139972,0.137844,0,0
188,New driverless car technology could make traffic lights and speeding tickets obsolete Pair of studies outline innovations that will improve coordination of traffic patterns and save fuel,"Imagine a daily commute that's orderly instead of chaotic. Connected and automated vehicles could provide that relief by adjusting to driving conditions with little to no input from drivers. When the car in front of you speeds up, yours would accelerate, and when the car in front of you screeches to a halt, your car would stop, too.At the University of Delaware, Andreas Malikopoulos uses control theory to develop algorithms that will enable this technology of the future. In two recently published papers, Malikopoulos, who was recently named the Terri Connor Kelly and John Kelly Career Development Professor of Mechanical Engineering, describes innovations in connected and automated vehicle technology pioneered in two laboratories at the University, the UD Scaled Smart City (UDSSC) testbed and a driving simulator facility.""We are developing solutions that could enable the future of energy efficient mobility systems,"" said Malikopoulos. ""We hope that our technologies will help people reach their destinations more quickly and safely while conserving fuel at the same time.""Making traffic lights obsoleteSomeday cars might talk to each other to coordinate traffic patterns. Malikopoulos and collaborators from Boston University recently developed a solution to control and minimize energy consumption in connected and automated vehicles crossing an urban intersection that lacked traffic signals. Then they used software to simulate their results and found that their framework allowed connected and automated vehicles to conserve momentum and fuel while also improving travel time. The results were published in the journal Automatica.Saving fuel and avoiding speeding ticketsImagine that when the speed limit goes from 65 to 45 mph, your car automatically slows down. Malikopoulos and collaborators from the University of Virginia formulated a solution that yields the optimal acceleration and deceleration in a speed reduction zone, avoiding rear-end crashes. What's more, simulations suggest that the connected vehicles use 19 to 22 percent less fuel and get to their destinations 26 to 30 percent faster than human-driven vehicles. The results of this research effort were published in IEEE Transactions on Intelligent Transportation Systems.Malikopoulos has received funding for this work from two U.S. Department of Energy programs -- the Smart Mobility Initiative and the Advanced Research Projects Agency -- Energy's NEXTCAR program.Malikopoulos is the principal investigator of a three-year project funded by the Advanced Research Projects Agency for Energy (ARPA-E) through its NEXT-Generation Energy Technologies for Connected and Automated On-Road Vehicles (NEXTCAR) program to improve the efficiency of an Audi A3 e-tron by at least 20 percent. The partners of this project are the University of Michigan, Boston University, Bosch Corporation, and Oak Ridge National Laboratory.Story Source:Materials provided by University of Delaware. ",0.0998908,0.101585,0.116318,0.182077,0.100238,0.174928,0.139971,0,0
189,How should autonomous vehicles be programmed? Massive global survey reveals ethics preferences and regional differences,"A massive new survey developed by MIT researchers reveals some distinct global preferences concerning the ethics of autonomous vehicles, as well as some regional variations in those preferences.The survey has global reach and a unique scale, with over 2 million online participants from over 200 countries weighing in on versions of a classic ethical conundrum, the ""Trolley Problem."" The problem involves scenarios in which an accident involving a vehicle is imminent, and the vehicle must opt for one of two potentially fatal options. In the case of driverless cars, that might mean swerving toward a couple of people, rather than a large group of bystanders.""The study is basically trying to understand the kinds of moral decisions that driverless cars might have to resort to,"" says Edmond Awad, a postdoc at the MIT Media Lab and lead author of a new paper outlining the results of the project. ""We don't know yet how they should do that.""Still, Awad adds, ""We found that there are three elements that people seem to approve of the most.""Indeed, the most emphatic global preferences in the survey are for sparing the lives of humans over the lives of other animals; sparing the lives of many people rather than a few; and preserving the lives of the young, rather than older people.""The main preferences were to some degree universally agreed upon,"" Awad notes. ""But the degree to which they agree with this or not varies among different groups or countries."" For instance, the researchers found a less pronounced tendency to favor younger people, rather than the elderly, in what they defined as an ""eastern"" cluster of countries, including many in Asia.The paper, ""The Moral Machine Experiment,"" is being published in Nature.The authors are Awad; Sohan Dsouza, a doctoral student in the Media Lab; Richard Kim, a research assistant in the Media Lab; Jonathan Schulz, a postdoc at Harvard University; Joseph Henrich, a professor at Harvard; Azim Shariff, an associate professor at the University of British Columbia; Jean-Franeois Bonnefon, a professor at the Toulouse School of Economics; and Iyad Rahwan, an associate professor of media arts and sciences at the Media Lab.Awad is a postdoc in the MIT Media Lab's Scalable Cooperation group, which is led by Rahwan.To conduct the survey, the researchers designed what they call ""Moral Machine,"" a multilingual online game in which participants could state their preferences concerning a series of dilemmas that autonomous vehicles might face. For instance: If it comes right down it, should autonomous vehicles spare the lives of law-abiding bystanders, or, alternately, law-breaking pedestrians who might be jaywalking? (Most people in the survey opted for the former.)All told, ""Moral Machine"" compiled nearly 40 million individual decisions from respondents in 233 countries; the survey collected 100 or more responses from 130 countries. The researchers analyzed the data as a whole, while also breaking participants into subgroups defined by age, education, gender, income, and political and religious views. There were 491,921 respondents who offered demographic data.The scholars did not find marked differences in moral preferences based on these demographic characteristics, but they did find larger ""clusters"" of moral preferences based on cultural and geographic affiliations. They defined ""western,"" ""eastern,"" and ""southern"" clusters of countries, and found some more pronounced variations along these lines. For instance: Respondents in southern countries had a relatively stronger tendency to favor sparing young people rather than the elderly, especially compared to the eastern cluster.Awad suggests that acknowledgement of these types of preferences should be a basic part of informing public-sphere discussion of these issues. In all regions, since there is a moderate preference for sparing law-abiding bystanders rather than jaywalkers, knowing these preferences could, in theory, inform the way software is written to control autonomous vehicles.""The question is whether these differences in preferences will matter in terms of people's adoption of the new technology when [vehicles] employ a specific rule,"" he says.Rahwan, for his part, notes that ""public interest in the platform surpassed our wildest expectations,"" allowing the researchers to conduct a survey that raised awareness about automation and ethics while also yielding specific public-opinion information.""On the one hand, we wanted to provide a simple way for the public to engage in an important societal discussion,"" Rahwan says. ""On the other hand, we wanted to collect data to identify which factors people think are important for autonomous cars to use in resolving ethical tradeoffs.""Beyond the results of the survey, Awad suggests, seeking public input about an issue of innovation and public safety should continue to become a larger part of the dialogue surrounding autonomous vehicles.""What we have tried to do in this project, and what I would hope becomes more common, is to create public engagement in these sorts of decisions,"" Awad says.Story Source:Materials provided by Massachusetts Institute of Technology. Original written by Peter. ",0.053928,0.0903278,0.154874,0.218864,0.0926141,0.116737,0.156915,0,0
190,"Emissions from most diesel cars in Europe greatly exceed laboratory testing levels Real-word driving produces up to 16 times more emissions, causing 2,700 premature deaths across the EU","In September 2015, the German automaker Volkswagen was found to have illegally cheated federal emissions tests in the United States, by intentionally programming emissions control devices to turn on only during laboratory testing. The devices enabled more than 11 million passenger vehicles to meet U.S. emissions standards in the laboratory despite producing emissions up to 40 times higher than the legal limit in real-world driving conditions.Now a new MIT study reports that Volkswagen is not the only auto manufacturer to make diesel cars that produce vastly more emissions on the road than in laboratory tests. The study, published this month in Atmospheric Environment, finds that in Europe, 10 major auto manufacturers produced diesel cars, sold between 2000 and 2015, that generate up to 16 times more emissions on the road than in regulatory tests -- a level that exceeds European limits but does not violate any EU laws.What's more, the researchers predict these excess emissions will have a significant health impact, causing approximately 2,700 premature deaths per year across Europe. These health effects, they found, are ""transboundary,"" meaning that diesel emissions produced in one country can adversely affect populations in other countries, thousands of kilometers away.""You might imagine that where the excess emissions occur is where people might die early,"" says study author Steven Barrett, the Raymond L. Bisplinghoff Professor of Aeronautics and Astronautics at MIT. ""But instead we find that 70 percent of the total [health] impacts are transboundary. It suggests coordination is needed not at the country, but at the continental scale, to try to solve this problem of excess emissions.""The 10 manufacturers' excess emissions may not be a result of unlawful violations, as was the case with Volkswagen. Instead, the team writes that ""permissive testing procedures at the EU level and defective emissions control strategies"" may be to blame.The researchers report a silver lining: If all 10 auto manufacturers were to improve their emissions control technologies to perform at the same level as the best manufacturer in the group, this would prevent up to 1,900 premature deaths per year.""That's pretty significant in terms of the number of premature mortalities that would be avoided,"" Barrett says.Barrett's co-authors at MIT are Guillaume Chossiere, Robert Malina (now at Hasselt University), Florian Allroggen, Sebastian Eastham, and Raymond Speth.Tuning the knobsThe study focuses on emissions of nitrogen oxides, or NOx, a type of gas that is produced in diesel exhaust. When the gas gets oxidized and reacts with ammonia in the atmosphere, it forms fine particles and can travel for long distances before settling. When these particles are inhaled, they can lodge deep in the lungs, causing respiratory disease, asthma, and other pulmonary and cardiac conditions. Additionally NOx emissions cause the formation of ozone, a pollutant long associated with adverse health outcomes.""There are many times the number of diesel cars in Europe compared to the U.S., partly because the EU started pushing diesel for environmental reasons, as it produces less carbon dioxide emissions compared with [gasoline],"" Barrett says. ""It's a case where diesel has probably been beneficial in terms of climate impacts, but it's come at the cost of human health.""Recently, the EU started tightening its standards for diesel exhaust to reduce NOx emissions and their associated health effects. However, independent investigations have found that most diesel cars on the road do not meet the new emissions standards in real driving conditions.""Initially manufacturers were able to genuinely meet regulations, but more recently it seems they've almost tweaked knobs to meet the regulations on paper, even if in reality that's not reproduced on the road,"" Barrett says. ""And that's not been illegal in Europe.""Life exposureIn this study, Barrett and his colleagues quantified the health impacts in Europe of excess NOx emissions -- emissions that were not accounted for in standard vehicle testing but are produced in actual driving conditions. They also estimated specific manufacturers' contributions to the total health impacts related to the excess emissions.The researchers considered 10 major auto manufacturers of diesel cars sold in Europe, for which lab and on-road emissions data were available: Volkswagen, Renault, Peugeot-Citroen, Fiat, Ford, General Motors, BMW, Daimler, Toyota, and Hyundai. Together, these groups represent more than 90 percent of the total number of diesel cars sold between 2000 and 2015, in 28 member states of the EU, along with Norway and Switzerland.For each manufacturer, the team calculated the total amount of excess emissions produced by that manufacturer's diesel car models, based on available emissions data from laboratory testing and independent on-road tests. They found that overall, diesel cars produce up to 16 times more NOx emissions on the road than in lab tests.They then calculated the excess emissions associated with each manufacturer's diesel car, by accounting for the number of those cars that were sold between 2000 and 2015, for each country in which those cars were sold.The team used GEOS-Chem, a chemistry transport model that simulates the circulation of chemicals and particles through the atmosphere, to track where each manufacturer's excess NOx emissions traveled over time. They then overlaid a population map of the EU onto the atmospheric model to identify specific populations that were most at risk of exposure to the excess NOx emissions.Finally, the team consulted epidemiological work to relate various populations' NOx exposure to their estimated health risk. The researchers considered four main populations in these calculations: adults with ischemic heart disease, stroke, chronic obstructive pulmonary disease, and lung cancer.Overall, they estimated that, each year, 2,700 people within these populations will lose at least a decade of their life due to exposure to excess NOx emissions from passenger cars. They broke this number down by manufacturer and found a wide spread of health impact contributions: Volkswagen, Renault, and General Motors produced diesel cars associated with the most yearly premature deaths, each numbering in the hundreds, while Toyota, Hyundai, and BMW were associated with fewer early deaths.""The variation across manufacturers was more than a factor of five, which was much bigger than we expected,"" Barrett says.""There's no safe level""For each country, the team also compared the excess emissions that it produced itself, versus the number of premature deaths that its population incurred, and found virtually no relationship. That is, some countries, such as Poland and Switzerland, produced very little NOx emissions and yet experienced a disproportionate number of premature deaths from excess emissions originating in other countries.Barrett says this transboundary effect may be due to the nature of NOx emissions. Unlike particulate matter spewed from smokestacks, such as soot, which mostly settles out in the local area, NOx is first emitted as a gas, which can be carried easily by the wind across thousands of kilometers, before reacting with ammonia to form particulates, a form of the chemical that can ultimately cause respiratory and cardiac problems.""There's almost no correlation between who drives [diesel cars] and who incurs the health disbenefits, because the impacts are so diffuse through all of Europe,"" Barrett says.The study ends with a final result: If all 10 manufacturers were to meet the on-road emissions performance of the best manufacturer in the group, this would avoid 1,900 premature deaths due to NOx exposure. But Barrett says ultimately, regulators and manufacturers will have to go even further to prevent emissions-associated mortalities.""The solution is to eliminate NOx altogether,"" Barrett says. ""We know there are human health impacts right down to pre-industrial levels, so there's no safe level. At this point in time, it's not that we have to go back to [gasoline]. It's more that electricification is the answer, and ultimately we do have to have zero emissions in cities.""Story Source:Materials provided by Massachusetts Institute of Technology. Original written by Jennifer Chu. ",0.0996258,0.143318,0.164788,0.177593,0.119307,0.182585,0.178764,0,0
191,Analyzing roadside dust to identify potential health concerns,"Everyone knows that cars contribute to air pollution. And when most people consider the source, exhaust is usually what comes to mind.However, new research led by the University of Pennsylvania's Reto Giere, working with collaborators across the world, is helping to illuminate another significant culprit when it comes to traffic-related air pollution: Tiny bits of tires, brake pads, and road materials that become suspended in the air when vehicles pass over.""More and more I've noticed that we don't know enough about what is on our roads,"" says Giere, professor and chair of Penn's Department of Earth and Environmental Science in the School of Arts and Sciences. ""If you have lots of traffic, cars, and trucks driving by, they re-suspend the dust on the roads into the atmosphere, and then it becomes breathable. To understand the potential health implications of these dust particles, it's really important to understand what's on the road.""While regulatory efforts have helped make cars cleaner and more efficient, those restrictions do not address the pollution that arises from tire and brake wear. Increasing urban congestion stands to aggravate these as sources of pollution and possibly adverse health effects.""About 4 million people die prematurely from air pollution each year,"" says Giere. ""From unsafe water the number is 2 million. Yet we have a United Nations Sustainable Development Goal about water pollution but not one about the air.""To shed light on the contents of traffic-related dust and the conditions that make it more likely to accumulate, Giere has teamed with German colleagues from the Federal Highway Research Institute, the German Meteorological Service, and the University of Freiburg to sample and analyze the air along roadsides. In 2017, they published the findings of a year-long sampling effort along two highly frequented motorways in Germany, one subject to more stop-and-go traffic and another in a more rural area bordered by agricultural fields.To passively collect the dust along the roadsides, they used customized cylindrical samplers with a transparent sticky foil at the bottom to trap particles that make their way in. The researchers checked the collection points and switched out the sticky ""trap"" weekly.Using optical microscopy to analyze the collected airborne particles, the team found that the site with busier traffic patterns had 30 percent more particles overall, with a greater fraction derived from tire wear. Weather factored significantly into the patterns they observed; dry and warm conditions were associated with a greater build-up of particles.""At higher temperatures we saw more tire abrasion, more pollution than at intermediate temperatures,"" Giere says. ""This was exactly analogous to what two laboratory studies found.""With higher temperatures and more dry spells predicted under climate change, Giere notes that this problem of tire abrasion may only get worse, ""which is significant,"" he says, ""because nearly 30 percent of the microplastics released globally to the oceans are from tires.""In a more recent study, published last month in Aerosol and Air Quality Research, Giere and colleagues used powerful scanning electron microscopy to more precisely identify the make-up of the particles collected from the two motorways studied in the 2017 report as well as a third collection site, at an urban highway with slower-moving traffic.""The optical microscope gives us a first approximation,"" Giere says, ""while the scanning electron microscope allows us to distinguish between tire abrasion, brake abrasion, carbon, or find out if there are minerals in there.""Taking a further step, the team also ran samples through an analysis that provides information about the elements that compose each specimen, called energy-dispersive X-ray spectroscopy.This study focused on the ""super coarse"" particles collected, those greater than 10 micrometers in size. (For comparison, a human hair is roughly 75 micrometers in diameter.) While still tiny, these particles pose less of a health threat than those even smaller, which are more easily inhaled. Still, these larger particles can wind up in waterways and soil, affecting wildlife or possibly even agricultural crops.Ninety percent of the dust particles collected from the three sites were traffic-related and the researchers again saw differences between the sites. The slower-moving traffic on the urban road generated fewer particles from brake wear but more from tires; they noted that the tire rubber became encrusted with minerals and other materials from the roads. The highway with more stop-and-go traffic generated more brake particles.Tire and brake pad manufacturers do not disclose all the contents of their products, but it's known that zinc, lead, antimony, silicates, cadmium, and asbestos are used by some. These are chemicals that can pose a health risk if they get into the environment or, if the tires are burned as they sometimes are by coal plants, the atmosphere.""These coarse particles aren't going to be transported very far, so pollution is going to be restricted to the vicinity of these roads, especially during congestion,"" Giere says. ""But they do also collect on the road and then wash into rivers. Our team believes that's a major pathway of how microplastics get into waterways.""One way to reduce this avenue of pollution would be traffic-calming measures, such as coordinated traffic lights, that reduce the amount of starting and stopping that drivers must perform. Giere and colleagues, including Ph.D. student Michael O'Shea, are also performing similar experiments on the streets of Philadelphia and comparing the pollution levels between different neighborhoods to see what is happening a little closer to home.The work was financially supported in part by the Federal Highway Research Institute in cooperation with the German Meteorological Service.Story Source:Materials provided by University of Pennsylvania. ",0.0628759,0.0968138,0.155077,0.132452,0.0826019,0.122417,0.120782,0,0
192,Swarmlike collective behavior in bicycling Study of large-scale collective behavior of bicycle racers in pelotons gives clues to cognition and crowd dynamics,"Whether it's the acrobatics of a flock of starlings or the synchronized swimming of a school of fish, nature is full of examples of large-scale collective behavior. Humans also exhibit this behavior, most notably in pelotons, the mass of riders in bicycle races.During the American Physical Society's Division of Fluid Dynamics 71st Annual Meeting, which will take place Nov. 18-20 at the Georgia World Congress Center in Atlanta, Georgia, Jesse Belden, a researcher at the Naval Undersea Warfare Center, will describe the research he and his colleagues have been conducting on collective behavior in pelotons.Using aerial video footage of bicycle races, Belden and colleagues analyzed peloton motion to determine what causes changes in the group's large-scale collective behavior. They found that riders move through the peloton in a manner similar to circulation in a fluid and observed two types of propagating waves within pelotons. ""You see all these patterns and motion behaviors emerge,"" said Belden.The researchers found two types of waves affect the structure of a peloton. First, the researchers found a wave that moves back and forth along the peloton, usually due to a rider suddenly hitting the brakes and others slowing to avoid a collision. The other type of wave is a transverse wave caused when riders move to the left or right to avoid an obstacle or to gain an advantageous position.Pelotons maintain a persistent structure, and researchers previously thought this form was driven by individual riders seeking an aerodynamic advantage. However, aerodynamics only come into play at the outside edges of the peloton. Instead, the researchers found that peloton dynamics are likely driven by rider vision, with each rider keeping other riders within a range of peripheral vision that is most sensitive to motion. Additionally, wave propagation speeds were consistent with human reaction time rather than conscious cognitive decisions like improving aerodynamics.These findings shed new light on large-scale collective behavior in humans and could apply to varied topics including traffic and crowd management. Additionally, understanding the role of sensory input in collective behavior is important to building better autonomous vehicles like self-driving cars. This research has also given insights into the cognitive processes involved with individual rider actions and their effects on broader peloton dynamics. ""Unlike birds or fish, you can talk to the cyclists,"" Belden said.Story Source:Materials provided by American Physical Society. ",0.106924,0.105074,0.129189,0.168661,0.111115,0.148888,0.154699,0,0
193,Where you go tells who you are -- and vice versa,"Estimating travel demand in a city is a critical tool for urban planners to understand traffic patterns, predict traffic congestion, and plan ahead for transportation infrastructure maintenance and replacement. For years, researchers have used the classic practice of multiplying the number of trips per day per person for different demographic groups to model activity-based travel demand. But because this method was developed before the current era of ubiquitous sensors -- GPS devices, smartphones, cameras on light poles, and connected vehicles, among them -- researchers have found it difficult to validate their estimates in real-world situations.Mining data to analyze tracking patterns, Sharon Di, assistant professor of civil engineering and engineering mechanics at Columbia Engineering, has discovered that she can infer the population travel demand level in a region from the trajectories of just a portion of travelers. She took data collected from the world's first and largest connected vehicle testbed in Ann Arbor, led by University of Michigan Transportation Institute (UMTRI), and analyzed 349 vehicles' continuous one-year mobile traces (19,130 travel activities). She found three distinct groups and inferred their demographics based on their travel patterns:She and her PhD student Zhenyu Shou then validated their inferred demographics using survey data from UMTRI. Their findings are outlined in a study published by Transportation Research Part C September 18.""With the popularity of sensors everywhere, from our pockets to our cars, we can now trace individuals in terms of where they go, at what time, and what activity they may perform -- essentially, where you go tells who you are, and vice versa,"" says Di, who is also a member of the Data Science Institute. ""What we've learned from our analysis of the Michigan data will help us utilize future data collected from New York City's connected vehicles testbed to understand mobility patterns in the city and help relieve traffic congestion.""Because people tend to visit the same places for daily activities such as work, shopping, and dining, everyday mobile traces tend to be repetitive, but random events create deviations. Because most existing studies use just a single day or a few days of a smaller subset of people's mobile traces, they do not accurately or fully capture their longer term travel routines. A day or two of mobile traces also fails to capture recurring traffic jams.Di believes her study is the first to use data from an entire year. She built a probability tree for each driver to describe the frequency of their traces in a year and then used data mining tools to see to what extent the similarity of socio-demographics could explain travel patterns. She discovered that those who have similar mobility patterns are likely to belong to the same demographic group.Her work can be extended either to infer an unknown user's demographic, or customer profiling, based on activity patterns, or to reconstruct an unknown user's frequent activity patterns based on demographics and similar travelers' patterns. By establishing a quantitative relation between human mobility patterns and demographics, Di has laid a theoretical foundation to use individual mobile traces, which contain a sequence of places people visit, to estimate travel demand.""Di's and Shou's work demonstrates the utility of data science tools for discovering human mobility patterns,"" says Gowtham Atluri, a computer science professor at the University of Cincinnati, an expert in spatial-temporal data mining who was not involved in the study. ""Their overall framework is innovative and highlights the need for collaborative endeavors between transportation and data science researchers.""Di is looking now at scaling up a small sample of mobility patterns to a larger city level. New York City has one of the three US Department of Transportation connected vehicle testbeds and Di plans to collect a large amount of vehicle mobile traces. Once she has this data, she will generate human mobility patterns using the City's demographics, easily obtained from national census data.""There are so many more connected vehicles on the roads now that can ""talk"" both to each other and to roadside infrastructure to communicate where their exact location is and at what time,"" Di observes. ""Our synthetic trajectories will help city planners to predict traffic congestion and actively manage traffic.""Story Source:Materials provided by Columbia University School of Engineering and Applied Science. ",0.0815452,0.106325,0.157459,0.213274,0.136771,0.117713,0.15146,0,0
194,Smart car technologies save drivers $6.2 billion on fuel costs each year,"On one of the busiest traveling holidays of the year, drivers may be focusing on getting to grandma's house for Thanksgiving dinner, not on what smart car technologies are saving them in fuel costs. But in the first study to assess the energy impact of smart technology in cars, researchers at Stevens Institute of Technology have put a number on the potential fuel-cost savings alone: $6.2 billion.""That's not insignificant,"" said Yeganeh Hayeri, an assistant professor at Stevens whose work lies at the intersection of civil and environmental engineering and public policy. ""That translates to between $60 and $266 in the pocket of car owners every year, not to mention additional savings created for each driver due to more smoothly-flowing traffic, fewer accidents and aerodynamic efficiency of all other vehicles on the road.""While many studies have looked at the social impacts of driverless cars with high levels of automation, this study is the first to look at the energy impact on lower levels of automation and individual technologies that are either already introduced in our cars, pickup trucks and sport-utility vehicles or will be in the next few years.To figure out the impact of these technologies on fuel-saving cost, Hayeri and her colleagues at Stevens, including Saeed Vasebi, a graduate student in Hayeri's lab, and Carnegie Mellon University conducted a comprehensive review of the literature on the energy and safety impacts of automated features, providing precise data for predicting how these features would affect fuel consumption nationwide.Using these data, they then analyzed the benefits and costs associated with each automation technology, which were categorized into three groups: warning systems (i.e. for lane departures, blind spots, forward collisions, speed limit detection, and traffic warnings), control systems (i.e. for adaptive cruise control, collision detection braking, active braking and cooperative adaptive cruise control) and information systems (i.e. parking aid system and dynamic route guidance.)In their work, recently reported in Transportation Research Record, Hayeri, Vasebi and colleagues show that drivers of low-level automated vehicles (those equipped with all technologies considered in this study) could reduce fuel consumption by 27 to 119 gallons per year for each vehicle. This saving equals from 6 to 23 percent of average fuel consumption in the U.S. and could save each vehicle owner up to $60 to $266, as mentioned above.This Thanksgiving weekend, it is estimated that 54.3 million Americans are planning a road trip, a 4.8 percent increase over last year, despite drivers expecting to pay the highest Thanksgiving gas prices in four years, according to AAA. That means traffic congestion, construction, a dearth of parking, idling engines at stoplights, intersections and construction zones, not to mention getting lost.""Knowing when and where congestion will build can help drivers avoid the stress of sitting in traffic,"" said Hayeri. ""What we did is put a number of fuel-saving costs that come with technologies that assist us with making smarter choices on the road. We hope to use this information for improving future transportation, and consequentially improve the environment, save lives and keep the air we breathe cleaner.""Story Source:Materials provided by Stevens Institute of Technology. ",0.0960769,0.0874574,0.123717,0.158498,0.0995057,0.160432,0.134735,0,0
195,"Extending the life of low-cost, compact, lightweight batteries ","Metal-air batteries are one of the lightest and most compact types of batteries available, but they can have a major limitation: When not in use, they degrade quickly, as corrosion eats away at their metal electrodes. Now, MIT researchers have found a way to substantially reduce that corrosion, making it possible for such batteries to have much longer shelf lives.While typical rechargeable lithium-ion batteries only lose about 5 percent of their charge after a month of storage, they are too costly, bulky, or heavy for many applications. Primary (nonrechargeable) aluminum-air batteries are much less expensive and more compact and lightweight, but they can lose 80 percent of their charge a month.The MIT design overcomes the problem of corrosion in aluminum-air batteries by introducing an oil barrier between the aluminum electrode and the electrolyte -- the fluid between the two battery electrodes that eats away at the aluminum when the battery is on standby. The oil is rapidly pumped away and replaced with electrolyte as soon as the battery is used. As a result, the energy loss is cut to just 0.02 percent a month -- more than a thousandfold improvement.The findings are reported today in the journal Science by former MIT graduate student Brandon J. Hopkins '18, W.M. Keck Professor of Energy Yang Shao-Horn, and professor of mechanical engineering Douglas P. Hart.While several other methods have been used to extend the shelf life of metal-air batteries (which can use other metals such as sodium, lithium, magnesium, zinc, or iron), these methods can sacrifice performance Hopkins says. Most of the other approaches involve replacing the electrolyte with a different, less corrosive chemical formulation, but these alternatives drastically reduce the battery power.Other methods involve pumping the liquid electrolyte out during storage and back in before use. These methods still enable significant corrosion and can clog plumbing systems in the battery pack. Because aluminum is hydrophilic (water-attracting) even after electrolyte is drained out of the pack, the remaining electrolyte will cling to the aluminum electrode surfaces. ""The batteries have complex structures, so there are many corners for electrolyte to get caught in,"" which results in continued corrosion, Hopkins explains.A key to the new system is a thin membrane placed between the battery electrodes. When the battery is in use, both sides of the membrane are filled with a liquid electrolyte, but when the battery is put on standby, oil is pumped into the side closest to the aluminum electrode, which protects the aluminum surface from the electrolyte on the other side of the membrane.The new battery system also takes advantage of a property of aluminum called ""underwater oleophobicity"" -- that is, when aluminum is immersed in water, it repels oil from its surface. As a result, when the battery is reactivated and electrolyte is pumped back in, the electrolyte easily displaces the oil from the aluminum surface, which restores the power capabilities of the battery. Ironically, the MIT method of corrosion suppression exploits the same property of aluminum that promotes corrosion in conventional systems.The result is an aluminum-air prototype with a much longer shelf life than that of conventional aluminum-air batteries. The researchers showed that when the battery was repeatedly used and then put on standby for one to two days, the MIT design lasted 24 days, while the conventional design lasted for only three. Even when oil and a pumping system are included in scaled-up primary aluminum-air battery packs, they are still five times lighter and twice as compact as rechargeable lithium-ion battery packs for electric vehicles, the researchers report.Hart explains that aluminum, besides being very inexpensive, is one of the ""highest chemical energy-density storage materials we know of"" -- that is, it is able to store and deliver more energy per pound than almost anything else, with only bromines, which are expensive and hazardous, being comparable. He says many experts think aluminum-air batteries may be the only viable replacement for lithium-ion batteries and for gasoline in cars.Aluminum-air batteries have been used as range extenders for electric vehicles to supplement built-in rechargeable batteries, to add many extra miles of driving when the built-in battery runs out. They are also sometimes used as power sources in remote locations or for some underwater vehicles. But while such batteries can be stored for long periods as long as they are unused, as soon as they are turned on for the first time, they start to degrade rapidly.Such applications could greatly benefit from this new system, Hart explains, because with the existing versions, ""you can't really shut it off. You can flush it and delay the process, but you can't really shut it off."" However, if the new system were used, for example, as a range extender in a car, ""you could use it and then pull into your driveway and park it for a month, and then come back and still expect it to have a usable battery. ... I really think this is a game-changer in terms of the use of these batteries.""With the greater shelf life that could be afforded by this new system, the use of aluminum-air batteries could ""extend beyond current niche applications,"" says Hopkins. The team has already filed for patents on the process.The research was supported by MIT Lincoln Laboratory.Story Source:Materials provided by Massachusetts Institute of Technology. ",0.10013,0.177438,0.140019,0.156898,0.129663,0.180137,0.158388,0,0
196,Breakthrough in process to produce hydrogen fuel,"Ben-Gurion University of the Negev (BGU) and the Technion Israel Institute of Technology researchers have cracked the chemical mechanism that will enable development of a new and more efficient photo-chemical process to produce hydrogen fuel from water, according to a new paper published in Nature Communications.The team is the first to successfully reveal the fundamental chemical reaction present in solar power that could form the missing link to generate the electricity necessary to accomplish this process. It allows the process to unfold naturally instead of relying on large amounts of human-made energy sources or precious metals to catalyze the reaction. Production of hydrogen does not emit greenhouse gases, but the process has until now required more energy than is generated and as a result has limited commercial viability.""This discovery could have a significant impact on efforts to replace carbon-based fuels with more environmentally friendly hydrogen fuels,"" according to the team led by BGU researchers Dr. Arik Yochelis and Dr. Iris Visoly-Fisher and Prof. Avner Rothschild of the Technion. ""Car manufacturers seek to develop hydrogen-powered vehicles that are considered efficient and environmentally friendly and unlike electric vehicles, allow for fast refueling and extended mileage.""Hydrogen production for fuel requires splitting water molecules (H2O) into two hydrogen atoms and one oxygen atom. The research reveals a breakthrough toward understanding the mechanism that occurs during the photochemical splitting of hydrogen peroxide (H2O2) over iron-oxide photo-electrodes, which involves splitting the photo-oxidation reaction from linear to two sites.After years of challenging experiments during which Prof. Rothschild's laboratory was unable to overcome the barrier in efficiency, he approached Drs. Yochelis and Visoly-Fisher to collaborate and complete the puzzle.""Beyond the scientific breakthrough, we have shown that the photo-electrochemical reaction mechanism is related to the family of chemical reactions for which Prof. Gerhard Ertl was awarded the 2007 Nobel Prize in Chemistry,"" says Dr. Yochelis of the BGU's Alexandre Yersin Department of Solar Energy and Environmental Physics in the Jacob Blaustein Institutes for Desert Research. ""Our discovery opens new strategies for photochemical processes.""Story Source:Materials provided by American Associates, Ben-Gurion University of the Negev. ",0.0894987,0.221207,0.133299,0.169083,0.135243,0.145096,0.168826,0,0
197,Future cars to be made from revolutionary new material,"A new material that is as stiff as metal but flexible enough to withstand strong vibrations could transform the car manufacturing industry, say experts from the University of Surrey.In a paper published in Scientific Reports by Nature, scientists from Surrey joined forces with Johns Hopkins University in Baltimore and the University of California to develop a material that has high stiffness and damping.The team achieved this near impossible combination in a material by using 3D woven technical textile composite sheets, with selected unbonded fibres -- allowing the inside of the material to move and absorb vibrations, while the surrounding material remains rigid.Researchers believe their new material could usher in a new wave of trains, cars, and aircrafts, allowing customers to experience little to no vibration during their travels.Dr Stefan Szyniszewski, lead author of the study and Assistant Professor of Materials and Structures at the University of Surrey, said: ""The idea of a composite the resolves the paradox of stiffness and damping was thought to be impossible -- yet here we are. This is an exciting development that could send shock waves through the car, train and aerospace manufacturing industries. This is a material that could make the vehicles of the near future more comfortable than ever before.""Story Source:Materials provided by University of Surrey. ",0.124492,0.144114,0.217154,0.197589,0.144293,0.203538,0.194911,0,0
198,Ride-hailing increases vehicle miles traveled,"Ride-hailing accounts for an 83 percent increase in the miles cars travel for ride-hailing passengers in Denver's metro area, according to a study published this week in the journal Transportation by researchers at the University of Colorado Denver.Lead author and CU Denver civil engineering Ph.D. graduate Alejandro Henao signed up as a driver for Uber and Lyft. He collected exclusive driver data providing hundreds of rides throughout the Denver metropolitan area in fall 2016, according to the paper, ""The impact of ride-hailing on vehicle miles traveled.""For this first-of-its-kind study, the researcher-driver collected real-time data and surveyed passengers for feedback and demographic information. By surveying passengers, Henao learned that a combined 34 percent of his ride-hailing passengers would have taken transit, walked, or bicycled if ride-hailing hadn't existed.""Vehicle miles traveled increased mainly due to two factors; additional empty miles from ride-hailing drivers going around without passengers, and ride-hailing substituting more efficient and sustainable modes such as transit, biking and walking,"" said Henao.There is decrease in overall transportation efficiency due to more car miles on the road, often traveling without passengers. For every 100 miles carrying passengers, Uber and Lyft drivers travel an additional 69 miles without a passenger, conservatively.""Given the lack of data and existing research, this study represents a nice step forward in helping us better understand how ride-haling impacts the transportation system,"" said co-author Wes Marshall, associate professor, Civil Engineering Department, College of Engineering and Applied Science, CU Denver. ""However, cities still need better data to inform policy decisions about the many mobility-disrupting companies, and we have reached a point where we should expect, and probably need to require, more data transparency.""In this study, passenger demographics were better distributed across Denver's income and education demographics compared to research conducted in other cities; however, the city's disadvantaged populations are underrepresented. Further research is needed to study the equity impacts of ride-haling services, Marshall said.""Studying ride-hailing has big implications for automated and autonomous vehicles, especially when it comes to empty miles and mode replacement. As an analogy, the empty miles that ride-hailing are putting into our systems today will be added by zero-occupancy driverless cars, or zombie cars, in the future,"" said Henao.Story Source:Materials provided by University of Colorado Denver. ",0.172195,0.16204,0.242241,0.26349,0.177996,0.222792,0.22485,0,0
199,Mathematicians calculate the safest way home for pedestrians,"A mobile app that guides pedestrians along the safest instead of quickest route to their destination is being developed by researchers at Cardiff University.Maths and computer science experts have devised a way of scoring the safety of any given area using sophisticated mathematical algorithms, which they believe could easily be implemented into a navigation mobile app to help reduce road traffic casualties.Each year about 1.24 million deaths worldwide result from road traffic crashes, making this the eighth leading cause of death globally.According to the UK Department for Transport, pedestrians accounted for 24% of all road deaths in Great Britain in 2015.At the moment, apps such as Google Maps do not account for pavements and will only give people the quickest route to their destination. These apps do not take into account the characteristics of pavements and roads and the dangers associated with them.In a new study, published in the journal Accident Analysis and Prevention, researchers have shown how a novel system for scoring the safety of an area can successfully predict the likely number of road casualties.The computer algorithm takes into account a number of factors, such as the types and number of crossings, the type of street, the possibility of jaywalking and the speed limits of each road in a given area.The scoring is done automatically by simply feeding in the raw data from a map of any given area, and has been tested on 15 cities in the UK -- Bath, Bedford, Blackpool, Bristol, Coventry, Leeds, Leicester, Liverpool, Manchester, Nottingham, Reading, Salford, Sheffield, Swindon and York. Of those 15 cities, Liverpool was ranked as having the most unsafe roads whereas Bath was deemed to have the safest.The researchers believe this novel system could be of great value to city planners and developers, specifically when assessing how changes to a city's infrastructure may affect road safety, such as the pedestrianizing of roads or the changing of speed limits.In the nearer term, the team are looking at developing an app which people could use to tell them the safest possible route to their destination.Lead author of the study Dr Padraig Corcoran, from Cardiff University' School of Computer Science and Informatics, said: ""Google Maps is used millions of times a day to get people from A to B, yet it completely overlooks the safety of the routes that it offers to pedestrians.""Considering the large amount of deaths caused each year by road traffic crashes, we decided to devise a way of mapping how safe a particular road is by using a wide range of variables, such as the number of crossings and the speed limit of the road. We know that our safety ratings are accurate as they directly correlate with the number of road casualties in a given area.""Our next aim is translate this research into a product that the public can use. We envisage something very similar to Google Maps in which a user can input their destination and then choose a route that utilises our algorithm and gives them the safest possible journey instead of the quickest. This could definitely save lives and would go some way to reducing the high levels of causalities both here in the UK and across the world.""Story Source:Materials provided by Cardiff University. ",0.102536,0.0996706,0.163884,0.215,0.118752,0.172677,0.161867,0,0
200,Large trucks are biggest culprits of near-road air pollution,"For the 30 per cent of Canadians who live within 500 metres of a major roadway, a new study reveals that the type of vehicles rolling past their homes can matter more than total traffic volume in determining the amount of air pollution they breathe.A two-year U of T Engineering study has revealed large trucks to be the greatest contributors to black carbon emissions close to major roadways. Professor Greg Evans hopes these results gets city planners and residents thinking more about the density of trucks, rather than the concentration of vehicle traffic, outside their homes, schools and daycares. The study was recently published in the journal Environmental Science & Technology.""I've been asked by people, 'We live near a high-traffic area, should we be worried?' My response is that it's not so much about how much traffic there is, it's more about the percentage of trucks, older trucks in particular.""The comprehensive study -- led by Evans and collaborators at Environment and Climate Change Canada, and the Ontario Ministry of the Environment, Conservation and Parks, as well as the Metro Vancouver Regional District -- involved measuring vehicle emissions near roads in Vancouver and Toronto, including the 401, North America's busiest stretch of highway.The difference between emission levels across the sites was more correlated with the number of large trucks on the road rather than number of cars.Researchers found that air pollution levels right beside a major trucking route within a city were close to levels seen beside Highway 401, despite the road carrying less than one-tenth of the vehicle traffic. ""This was in part due to differences in wind and proximity to the road but, surprisingly, the number of vehicles didn't make that much of a difference,"" said Evans.The data also revealed a significant drop in emissions on the 401 on the weekends, when personal vehicle traffic is still very high, but the volume of large truck traffic is low.Research consistently links traffic emissions to negative effects on both the environment and human health. ""Whether it be cancer, respiratory problems, cardiac problems or neurodegenerative problems, there are numerous adverse health effects associated with the chemicals in these emissions,"" said Evans. ""If we were able to reduce emission of pollutants like black carbon, we would also see an immediate climate benefit."" Black carbon -- commonly called soot -- is a marker for exposure to diesel exhaust which is known to have negative health effects.Evans points out that modern trucks have made large improvements in their emissions -- it's the older diesel trucks that are the real culprits. ""Those big, 18-wheeler diesel trucks last for a long time. We need to push to retrofit these old trucks with better emission treatment systems. Simply retrofitting the worse offending trucks, or getting them off the road, is a tremendous opportunity to improve air quality in our cities.""The study will be part of a larger report in December that will stress the importance of implementing long-term monitoring of traffic related air pollution in Canada, and indicating that targeting high-emitting vehicles such as old trucks can provide a path towards improving near-road air quality.In the meantime, Evans hopes the study gets Canadians thinking about the effects of working, playing and living near truck-related air pollution. ""When a cyclist is riding near a large truck and they see a large plume of soot coming out -- it's important for them to be aware. Although shipping freight and construction by truck are critical to our economy, people need to know about the negative effects. There are ways that we can achieve a better balance.""Story Source:Materials provided by University of Toronto Faculty of Applied Science & Engineering. ",0.123482,0.124044,0.165184,0.158309,0.105833,0.197512,0.155852,0,0
201,Towards animal-friendly machines Machine ethics researchers help develop prototypes for animal-friendly machines,"Semi-autonomous and autonomous machines and robots can become moral machines using annotated decision trees containing ethical assumptions or justifications for interactions with animals.Machine ethics is a young, dynamic discipline, which primarily targets people, not animals. However, it is important that animals are kept from harm when encountering these machines since animals cannot make informed decisions or react as humans would.Several prototypes of semi-autonomous and autonomous machines that do not startle animals in the wild have been developed at the FHNW University in Brugg-Windisch, Switzerland. The prototypes are a ladybird-friendly robot vacuum cleaner, a self-driving car, a drone study for nature photography and advanced driver assistance systems.The article ""Towards animal-friendly machines"" by Professor Oliver Bendel of the FHNW School of Business, published in De Gruyter's open access journal Paladyn, Journal of Behavioral Robotics, describes how annotated decision trees for animal-friendly moral machines are being developed and compared while making the moral justifications transparent.The modeling for the drone, for example, was presented in 2015 and instructed it to ignore humans, to avoid harming flying birds and to identify skittish animals and only photograph them from an appropriate height.The robot vacuum cleaner was programmed to identify ladybirds by their coloring, and stop vacuuming until the insect had moved on. Furthermore, the owner could control the morality of the machine by presetting it to spare ladybirds, but vacuum other invasive or undesirable species. This may not seem animal-friendly, but absolute moral rules need not be enforced consistently if, for example, a vermin-free house is justified.Programming advanced driver assistance systems (ADAS) in terms of decisions they can make with respect to animals is the main focus of the Robocar design study. The study posits that ADAS should recognize warning signs for toad migration, hedgehog populations or deer crossings and adapt the car's reactions (emergency brake, reduced speed, etc) accordingly. In short, ADAS systems should identify such animals and animal species directly and react appropriately.""Both robotics and computer science must be sensitized to animal protection and advocates for animal ethics should follow developments in robotics and artificial intelligences and should be involved in both,"" said Professor Bendel.Story Source:Materials provided by De Gruyter. ",0.0985448,0.143749,0.154241,0.292774,0.134101,0.157191,0.233062,0,0
202,Autonomous vehicles could shape the future of urban tourism,"In the first study of its kind, published in the Annals of Tourism Research, academics from the University of Surrey and the University of Oxford have examined how Autonomous Vehicles (AVs) may have a substantial impact on the future of urban tourism.When we think of automated vehicles it seems to be a topic that sits firmly in science fiction, from cars with character in The Love Bug (1969) and Knight Rider (1982) to more recent practical representations in Minority Report (2002). However, according to the new research by Professor Scott Cohen (University of Surrey) and Dr Debbie Hopkins (University of Oxford), AVs may be spotted on our roads as soon 2025 and could lead to far-reaching impacts on urban tourism.The conceptual paper entitled Autonomous vehicles and the future of urban tourism imagines the impact of AVs in future urban tourism and focuses on the pros and cons of these impacts with regards to the transformation of urban space, the rise of autonomous taxis, and changes to city sightseeing and hospitality in the urban night.Potential benefits include reduced traffic congestion and emissions, improved foreign car hire processes, reduced parking requirements and cheaper taxi fares. AVs may impact other industries in radical ways too, such as Amsterdam's Red Light District, which could become operated out of moving AVs, and restaurants and hotels may encounter new competition in the form of AV dining cars and passengers sleeping in their moving vehicles.AVs are also the subject of many concerns. More time spent in cars on longer journeys could facilitate greater urban sprawl and increase car dependency. AVs may reduce demand for train travel, coach tours, public transport and driven taxis -- all resulting in future job losses. The potential for terrorism facilitated by AVs also raises genuine security fears.Professor Cohen, Head of Tourism and Transport at Surrey's School of Hospitality and Tourism Management, said: ""This groundbreaking study will benefit urban planners, policy makers and the tourism and hospitality industries, who will face a range of threats and opportunities as AVs begin to reach the mass market in the coming decade.""The visitor economy will be gradually transformed if AVs become fully automated and mainstream, leading to a future where hordes of small AVs could congest urban attractions, hop-on hop-off city bus tours may go out of business altogether, motorways between cities could fill at night with slow-moving AVs carrying sleeping occupants and commercial sex in moving AVs becomes a growing phenomenon.""The paper calls for future work that provides context-specific analyses that may reveal alternative ways of thinking about AVs for urban tourism. Its release coincides this week with the World Travel Market and European Tourism Day on 7 November and acts as a timely reminder of the rising importance and significance of AVs in tourism to industry and policy makers.Story Source:Materials provided by University of Surrey. ",0.145613,0.145088,0.20809,0.226504,0.148248,0.222544,0.198388,0,0
203,Location makes or breaks many forms of public housing Not so affordable,"University of Texas at Arlington researchers determined in a new study that subsidized housing is not affordable in the Dallas-Fort Worth region because its location does not make it transportation friendly.Shima Hamidi, assistant professor in the College of Architecture, Planning and Public Affairs, and two of her doctoral students, Jinat Jahan and Somayeh Moazzeni, published ""Does Location Matter? Performance Analysis of the Affordable Housing Programs with Respect to Transportation Affordability in Dallas Fort Worth (DFW) Metropolis"" in the Transportation Research Record journal. The journal is sponsored by Transportation Research Board, one of seven program units of the National Academies of Sciences, Engineering, and Medicine.""We found out that Housing and Urban Development housing programs aren't affordable because of transportation costs,"" said Hamidi, who also is director of CAPPA's Institute of Urban Studies. ""When people can't access mass transportation or if there isn't any mass transportation in a certain area, the transportation costs are prohibitive for where the public housing is located.""Adrian Parr, dean of the College of Architecture, Planning and Public Affairs, said Hamidi's work is essential to this region, which is one of the fastest growing parts of the country.""It is important that as we set out to identify solutions to the challenges North Texas faces, we articulate the problems that frame our solutions from the vantage point of who is most impacted and how,"" Parr said. ""This is one of the primary strengths of Dr. Hamidi's study. Namely, inequity is a product of how policy, planning and design overlap. Mitigating inequity then necessarily involves deliberately working at their points of intersection.""Federal authorities define affordable as a household spending less than 30 percent of its budget on housing.Hamidi said that planners believe any household spending more than 15 percent of its budget on transportation is really over a realistic threshold.""When you roll in the purchase or lease of a vehicle, insurance, upkeep, gasoline, parking fees and everything else, transportation becomes unaffordable for many people in the DFW region,"" Hamidi said.She said millions are being spent on local HUD housing but users of that housing are spending even more money on transportation.""The reason for the unaffordability is that many times, public housing users have no alternative than to purchase a car,"" Hamidi said. ""There are no other transportation options for them.""About 69 percent of the public housing projects in DFW are spending more than 15 percent of their income for transportation and thus making it unaffordable. The majority of those public housing users are spending about 17 to 20 percent of their income on transportation.The lowest expenditure, according to the study, found to be 2.14 percent for a home property located in downtown Dallas. In contrast, a property supported federal housing program located in a suburban area, 35 miles north from downtown Dallas, is spending 25.76 percent of their budget on transportation, the highest in the area.For all cases in the study, location and distance from daily destinations, availability of transit services, accessibility, street connectivity and mixed-use development are found to be influential for properties' affordability in terms of transportation costs.The findings make it clear that location matters for assisted housing properties to be truly affordable in terms of both housing and transportation costs.Story Source:Materials provided by University of Texas at Arlington. ",0.138425,0.163402,0.277741,0.254519,0.186309,0.19803,0.212137,0,0
204,New sensor could help doctors monitor patient progress from a distance,"A self-powered sensor developed at the University of Waterloo could allow doctors to remotely monitor the recovery of surgical patients.The small, tube-like device is designed to be fitted to braces after joint surgery to wirelessly send information to computers, smartphones or smartwatches to track range of motion and other indicators of improvement.""That data would be continuously collected, so it would be as though the physician or physiotherapist was always there, always observing the patient,"" said Hassan Askari, an engineering doctoral candidate at Waterloo.The same sensor could also be used in a variety of other ways, including in the tires of autonomous vehicles to detect and respond to icy roads.A prototype built and tested by the researchers combines electromagnetism and triboelectricity, a relatively new energy harvesting technique that involves bringing different materials together to produce current.When bent or twisted, the device generates enough electricity for sensing and powering electronic circuits for processing and wireless signal transmission.""The aim was to develop a sensor that works without having a battery attached to it,"" said Askari. ""It is its own power source.""That makes the device well-suited for applications that put a premium on reliability and where it would be difficult or expensive to replace worn-out batteries.Askari estimated the sensors -- about six centimetres long and one centimetre wide -- could be commercially manufactured for $5 to $10 each.Research is now focused on making them smaller and more sensitive using triboelectricity alone. Software is also being developed to process signals for the tire application.When attached to the inside of tires, they could sense changing road conditions and instantly send information to control systems to enable self-driving vehicles to make adjustments.""Based on the forces, the interaction between the road and the tires, we could actually detect ice or rain,"" said Askari. ""That is extremely important information for autonomous driving.""Askari collaborated at Waterloo with fellow PhD student Ehsan Asadi, and engineering professors Amir Khajepour and Mir Behrad Khamesee, as well as doctoral student Zia Saadatnia and professor Jean Zu at the University of Toronto.A study detailing their work appears in the journal Sensors and Actuators A: Physical.Story Source:Materials provided by University of Waterloo. ",0.0903633,0.0950441,0.115695,0.186734,0.141571,0.148604,0.154649,0,0
205,Scientists present concept for the elimination of traffic jams,"In the current issue of Nature, the economists Peter Cramton, Axel Ockenfels (both University of Cologne) and R. Richard Geddes (Cornell University) outline a concept for the future of traffic control. In this concept, drivers would have to pay a dynamic fee for the use of roads. This would contribute to avoiding traffic jams and protecting the environment, the researchers argue. Fees that respond to traffic volumes in real time and with site precision, taking into account factors such as vehicle type and exhaust emissions, can significantly improve traffic flow and contribute to reducing air pollution.Traffic jams are not only annoying and time-consuming, they are also costly. In Germany, the economic damage caused by congested roads in 2017 totaled approximately 80 billion euros. 'Currently, road users who cause traffic jams, damage the environment and even incur costs are paying just as much as those who are not involved,' says Ockenfels. 'Without a toll, this means that the general public is subsidizing these road users. That's unfair.' A toll for road use would bring these costs to light and reduce congestion. 'If the fee adapts to the volume of traffic and the situation on the road in real time, i.e. is more expensive at rush hour than around noon, everyone can choose the route that suits them best. This already works for navigation systems,' explains Cramton. 'Ultimately, this would reduce the load on main traffic arteries, improve traffic flow and reduce CO2 emissions.Technically, a dynamic road toll could already be implemented in real time today. Navigation and telecommunications systems, GPS data and apps can provide drivers with information and predict traffic volumes. 'Of course, you have to develop a system that is an acceptable compromise between collecting personal data and protecting privacy,' says Cramton. Modern cryptology could allow system operators to charge tolls without people having access to the data at all.The scientists do not believe that the toll would disadvantage people who cannot afford the tolls. 'Pricing must be dynamic and offer options. Imagine pricing the left lane of regularly congested, multi-lane roads. A lower traffic volume on the left lane would be the result. This in turn means that the flow of traffic on the right-hand lane also increases,' says Ockenfels. 'That way, everyone benefits.'Story Source:Materials provided by University of Cologne. ",0.141991,0.120333,0.190429,0.237611,0.141841,0.20695,0.182535,0,0
206,Electric vehicle charging in cold temperatures could pose challenges for drivers,"New research from Idaho National Laboratory suggests that electric vehicle drivers could face longer charging times when temperatures drop. The reason: cold temperatures impact the electrochemical reactions within the cell, and onboard battery management systems limit the charging rate to avoid damage to the battery.The new study, which looked at data from a fleet of electric vehicle (EV) taxis in New York City, was posted last week by the journal Energy Policy.""Battery researchers have known about the degradation of charging efficiency under cold temperatures for a long time,"" said Yutaka Motoaki, an EV researcher with INL's Advanced Vehicles research group.But most of the current knowledge comes from experiments with smaller batteries in the lab, not data from large, electric vehicle batteries in real-world conditions. Further, EV manufacturers often provide consumers with only rough estimates of charging times, and they typically do not specify the range of conditions for which those estimates apply.""We wanted to ask the question: What is the temperature effect on that battery pack?"" Motoaki said. ""What is the effect of degradation of charging efficiency on vehicle performance?""Motoaki and his colleagues analyzed data from a fleet of Nissan Leafs operated as taxis over roughly 500 Direct Current Fast Charge (DCFC) events. Temperatures for the charging events ranged from 15 to 103 degrees Fahrenheit.The researchers found that charging times increased significantly when the weather got cold. When an EV battery was charged at 77 degrees, a DCFC charger might charge a battery to 80 percent capacity in 30 minutes. But at 32 degrees, the battery's state of charge was 36 percent less after the same amount of time.And, the more the temperature dropped, the longer it took to charge the battery. Under the coldest conditions, the rate of charging was roughly three times slower than at warmer temperatures.It's important to note that cold weather would only impact EV drivers under specific circumstances, Motoaki said. For instance, people who charge their EVs in a warm garage and use their EVs for commuting within the range of their battery might not experience much inconvenience. Decreased fuel economy in cold weather is also a well-known phenomenon with gasoline and diesel-powered vehicles.But time spent charging in cold temperatures could make a big difference for a taxi driver, since every minute spent charging a vehicle is a minute the driver is not making money.""There's a lot of uncertainty about what the vehicle owner's experience would be if they drive the vehicle in Maine or Michigan,"" Motoaki said.The research poses questions not only for EV customers, but also for utilities and charging infrastructure providers. For instance, the location or abundance of charging infrastructure may need to be different in colder climates, and electric utilities might see electricity use vary as the seasons change.Story Source:Materials provided by DOE/Idaho National Laboratory. ",0.132326,0.170484,0.178422,0.197565,0.16336,0.195562,0.181798,0,0
207,Why pulsed sparks make for better ignition,"Researchers in the Oregon State University College of Engineering have learned the mechanisms behind a means of improved ignition, helping to open the door to better performance in combustion systems ranging from car engines to jet propulsion.The findings are important because reliable ignition is a linchpin for the safe and efficient operation of every type of combustion system.Supported by the Office of Naval Research and the Air Force Office of Scientific Research, the study was built around trying to identify why an improved ignition approach increased the probability of ignition of scramjets, a high-propulsion system that evolved from the ramjet technology of the 1960s. Scramjet is shorthand for supersonic combustion ramjet.Both ramjets and scramjets use their vehicles' supersonic airspeed -- greater than the speed of sound, 767 mph at sea level -- and the shape of the inlet valve to compress air prior to the adding of fuel and ignition in a combustion chamber.In a ramjet, the air being compressed is slowed to subsonic speeds within the engine, but in a scramjet, it is not. That gives scramjet-powered vehicles the potential to reach 20 times the speed of sound -- or Mach 20 -- compared to the top speed of about Mach 5 attainable with a ramjet.Studying under OSU assistant professor of mechanical engineering David Blunck, doctoral student Jonathan Bonebrake traveled to Dayton, Ohio, to work with a research group headed by Tim Ombrello of the U.S. Air Force Research Laboratory.There, Bonebrake used a high-speed infrared camera to record the effects of nanosecond pulsed high-frequency discharges on ignition kernels, the balls of hot gas that form just after ignition but before the development of a freely propagating flame.Infrared refers to the part of the electromagnetic spectrum where the wavelengths are longer than visible light but shorter than microwaves. The fuel in the study was a methane and air mixture.It was already known that the nanosecond pulsed high-frequency discharges -- sparks occurring with a frequency in excess of 10 kilohertz, or more than 10,000 times per second -- led to improved ignition.""But why was there enhanced ignition?"" Blunck said. ""There were some strong theories, but no one knew for sure.""Bonebrake and Blunck developed an ""inverse deconvolution technique"" to determine the changing temperature and size of the ignition kernels based on the infrared images and tested the technique successfully against experiments conducted with a McKenna burner, a standard tool in combustion studies.""With our camera, we could measure the radiation and then back out the temperatures of those balls of hot gas as those flame kernels were growing,"" Blunck said. ""We could quantify the differences in the temperatures of the kernels.""As we increased the increased the frequency of the spark discharges, there was more energy and more ignitability,"" he said. ""If the sparks were too far apart, they couldn't interact, but when the frequency was higher, energy was deposited more efficiently, resulting in higher temperatures and better ignition.""And not only did increased frequencies lead to an increase in kernel temperatures, Bonebrake noted, but also an increase in how quickly the kernel temperatures grew. The rate of kernel growth can be crucial to ignition success and combustion completion, as the developing kernel can be affected by whatever fluid motion is taking place within the combustion system.""Tim's group had observed what the nanosecond pulsed high-frequency discharges were doing, and we were able to explain the 'why,'"" Blunck said. ""Now there's a stronger case for maturing that technology if they choose to try to do that.""Findings were published in Proceedings of the Combustion Institute.Story Source:Materials provided by Oregon State University. Original written by Steve Lundeberg. ",0.126494,0.10903,0.127207,0.17743,0.135405,0.157887,0.177974,0,0
208,"Fleet of aerial, surface, and underwater robots maps ocean front ","Using multiple autonomous vehicles simultaneously, an interdisciplinary team of scientists and engineers returns to the United States after exploring the North Pacific Subtropical front -- a sharp boundary where cold fresh waters from the north meet warm salty waters from the south. The fronts are the most conspicuous oceanographic phenomena and the goal of this project was to demonstrate the use of distributed autonomous robotics to detect, track, and characterize these complex and dynamic processes with high accuracy across large spatial and temporal scales.Bringing together aerial, surface, and underwater robotic vehicles with support from the research vessel Falkor, allowed the team to locate, map, and explore the front. It was detected approximately 1,000 nautical miles off the coast of Southern California several days ahead of the research party's arrival using autonomous surface vehicles. Multiple low cost submarine, surface, and aerial robots joined the front exploration from Falkor. The resulting distributed robotic system and intelligent sensor network helped the research team to track the complex oceanic front dynamically, in an easier, faster, and more cost effective way that would be possible with traditional means, such as only relying on ships.Satellite observations are often not enough to track continuously changing ocean system. To locate the North Pacific front in the first place, this international group of scientists and engineers led by Dr. Joao Borges de Sousa, of the Laboratorio de Sistemas e Tecnologia Subaquatica (LSTS) from Porto University deployed one WaveGlider and two Saildrones into the target research area ahead of the ship's arrival. The data they acquired and transmitted to Falkor over satellite allowed the research party to optimize the deployment plans for all autonomous vehicles, including those that were delivered by and deployed from the vessel. Intelligent multi-vehicle control software, Ocean Space Center, developed at LSTS and deployed on Falkor was refined throughout the cruise to automatically optimize the operations of all deployed robots and facilitate the control of the robotic fleet for human operators. For the first time ever, a section of a major open-ocean front was mapped with sub-mesoscale resolution using a controlled fleet of dozens of autonomous robots. This detailed map of the ocean front dynamically composed from the acquired data enabled scientists to identify hotspots in which coordinated ship-robotic surveys were conducted with adaptive spatial and temporal resolution.Operation of multiple assets in open sea can be difficult but, in less than three weeks, the underwater robots travelled over 1,000 nautical miles for approximately 500 hours, while the autonomous surface vehicles operated continuously, and the unmanned aerial vehicles performed over 25 flights totaling 10 hours. This project demonstrates a novel approach for distributing the observations of complex ocean dynamics across multiple underwater, surface, and air vehicles. Instead of sampling from one vessel in a single location, researchers can now monitor a much larger area with high resolution in space and time in a scalable and cost-efficient manner using a networked fleet of robotic vehicles supported by a vessel-based command center.""I think it is essential for humankind to understand the big picture because, in the end, we are talking about the life-support system for the Earth,"" said Joeo Sousa. ""The oceans are an important component of that life-support system and they are not as tremendously huge as people tend to assume. In fact, if all of the oceans' water were put into a bubble, most of us would be stunned to see how impressively small it looks when compared with the Earth's size. And, yet, science still lacks the technology and tools to study the oceans overall health and functioning. ""To enable a sustainable presence in the oceans, the group further developed their specific software called Neptus and Ripples, the software behind the Open Space Center, that allowed them to view and control the robots in real time, gaining advanced situational awareness, remote visualization, and control, all via the Internet. This software toolchain enabled them to control the ensemble of vehicles in several unprecedented ways, notably operating non-stop for most of the expedition with just one operator.Lessons learned and results obtained in this cruise can be applied to other frontal regions, as well as to other phenomena of the world Ocean. Thus, the technologies demonstrated in this expedition will help us to understand and monitor how key issues such as climate change, ocean acidification, unsustainable fishing, pollution, waste, loss of habitats and biodiversity, shipping, security, and mining are affecting global ocean sustainability and stewardship.Story Source:Materials provided by Schmidt Ocean Institute. ",0.0732924,0.0684788,0.1154,0.146595,0.0715607,0.119019,0.116859,0,0
209,Marine mammals most at risk from increased Arctic ship traffic,"In August 2016, the first large cruise ship traveled through the Northwest Passage, the northern waterway linking the Atlantic and Pacific oceans. The following year, the first ship without an icebreaker plied the Northern Sea Route, a path along Russia's Arctic coast that was, until recently, impassable by unescorted commercial vessels.In recent decades parts of the Arctic seas have become increasingly ice-free in late summer and early fall. As sea ice is expected to continue to recede due to climate change, seasonal ship traffic from tourism and freight is projected to rise. A study from the University of Washington and the University of Alaska Fairbanks is the first to consider potential impacts on the marine mammals that use this region during fall and identify which will be most vulnerable.The study is published the week of July 2 in the Proceedings of the National Academy of Sciences. ""We know from more temperate regions that vessels and whales don't always mix well, and yet vessels are poised to expand into this sensitive region,"" said lead author Donna Hauser, who did the research as a postdoctoral researcher at the UW and is now a research assistant professor at the University of Alaska Fairbanks. ""Even going right over the North Pole may be passable within a matter of decades. It raises questions of how to allow economic development while also protecting Arctic marine species.""The study looked at 80 subpopulations of the seven marine mammals that live in the Arctic and identified their risks on or near major shipping routes in September, a month when the Arctic Ocean has the most open water.Forty-two of these subpopulations would be exposed to vessel traffic, and the degree of exposure plus the particular characteristics of each species determine which are most sensitive.The most vulnerable marine mammals were found to be narwhals, or tusked whales. These animals migrate through parts of the Northwest Passage to and from their summertime habitats.""Narwhals have all the traits that make them vulnerable to vessel disturbances -- they stick to really specific areas, they're pretty inflexible in where they spend the summer, they live in only about a quarter of the Arctic, and they're smack dab in the middle of shipping routes,"" said co-author Kristin Laidre, a polar scientist at UW Applied Physics Laboratory's Polar Science Center. ""They also rely on sound, and are notoriously skittish and sensitive to any kind of disturbance.""Other mammals found to be vulnerable were beluga and bowhead whales. Walruses also were vulnerable because some populations are relatively small and known to live along shipping routes, compared to generally large and widely distributed populations of ringed and bearded seals, which were shown to be less vulnerable.The study found the least vulnerable animals were polar bears, which are largely on land during September, and don't rely on underwater sound for communication or navigation. Shipping in other seasons may have a greater impact.The paper also identified two ""pinch points,"" narrow passageways where ships and animals are most likely to intersect. These are the Bering Strait that separates the U.S. and Russia, and Lancaster Sound in the northern Canadian territory of Nunavut. These regions had a risk of conflicts two to three times higher than on other parts of the shipping route.""These obligatory pinch points are used by migratory species to get in and out of the Arctic, but they are also necessary passageways for vessels using these sea routes,"" Hauser said. ""Identifying the relative risks in Arctic regions and among marine mammals can be helpful when establishing strategies to deal with potential effects.""Travel through the Arctic Ocean is already beginning, with the Russian route having the most potential for commercial ships. The Northern Sea Route had more than 200 ships from 2011 to 2016, all of which were large vessels. More than 100 vessels passed through the Northwest Passage during that time, with more than half being small, private vessels like personal yachts.The International Maritime Organization in May established the first international guidelines for vessel traffic in the Arctic Ocean. The voluntary code was proposed by the U.S. and Russia to identify safe routes through the Bering Strait.The new study could help to create future guidelines, prioritize different measures to protect marine mammals and identify areas needing further study, the authors said.""I think we can learn a lot from areas that have already been thinking about these kinds of conflicts between ships and marine mammal populations -- for example the North Atlantic right whale, or fin and blue whales around California,"" Laidre said. ""We could aim to develop some mitigation strategies in the Arctic that help ships avoid key habitats, adjust their timing taking into account the migration of animals, make efforts to minimize sound disturbance, or in general help ships detect and deviate from animals.""Story Source:Materials provided by University of Washington. Original written by Hannah Hickey. ",0.1059,0.12168,0.168694,0.168196,0.113791,0.174162,0.153725,0,0
210,"Self-heating, fast-charging battery makes electric vehicles climate-immune ","Californians do not purchase electric vehicles because they are cool, they buy EVs because they live in a warm climate. Conventional lithium-ion batteries cannot be rapidly charged at temperatures below 50 degrees Fahrenheit, but now a team of Penn State engineers has created a battery that can self-heat, allowing rapid charging regardless of the outside chill.""Electric vehicles are popular on the west coast because the weather is conducive,"" said Xiao-Guang Yang, assistant research professor in mechanical engineering, Penn State. ""Once you move them to the east coast or Canada, then there is a tremendous issue. We demonstrated that the batteries can be rapidly charged independently of outside temperature.""When owners can recharge car batteries in 15 minutes at a charging station, electric vehicle refueling becomes nearly equivalent to gasoline refueling in the time it takes. Assuming that charging stations are liberally placed, drivers can lose their ""range anxiety"" and drive long distances without worries.Previously, the researchers developed a battery that could self-heat to avoid below-freezing power drain. Now, the same principle is being applied to batteries to allow 15-minute rapid charging at all temperatures, even as low as minus 45 degrees F.The self-heating battery uses a thin nickel foil with one end attached to the negative terminal and the other extending outside the cell to create a third terminal. A temperature sensor attached to a switch causes electrons to flow through the nickel foil to complete the circuit when the temperature is below room temperature. This rapidly heats up the nickel foil through resistance heating and warms the inside of the battery. Once the battery's internal temperature is above room temperature, the switch turns opens and the electric current flows into the battery to rapidly charge it. ""One unique feature of our cell is that it will do the heating and then switch to charging automatically,"" said Chao-Yang Wang, William E.Diefenderfer Chair of mechanical engineering, professor of chemical engineering and professor of materials science and engineering, and director of the Electrochemical Engine Center. ""Also, the stations already out there do not have to be changed. Control off heating and charging is within the battery, not the chargers.""The researchers report the results of their prototype testing in this week's edition of the Proceedings of the National Academy of Sciences. They found that their self-heating battery could withstand 4,500 cycles of 15-minute charging at 32 degrees F with only a 20-percent capacity loss. This provides approximately 280,000 miles of driving and a lifetime of 12.5 years, longer than most warranties.A conventional battery tested under the same conditions lost 20-percent capacity in 50 charging cycles.Lithium-ion batteries degrade when rapidly charged under 50 degrees F because, rather than the lithium ions smoothly integrating with the carbon anodes, the lithium deposits in spikes on the anode surface. This lithium plating reduces cell capacity, but also can cause electrical spikes and unsafe battery conditions. Currently, long, slow charging is the only way to avoid lithium plating under 50 degrees F.Batteries heated above the lithium plating threshold, whether by ambient temperature or by internal heating, will not exhibit lithium plating and will not lose capacity.""This ubiquitous fast-charging method will also allow manufacturers to use smaller batteries that are lighter and also safer in a vehicle,"" said Wang.Story Source:Materials provided by Penn State. Original written by A'ndrea Elyse Messer. ",0.123784,0.201303,0.140153,0.135569,0.162303,0.192354,0.153084,0,0
211,"Path to zero emissions starts out easy, but gets steep Energy innovation necessary to overcome biggest zero emissions challenges","Carbon dioxide emissions from human activities must approach zero within several decades to avoid risking grave damage from the effects of climate change. This will require creativity and innovation, because some types of industrial sources of atmospheric carbon lack affordable emissions-free substitutes, according to a new paper in Science from team of experts led by University of California Irvine's Steven Davis and Carnegie's Ken Caldeira.In addition to heating, cooling, lighting, and powering individual vehicles -- subjects that are often the focus of the emissions discussion -- there are other major contributors to atmospheric carbon that are much more challenging to address. These tough nuts to crack include air travel; long-distance freight by truck, train, or ship; and the manufacture of steel and cement.""We wanted to look closely at the barriers and opportunities related to the most difficult-to-decarbonize services,"" said lead author Davis.The barriers they analyzed included:""Taken together these 'tough-nut' sources account for a substantial fraction of global emissions,"" Caldeira said. ""To effectively address them, we will need to develop new processes and systems. This will require both development of new technologies and coordination and integration across industries.""Possibilities that the team analyzed include, but aren't limited to, the synthesis of energy dense hydrogen or ammonia-based fuels for aviation and shipping, new furnace technologies in the manufacture of concrete and steel, and tools to capture and safely store hydrocarbon emissions.But the costs of implementing and scaling up these technologies to overhaul the transportation, construction, and energy storage industries will present hurdles, they warn. Plus, it will be necessary to overcome the inertia of existing systems and policies to create something new and better.""We don't have a crystal ball to foresee what technologies will exist a century from now,"" Caldeira continued. ""But we know that people will want buildings, transportation, and other energy services and we can try to design our energy system so that it is able to take advantage of new inventions as they come along.""Story Source:Materials provided by Carnegie Institution for Science. ",0.0713913,0.116955,0.149204,0.164893,0.092434,0.145083,0.14925,0,0
212,How smart technology gadgets can avoid speed limits,"Speed limits apply not only to traffic. There are limitations on the control of light as well, in optical switches for internet traffic, for example. Physicists at Chalmers University of Technology now understand why it is not possible to increase the speed beyond a certain limit -- and know the circumstances in which it is best to opt for a different route.Light and other electromagnetic waves play a crucial role in almost all modern electronics, for example in our mobile phones. In recent years researchers have developed artificial speciality materials -- known as optomechanical metamaterials -- which overcome the limitations inherent in natural materials, in order to control the properties of light with a high degree of precision.For example, what are termed optical switches are used to change the colour or intensity of light. In internet traffic these switches can be switched on and off up to 100 billion times in a single second. But beyond that the speed cannot be increased any further. These unique speciality materials are also subject to this limit.""Researchers had high hopes of achieving higher and higher speeds in optical switches by further developing optomechanical metamaterials. We now know why these materials failed to outcompete existing technology in internet traffic and mobile communication networks,"" says Sophie Viaene, a nanophotonics researcher at the Department of Physics at Chalmers.To find out why there are speed limits and what they mean, Viaene went outside the field of optics and analysed the phenomenon using what is termed non-linear dynamics in her doctoral thesis. The conclusion she reached is that it is necessary to choose a different route to circumvent the speed limits: instead of controlling an entire surface at once, the interaction with light can be controlled more efficiently by manipulating one particle at a time. Another way of solving the problem is to allow the speciality material to remain in constant motion at a constant speed and to measure the variations from this movement.But Viaene and her supervisor, Associate Professor Philippe Tassin, say that the speed limit does not pose a problem for all applications. It is not necessary to change the properties of light at such high speeds for screens and various types of displays. So there is great potential for the use of these speciality materials here, since they are thin and can be flexible.Their results have determined the direction researchers should take in this area of research, and the scientific article was recently published in the journal Physical Review Letters. The pathway is now open for the ever smarter watches, screens and glasses of the future.""The switching speed limit is not a problem in applications where we see the light, because our eyes do not react all that rapidly. We see a great potential for optomechanical metamaterials in the development of thin, flexible gadgets for interactive visualisation technology,"" says Tassin, an associate professor in the Department of Physics at Chalmers.Story Source:Materials provided by Chalmers University of Technology. ",0.101403,0.101498,0.103687,0.165267,0.130784,0.148709,0.137686,0,0
213,Polymer professor develops safer component for lithium batteries,"The power source for things like implanted medical devices, electric cars or unmanned aerial vehicles are vital to their performance. So, what would happen if that powerhouse of energy -- a lithium battery -- failed? An electric or hybrid car would render useless and a much-needed biomedical device would hamper a patient's health.These are the kinds of things Polymer Science Professor Dr. Yu Zhu, along with other researchers, is trying to prevent.A recent paper from Zhu's research group, ""A Superionic Conductive, Electrochemically Stable Dual-Salt Polymer Electrolyte,"" will be published Tuesday in the journal Joule, Cell Press's forward-looking journal spanning energy research across disciplines.Specifically, Zhu and his research team developed a solid polymer electrolyte that can be used in lithium ion batteries to replace the current liquid electrolyte to improve the safety and performance of lithium batteries.Zhu says solid electrolytes has not been commercialized in lithium batteries because of drawbacks like low ionic conductivity and high interfacial resistance with electrodes. However, Zhu and his team demonstrated that a dual-salt based polymer solid electrolyte exhibited superionic conductivity at room temperature and outstanding electrochemical stability with lithium battery electrode materials.""A solid electrolyte has long been thought for lithium ion batteries due to its nonflammable property and high mechanical strength that may mitigate the disaster caused by battery failure,"" says Zhu. ""Battery safety and energy density are major concerns for emerging applications of lithium batteries, such as for use in electrical vehicles. If the solid polymer electrolyte is successfully developed, the energy density of the battery could be doubled and the safety concerns for lithium batteries could be removed. This research sets up a strong base to develop such a promising solid electrolyte for lithium batteries.""The research team has formed a company, Akron PolyEnergy Inc., that will further develop this technique and produce a large prototype for future commercialization.Zhu's graduate students Si Li and Yu-Ming Chen are leading authors for this research. Other researchers include graduate students Wenfeng Liang, Yunfan Shao and Kewei Liu, and Dr. Zhorro Nikolov from the National Polymer Innovation Center.Story Source:Materials provided by University of Akron. ",0.120008,0.251588,0.156518,0.179416,0.156373,0.187286,0.193123,0,0
214,Many wildlife-vehicle collisions preventable,"A new study from the University of Waterloo has found that Ontario could save millions by implementing simple measures to help prevent vehicle accidents involving wildlife.The Waterloo study focused on Ontario. It concluded that many of the thousands of wildlife-vehicle collisions -- some fatal -- occurring each year could be prevented if authorities implemented a few, cost-effective strategies to minimize the occurrences.Implementing strategies such as better signage, wildlife detection systems, fencing and wildlife crossings could help reduce financial and health-related impacts for people, emergency services and the insurance industry. The measures could also help prevent unnecessary loss in the wildlife populations.""These collisions cost Canadians hundreds of millions a year in vehicle damage and medical costs, as well as traffic delays, emergency services use and increases in insurance premiums,"" said Waterloo's Associate Professor Michael Drescher, who co-authored the study with graduate student Kristin Elton. ""Ontario is missing an opportunity here. The most efficient way to prevent these accidents is to integrate effective measures in wildlife conflict zones every time major road work is undertaken.""The study looked specifically at Southern Ontario, which has the highest concentration of roads in Canada. With ever-expanding infrastructure having a significant impact on wildlife and their habitat, many of the cost-effective measures that could help reduce wildlife-vehicle collisions are underdeveloped in Ontario.""Within Canada, the Rocky Mountain region is renowned as a leader in the management of wildlife-road conflicts, said Drescher, who estimates five to 10 per cent of car insurance premiums go to animal-related incidents. ""They've realized the economics are simple. Adding measures to road construction projects only marginally impacts the overall budget, while saving millions in taxpayer money, insurance costs, and potentially lives.""Governments have to balance many competing factors next with the technical elements of this problem, such as perceived financial constraints, public opinion and political motivations,"" added Drescher. ""When these factors are not managed right, wildlife management measures are delayed or never started.""Story Source:Materials provided by University of Waterloo. ",0.14138,0.169559,0.197666,0.220877,0.153194,0.209498,0.193993,0,0
215,Self-driving cars must reduce traffic fatalities by at least 75 percent to stay on the roads,"The race is on for companies to present their driverless cars to the public, but recent collisions involving autonomous vehicles developed by Uber Technologies Inc. and Tesla Inc. have led consumers to questions whether these vehicles can alleviate traffic issues and increase safety. A new study published in Risk Analysis examined the question ""How safe is safe enough for self-driving vehicles (SDVs)?""To answer this question, researchers employed an expressed-preference approach -- a method that has not previously been employed in this setting -- to determine the socially acceptable risk of SDVs. The results showed that the public will not accept this new technology unless it is shown to be safer, approximately four to five times as safe as human-driven vehicles (HDVs). Despite the conveniences SDVs would bring to individuals, such as the ability to watch a movie, read a book, sleep or surf the internet, the public will be much less likely to accept, or even tolerate, SDVs if they have the same risk level as human driving. As suggested by previous studies, an individual increases his or her demand for safety when that safety is entrusted to an external factor, such as an automated vehicle.One of the major motivations behind the development of SDVs is to improve road safety. Human error is believed to cause 94 percent of all traffic crashes in the U.S., and 75 percent in the U.K. While SDVs have the potential to significantly reduce these types of crashes, they also introduce several new road risks, including accidents caused by cyber-attacks. Creating perfectly safe SDVs is both technologically and economically infeasible, but policies can require that the risk of having them on the road be as low as technically achievable.The study was conducted by Peng Liu and Run Yang, Tianjin University, and Zhigang Xu, of Chang'an University. The survey was distributed to a convenience sample of residents in Tianjin, China. Of the 499 respondents, half were randomly assigned to complete a version of the survey for HDVs, while the other half completed an SDV version. Risk frequencies were expressed as one fatality per a certain number of vehicle-kilometers traveled and as one fatality per a certain number of population, respectively. Respondents were asked to accept or reject each traffic risk scenario at one of four levels: never accept, hard to accept, easy to accept and fully accept.The results show that the respondents believe that SDVs should be four to five times as safe as HDVs. Current global traffic fatal risk is estimated at 17.4 per 100,000, which is 350 times greater than the frequency accepted by 50 percent of the respondents for SDVs. This implies that respondents expect SDVs to improve safety by two orders of magnitude against the current traffic risk.Based on the results, the researchers propose the following requirements for SDVs based on the tolerability of risk in industrial safety (a concept developed in the health and safety field) in which risks are distinguished by three criteria: unacceptable, tolerable and broadly acceptable. SDVs that are less safe than human drivers would be set as the unacceptable risk criterion. The tolerable risk is that SDVs be four to five times as safe, meaning they should be able to reduce 75-80 percent of current traffic fatalities. The broadly acceptable risk criterion for SDVs is set as two orders of magnitude lower than current global traffic risk, indicating a hundredfold improvement over current traffic risks, or the same order of magnitude experienced in public transportation modes, such as rail and commercial aviation.""Our results and method may help government authorities to establish clear safety requirements for regulating SDVs and also help SDV manufacturers find consumers' expectations for SDVs that must be met,"" states Liu, Ph.D., assistant professor of industrial engineering.Story Source:Materials provided by Society for Risk Analysis. ",0.106313,0.0956885,0.146046,0.18406,0.114975,0.168028,0.139109,0,0
216,Kicking the car(bon) habit better for air pollution than technology revolution,"Changing our lifestyles and the way we travel could have as big -- if not more of an impact on CO2 transport emissions, as electric vehicles and the transport technology revolution, according to new Oxford University research.Published in Energy Efficiency, the study uses Scotland as an example and suggests that, radical lifestyle change can show quicker results than the gradual transition to Electric Vehicles and phasing out of conventional petrol and diesel vehicles.Scotland has committed itself to reduce carbon emissions by 80% of 1990 levels by 2050. For transport, this includes international aviation and shipping which makes the targets more difficult to achieve.Led by Dr Christian Brand, Senior Research Fellow and Associate Professor at the Environmental Change Institute and Transport Studies Unit, in collaboration with colleagues Jillian Anable from the University of Leeds and Craig Morton at the University of Loughborough, the paper explores how plausible changes in the way we travel might reduce energy use and emissions in Scotland over the next three decades, in light of the 5-year carbon budgets up to 2050 and beyond.""Our study explores how Scotland might achieve these targets in the transport sector. We find that both lifestyle change -- such as making fewer and shorter journeys, sharing existing journeys, or shifting to walking, cycling and clean public transport -- and a comprehensive strategy around zero emission technologies are needed, but that they have limits to meeting our CO2 targets, in particular beyond 2030"" explains lead author, Oxford Scientist Dr Christian Brand.The findings suggest that, only through prioritisation of both demand- (lifestyle, social and cultural change) and supply-side (new technology) transport solutions, might we have a chance of curbing carbon emissions in line with the United Nation's 1.5C Climate Change Agreement. The co-benefits of such change to human health and the NHS are enormous.""The newfound urgency of 'cleaning up our act' since the Paris Climate Change Agreement in 2016 and Dieselgate scandal suggests that we cannot just wait for the technology fix,"" says Dr Christian Brand.Traditionally governments have prioritised technology fixes and supply-side transport solutions to the carbon emission problem.However, the authors suggest that a long-term carbon and air quality emission-cutting strategy should consider both demand- and supply-side transport solutions, for the best chance of success.Change will need to be led by consumers, policy makers and industry alike, they say.""We need to look at how we can inspire and support consumer lifestyle changes -- in travel patterns, mode and vehicle choice, vehicle occupancy -- to be in with a chance of reducing our carbon emissions in line with legislated targets and travelling on the 'Road to Zero' faster, further and more flexible.""Story Source:Materials provided by University of Oxford. ",0.11323,0.153835,0.155259,0.172153,0.121959,0.193787,0.172556,0,0
217,How to melt gold at room temperature,"When the tension rises, unexpected things can happen -- not least when it comes to gold atoms. Researchers from, among others, Chalmers University of Technology, Sweden, have now managed, for the first time, to make the surface of a gold object melt at room temperature.Ludvig de Knoop, from Chalmers' Department of Physics, placed a small piece of gold in an electron microscope. Observing it at the highest level of magnification and increasing the electric field step-by-step to extremely high levels, he was interested to see how it influenced the gold atoms.It was when he studied the atoms in the recordings from the microscope, that he saw something exciting. The surface layers of gold had actually melted -- at room temperature.""I was really stunned by the discovery. This is an extraordinary phenomenon, and it gives us new, foundational knowledge of gold,"" says Ludvig de Knoop.What happened was that the gold atoms became excited. Under the influence of the electric field, they suddenly lost their ordered structure and released almost all their connections to each other. Upon further experimentation, the researchers discovered that it was also possible to switch between a solid and a molten structure.The discovery of how gold atoms can lose their structure in this way is not just spectacular, but also groundbreaking scientifically. Together with the theoretician Mikael Juhani Kuisma, from the University of Jyveskyle in Finland, Ludvig de Knoop and colleagues have opened up new avenues in materials science. The results are now published in the journal Physical Review Materials.Thanks to theoretical calculations, the researchers are able to suggest why gold can melt at room temperature. Possibly, the surface melting can be seen as a so-called low-dimensional phase transition. In that case, the discovery is connected to the research field of topology, where pioneers David Thouless, Duncan Haldane and Michael Kosterlitz received the Nobel Prize in Physics 2016. With Mikael Juhani Kuisma in the lead, the researchers are now looking into that possibility.In any case, the ability to melt surface layers of gold in this manner enables various novel practical applications in the future.""Because we can control and change the properties of the surface atom layers, it opens doors for different kinds of applications. For example, the technology could be used in different types of sensors, catalysts and transistors. There could also be opportunities for new concepts for contactless components,"" says Eva Olsson, Professor at the Department of Physics at Chalmers.But for now, for those who want to melt gold without an electron microscope, a trip to the goldsmith is still in order.Story Source:Materials provided by Chalmers University of Technology. ",0.100945,0.186771,0.131415,0.187425,0.152069,0.157126,0.162155,0,0
218,Purple bacteria 'batteries' turn sewage into clean energy,"You've flushed something valuable down the toilet today.Organic compounds in household sewage and industrial wastewater are a rich potential source of energy, bioplastics and even proteins for animal feed -- but with no efficient extraction method, treatment plants discard them as contaminants. Now researchers have found an environmentally-friendly and cost-effective solution.Published in Frontiers in Energy Research, their study is the first to show that purple phototrophic bacteria -- which can store energy from light -- when supplied with an electric current can recover near to 100% of carbon from any type of organic waste, while generating hydrogen gas for electricity production.""One of the most important problems of current wastewater treatment plants is high carbon emissions,"" says co-author Dr Daniel Puyol of King Juan Carlos University, Spain. ""Our light-based biorefinery process could provide a means to harvest green energy from wastewater, with zero carbon footprint.""Purple photosynthetic bacteriaWhen it comes to photosynthesis, green hogs the limelight. But as chlorophyll retreats from autumn foliage, it leaves behind its yellow, orange and red cousins. In fact, photosynthetic pigments come in all sorts of colors -- and all sorts of organisms.Cue purple phototrophic bacteria. They capture energy from sunlight using a variety of pigments, which turn them shades of orange, red or brown -- as well as purple. But it is the versatility of their metabolism, not their color, which makes them so interesting to scientists.""Purple phototrophic bacteria make an ideal tool for resource recovery from organic waste, thanks to their highly diverse metabolism,"" explains Puyol.The bacteria can use organic molecules and nitrogen gas -- instead of CO2 and H2O -- to provide carbon, electrons and nitrogen for photosynthesis. This means that they grow faster than alternative phototrophic bacteria and algae, and can generate hydrogen gas, proteins or a type of biodegradable polyester as byproducts of metabolism.Tuning metabolic output with electricityWhich metabolic product predominates depends on the bacteria's environmental conditions -- like light intensity, temperature, and the types of organics and nutrients available.""Our group manipulates these conditions to tune the metabolism of purple bacteria to different applications, depending on the organic waste source and market requirements,"" says co-author Professor Abraham Esteve-Neeez of University of Alcale, Spain.""But what is unique about our approach is the use of an external electric current to optimize the productive output of purple bacteria.""This concept, known as a ""bioelectrochemical system,"" works because the diverse metabolic pathways in purple bacteria are connected by a common currency: electrons. For example, a supply of electrons is required for capturing light energy, while turning nitrogen into ammonia releases excess electrons, which must be dissipated. By optimizing electron flow within the bacteria, an electric current -- provided via positive and negative electrodes, as in a battery -- can delimit these processes and maximize the rate of synthesis.Maximum biofuel, minimum carbon footprintIn their latest study, the group analyzed the optimum conditions for maximizing hydrogen production by a mixture of purple phototrophic bacteria species. They also tested the effect of a negative current -- that is, electrons supplied by metal electrodes in the growth medium -- on the metabolic behavior of the bacteria.Their first key finding was that the nutrient blend that fed the highest rate of hydrogen production also minimized the production of CO2.""This demonstrates that purple bacteria can be used to recover valuable biofuel from organics typically found in wastewater -- malic acid and sodium glutamate -- with a low carbon footprint,"" reports Esteve-Neeez.Even more striking were the results using electrodes, which demonstrated for the first time that purple bacteria are capable of using electrons from a negative electrode or ""cathode"" to capture CO2 via photosynthesis.""Recordings from our bioelectrochemical system showed a clear interaction between the purple bacteria and the electrodes: negative polarization of the electrode caused a detectable consumption of electrons, associated with a reduction in carbon dioxide production.""This indicates that the purple bacteria were using electrons from the cathode to capture more carbon from organic compounds via photosynthesis, so less is released as CO2.""Towards bioelectrochemical systems for hydrogen productionAccording to the authors, this was the first reported use of mixed cultures of purple bacteria in a bioelectrochemical system -- and the first demonstration of any phototroph shifting metabolism due to interaction with a cathode.Capturing excess CO2 produced by purple bacteria could be useful not only for reducing carbon emissions, but also for refining biogas from organic waste for use as fuel.However, Puyol admits that the group's true goal lies further ahead.""One of the original aims of the study was to increase biohydrogen production by donating electrons from the cathode to purple bacteria metabolism. However, it seems that the PPB bacteria prefer to use these electrons for fixing CO2 instead of creating H2.""We recently obtained funding to pursue this aim with further research, and will work on this for the following years. Stay tuned for more metabolic tuning.""Story Source:Materials provided by Frontiers. ",0.0712672,0.161099,0.173055,0.141728,0.11609,0.138132,0.143742,0,0
219,Making a transparent flexible material of silk and nanotubes,"The silk fibers produced by Bombyx mori, the domestic silkworm, has been prized for millennia as a strong yet lightweight and luxurious material. Although synthetic polymers like nylon and polyester are less costly, they do not compare to silk's natural qualities and mechanical properties. And according to research from the University of Pittsburgh's Swanson School of Engineering, silk combined with carbon nanotubes may lead to a new generation of biomedical devices and so-called transient, biodegradable electronics.The study, ""Promoting Helix-Rich Structure in Silk Fibroin Films through Molecular Interactions with Carbon Nanotubes and Selective Heating for Transparent Biodegradable Devices,"" was featured on the Oct. 26 cover of the American Chemistry Society journal Applied Nano Materials.""Silk is a very interesting material. It is made of natural fibers that humans have been using for thousands of years to make high quality textiles, but we as engineers have recently started to appreciate silk's potential for many emerging applications such as flexible bioelectronics due to its unique biocompatibility, biodegradability and mechanical flexibility,"" noted Mostafa Bedewy, assistant professor of industrial engineering at the Swanson School and lead author of the paper. ""The issue is that if we want to use silk for such applications, we don't want it to be in the form of fibers. Rather, we want to regenerate silk proteins, called fibroins, in the form of films that exhibit desired optical, mechanical and chemical properties.""As explained by the authors in the video below, these regenerated silk fibroins (RSFs) however typically are chemically unstable in water and suffer from inferior mechanical properties, owing to the difficulty in precisely controlling the molecular structure of the fibroin proteins in RSF films. Bedewy and his NanoProduct Lab group, which also work extensively on carbon nanotubes (CNTs), thought that perhaps the molecular interactions between nanotubes and fibroins could enable ""tuning"" the structure of RSF proteins.""One of the interesting aspects of CNTs is that, when they are dispersed in a polymer matrix and exposed to microwave radiation, they locally heat up,"" Dr. Bedewy explained. ""So we wondered whether we could leverage this unique phenomenon to create desired transformations in the fibroin structure around the CNTs in an ""RSF-CNT"" composite.""According to Dr. Bedewy, the microwave irradiation, coupled with a solvent vapor treatment, provided a unique control mechanism for the protein structure and resulted in a flexible and transparent film comparable to synthetic polymers but one that could be both more sustainable and degradable. These RSF-CNT films have potential for use in flexible electronics, biomedical devices and transient electronics such as sensors that would be used for a desired period inside the body ranging from hours to weeks, and then naturally dissolve.""We are excited about advancing this work further in the future, as we are looking forward to developing the science and technology aspects of these unique functional materials,"" Dr. Bedewy said. "" From a scientific perspective, there is still a lot more to understand about the molecular interactions between the functionalization on nanotube surfaces and protein molecules. From an engineering perspective, we want to develop scalable manufacturing processes for taking cocoons of natural silk and transforming them into functional thin films for next generation wearable and implantable electronic devices.""Video: https://www.youtube.com/watch?v=c10f80FortUStory Source:Materials provided by University of Pittsburgh. ",0.0829827,0.184295,0.152119,0.157505,0.156588,0.128365,0.161114,0,0
220,Interfacial electronic state improving hydrogen storage capacity in Pd-MOF materials,"NIMS, Kyushu University and Kyoto University jointly identified a mechanism by which a hybrid material composed of palladium (Pd) and metal-organic frameworks (MOFs) is capable of storing approximately twice as much hydrogen as a material composed solely of Pd. The greater hydrogen storage capacity of the hybrid material is associated with a slight change in its electronic state caused by the transfer of an electric charge -- amounting to approximately 0.4 electrons -- from the Pd to the MOFs. The joint research team therefore successfully determined the quantitative relationships between the materials' electronic states and their hydrogen storage properties. These findings may facilitate the development of new hybrid materials with superior hydrogen storage properties or with the capability to efficiently catalyze hydrogenation reactions.Hydrogen is a viable next-generation energy source. Widespread use of hydrogen will require efficient hydrogen storage methods. Transition metals, such as Pd, are known to possess excellent hydrogen storage properties. Recent reports indicated that the hydrogen storage capabilities of materials composed of transition metal nanoparticles and MOFs are significantly higher than those of materials composed solely of a transition metal. It had been predicted that these increased hydrogen storage capabilities are associated with the transfer of electric charge at the interface between the transition metals and the MOFs. However, the mechanisms responsible for the increased hydrogen storage capabilities were not quantitatively understood (e.g., the amount of charge transferred).We investigated the electronic state of a hybrid material, Pd@HKUST-1, which is composed of Pd nanocubes and MOFs (specifically, copper(II) 1, 3, 5-benzenetricarboxylate, or HKUST-1) and capable of storing approximately twice the amount of hydrogen of materials composed solely of Pd nanocubes. For this investigation, we used NIMS's synchrotron X-ray beamline at SPring-8, the world's largest synchrotron radiation facility. In addition, we calculated the electronic states of Pd and HKUST-1 separately and compared them with the electronic state of Pd@HKUST-1. As a result, we found that an electric charge amounting to approximately 0.4 electrons had been transferred from the Pd nanocubes to the MOFs. This small charge transfer presumably enabled the electron bands in the Pd nanocubes to store more hydrogen, resulting in approximately doubled hydrogen storage capacity for the hybrid material compared to a material composed solely of Pd nanocubes.Hybrid materials composed of transition metal nanoparticles and MOFs are potentially capable not only of storing large amounts of hydrogen but also of efficiently catalyzing hydrogenation reactions. The methods developed and used in this study to measure and analyze electronic states may accelerate the development of new hybrid materials with greatly increased hydrogen storage and catalytic capabilities.Story Source:Materials provided by National Institute for Materials Science, Japan. ",0.151563,0.256354,0.181941,0.226544,0.195856,0.231713,0.226271,0,0
221,Draw-your-own electrodes set to speed up development of micro detection devices,"Miniature devices for sensing biological molecules could be developed quicker thanks to a rapid prototyping method. Devices that sense and measure biological molecules important for healthcare, such as detecting diseases in blood samples, rely on electrodes to carry out their tasks.New generations of these devices are being made that manipulate molecules or work with smaller concentrations of molecules, for example detecting rare cancer cells in blood samples.These require intricate patterning of minute electrodes. Getting the right pattern is key, but building prototypes of different electrode designs can be expensive and time-consuming, often requiring specialist equipment and expertise.Now, researchers at Imperial College London, have created a method that allows intricate electrode patterns to be printed in community labs and hackspaces at a fraction of the time and cost. The details of their method are published in Scientific Reports.Lead researcher Dr Ali Salehi-Reyhani, from the Department of Chemistry at Imperial, said: ""With our method researchers and startups can more easily design and develop analytical devices, even when they need electronics that can't be bought off-the-shelf.""Community hackspaces are great for democratising science, allowing more people to try out new technology solutions. We hope this method will allow bioelectronics to benefit from that ecosystem of hackers getting hands-on with problems and solutions in healthcare.""The method allows researchers to design electrode patterns on computers before printing them off using a laser-cutting printer. The cavities are then filled with metal using microfluidic techniques -- using the science of how fluids move through confined spaces.In this way, researchers could print several sheets of electrodes, each with a slightly different design, allowing them to be tested in rapid succession to find the best design. Previously, designs may have had to be sent away to be manufactured, taking weeks or even months to arrive at the best design, but now the whole process can be reduced to a matter of days.The team at fabriCELL, a centre of excellence in artificial cell science run by Imperial College London and King's College London, are now using the technique to prototype devices for manipulating and analysing cells.They say the technique could be used to speed up the development of flexible wearable devices, such as skin patches that monitor health signals and devices, and devices that could be used in hospitals or GP surgeries, such as ones that can quickly distinguish between viral and bacterial infections with just a drop of blood.Story Source:Materials provided by Imperial College London. Original written by Hayley Dunning. ",0.073595,0.133425,0.12277,0.175236,0.129082,0.117412,0.153177,0,0
222,Racing electrons under control Physicists precisely control electrons on a time scale of under one femtosecond,"Being able to control electronic systems using light waves instead of voltage signals is the dream of physicists all over the world. The advantage is that electromagnetic light waves oscillate at petaherz frequency. This means that computers in the future could operate at speeds a million times faster than those of today. Scientists at Friedrich-Alexander-Universitet Erlangen-Nernberg (FAU) have now come one step closer to achieving this goal as they have succeeded in using ultra-short laser impulses to precisely control electrons in graphene.Current control in electronics that is one million times faster than in today's systems is a dream for many. Ultimately, current control is one of the most important components as it is responsible for data and signal transmission. Controlling the flow of electrons using light waves instead of voltage signals, as is now the case, could make this dream a reality. However, up to now, it has been difficult to control the flow of electrons in metals as metals reflect light waves and the electrons inside them cannot be influenced by these light waves.Physicists at FAU have therefore turned to graphene, a semi-metal that comprises only one single layer of carbon and is so thin that enough light can penetrate to enable electrons to be set in motion. In an earlier study, physicists at the Chair for Laser Physics had already succeeded in generating an electric signal at a time scale of only one femtosecond by using a very short laser pulse. This is equivalent to one millionth of one billionth of a second. In these extreme time scales, electrons reveal their quantum nature as they behave like a wave. The wave of electrons glides through the material as it is driven by the light field (the laser pulse).Under controlThe researchers went one step further in the current study. They aimed a second laser pulse at this light-driven wave. This second pulse now enables the electron wave to pass through the material in two dimensions. The second laser pulse can be used to deflect, accelerate or even change the direction of the electron wave. This enables information to be transmitted by this wave, depending on the exact time, strength and direction of the second pulse. It's possible to go one step further. 'Imagine the electron wave is a wave in water. Waves in water can split because of an obstacle and converge and interfere when they have passed the obstacle. Depending on how the sub-waves stand in relation to one another, they either amplify or cancel each other out. We can use the second laser pulse to modify the individual sub-waves in a targeted manner and thus control their interference', explains Christian Heide from the Chair of Laser Physics. 'In general, it's very difficult to control quantum phenomena, such as the wave characteristics of electrons in this instance. This is because it's very difficult to maintain the electron wave in a material as the electron wave scatters with other electrons and loses its wave characteristics. Experiments in this field are typically performed at extremely low temperatures. We can now carry out these experiments at room temperature, since we can control the electrons using laser pulses at such high speeds that there is no time left for the scatter processes with other electrons. This enables us to research several new physical processes that were previously not accessible.'It means the scientists have made significant progress towards realising electronic systems that can be controlled using light waves. In the next few years they will be investigating whether electrons in other two-dimensional materials can also be controlled in the same way. 'Maybe we will be able to use materials research to modify the characteristics of materials in such a way that it will soon be possible to build small transistors that can be controlled by light', says Heide.Story Source:Materials provided by University of Erlangen-Nuremberg. ",0.107963,0.122049,0.113501,0.136098,0.158582,0.164149,0.128891,0,0
223,Environment turns molecule into a switch,"It looks like a cross with four arms of equal length that have a central atom at their intersection. All atoms are arranged in one plane so that the molecule is absolutely planar -- at least in the normal state.Physicists from the University of Werzburg have now succeeded in manipulating this molecule using a special deposit and an electrical field to permanently take on two different states. This could make the molecule suitable as a kind of ""molecular switch"" for spintronics applications -- a trailblazing data processing technology based on electron spin.The molecular switch is the fruit of a collaboration of members from the Departments of Experimental and Theoretical Physics at the University of Werzburg: Dr. Jens Kegel, a postdoc at the Department of Experimental Physics II, devised and ran the experiments. Giorgio Sangiovanni, a Professor of Theoretical Physics at the Institute of Theoretical Physics and Astrophysics, was in charge of interpreting them. The team has recently published their research results in the current issue of the journal npj Quantum Materials.Building a bridge with a dye molecule""We used a manganese phthalocyanine molecule, a dye which cannot be normally switched,"" Sangiovanni describes the physicists' approach. Jens Kegel had to resort to a trick to turn it into a molecular switch: He mounted the molecule on a very special metallic surface built of silver and bismuth atoms.Because bismuth atoms are much bigger than silver atoms, their regular arrangement covers the metal surface like low walls. Irregularities in this structure result in a larger distance between two bismuth areas like a dried-up riverbed. The manganese phthalocyanine molecule then builds a bridge across this riverbed to continue the metaphor.Switched by an electric fieldJens Kegel used a special technique to give the molecule its switching property. When he approached the manganese atom at the centre of the molecule with a very fine tip that emitted an electric field, the central atom changed its position and moved down a bit towards the metallic surface out of the molecular plane. ""In this way, the molecule took on two stable switchable states,"" the physicist says.Physically, the molecule creates a large magnetic moment due to the position change of its central atom. Because of special quantum physics phenomena, this change of position affects the entire molecule, manifesting externally through distinctly different magnetic properties. Physicists refer to this as the Kondo effect.A new concept to build molecular switchesNormally, molecular switches are synthesised to be intrinsically stable in multiple states. ""We have now demonstrated that this functionality can be created also in non-switchable molecules by selectively manipulating the molecule's environment,"" Kegel and Sangiovanni explain the central result of their paper. The physicists have thus developed a new concept to build molecular switches which they believe will open up new design possibilities in molecular electronics in the future.Successful cooperation in the Collaborative Research CenterThe successful collaboration of theoretical and experimental physicists at the University of Werzburg is also based on the Collaborative Research Center ""Topological and Correlated Electronics at Surfaces and Interfaces,"" short ToCoTronics, which is located in Werzburg. Its focus is on special physical phenomena -- electronic correlations and topological physics, and above all, their interactions which have huge application potential for novel, trailblazing technologies of tomorrow.Story Source:Materials provided by University of Werzburg. ",0.0958936,0.194868,0.138478,0.165673,0.151608,0.152658,0.164802,0,0
224,"Epoxy compound gets a graphene bump Scientists combine graphene foam, epoxy into tough, conductive composite","Rice University scientists have built a better epoxy for electronic applications.Epoxy combined with ""ultrastiff"" graphene foam invented in the Rice lab of chemist James Tour is substantially tougher than pure epoxy and far more conductive than other epoxy composites while retaining the material's low density. It could improve upon epoxies in current use that weaken the material's structure with the addition of conductive fillers.The new material is detailed in the American Chemical Society journal ACS Nano.By itself, epoxy is an insulator, and is commonly used in coatings, adhesives, electronics, industrial tooling and structural composites. Metal or carbon fillers are often added for applications where conductivity is desired, like electromagnetic shielding.But there's a trade-off: More filler brings better conductivity at the cost of weight and compressive strength, and the composite becomes harder to process.The Rice solution replaces metal or carbon powders with a three-dimensional foam made of nanoscale sheets of graphene, the atom-thick form of carbon.The Tour lab, in collaboration with Rice materials scientists Pulickel Ajayan, Rouzbeh Shahsavari and Jun Lou and Yan Zhao of Beihang University in Beijing, took their inspiration from projects to inject epoxy into 3D scaffolds including graphene aerogels, foams and skeletons from various processes.The new scheme makes much stronger scaffolds from polyacrylonitrile (PAN), a powdered polymer resin they use as a source of carbon, mixed with nickel powder. In the four-step process, they cold-press the materials to make them dense, heat them in a furnace to turn the PAN into graphene, chemically treat the resulting material to remove the nickel and use a vacuum to pull the epoxy into the now-porous material.""The graphene foam is a single piece of few-layer graphene,"" Tour said. ""Therefore, in reality, the entire foam is one large molecule. When the epoxy infiltrates the foam and then hardens, any bending in the epoxy in one place will stress the monolith at many other locations due to the embedded graphene scaffolding. This ultimately stiffens the entire structure.""The puck-shaped composites with 32 percent foam were marginally denser, but had an electrical conductivity of about 14 Siemens (a measure of conductivity, or inverse ohms) per centimeter, according to the researchers. The foam did not add significant weight to the compound, but gave it seven times the compressive strength of pure epoxy.Easy interlocking between the graphene and epoxy helped stabilize the structure of the graphene as well. ""When the epoxy infiltrates the graphene foam and then hardens, the epoxy is captured in micron-sized domains of the graphene foam,"" Tour said.The lab upped the ante by mixing multiwalled carbon nanotubes into the graphene foam. The nanotubes acted as reinforcement bars that bonded with the graphene and made the composite 1,732 percent stiffer than pure epoxy and nearly three times as conductive, at about 41 Siemens per centimeter, far greater than nearly all of the scaffold-based epoxy composites reported to date, according to the researchers.Tour expects the process will scale for industry. ""One just needs a furnace large enough to produce the ultimate part,"" he said. ""But that is done all the time to make large metal parts by cold-pressing and then heating them.""He said the material could initially replace the carbon-composite resins used to pre-impregnate and reinforce fabric used in materials from aerospace structures to tennis rackets.Story Source:Materials provided by Rice University. Original written by Mike Williams. ",0.151027,0.226181,0.231729,0.200753,0.209605,0.201747,0.212504,0,0
225,Scientists shed light on semiconductor degradation mechanism,"Scientists at Nagoya Institute of Technology (NITech) and collaborating universities in Japan have gained new insights into the mechanisms behind degradation of a semiconductor material that is used in electronic devices. By highlighting the specific science behind how the material degrades, they are making way for potential discoveries that may prevent the performance degradation of the material.The study was published in the Journal of Applied Physics in September of 2018. The scientists used Silicon Carbide (SiC) material for the experiment. SiC is becoming a more popular alternative to standard semiconductor materials for electronic devices. The study is based on a specific type of SiC material that is characteristic for its structure, or 4H-SiC. This material was exposed to both photoluminescence as well as various temperatures as a means to create specific kinds of deformations that lead to the degradation of SiC-based devices. The scientists were able to observe how these deformations actually take place on an atomic level.""We quantified the speed at which electric charge particles move in regions of 4H-SiC material where the atomic structure has been defected. This will usher discoveries of ways to suppress degradation of SiC-based devices such as power electronic systems,"" states Dr. Masashi Kato, an associate professor at the Frontier Research Institute for Materials Science in NITech.In order to better understand the actual mechanism behind atomic deformation that lead to degradations, the researchers used photoluminescence to induce movement of electric charge particles and measured the speeds at which that took place. They looked for specific factors that may limit particle movement, including the material that was used.They also tested the effects of increasing temperature, specifically looking to see if higher temperatures will increase or decrease rate of deformation.According to Dr. Kato, the presence of a particular kind of atomic deformation that causes the material degrade is particularly problematic for SiC-based power devices. ""While a particular SiC-based device is in operation, the atoms of the material deform, which leads to degradation. The process by which these atoms deform is not clear yet. What is known, however, is that movement of electric charge within the material as well as areas where the material has become defect already contribute to the aforementioned atomic deformation,"" he states.So far similar experiments have been conducted in the past by other researchers, the results that have been reported are not consistent. Here, the result of experiments with photoluminescence indicates that the carrier recombination in single Shockley stacking faults (1SSFs) and at partial dislocations (PDs) is faster than that in regions without 1SSFs in 4H-SiC. Such fast recombination will induce the degradation of the device with 1SSFs. In addition, 1SSF expansion velocity also increases with temperature increase.As such, they pave the way for research that will revolve around the slowing of SiC-based devices degradation. This, in turn, could potentially result in higher quality and more durable devices.Along those lines, the authors state that their future research endeavors will focus on finding out ways to prevent SiC-based devices from degrading as well as creating devices that will not wear down over time.Story Source:Materials provided by Nagoya Institute of Technology. ",0.107109,0.150203,0.186121,0.197541,0.16115,0.161668,0.189218,0,0
226,Unlocking the secrets of metal-insulator transitions,"By using an x-ray technique available at the National Synchrotron Light Source II (NSLS-II), scientists found that the metal-insulator transition in the correlated material magnetite is a two-step process. The researchers from the University of California Davis published their paper in the journal Physical Review Letters. NSLS-II, a U.S. Department of Energy (DOE) Office of Science user facility located at Brookhaven National Laboratory, has unique features that allow the technique to be applied with stability and control over long periods of time.""Correlated materials have interesting electronic, magnetic, and structural properties, and we try to understand how those properties change when their temperature is changed or under the application of light pulses, or an electric field"" said Roopali Kukreja, a UC Davis professor and the lead author of the paper. One such property is electrical conductivity, which determines whether a material is metallic or an insulator.If a material is a good conductor of electricity, it is usually metallic, and if it is not, it is then known as an insulator. In the case of magnetite, temperature can change whether the material is a conductor or insulator. For the published study, the researchers' goal was to see how the magnetite changed from insulator to metallic at the atomic level as it got hotter.In any material, there is a specific arrangement of electrons within each of its billions of atoms. This ordering of electrons is important because it dictates a material's properties, for example its conductivity. To understand the metal-insulator transition of magnetite, the researchers needed a way to watch how the arrangement of the electrons in the material changed with the alteration of temperature.""This electronic arrangement is related to why we believe magnetite becomes an insulator,"" said Kukreja. However, studying this arrangement and how it changes under different conditions required the scientists to be able to look at the magnetite at a super-tiny scale.The technique, known as x-ray photon correlation spectroscopy (XPCS), available at NSLS-II's Coherent Soft X-ray scattering (CSX) beamline, allowed the researchers to look at how the material changed at the nanoscale -- on the order of billionths of a meter.""CSX is designed for soft x-ray coherent scattering. This means that the beamline exploits our ultrabright, stable and coherent source of x-rays to analyze how the electron's arrangement changes over time,"" explained Andi Barbour, a CSX scientist who is a coauthor on the paper. ""The excellent stability allows researchers to investigate tiny variations over hours so that the intrinsic electron behavior in materials can be revealed.""However, this is not directly visible so XPCS uses a trick to reveal the information.""The XPCS technique is a coherent scattering method capable of probing dynamics in a condensed matter system. A speckle pattern is generated when a coherent x-ray beam is scattered from a sample, as a fingerprint of its inhomogeneity in real space,"" said Wen Hu, a scientist at CSX and co-author of the paper.Scientists can then apply different conditions to their material and if the speckle pattern changes, it means the electron ordering in the sample is changing. ""Essentially, XPCS measures how much time it takes for a speckle's intensity to become very different from the average intensity, which is known as decorrelation,"" said Claudio Mazzoli, the lead beamline scientist at the CSX beamline. ""Considering many speckles at once, the ensemble decorrelation time is the signature of the dynamic timescale for a given sample condition.""The technique revealed that the metal-insulator transition is not a one step process, as was previously thought, but actually happens in two steps.""What we expected was that things would go faster and faster while warming up. What we saw was that things get faster and faster and then they slow down. So the fast phase is one step and the second step is the slowing down, and that needs to happen before the material becomes metallic,"" said Kukreja. The scientists suspect that the slowing down occurs because, during the phase change, the metallic and insulating properties actually exist at the same time in the material.""This study shows that these nanometer length scales are really important for these materials,"" said Kukreja. ""We can't access this information and these experimental parameters anywhere else than at the CSX beamline of NSLS-II.""Story Source:Materials provided by DOE/Brookhaven National Laboratory. ",0.070612,0.124756,0.109472,0.14799,0.112471,0.133242,0.137917,0,0
227,Making steps toward improved data storage Mapping nanoscale changes in memory material,"A team of scientists has created the world's most powerful electromagnetic pulses in the terahertz range to control in fine detail how a data-storage material switches physical form. This discovery could help find a way to scale down memory devices, eventually revolutionizing how computers handle information.Compact discs might be falling out of fashion, but they may have inspired the next generation of computer nanotechnology. A glass layer in CDs consists of a phase-change material that can be encoded with information when light pulses cause crystals in small regions of the layer to either grow or melt.Phase-change materials triggered by electrical impulses -- rather than light -- would offer new memory technologies with more stable and faster operation than that possible in many current types of memory devices. In addition, downscaling memory sites in phase-change materials could increase memory density. But this remains challenging because of the difficulty of controlling the crystal growth -- crystallization -- and melting -- amorphization -- processes.Addressing this issue in an article in Physical Review Letters, a team of scientists led by Kyoto University observed nanometer-scale growth of individual crystals in a phase-change material composed of germanium, antimony and tellurium -- or GST -- after applying high-powered terahertz pulses as a trigger.""One reason crystallization and amorphization of GST under an electric field are difficult to control is the heat diffusion effects in the micrometer scale associated with electrical inputs, which also contribute to the crystallization,"" explains group leader Hideki Hirori. ""Fortunately, terahertz technologies have matured to the point where we can use short pulses to generate strong electric fields while suppressing heating effects.""Hirori and his coworkers developed a terahertz pulse generator that delivered ultra-short and highly intense terahertz pulses across a pair of gold antennas. These pulses created an electric field in the GST sample comparable to that of an electrically switched device. Importantly, this approach greatly reduced the heat diffusion because of the extremely short duration of terahertz pulses -- around 1 picosecond, or 10?12 s -- enabling fine control over the rate and direction of GST crystallization. A region of crystallization grew in a straight line between the gold antennas in the direction of the field, at a few nanometers per pulse.When the team tracked stepwise changes in crystallization while increasing the number of terahertz pulses, they were surprised to find that after a certain point, crystal conductivity rapidly sped up instead of rising in line with the increase in terahertz strength. The researchers hypothesize that electrons jumping between states in the crystal added an unexpected source of heat to the system, boosting crystallization.Hirori explains: ""Our experiment reveals how nanoscale and direction-controlled growth of crystals in GST can be achieved. We also identified a phenomenon which should assist in the design of new devices and ultimately realize the fast and stable digital information handling potential that this material promises.""Story Source:Materials provided by Kyoto University. ",0.0800443,0.121197,0.112417,0.148162,0.13915,0.143387,0.133413,0,0
228,Experimental plasma generator offers path forward for better use of landfill gas as energy,"Methane gas released from landfills has long been a topic of interest for alternative energy. One issue, however, is that landfill gases contain numerous contaminants, such as volatile methyl siloxanes, whose silica deposits put extra wear and tear on the natural gas generators when they combust. One group has demonstrated a promising new application of plasma technology capable of removing such compounds.Researchers at the University of South Carolina in Columbia have demonstrated an experimental plasma device capable of cleaning gas samples of D4, one of the most common siloxanes. Drawing on a technique for creating plasma called dielectric barrier discharge, the group was able to significantly reduce the amount of D4 samples after treating it with a helium-based plasma.The findings point to a new potential solution for accommodating landfill gas rich in siloxanes. They will be presented at the American Physical Society 71st Annual Gaseous Electronics Conference and 60th Annual meeting of the APS Division of Plasma Physics, which takes place Nov. 5-9 at the Oregon Convention Center in Portland.""This is the first time dielectric barrier discharge has been used to remove volatile organic silicate compounds,"" said Malik Tahiyat, one of the researchers involved with the study. ""In our case, there's no wait for removing it or material that has to be thrown out after a certain amount of time.""Silicates erode the engines that drive natural gas electricity production, requiring extra costly maintenance. Most current methods of removing them from cleaner burning methane, such as carbon filters and silica gel, suffer from diminished performance and can be costly to reuse.The group created a dielectric barrier discharge plasma to render D4 inert by polymerizing it out of the gas phase. Helium gas was bubbled through liquid siloxane, which is then passed through a tubular dielectric barrier discharge plasma reactor.Samples that were treated with electric discharges were compared to samples that were not. Gas chromatography-mass spectrometry and nuclear magnetic resonance techniques were used to evaluate the quantity and identity of the products from the plasma reactions.Depending on how long the helium with D4 was exposed to the plasma, up to 85 percent of the D4 was converted to removable deposits of silica compounds, confirming that the silicon was removed from the gas mixture.""Our findings have shown that plasma can remove siloxane successfully,"" said Shamia Hoque, another researcher involved with the study. ""When it is removed, it comes out in a form that wouldn't re-enter the waste supply, something that's a problem with the other approaches.""Tanvir Farouk, a third researcher involved with the study, said the group hopes to improve on the laboratory-based system with the hopes of one day scaling it up to a commercially viable product.Story Source:Materials provided by American Physical Society. ",0.099678,0.18172,0.171203,0.166089,0.143246,0.168059,0.171407,0,0
229,"Why a stream of plasma makes chemical reactions more efficient Modeling shows that plasmas may activate the metal catalyst in a packed bed reactor, resulting in faster chemical reactions","A whiff of plasma, when combined with a nanosized catalyst, can cause chemical reactions to proceed faster, more selectively, at lower temperatures, or at lower voltages than without plasma -- and nobody really knows why.Using computer modeling, Juliusz Kruszelnicki of the University of Michigan investigated the interactions between plasmas and metal catalysts embedded into ceramic beads in a packed bed reactor. He discovered that together, the metals, beads and gas create plasma that intensifies electric fields and locally heats the catalyst, which can then accelerate reactions.Kruszelnicki will talk about this work at the American Physical Society 71st Annual Gaseous Electronics Conference and 60th Annual meeting of the APS Division of Plasma Physics, which will take place next week, Nov. 5-9 at the Oregon Convention Center in Portland.These plasma reactors have tremendous potential to make valuable chemical processes more efficient and cost-effective, such as removing air pollution, converting carbon dioxide into fuels and producing ammonia for fertilizer, through ""plasma chemical conversion.""""Combining thermocatalytic systems and plasmas allows new avenues to produce chemical products you otherwise might not be able to, or perhaps to do so at higher efficiency,"" Kruszelnicki said.Kruszelnicki modeled the interactions of plasma and catalysts using advanced multiphysics codes developed in the lab of Mark J. Kushner at the University of Michigan. These include modules for phenomena such as electromagnetics, surface chemistry, fluid dynamics and chemical kinetics. He modeled a packed bed reactor, which is a tube filled with ceramic beads, with an electrical current passing through concentric electrodes. When gases move through the reactor, catalysts cause them to react in specific ways, such as combining nitrogen and hydrogen to generate ammonia.Kruszelnicki found that when the beads are embedded with metallic catalyst particles and then electrified, field emission of electrons takes place, which enables higher densities of plasma. The plasma heats the catalyst, which can cause the chemical reaction to proceed faster and more efficiently, potentially lowering the applied power needed for the reaction.""Through this process of localizing the electric field, electrons can be emitted from the surface of the metal particles and start a plasma, where it otherwise wouldn't occur,"" Kruszelnicki said.By simulating low-temperature plasma chemistry, Kruszelnicki and other members of the Kushner lab are discovering new ways that plasma and catalysts work together to make plasma chemical conversion more efficient than traditional chemical conversion. Currently they are working with the National Science Foundation's Industry-University Cooperative Research Centers Program to collaborate with companies to translate this research for use in industry. They also hope that these more efficient processes will be compatible with off-the-grid applications, such as making fertilizer for subsistence farmers using solar power.Story Source:Materials provided by American Physical Society. ",0.110208,0.227022,0.138856,0.16495,0.147921,0.16967,0.175059,0,0
230,New technique to understand biology at the nanoscale Applications include detecting infectiousness of HIV virus,"Washington State University researchers for the first time have shown that they can use electrical fields to gain valuable information about the tiny, floating vesicles that move around in animals and plants and are critically important to many biological functions.The new technique could make it easier and less expensive for researchers to gain important information about many biological processes -- from understanding the spread of infection in people to improving drug delivery techniques. Led by graduate student Adnan Morshed and Prashanta Dutta, professor in the School of Mechanical and Materials Engineering, the work was published in Physical Review Fluids.At the basis of much of biology are cells and, at even smaller scales, cell-like bubbles that float around in liquid doing critically important jobs. So, for instance, neurons communicate in our brain through vesicles that carry information and chemicals from one neuron to the next. The HIV virus is another tiny vesicle. Over time, the vesicle carrying HIV changes and becomes stiffer, which indicates that the virus is becoming more infectious.But studying the properties of these tiny and critically important cellular sacs that travel through organisms in fluids has been difficult, especially when researchers get to the smallest floaters that are 40-100 nanometers in size. To study biological processes at tiny scales, the researchers use atomic force microscopes, which require removing the vesicles from their natural floating homes. The process is expensive, cumbersome, and slow. Furthermore, by taking them out of their natural settings, the biological materials also don't necessarily exhibit their natural behavior, said Dutta.The WSU research team has developed a system that uses a microfluidic-based system and electric fields to better understand vesicles. Similar to a grocery store checker who identifies products as they are passed over a scanner, the researchers apply electrical fields in a liquid as the vesicle passes through a narrow pore. Because of the electric field, the vesicle moves, deforms, or reacts differently depending on its chemical make-up. In the case of the HIV vesicles, for instance, the researchers should be able to see the electric field affect the stiffer, more infectious vesicle in a different way than a more flexible, less infectious vesicle. For drug delivery, the system could differentiate a vesicle that contains more or less of a drug -- even if the two cells might look identical under a microscope.""Our system is low-cost and high throughput,"" said Dutta. ""We can really scan hundreds of samples at a time.""He added that they can change the speed of the process to allow researchers to more carefully observe property changes.The researchers developed a model and tested it with synthetic liposomes, tiny sacs that are used for targeted drug delivery. They hope to begin testing the process soon with more realistic biological materials.Story Source:Materials provided by Washington State University. ",0.0773186,0.130118,0.128496,0.179981,0.117029,0.133878,0.162172,0,0
231,How to mass produce cell-sized robots,"Tiny robots no bigger than a cell could be mass-produced using a new method developed by researchers at MIT. The microscopic devices, which the team calls ""syncells"" (short for synthetic cells), might eventually be used to monitor conditions inside an oil or gas pipeline, or to search out disease while floating through the bloodstream.The key to making such tiny devices in large quantities lies in a method the team developed for controlling the natural fracturing process of atomically-thin, brittle materials, directing the fracture lines so that they produce miniscule pockets of a predictable size and shape. Embedded inside these pockets are electronic circuits and materials that can collect, record, and output data.The novel process, called ""autoperforation,"" is described in a paper published today in the journal Nature Materials, by MIT Professor Michael Strano, postdoc Pingwei Liu, graduate student Albert Liu, and eight others at MIT.The system uses a two-dimensional form of carbon called graphene, which forms the outer structure of the tiny syncells. One layer of the material is laid down on a surface, then tiny dots of a polymer material, containing the electronics for the devices, are deposited by a sophisticated laboratory version of an inkjet printer. Then, a second layer of graphene is laid on top.Controlled fracturingPeople think of graphene, an ultrathin but extremely strong material, as being ""floppy,"" but it is actually brittle, Strano explains. But rather than considering that brittleness a problem, the team figured out that it could be used to their advantage.""We discovered that you can use the brittleness,"" says Strano, who is the Carbon P. Dubbs Professor of Chemical Engineering at MIT. ""It's counterintuitive. Before this work, if you told me you could fracture a material to control its shape at the nanoscale, I would have been incredulous.""But the new system does just that. It controls the fracturing process so that rather than generating random shards of material, like the remains of a broken window, it produces pieces of uniform shape and size. ""What we discovered is that you can impose a strain field to cause the fracture to be guided, and you can use that for controlled fabrication,"" Strano says.When the top layer of graphene is placed over the array of polymer dots, which form round pillar shapes, the places where the graphene drapes over the round edges of the pillars form lines of high strain in the material. As Albert Liu describes it, ""imagine a tablecloth falling slowly down onto the surface of a circular table. One can very easily visualize the developing circular strain toward the table edges, and that's very much analogous to what happens when a flat sheet of graphene folds around these printed polymer pillars.""As a result, the fractures are concentrated right along those boundaries, Strano says. ""And then something pretty amazing happens: The graphene will completely fracture, but the fracture will be guided around the periphery of the pillar."" The result is a neat, round piece of graphene that looks as if it had been cleanly cut out by a microscopic hole punch.Because there are two layers of graphene, above and below the polymer pillars, the two resulting disks adhere at their edges to form something like a tiny pita bread pocket, with the polymer sealed inside. ""And the advantage here is that this is essentially a single step,"" in contrast to many complex clean-room steps needed by other processes to try to make microscopic robotic devices, Strano says.The researchers have also shown that other two-dimensional materials in addition to graphene, such as molybdenum disulfide and hexagonal boronitride, work just as well.Cell-like robotsRanging in size from that of a human red blood cell, about 10 micrometers across, up to about 10 times that size, these tiny objects ""start to look and behave like a living biological cell. In fact, under a microscope, you could probably convince most people that it is a cell,"" Strano says.This work follows up on earlier research by Strano and his students on developing syncells that could gather information about the chemistry or other properties of their surroundings using sensors on their surface, and store the information for later retrieval, for example injecting a swarm of such particles in one end of a pipeline and retrieving them at the other to gain data about conditions inside it. While the new syncells do not yet have as many capabilities as the earlier ones, those were assembled individually, whereas this work demonstrates a way of easily mass-producing such devices.Apart from the syncells' potential uses for industrial or biomedical monitoring, the way the tiny devices are made is itself an innovation with great potential, according to Albert Liu. ""This general procedure of using controlled fracture as a production method can be extended across many length scales,"" he says. ""[It could potentially be used with] essentially any 2-D materials of choice, in principle allowing future researchers to tailor these atomically thin surfaces into any desired shape or form for applications in other disciplines.""This is, Albert Liu says, ""one of the only ways available right now to produce stand-alone integrated microelectronics on a large scale"" that can function as independent, free-floating devices. Depending on the nature of the electronics inside, the devices could be provided with capabilities for movement, detection of various chemicals or other parameters, and memory storage.There are a wide range of potential new applications for such cell-sized robotic devices, says Strano, who details many such possible uses in a book he co-authored with Shawn Walsh, an expert at Army Research Laboratories, on the subject, called ""Robotic Systems and Autonomous Platforms,"" which is being published this month by Elsevier Press.As a demonstration, the team ""wrote"" the letters M, I, and T into a memory array within a syncell, which stores the information as varying levels of electrical conductivity. This information can then be ""read"" using an electrical probe, showing that the material can function as a form of electronic memory into which data can be written, read, and erased at will. It can also retain the data without the need for power, allowing information to be collected at a later time. The researchers have demonstrated that the particles are stable over a period of months even when floating around in water, which is a harsh solvent for electronics, according to Strano.""I think it opens up a whole new toolkit for micro- and nanofabrication,"" he says.Story Source:Materials provided by Massachusetts Institute of Technology. Original written by David L. Chandler. ",0.0459517,0.0901774,0.0915668,0.141619,0.0839756,0.0907576,0.121247,0,0
232,3D-printed lithium-ion batteries,"Electric vehicles and most electronic devices, such as cell phones and laptop computers, are powered by lithium-ion batteries. Until now, manufacturers have had to design their devices around the size and shape of commercially available batteries. But researchers have developed a new method to 3D print lithium-ion batteries in virtually any shape. They report their results in ACS Applied Energy Materials.Most lithium-ion batteries on the market come in cylindrical or rectangular shapes. Therefore, when a manufacturer is designing a product -- such as a cell phone -- they must dedicate a certain size and shape to the battery, which could waste space and limit design options. Theoretically, 3D-printing technologies can fabricate an entire device, including the battery and structural and electronic components, in almost any shape. However, the polymers used for 3D printing, such as poly(lactic acid) (PLA), are not ionic conductors, creating a major hurdle for printing batteries. Christopher Reyes, Benjamin Wiley and colleagues wanted to develop a process to print complete lithium-ion batteries with an inexpensive 3D printer.The researchers increased the ionic conductivity of PLA by infusing it with an electrolyte solution. In addition, they boosted the battery's electrical conductivity by incorporating graphene or multi-walled carbon nanotubes into the anode or cathode, respectively. To demonstrate the battery's potential, the team 3D printed an LED bangle bracelet with an integrated lithium-ion battery. The bangle battery could power a green LED for about 60 seconds. According to the researchers, the capacity of the first-generation 3D-printed battery is about two orders of magnitude lower than that of commercial batteries, which is too low for practical use. However, they say that they have several ideas for increasing the capacity, such as replacing the PLA-based materials with 3D-printable pastes.Story Source:Materials provided by American Chemical Society. ",0.117422,0.227717,0.151209,0.17467,0.172731,0.181384,0.174632,0,0
233,Electrical enhancement: Engineers speed up electrons in semiconductors,"Researchers from Graduate School of Bio-Applications and Systems Engineering at Tokyo University of Agriculture and Technology (TUAT) have sped up the movement of electrons in organic semiconductor films by two to three orders of magnitude. The speedier electronics could lead to improved solar power and transistor use across the world, according to the scientists.They published their results in the September issue of Macromolecular Chemistry and Physics.Led by Kenji Ogino, a professor at Graduate School of Bio-Applications and Systems Engineering at TUAT, Japan, the team found that adding polystyrene, commonly known as Styrofoam in North America, could enhance the semiconducting polymer by allowing electrons to move from plane to plane quickly. The process, called hole mobility, is how electrons move through an electric field consisting of multiple layers. When a molecule is missing an electron, an electron from a different plane can jump or fall and take its place.Through various imaging techniques, it's fairly easy to follow the electron trail in the crystal-based structures. In many semiconducting polymers, however, the clean, defined lines of the crystalline skeleton intertwine with a much more difficult-to-define region. It's actually called the amorphous domain.""[Electrons] transport in both crystalline and amorphous domains. To improve the total electron mobility, it is necessary to control the nature of the amorphous domain,"" Ogino said. ""We found that hole mobility extraordinarily improved by the introduction of polystyrene block accompanied by the increase of the ratio of rigid amorphous domain.""The researchers believe that the way the crystalline domain connects within itself occurs most effectively through the rigid amorphous domain. The addition of polystyrene introduced more amorphous domain, but contained by flexible chains of carbon and hydrogen atoms. Even though the chains are flexible, it provides rigidity, and some degree of control, to the amorphous domain.Electrons moved two to three times quicker than normal.""The introduction of a flexible chain in semicrystalline polymers is one of the promising strategies to improve the various functionalities of polymer films by altering the characteristics of the amorphous domain,"" Ogino said. ""We propose that the rigid amorphous domain plays an important role in the hole transporting process.""Enhanced hole mobility is a critical factor in developing more efficient solar devices, according to Ogino. Next, Ogino and the researchers plan to examine how the enhanced hole mobility affected other parameters, such as the chemical composition and position of the structures within the polymer film.Story Source:Materials provided by Tokyo University of Agriculture and Technology. ",0.0909873,0.159499,0.167722,0.166818,0.161742,0.145289,0.164774,0,0
234,Controlling organic semiconductor band gaps by electron-acceptor fluorination Researchers develop a fluorinated electron-accepter unit for high performance organic semiconductors,"Organic semiconductor materials have the potential to be used in innovative applications such as transparent and flexible devices, and their low cost makes their potential use particularly attractive. The properties of organic semiconductor materials can be tuned by controlling their structure at the molecular level through parts of the structure known as electron-accepting units. A group of researchers centered at Osaka University has specifically tailored an electron-accepting unit that was then successfully used in an organic semiconductor applied in solar cell device that showed high photovoltaic performance. Their findings were published in NPG Asia Materials.""Electron-accepting units are important elements of organic semiconductors,"" study corresponding author Yoshio Aso says. ""Through the controlled addition of electronegative fluorine groups to a widely used electron-accepting material, we were able to show precise control of the energy levels within the resulting semiconductor. This ability to tune the band gap translates to selectivity over the injection and transport of holes and/or electrons within the material, which is important in potential applications.""The fluorinated electron-acceptor unit was used to prepare a thin film solar cell that was compared with a cell based on a non-fluorinated analogue. The researchers found that the fluorinated material showed enhanced power conversion efficiency, up to 3.12%. The morphology of the fluorinated film was also found to be good, which supported the efficient charge generation and transport that is necessary for successful application.""The more we are able to fine tune organic semiconductor behavior on the molecular level, the more possibilities there will be for demonstrating their macroscopic applications,"" co-author Yutaka Ie says. ""It is our hope that the band gap control and high photovoltaic performance we have demonstrated will lead to our material being applied in devices such as organic light-emitted diodes, field-effect transistors, and thin film solar cells.""The straightforward demonstration of the link between high electronegativity, greater electron-accepting tendency, and enhanced semiconductor performance, highlights both the potential and versatility of organic semiconductors. Further elegant solutions such as this one could substantially broaden the range of ee-conjugated materials, and reinforce the case for organic electronics.Story Source:Materials provided by Osaka University. ",0.0991349,0.1831,0.143185,0.170505,0.176759,0.158086,0.162211,0,0
235,"When it comes to smartphone lifespan, brand name matters more than hardware Popularity of iPhone contributes to its longevity -- and sustainability","Many critics have denounced smartphone manufacturers in recent years for producing devices that quickly become obsolete, creating a ""planned obsolescence"" that is costly for consumers and the global environment.Yet while many consumers clamor for increased ""repairability"" -- and thus longer lifespans -- for these devices, a new Yale-led study finds that there is a more important factor in determining how long smartphones remain in use: Brand cache.In an analysis of roughly 500,000 listings of secondhand Apple and Samsung phones sold on eBay, researchers found that brand, an intangible property, is more important than repairability or memory size in extending the life of a product.Writing in the Journal of Industrial Ecology, they conclude that iPhones on average have an additional year of use even though the two brands are comparable in quality and technological innovation.""I know that people bash the phone companies for 'planned obsolescence' of their products, but in most cases phones are replaced when they are still working fine, so improving repairability won't necessarily help much. "" said Tamar Makov, a doctoral student at the Yale School of Forestry & Environmental Studies (F&ES). ""Perhaps we should be focused on what really makes us replace phones so quickly, or we should be angry at manufacturers for making really good commercials. But it's likely that the problem is not the hardware.""The findings, she said, illustrate the potential of utilizing such intangible properties to promote sustainable consumption of these and other popular consumer goods.The paper was co-authored by Marian Chertow, associate professor of industrial environmental management at F&ES, Tomer Fishman, a postdoctoral associate at F&ES, and Vered Blass of Tel Aviv University.With penetration rates of almost 90 percent in the world's developed countries, mobile phones present a critical challenge in terms of environmental sustainability. Smartphones can contain as many as 50 different elements, including minerals linked to civil unrest, rare earth metals whose availabilities are dwindling, and various toxic materials that can degrade the natural world and threaten public health.These environmental costs are exacerbated by the relatively short lifespans of most smartphones. According to recent estimates the use phase of the average phone is less than two years.In recent years, a growing secondhand market for used smartphones has extended the average lifespan of these devices. But just how much longer, the researchers say, depends on the name on the phone.For their study, they examined the resale performance of the two largest manufacturers of smartphones in the U.S. -- Apple and Samsung -- which combined command about 70 percent of the smartphone market. They each produce both phone models that are relatively easy to repair and harder to repair. And while the products are similar enough that the two manufacturers are engaged in an ongoing patent infringement lawsuit, Apple ranks higher in terms of brand equity. While Apple typically is ranked at the top of the world's most valuable brands, Samsung usually falls below the top 10.For their comparison, they extracted resale prices and detailed device information for nearly a half-million Apple and Samsung phones listed on eBay, and then calculated the percentage of overall value each device had lost by the time it was resold based on its original retail price.After evaluating depreciation rates based on a range of variables -- including repairability, battery size, data capacity, and screen size -- they found that Samsung smartphones lost their value faster and reached the end of their ""economic"" life after 54 e months, while Apple phones reach it after about 67 months, a difference of about one year.In other words, Apple phones had longer lifespans even if they were the same age, size, and functional capability.""It's not that technical specifications don't matter,"" Makov said. ""But no matter what combination of specs were included in our analysis, brand name had a substantial impact.""""Some phones last longer, and it's not just because they're repairable or more functionally durable,"" she added. ""It's also the psychological aspects that make them more durable. That's important to remember.""Story Source:Materials provided by Yale School of Forestry & Environmental Studies. ",0.0554426,0.09279,0.130494,0.195534,0.118792,0.109025,0.125907,0,0
236,Scientists achieve first ever acceleration of electrons in plasma waves,"The Large Hadron Collider (LHC) at the European Organization for Nuclear Research (CERN) -- operators of the world's largest particle physics lab -- near Geneva, Switzerland, is said to be the largest particle accelerator in the world. The accelerator lies in a tunnel 27 kilometers in circumference, as deep as 175 meters beneath the French-Swiss border. This, by the way, has helped scientists uncover the Higgs boson, the last particle predicted by the Standard Model in 2012.Following the discovery of Higgs, a primary scientific goal of high energy physicits has been to characterize the properties of this new particle and to discover oher high-energy physics phenomenon. As a result, there have been some rapid developments in high-energy particle accelerator technology to support high-energy physics research. However, the technologies used to date can only be improved and expanded at great expense. For this reason, making high-energy accelerators more affordable is urgently needed.An international team of physicists, working on the Advanced Proton Driven Plasma Wakefield Acceleration Experiment (AWAKE) at CERN, reported that they have conducted a groundbreaking experiment demonstrating a new way of accelerating electrons to high energies -- one that could dramatically shrink the size of future particle accelerators and lower their costs. A paper describing this important result was published in Nature on August 29, 2018.AWAKE is an international scientific collaboration, made up of engineers and scientists from 18 institutes, including CERN and the Max Planck Institute for Physics in Germany. A UNIST-based research group, led by Professor Moses Chung in the Department of Physics is also part of this AWAKE collaboration and made a number of important contributions to AWAKE. This includes the design of beamlines and the optimization of electron beam injection.""AWAKE's technology will bring about a paridigm shift in the development of future high-energy particle accelerators, following LHC,"" says Professor Chung. ""The latest achievement could enable engineers to drastically reduce the size of future particle accelerators, cutting down on the vast amounts of money normally required to build them."" He adds, ""The high-energy particle collisions these facilities produce enable physicists to probe the fundamental laws of nature, providing the basis for advancements in a huge variety of different fields.""Typically, particle physics experiments use oscillating electric fields, called radiofrequency cavities, and high-powered magnets to accelerate particles to high energies. But these experiments must grow quite large -- they have to be, in order to accelerate particles with enough energy to properly study them.As an alternative cost-cutting option to accelerate particles more efficiently, the wakefield accelerator has been suggested. Physicists send a beam of either electrons, protons, or a laser through a plasma. Free electrons in the plasma move toward the beam, but overshoot it, then come crashing back, creating a bubble structure behind the beam and intense electric fields. If you inject particles, like more electrons, into the wake, it can accelerate the injected particles in a shorter amount of time with an electric field 10 or more times stronger.In the study, proton-driven plasma wakefield acceleration has been demonstrated for the first time. The strong electric fields, generated by a series of proton microbunches, were sampled with a bunch of electrons. These electrons were accelerated up to 2 GeV in approximately 10 m of plasma and measured using a magnetic spectrometer. This technique has the potential to accelerate electrons to the TeV scale in a single accelerating stage.Although still in the early stages of its programme, the AWAKE collaboration has taken an important step on the way to realising new high energy particle physics experiments.Story Source:Materials provided by Ulsan National Institute of Science and Technology(UNIST). ",0.0957877,0.113143,0.115801,0.147368,0.112191,0.157073,0.134504,0,0
237,Building a better battery layer by layer,"A team of researchers from Shinshu University in Nagano, Japan is now closer to a thin, high-capacity lithium-ion battery that could open the gates to better energy storage systems for electric vehicles.The research team was led by professor Katsuya Teshima, director of the Center for Energy and Environmental Science (hereinafter called CEES) at Shinshu University in Japan. They published their insights online in August in Scientific Reports.""Lithium-ion batteries are very promising energy storage systems for electric vehicles that require relatively high energy densities,"" said the study's author Nobuyuki Zettsu, a professor in the CEES and in the Department of Materials Chemistry at Shinshu University. ""However, their high operating voltages commonly result in the oxidative decomposition of the electrode surface, which subsequently promotes various side reactions.""Lithium-ion batteries store a lot of energy, but the force it takes to make the battery disperse the energy is too much -- so much, in fact, that the resulting damage makes the battery lose storage capacity.To combat this issue, Zettsu and colleagues examined the electric and electrochemical properties of the high-voltage (>4.8 V, vs Li+/Li) cathode, where the electrons enter the battery cell.""Many researchers have attempted to mitigate the observed capacity fading through the reduction of the direct contact area,"" Zettsu said, pointing to research projects where scientists covered the surface of the cathode with different materials in an attempt to reduce erosion. ""Various fundamental studies have been performed to investigate the effects of the surface coating modification; however, none of them led to a considerable performance enhancement of high-voltage cathode-based battery cells.""Zettsu may have turned the tide on surface modifiers through the use of a self-assembled monolayer. His team applied an ultra-thin coating of fluoroalkylisilane to the surface of the cathodes. Fluoroalkylisilane, a type of silicone, organizes itself into the most efficient arrangement to conduct lithium ions and insulate electrons while remaining only one atom thick.""We discovered... that coating the surface of the active material with a self-assembled monolayer... promoted efficient transportation within the electrodes, while also suppressing the side reactions occurring at the electrode and electrolyte interface,"" Zettsu said. ""This coating provided improvement in both the power density and the cyclability in high-voltage lithium-ion batteries.""The researchers saw that the direct contact between the cathode and the electrolyte entering the battery was minimized, and that the capacity of the battery did not degrade even after it was cycled one hundred times.""The deposited self-assembled monolayer coatings reduced the activation barrier for the lithium ion transfer and stabilized ions near the surface, which positively affected the electrochemical reactions occurring at the interface between the electrode and electrolyte,"" Zettsu said. ""The surface stabilization coatings represent a game-changing technology for the development of high-voltage cathode materials without the limitation of the electrochemical dilemma of efficiency versus stabilization.""However, Zettsu said, the full effects produced by the surface coating on the full battery system need to be examined in greater detail to better understand any potential negative side-effects.""Our results can provide new directions for designing lithium-ion batteries based on high-voltage systems with superior electrochemical performance,"" Zettsu said.Zettsu plans to introduce this surface treatment technology to the market by 2022 in cooperation with automobile and cell manufacturers, with the aim of creating high-energy batteries that are also environmentally friendly.""Due to the world's environmental regulations, the push towards electric and hybrid automobiles is proceeding at a steady momentum. The performance level required for lithium ion batteries is very high,"" Zettsu said. ""Currently, we are working on manufacturing real battery cells for plug-in hybrid vehicles and battery electric vehicles using the coating process and experiments in automatic driving modes.""This work was supported by Japan Science and Technology Agency and the Japan Society for the Promotion of Science.Other contributors include Dae-Wook Kim, Shuhei Uchida, Hiromasa Shiiba, and Katsuya Teshima, all of whom are affiliated with the Department of Materials Chemistry at Shinshu University. Teshima is the director of the Center for Energy & Environmental Science at Shinshu University.Story Source:Materials provided by Shinshu University. ",0.0705469,0.190696,0.101817,0.107981,0.112653,0.121094,0.120064,0,0
238,Light switch: Scientists develop method to control nanoscale manipulation in high-powered microscopes,"Researchers from Japan have taken a step toward faster and more advanced electronics by developing a way to better measure and manipulate conductive materials through scanning tunneling microscopy.The team published their results in July in Nano Letters, an American Chemical Society journal. Scientists from the University of Tokyo, Yokohama National University, and the Central Research Laboratory of Hamamatsu Photonics contributed to this paper.Scanning tunneling microscopy (STM) involves placing a conducting tip close to the surface of the conductive material to be imaged. A voltage is applied through the tip to the surface, creating a ""tunnel junction"" between the two through which electrons travel.The shape and position of the tip, the voltage strength, and the conductivity and density of the material's surface all come together to provide the scientist with a better understanding of the atomic structure of the material being imaged. With that information, the scientist should be able to change the variables to manipulate the material itself.Precise manipulation, however, has been a problem -- until now.The researchers designed a custom terahertz pulse cycle that quickly oscillates between near and far fields within the desired electrical current.""The characterization and active control of near fields in a tunnel junction are essential for advancing elaborate manipulation of light-field-driven processes at the nanoscale,"" said Jun Takeda, a professor in the department of physics in the Graduate School of Engineering at Yokohama National University. ""We demonstrated that desirable phase-controlled near fields can be produced in a tunnel junction via terahertz scanning tunneling microscopy with a phase shifter.""According to Takeda, previous studies in this area assumed that the near and far fields were the same -- spatially and temporally. His team examined the fields closely and not only identified that there was a difference between the two, but realized that the pulse of fast laser could prompt the needed phase shift of the terahertz pulse to switch the current to the near field.""Our work holds enormous promise for advancing strong-field physics in nano-scale solid state systems, such as the phase change materials used for optical storage media in DVDs and Blu-ray, as well as next-generation ultrafast electronics and microscopies,"" Takeda said.Story Source:Materials provided by Yokohama National University. ",0.0902236,0.136401,0.129319,0.151445,0.144073,0.134203,0.143264,0,0
239,Brilliant iron molecule could provide cheaper solar energy,"For the first time, researchers have succeeded in creating an iron molecule that can function both as a photocatalyst to produce fuel and in solar cells to produce electricity. The results indicate that the iron molecule could replace the more expensive and rarer metals used today.Some photocatalysts and solar cells are based on a technology that involves molecules containing metals, known as metal complexes. The task of the metal complexes in this context is to absorb solar rays and utilise their energy. The metals in these molecules pose a major problem, however, as they are rare and expensive metals, such as the noble metals ruthenium, osmium and iridium.""Our results now show that by using advanced molecule design, it is possible to replace the rare metals with iron, which is common in the Earth's crust and therefore cheap,"" says Chemistry Professor Kenneth Wernmark of Lund University in Sweden.Together with colleagues, Kenneth Wernmark has for a long time worked to find alternatives to the expensive metals. The researchers focused on iron which, with its six per cent prevalence in the Earth's crust, is significantly easier to source. The researchers have produced their own iron-based molecules whose potential for use in solar energy applications has been proven in previous studies.In this new study, the researchers have moved one step further and developed a new iron-based molecule with the ability to capture and utilise the energy of solar light for a sufficiently long time for it to react with another molecule. The new iron molecule also has the ability to glow long enough to enable researchers to see iron-based light with the naked eye at room temperature for the first time.""The good result depends on the fact that we have optimised the molecular structure around the iron atom,"" explains colleague Petter Persson of Lund University.The study is now published in the journal Science. According to the researchers, the iron molecule in question could be used in new types of photocatalysts for the production of solar fuel, either as hydrogen through water splitting or as methanol from carbon dioxide. Furthermore, the new findings open up other potential areas of application for iron molecules, e.g. as materials in light diodes (LEDs).What surprised the Lund researchers is that they arrived at good results so quickly. In just over five years, they succeeded in making iron interesting for photochemical applications, with properties largely as good as those of the best noble metals.""We believed it would take at least ten years,"" says Kenneth Wernmark.Besides the researchers from Lund University, colleagues from Uppsala University and the University of Copenhagen were also involved in the collaboration.Story Source:Materials provided by Lund University. ",0.0889577,0.219192,0.140486,0.172689,0.138429,0.147968,0.173266,0,0
240,Great strides for carbon capture using earth-abundant elements as photocatalytic system,"Researchers at Tokyo Institute of Technology have designed a CO2 reduction method based only on commonly occurring elements. Achieving a 57% overall quantum yield of CO2 reduction products, it is the highest performing system of its kind reported to date, raising prospects for cost-effective carbon capture solutions.As global warming presents one of the biggest challenges to humanity in the 21st century, the quest to curb mounting CO2 emissions is more pressing than ever.In a study published in the Journal of the American Chemical Society, Osamu Ishitani and colleagues at Tokyo Institute of Technology (Tokyo Tech) and Japan's National Institute of Advanced Industrial Science and Technology report a photocatalytic[1] system that brings scientists closer to achieving artificial photosynthesis -- the goal of creating a sustainable system similar to the way that plants convert CO2 to useful energy by using earth abundant metals.Although metal-complex photocatalytic systems have been reported for CO2 reduction, many of them used noble- and/or rare-metal complexes. Compared to these approaches that utilize rare metals (such as ruthenium and rhenium), the use of earth abundant metals is ""greener"" and inexpensive, and has thus attracted much interest.Their new process is made up of two components: 1) a copper complex (CuPS) that behaves as a redox photosensitizer[2] and 2) a manganese-based catalyst, Mn(4OMe).CuPS proved to be a stable and efficient redox photosensitizer, as decomposition was only 2% after 12 hours of irradiation. In addition, CuPS exhibited a much stronger reduction capability compared to other photosensitizers investigated to date.The team reported that the total quantum yield of CO2 reduction products was 57%, the turnover number based on the manganese catalyst was over 1300 and the selectivity of CO2 reduction was 95%.In particular, the figure of 57% is remarkable, as the researchers comment: ""To the best of our knowledge, this is the highest quantum yield for CO2 reduction using abundant elements and the yield would be comparable to that obtained with rare metals.""The study highlights the way that incremental advances in chemistry may have a large impact on the wider goal of working towards a fossil-fuel-free future.The research was supported by the Japan Science and Technology Agency's CREST program aimed at accelerating strategic innovation.Technical terms[1] Photocatalytic: Referring to a light-driven process that can accelerate a particular reaction of interest.[2] Redox photosensitizer: A component that initiates the photochemical one-electron transfer from a reductant to a catalyst.Story Source:Materials provided by Tokyo Institute of Technology. ",0.0758096,0.18643,0.140166,0.158565,0.113949,0.135451,0.156743,0,0
241,New catalyst produces cheap hydrogen fuel,"Professor Anthony O'Mullane said the potential for the chemical storage of renewable energy in the form of hydrogen was being investigated around the world.""The Australian Government is interested in developing a hydrogen export industry to export our abundant renewable energy,"" said Professor O'Mullane from QUT's Science and Engineering Faculty.""In principle, hydrogen offers a way to store clean energy at a scale that is required to make the rollout of large-scale solar and wind farms as well as the export of green energy viable.""However, current methods that use carbon sources to produce hydrogen emit carbon dioxide, a greenhouse gas that mitigates the benefits of using renewable energy from the sun and wind.""Electrochemical water splitting driven by electricity sourced from renewable energy technology has been identified as one of the most sustainable methods of producing high-purity hydrogen.""Professor O'Mullane said the new composite material he and PhD student Ummul Sultana had developed enabled electrochemical water splitting into hydrogen and oxygen using cheap and readily available elements as catalysts.""Traditionally, catalysts for splitting water involve expensive precious metals such as iridium oxide, ruthenium oxide and platinum,"" he said.""An additional problem has been stability, especially for the oxygen evolution part of the process.""What we have found is that we can use two earth-abundant cheaper alternatives -- cobalt and nickel oxide with only a fraction of gold nanoparticles -- to create a stable bi-functional catalyst to split water and produce hydrogen without emissions.""From an industry point of view, it makes a lot of sense to use one catalyst material instead of two different catalysts to produce hydrogen from water.""Professor O'Mullane said the stored hydrogen could then be used in fuel cells.""Fuel cells are a mature technology, already being rolled out in many makes of vehicle. They use hydrogen and oxygen as fuels to generate electricity -- essentially the opposite of water splitting.""With a lot of cheaply 'made' hydrogen we can feed fuel cell-generated electricity back into the grid when required during peak demand or power our transportation system and the only thing emitted is water.""Story Source:Materials provided by Queensland University of Technology. ",0.0970373,0.24094,0.157053,0.156195,0.136304,0.175725,0.180813,0,0
242,"Light-activated, single-ion catalyst breaks down carbon dioxide Designing better catalysts for converting pollutant gas into useful products","A team of scientists has discovered a single-site, visible-light-activated catalyst that converts carbon dioxide (CO2) into ""building block"" molecules that could be used for creating useful chemicals. The discovery opens the possibility of using sunlight to turn a greenhouse gas into hydrocarbon fuels.The scientists used the National Synchrotron Light Source II, a U.S. Department of Energy (DOE) Office of Science user facility at Brookhaven National Laboratory, to uncover details of the efficient reaction, which used a single ion of cobalt to help lower the energy barrier for breaking down CO2. The team describes this single-site catalyst in a paper just published in the Journal of the American Chemical Society.Converting CO2 into simpler parts -- carbon monoxide (CO) and oxygen -- has valuable real-world applications. ""By breaking CO2, we can kill two birds with one stone -- remove CO2 from the atmosphere and make building blocks for making fuel,"" said Anatoly Frenkel, a chemist with a joint appointment at Brookhaven Lab and Stony Brook University. Frenkel led the effort to understand the activity of the catalyst, which was made by Gonghu Li, a physical chemist at the University of New Hampshire.""We now have evidence that we have made a single-site catalyst. No previous work has reported solar CO2 reduction using a single ion,"" said Frenkel.Breaking the bonds that hold CO2 together takes a lot of energy and a long time. So, Li set out to develop a catalyst to lower the energy barrier and speed up the process.""The question is, between several possible catalysts, which are efficient and practical to implement in industry?"" said Frenkel.One key ingredient required to break the bonds of CO2 is a supply of electrons. These electrons can be generated when a material known as a semiconductor gets activated by energy in the form of light. The light ""kicks"" electrons out, so to speak, making them available to the catalyst for chemical reactions. Sunlight could be a natural source of such light. But many semiconductors can only be activated by ultraviolet light, which makes up less than a five percent of the solar spectrum.""The challenge is to find another semiconductor material where the energy of natural sunlight will make a perfect match to kick out the electrons,"" Frenkel said.The scientists also needed the semiconductor to be bound to a catalyst made from materials that could be found abundantly in nature, rather than rare, expensive metals such as platinum. And they wanted the catalyst to be selective enough to drive only the reaction that converts CO2 to CO.""We don't want the electrons to be used for reactions other than reducing CO2,"" Frenkel said.Cobalt ions bound to graphitic carbon nitride (C3N4), a semiconductor made of carbon, nitrogen, and hydrogen atoms, ticked all the boxes for these requirements.""There has been significant interest in using C3N4 as a metal-free semiconductor to harvest visible light and drive chemical reactions,"" said Li. ""Electrons generated by C3N4 under light irradiation have energy high enough to reduce CO2. Such electrons often don't have lifetimes long enough to allow them to travel to the semiconductor surface for use in chemical reactions. In our study, we adopted a common and effective strategy to build up enough energetic electrons for the catalyst by using a sacrificial electron donor. This strategy allowed us to focus on the catalysis for CO2 reduction. Ultimately, we want to use water molecules as the electron donor for our catalysis,"" he added.Peipei Huang, a postdoctoral researcher in Li's lab, made the catalyst by simply depositing cobalt ions on a C3N4 material made from commercially available urea. The team then extensively examined the synthesized catalyst using a variety of techniques in collaboration with Christine Caputo at the University of New Hampshire and Ronald Grimm at Worcester Polytechnic Institute.The catalyst worked in CO2 reduction under visible-light irradiation.""This catalyst did what it was supposed to do -- break down CO2 and make CO with very good selectivity in visible light,"" Frenkel said. ""But the next goal was to see why it worked. If you can understand why it works you can make new and better materials based on those principles.""So Frenkel and Li brainstormed experiments that would show the structure of the catalyst with precision. Structural studies would give the scientists information about the number of cobalt atoms, their location relative to the carbon and nitrogen atoms, and other characteristics the scientists could potentially adjust to try to improve the catalyst further.They turned to the Quick x-ray Absorption and Scattering (QAS) beamline at NSLS-II to use x-ray absorption spectroscopy. With help from lead beamline scientist Steven Ehrlich, Frenkel's student Jiahao Huang took the data and analyzed the spectra.In this technique, the x-rays from NSLS-II get absorbed by atoms in the sample, which then eject waves of electrons. The spectra show how these electron waves interact with surrounding atoms, similar to the way ripples on the surface of a lake get disrupted when they encounter rocks.""To be able to do x-ray absorption spectroscopy (XAS), we need to tune and scan the energy of the x-ray beam hitting the sample,"" said Ehrlich. ""Each element can absorb x-rays at distinct energies, called absorption edges. At the new QAS beamline we can scan the energy of the x-rays across the absorption edge energy of different elements, such as cobalt in this case. We then measure the number of photons absorbed by the sample for each value of the x-ray energy.""In addition, Frenkel explained, ""each type of atom produces a different kind of electronic ripple, when excited by x-rays, or when hit by other ripples, so the x-ray absorption spectrum tells you what the surrounding atoms are as well as how far apart and how many there are.""The analysis showed that the catalyst breaking down CO2 was made of single ions of cobalt surrounded on all sides by nitrogen atoms.""There were no cobalt-cobalt pairs. So, this was evidence that they were in fact single atoms of cobalt dispersed on the surface,"" Frenkel said.""This data also narrows down the possible structural arrangements, which provides information for theorists to fully evaluate and understand the reactions,"" Frenkel added.Though the science outlined in the paper is not yet in practical use, there are abundant possibilities for applications, Frenkel said. In the future, such single-site catalysts could be used in large-scale areas with abundant sunlight to break down excess CO2 in the atmosphere, similar to the way plants break down CO2 and reuse its building blocks to build sugars in the process of photosynthesis. But instead of making sugars, scientists might use the CO building blocks to generate synthetic fuels or other useful chemicals.This research was supported by the DOE Office of Science and partly by the National Science Foundation.Story Source:Materials provided by DOE/Brookhaven National Laboratory. ",0.0505842,0.179021,0.103183,0.116747,0.10204,0.0876814,0.125823,0,0
243,A life cycle solution to fossil fuel impacts,"Pennsylvania's energy history is rich with the quantities of fossil fuels that it has produced, but is also rife with the environmental legacies of coal mining and, more recently, hydrofracturing. Water that finds its way into abandoned coal mines dotted throughout the Commonwealth resurfaces as acid mine drainage (AMD), while freshwater used to fracture or ""frack"" oil and natural gas deposits reemerges as ""produced"" water contaminated with salts, metals, and radioactive material.Remediating both AMD and produced water is an expensive process and federal law prohibits produced water disposal at municipal water treatment plants. However, research from the University of Pittsburgh Swanson School of Engineering, published recently in Environmental Science & Technology, found that co-treatment of the two fluids may not only solve two environmental issues at once, but also reduce the environmental impact of both legacy wastes.Leanne Gilbertson, assistant professor of civil and environmental engineering, is principal investigator of the research, ""Life Cycle Impact and Benefit Tradeoffs of a Produced Water and Abandoned Mine Drainage Co-Treatment Process."" The article, authored by graduate student Yan Wang, incorporates related research by her Swanson School colleagues, Radisav Vidic, the William Kepler Whiteford Professor and Department Chair of Civil and Environmental Engineering, and Associate Professor Vikas Khanna.""This study is the serendipitous result of three different researchers finding a common theme to unite the collaboration. Radisav's group developed the method for co-treating AMD and produced water and he is a leading researcher in the field of produced water treatment via membrane distillation, while Vikas's group focuses on complex systems analysis,"" Dr. Gilbertson explained. ""My expertise in life cycle assessment brings a new perspective to these industries and a way to quantify the environmental and human health impact tradeoffs of alternative approaches to utilizing these two wastewaters.""Dr. Gilbertson and her group focused on a five county region of southwestern Pennsylvania impacted by both AMD and hydrofracturing - Allegheny, Fayette, Greene, Washington, and Westmoreland counties. The research targeted three critical, mutual aspects of remediation - co-treatment of produced water and AMD, transportation of water to and from mine and drill sites, and avoiding AMD discharge to the environment. Dr. Gilbertson's LCA found that co-treating AMD and produced water is beneficial because, while the chemical composition of each fluid varies from site to site, the two byproducts share opposite amounts of barium and sulfates which, when combined, can be removed via precipitation. The resulting fluid can then be used to replace freshwater in future fracking operations while the barite produced by this process can be used in drilling operations.Dr. Gilbertson noted that this result is important because it creates value of out two significant waste products and precludes environmental impacts of AMD. ""While the combined produced water volume from fracking is 4,450 cubic meters per day, there is a staggering 281,000 cubic meters of orphaned AMD produced daily in the region. Mitigating the two via co-treatment would result in reduced freshwater use and become a net environmental benefit.""But even with the potential positive impact of co-treatment, transporting the fluids between mining and drilling sites could create a significant, negative tradeoff. It will be a balance between the proposed and current handling of produced water, which his often transported significant distances for treatment, or out of state for disposal via large trucks logging several hundred thousand miles per year.To minimize these significant impacts, which include not only fuel use but also road wear and truck exhaust, Dr. Khanna and his PhD student, Sakineh Tavakoli, developed a model to identify the optimal locations for co-treatment sites between AMD and gas wells in the five-county region. Although costs associated with optimized co-treatment may be higher than using freshwater, the environmental benefits could be significant. Another potential option currently being piloted by Drs. Vidic and Khanna is a mobile membrane distillation system that would be powered by waste heat generated during drilling to treat produced water on site.And although the optimization model was developed using mining and gas well sites in the five-county region, the researchers note that this approach can be applied to other areas in Pennsylvania, and throughout the U.S. using similar data.  Ms. Wang added that what is novel about this research is that the group attempted to quantify the benefits of not releasing AMD into ecosystems and the environment.""These are ""credits"" to the system that you wouldn't necessarily think about. For example, by utilizing AMD as a fracking fluid, we're greatly reducing the amount of freshwater that would be wasted. Similarly, by optimizing transportation routes and developing mobile treatment sites, we are significantly reducing the environmental impact of long-haul trucking,"" Ms. Wang said. ""Most importantly, by using AMD as a resource, we are helping to mitigate a legacy waste from the environment that then improves remediation efforts. In short, the cascade effect of co-treating these two waste products can be a net benefit for Pennsylvania.""eStory Source:Materials provided by University of Pittsburgh. ",0.0697374,0.128009,0.196044,0.169952,0.10963,0.132616,0.154737,0,0
244,Living electrodes with bacteria and organic electronics,"Adding bacteria to electrochemical systems is often an environmentally sensitive means to convert chemical energy to electricity. Applications include water purification, bioelectronics, biosensors, and for the harvesting and storage of energy in fuel cells. One problem that miniaturisation of the processes has encountered is that a high signal strength requires large electrodes and a large volume of liquid.Researchers at Linkeping University, together with colleagues at the Lawrence Berkeley National Laboratory in Berkeley, California, USA, have now developed a method in which they embed the electroactive bacterium Shewanella oneidensis into PEDOT:PSS, an electrically conducting polymer, on a substrate of carbon felt.The researchers call the result a ""multilayer conductive bacterial-composite film,"" abbreviated as MCBF. Microscopic analysis of the film shows an interleaved structure of bacteria and conducting polymers that can be up to 80 ?m thick, much thicker than it can be without this specific technique.""Our experiments show that more than 90% of the bacteria are viable, and that the MCBF increases the flow of electrons in the external circuit. When our film is used as anode in microbial electrochemical cells, the current is 20 times higher than it is when using unmodified anodes, and remains so for at least several days,"" says Gebor Mehes, researcher at Linkeping University and one of the lead authors of the scientific article recently published in Scientific Reports.Previous work has tested, among other things, carbon nanotubes to increase the surface area at the anode, but the results were poor.The possibility to couple biological processes with readable electrical signals is also valuable, for example for environmental sensors which require rapid response times, low energy consumption, and the ability to use many different receptors. Researchers have recently demonstrated how to use Shewanella oneidensis to produce electrical currents in response to arsenic, arabinose (a type of sugar) and organic acids, among others.""This technology represents a type of ""living electrode"" where the electrode material and the bacteria are amalgamated into a single electronic biofilm. As we discover more about the essential role that bacteria play in our own health and wellness, such living electrodes will likely become versatile and adaptable tools for developing new forms of bioelectronic technologies and therapies,"" says Daniel Simon, principal investigator in Organic Bioelectronics at the Laboratory of Organic Electronics.Story Source:Materials provided by Linkeping University. Original written by Monica Westman Svenselius; translated by George Farrants. ",0.0575915,0.142608,0.136152,0.131329,0.11025,0.110091,0.124447,0,0
245,"The shape of things to come: Flexible, foldable supercapacitors for energy storage Potential uses of new technology range from medical devices and wearables to smart packaging","A team of researchers from the Plasma Physics Research Centre, Science and Research Branch of Islamic Azad University in Tehran, Iran, have discovered a way of making paper supercapacitors for electricity storage, according to a new study published in the journal Heliyon . At one sheet thick, these new supercapacitors can bend, fold, flex, and still hold electricity.The term ""supercapacitors"" is reserved for devices that hold over 10 times as much energy per unit volume as a traditional capacitor, and that can charge and discharge quickly. Paper supercapacitors are lighter and cheaper than other types and those developed by lead author, Dr. Leila Fekri Aval's group are more flexible than earlier paper supercapacitors, giving them a whole new range of potential uses. ""In the near future, the industrial and homemade applications for these types of supercapacitors will increase and the cost reduce, making them available to the public,"" explains Dr. Aval.Today, if you need to store a large amount of energy, you will typically need to use large, heavy rechargeable batteries. Supercapacitors can do this too, but at a step up: They charge and discharge more quickly than conventional batteries -- in minutes rather than hours -- and they can be charged and discharged more times over their lifespan.Carbon, taking the form of carbon nanotubes in today's capacitors and supercapacitors, contains the ideal properties for storing energy efficiently. Since the 1950s, researchers have exploited its strength and excellent thermal and electrical conductivity; carbon is also strong, elastic and flexible so that it can bend and stretch easily.The team of researchers investigated the structure of commercial supercapacitors and produced one that uses one sheet of carbon nanotube paper with different layers. They used barium titanate to separate the layers, which is more economical than any alternative compounds. The new paper superconductors can store energy efficiently even if they are rolled or folded.The potential applications of these new devices are vast: Medical implants, skin patches, wearable tech, and novel large-scale energy storage for domestic and commercial transport and smart packaging. Imagine, for example, using a computer tablet that can roll up and fit in your pocket or a phone that is part of your coat, or charging your phone with a battery that is part of your clothing.Dr. Aval anticipates that the commercial and domestic applications of these supercapacitors will soon increase and the cost decrease, so the technology will become available to the mass market. ""Energy is our most important challenge in the future,"" said Dr. Aval. ""It is important to build a device that stores energy, has high power and energy density, but at a low cost. This is what inspired our research into paper supercapacitors.""Story Source:Materials provided by Elsevier. ",0.0686414,0.111753,0.106066,0.142569,0.112569,0.121058,0.122962,0,0
246,Next-gen batteries possible with new engineering approach,"Dramatically longer-lasting, faster-charging and safer lithium metal batteries may be possible, according to Penn State research, recently published in Nature Energy.The researchers developed a three-dimensional, cross-linked polymer sponge that attaches to the metal plating of a battery anode.""This project aims to develop the next generation of metal batteries,"" said Donghai Wang, professor of mechanical engineering and the principal investigator of the project. ""Lithium metal has been tried in batteries for decades, but there are some fundamental issues that inhibit their advancement.""Under additional strain, like in the fast-charging methods desired in electrical vehicles, lithium ion (Li) batteries are vulnerable to dendritic growth -- needle-like formations that can reduce cycle life and potentially cause safety issues -- including fires or explosions.""Our approach was to use a polymer on the interface of Li metal,"" Wang explained. The material acts as a porous sponge that not only promotes ion transfer, but also inhibits deterioration.""This allowed the metal plating to be free of dendrites, even at low temperatures and fast charge conditions,"" he said.Wang, who is an affiliated faculty member at the Penn State Institutes of Energy and the Environment, also belongs to the Battery Energy and Storage Technology Center, a leading research institute in energy storage.A critical component of both IEE and the BEST Center's mission, this project brought together researchers from different disciplines within the University.""The collaboration in this cohort really helped drive this paper forward,"" Wang explained. ""It allowed us to examine the different aspects of this problem, from materials science, chemical engineering, chemistry, and mechanical engineering perspectives.""In this collaborative work, Long-Qing Chen's group in the Department of Materials Science and Engineering conducted modeling work to understand the improvement of Li metal anodes.The practical applications of this work could enable more powerful and stable metal battery technologies integral to everyday life, according to the researchers.""In an electric vehicle, it could increase the range of a drive before needing a charge by hundreds of miles,"" said Wang. ""It could also give smartphones a longer battery life.""Looking to the future, the team will explore the practical applications in a large-format battery cell to demonstrate its advantages and feasibility.Wang said, ""We want to push these technologies forward. With this work, I'm positive we can double the life cycle of these Li metal batteries.""Story Source:Materials provided by Penn State. ",0.0795955,0.149848,0.115163,0.161105,0.112891,0.142101,0.154897,0,0
247,"Structure of fossil-fuel source rocks is finally decoded Detailed 3D imaging of kerogen, a source of petroleum and natural gas, could improve estimates of recoverable amounts","The fossil fuels that provide much of the world's energy orginate in a type of rock known as kerogen, and the potential for recovering these fuels depends crucially on the size and connectedness of the rocks' internal pore spaces.Now, for the first time, a team of researchers at MIT and elsewhere has captured three-dimensional images of kerogen's internal structure, with a level of detail more than 50 times greater than has been previously achieved. These images should allow more accurate predictions of how much oil or gas can be recovered from any given formation. This wouldn't change the capability for recovering these fuels, but it could, for example, lead to better estimates of the recoverable reserves of natural gas, which is seen as an important transition fuel as the world tries to curb the use of coal and oil.The findings are reported this week in the Proceedings of the National Academy of Science, in a paper by MIT Senior Research Scientist Roland Pellenq, MIT Professor Franz-Josef Ulm, and others at MIT, CNRS and Aix-Marseille Universite (AMU) in France, and Shell Technology Center in Houston.The team, which published results two years ago on an investigation of kerogen pore structure based on computer simulations, used a relatively new method called electron tomography to produce the new 3-D images, which have a resolution of less than 1 nanometer, or billionth of a meter. Previous attempts to study kerogen structure had never imaged the material below 50 nanometers resolution, Pellenq says.Fossil fuels, as their name suggests, form when organic matter such as dead plants gets buried and mixed with fine-grained silt. As these materials get buried deeper, over millions of years the mix gets cooked into a mineral matrix interspersed with a mix of carbon-based molecules. Over time, with more heat and pressure, the nature of that complex structure changes.The process, a slow pyrolysis, involves ""cooking oxygen and hydrogen, and at the end, you get a piece of charcoal,"" Pellenq explains. ""But in between, you get this whole gradation of molecules,"" many of them useful fuels, lubricants, and chemical feedstocks.The new results show for the first time a dramatic difference in the nanostructure of kerogen depending on its age. Relatively immature kerogen (whose actual age depends of the combination of temperatures and pressures it has been subjected to) tends to have much larger pores but almost no connections among those pores, making it much harder to extract the fuel. Mature kerogen, by contrast, tends to have much tinier pores, but these are well-connected in a network that allow the gas or oil to flow easily, making much more of it recoverable, Pellenq explains.The study also reveals that the typical pore sizes in these formations are so small that normal hydrodynamic equations used to calculate the way fluids move through porous materials won't work. At this scale the material is in such close contact with the pore walls that interactions with the wall dominate its behavior. The research team thus had to develop new ways of calculating the flow behavior.""There's no fluid dynamics equation that works in these subnanoscale pores,"" he says. ""No continuum physics works at that scale.""To get these detailed images of the structure, the team used electron tomography, in which a small sample of the material is rotated within the microscope as a beam of elecrons probes the structure to provide cross-sections at one angle after another. These are then combined to produce a full 3-D reconstruction of the pore structure. While scientists had been using the technique for a few years, they hadn't applied it to kerogen structures until now. The imaging was carried out at the CINaM lab of CNRS and AMU, in France (in the group of Daniel Ferry), as part of a long-term collaboration with MultiScale Materials Science for Energy and Environment, the MIT/CNRS/AMU joint lab located at MIT.""With this new nanoscale tomography, we can see where the hydrocarbon molecules are actually sitting inside the rock,"" Pellenq says. Once they obtained the images, the researchers were able to use them together with with molecular models of the structure, to improve the fidelity of their simulations and calculations of flow rates and mechanical properties. This could shed light on how production rates decline in oil and gas wells, and perhaps on how to slow that decline.So far, the team has studied samples from three different kerogen locations and found a strong correlation between the maturity of the formation and its pore size distribution and pore void connectivity. The researchers now hope to expand the study to many more sites and to derive a robust formula for predicting pore structure based on a given site's maturity.Story Source:Materials provided by Massachusetts Institute of Technology. ",0.0422474,0.0879694,0.119495,0.123029,0.0674359,0.0852493,0.113477,0,0
248,Affordable catalyst for CO2 recycling,"A catalyst for carbon dioxide recycling, Mineral pentlandite may also be a conceivable alternative to expensive precious metal catalysts. This is the result of a study conducted by researchers from Ruhr-Universitet Bochum (RUB), Fritz-Haber Institute Berlin and Fraunhofer Umsicht in Oberhausen. Pentlandite had previously been known as a catalyst for hydrogen production. By adding a suitable solvent, the researchers successfully utilised it to convert carbon dioxide into carbon monoxide. The latter is a common source material in the chemical industry.The research team headed by Dr. Ulf-Peter Apfel, Chair of Inorganic Chemistry I in Bochum, describes the findings together with their colleagues in the journal Chemical Science from 5 November 2018.CO2 conversion replaces hydrogen production""The conversion of CO2 into valuable source materials for the chemical industry is a promising approach to combatting climate change,"" says Ulf-Peter Apfel. ""However, we currently don't know many cheap and readily available catalysts for CO2 reduction."" Moreover, potentially suitable catalysts primarily facilitate another chemical reaction, i.e. the synthesis of hydrogen -- these including pentlandite. Nevertheless, the researchers have successfully converted the mineral to be a CO2 catalyst.They generated electrodes from pentlandite and analysed under which conditions production of hydrogen or carbon monoxide took place at their surface. ""The decisive factor was water being present at the electrode surface,"" summarises Ulf-Peter Apfel. A lot of water shifted the reaction towards hydrogen production, a little water towards carbon monoxide production. By adjusting the water content, the researchers were thus able to generate carbon monoxide and hydrogen mixtures. ""Synthetic gas mixtures like this one play a crucial role in the chemical industry,"" points out Apfel.Stabile catalystPentlandite consists of iron, nickel and sulphur and resembles catalytically active enzyme centres that occur in nature, such as hydrogen-producing hydrogenases. ""A huge advantage of this mineral is the fact that it remains stable when confronted with other chemical compounds that occur in industrial emissions and are poison to many catalysts,"" explains Apfel.Story Source:Materials provided by Ruhr-University Bochum. ",0.0899123,0.28869,0.151495,0.152764,0.146491,0.153228,0.175815,0,0
249,Coordinated development could help wind farms be better neighbors,"As onshore and offshore wind energy farms have proliferated globally in recent years, new research led by the University of Colorado Boulder highlights a previously underexplored consequence: a wake effect from upwind wind farms that can reduce the energy production of their downwind neighbors.The study, undertaken in collaboration with the University of Denver (DU) and the National Renewable Energy Laboratory (NREL), combines legal and economic analysis with atmospheric modeling to demonstrate that wake effects -- which occur when groups of turbines reduce wind speed for up to several miles behind them -- are measurable and predictable, yet remain largely unaccounted for in current U.S. property law.The research appears today in the journal Nature Energy.""The findings highlight the need for coordinated development and awareness of the big picture in order to maximize wind energy generation nationwide,"" said Julie Lundquist, lead author of the study and an associate professor in CU Boulder's Department of Atmospheric and Oceanic Sciences (ATOC).Wind energy developers seek out sites with reliable wind resources as well as convenient access to electrical transmission lines, leading to dense turbine deployment in choice areas with little coordination between competitors. Nearly 90 percent of U.S. wind farms are located within 25 miles of another wind farm -- and often much closer.Yet much like a homeowner whose once-unimpeded views are blocked by new construction, the study shows that existing wind farms stand to lose valuable energy production if they suddenly find themselves downwind of a new neighbor and its wind- and energy-reducing wake.Using publicly-available data on monthly energy generation and dominant wind direction, the researchers modeled the wake effect on a pair of West Texas wind farms, using a third as a control. They found that wind speed reductions due to the establishment of the upwind farm reduced generation at the now-downwind farm by 5 percent from 2011-2015, an estimated revenue loss of around $3.7 million.""Just as upstream water users can knowingly or unknowingly impose additional costs downstream, the same effect is in play here,"" said Daniel Kaffine, a co-author of the study and a professor in CU Boulder's Department of Economics. ""We had a general sense of this interaction before and wanted to explore it further using a mix of social science and atmospheric data.""Kaffine and Lundquist, both fellows in the Renewable and Sustainable Energy Institute (RASEI), noted that Texas made for a good case study because of its current and future stake in wind energy development, as well as historical parallels concerning usage rights for the state's oil and gas resources.Over a century ago, oil and gas law recognized the ""rule of capture,"" which allows a landowner to capture a neighbor's oil resources by sucking them from an adjacent well. This resulted in poor well-field recoveries and waste, so states intervened to regulate production through well-spacing, pooling and other coordination measures. Similarly, states have developed water law regimes that protect rights for prior users. But current U.S. property law contains no such provisions for wind energy.""It took oil and gas and water law decades to create these legal regimes,"" said K.K. DuVivier, a co-author of the study and a professor at DU's Sturm College of Law. ""It appears that wind energy has not yet reached a similar stage of maturity. I hope this research will help raise awareness about the extent of the waking problem and educate those seeking a legal remedy.""Turbine wakes have been observed to extend up to 25 miles, potentially spanning multiple state and county jurisdictions and complicating matters even further.""In the immediate future, some business incentives might change,"" Kaffine said. ""Energy firms might put more emphasis on securing leases upwind to ensure they have enough of a buffer.""But while legal guidance may lag behind, Lundquist emphasized that the team's sophisticated atmospheric science simulations show the wake effect is a highly predictable atmospheric phenomenon that can be modeled and planned for in light of the new data.""It's encouraging that the wake effects seen in the economic analysis are captured by the atmospheric simulations,"" said Jessica Tomaszewski, a graduate student in ATOC who co-authored the new study.""The strongest wakes occur at night, when the atmosphere is stable and wind speeds and directions meet specific criteria,"" said Lundquist. ""Out of the month of wakes simulated, only 28 hours, or less than 4 percent of the time, had wakes in excess of 20 percent of the downwind farm's capacity, suggesting this episodic issue can be predicted and managed.""""Owners and grid managers can use this information to get ahead of the issue. New research at NREL in manipulating wakes might be helpful,"" said Tomaszewski. ""Ultimately, however, better coordination will be necessary to maximize the public benefits of wind energy.""Taking the wake effect into account will be especially crucial for the large number of offshore wind farms currently in development off the U.S. east coast, Lundquist added.""Offshore wind farms are important, but they are expensive to build and mistakes will be equally expensive,"" she said.""This project explores the links among economic, legal, and social issues and the geophysical processes in wind energy production,"" said Tom Baerwald, program director for the National Science Foundation's Dynamics of Coupled Natural and Human Systems program, which funded the research. ""These findings advance our understanding of these interactions, and provide guidance for successful development of renewal energy for the future.""The researchers hope to expand their efforts to measure wind farm wake effects by incorporating aircraft measurements as well as capturing data on a more frequent timescale from other sites around the country.The National Science Foundation (NSF) provided additional funding via a Graduate Research Fellowship. NREL and the U.S. Department of Energy's Wind Energy Technologies Office provided additional funding.Story Source:Materials provided by University of Colorado at Boulder. Original written by Trent Knoss. ",0.0816116,0.0918431,0.131456,0.149171,0.0880456,0.133118,0.119842,0,0
250,Solar panels for yeast cell biofactories,"Genetically engineered microbes such as bacteria and yeasts have long been used as living factories to produce drugs and fine chemicals. More recently, researchers have started to combine bacteria with semiconductor technology that, similar to solar panels on the roof of a house, harvests energy from light and, when coupled to the microbes' surface, can boost their biosynthetic potential.The first ""biological-inorganic hybrid systems"" (biohybrids) mostly focused on the fixation of atmospheric carbon dioxide and the production of alternative energies, and although promising, they also revealed key challenges. For example, semiconductors, which are made from toxic metals, thus far are assembled directly on bacterial cells and often harm them in the process. In addition, the initial focus on carbon-fixing microbes has limited the range of products to relatively simple molecules; if biohybrids could be created based on microorganisms equipped with more complex metabolisms, it would open new paths for the production of a much larger range of chemicals useful for many applications.Now, in a study in Science, a multidisciplinary team led by Core Faculty member Neel Joshi and Postdoctoral Fellows Junling Guo and Miguel Suestegui at Harvard's Wyss Institute for Biologically Inspired Engineering and John A. Paulson School of Engineering and Applied Sciences (SEAS) presents a highly adaptable solution to these challenges.""While our strategy conceptually builds on earlier bacterial biohybrid systems that were engineered by our collaborator Daniel Nocera and others, we expanded the concept to yeast -- an organism that is already an industrial workhorse and is genetically easy to manipulate -- with a modular semiconductor component that provides biochemical energy to yeast's metabolic machinery without being toxic,"" said Joshi, Ph.D., who is a Core Faculty member at the Wyss Institute and Associate Professor at SEAS. Co-author Nocera is the Patterson Rockwood Professor of Energy at Harvard University. As a result of the combined manipulations, yeasts' ability to produce shikimic acid, an important precursor of the anti-viral drug Tamiflu, several other medicines, nutraceuticals, and fine chemicals, was significantly enhanced.The baker's yeast Saccharomyces cerevisiae naturally produces shikimic acid to generate some of its building blocks for the synthesis of proteins and other biomolecules. However, by genetically modifying the yeast's central metabolism, the researchers enabled the cells to funnel more of the carbon atoms that their main nutrient source, the sugar glucose, contains into the pathway that produces shikimic acid and prevent the loss of carbon to alternative pathways by disrupting one of them.""In principle, the increased 'carbon flux' towards shikimic acid should lead to higher product levels, but in normal yeast cells, the alternative pathway that we disrupted to increase yields, importantly, also provides the energy needed to fuel the final step of shikimic acid production,"" said co-first author Miguel Suestegui, Ph.D., a chemical engineer and former Postdoctoral Fellow in Joshi's team and now Scientist at Joyn Bio LLC. To boost the more carbon-effective but energy-depleted engineered shikimic acid pathway, ""we hypothesized that we could generate the relevant energy-carrying molecule NADPH instead in a biohybrid approach with light-harvesting semiconductors.""Toward this goal, Suestegui collaborated with Junling Guo, Ph.D., the study's other co-corresponding and co-first author and presently a Postdoctoral Fellow with experience in chemistry and materials science in Joshi's lab. They designed a strategy that uses indium phosphide as a semiconductor material. ""To make the semiconductor component truly modular and non-toxic, we coated indium phosphide nanoparticles with a natural polyphenol-based ""glue,"" which allowed us to attach them to the surface of yeast cells while at the same time insulating the cells from the metal's toxicity,"" said Guo.When tethered to the cell surface and illuminated, the semiconductor nanoparticles harvest electrons (energy) from light and hand them over to the yeast cells, which shuttle them across their cell walls into their cytoplasm. There the electrons elevate the levels of NADPH molecules, which now can fuel shikimic acid biosynthesis. ""The yeast biohybrid cells, when kept in the dark, mostly produced simpler organic molecules such as glycerol and ethanol; but when exposed to light, they readily switched into shikimic acid production mode with an 11-fold increase in product levels, showing us that the energy transfer from light into the cell works very efficiently,"" said Joshi.""This scalable approach creates an entirely new design space for future biohybrid technologies. In future efforts, the nature of semiconductors and the type of genetically engineered yeast cells can be varied in a plug-and-play fashion to expand the type of manufacturing processes and range of bioproducts,"" said Guo.""The creation of light-harvesting, living cellular devices could fundamentally change the way we interact with our natural environment and allow us to be more creative and effective in the design and production of energy, medicines and chemical commodities,"" said Wyss Institute Founding Director Donald Ingber, M.D., Ph.D., who is also the Judah Folkman Professor of Vascular Biology at HMS and the Vascular Biology Program at Boston Children's Hospital, as well as Professor of Bioengineering at SEAS.Story Source:Materials provided by Wyss Institute for Biologically Inspired Engineering at Harvard. Original written by Benjamin Boettner. ",0.0409238,0.136239,0.102431,0.118555,0.0834784,0.0890268,0.12407,0,0
251,Next step on the path towards an efficient biofuel cell Researchers have combined two concepts that make the system as efficient as precious metal catalysts,"Fuel cells that work with the enzyme hydrogenase are, in principle, just as efficient as those that contain the expensive precious metal platinum as a catalyst. However, the enzymes need an aqueous environment, which makes it difficult for the starting material for the reaction -- hydrogen -- to reach the enzyme-loaded electrode. Researchers solved this problem by combining previously developed concepts for packaging the enzymes with gas diffusion electrode technology. The system developed in this way achieved significantly higher current densities than previously achieved with hydrogenase fuel cells.In the journal Nature Communications, a team from the Center for Electrochemical Sciences at Ruhr-Universitet Bochum, together with colleagues from the Max Planck Institute for Chemical Energy Conversion in Melheim an der Ruhr and the University of Lisbon, describes how they developed and tested the electrodes. The article was published on 9 November 2018.Advantages and disadvantages of gas diffusion electrodesGas diffusion electrodes can efficiently transport gaseous raw materials for a chemical reaction to the electrode surface with the catalyst. They have already been tested in various systems, but the catalyst was electrically wired directly to the electrode surface. ""In this type of system, only a single layer of enzyme can be applied to the electrode, which limits the flow of current,"" says Bochum chemist Dr. Adrian Ruff, describing a disadvantage. In addition, the enzymes were not protected from harmful environmental influences. In the case of hydrogenase, however, this is necessary because it is unstable in the presence of oxygen.Redox polymer as an oxygen protection shieldIn recent years, the chemists from the Center for Electrochemical Sciences in Bochum have developed a redox polymer in which they can embed hydrogenases and protect them from oxygen. Previously, however, they had only tested this polymer matrix on flat electrodes, not on porous three-dimensional structures such as those employed in gas diffusion electrodes.""The porous structures offer a large surface area and thus enable a high enzyme load,"" says Professor Wolfgang Schuhmann, Head of the Center for Electrochemical Sciences. ""But it was not clear whether the oxygen protection shield on these structures would work and whether the system would then still be gas-permeable.""Applying enzymes to electrodesOne of the problems with the manufacturing process is that the electrodes are hydrophobic, i.e. water-repellent, while the enzymes are hydrophilic, i.e. water-friendly. The two surfaces therefore tend to repel each other. For this reason, the researchers first applied an adhesive yet electron transferring layer to the electrode surface, onto which they then applied the polymer matrix with the enzyme in a second step. ""We specifically synthesised a polymer matrix with an optimal balance of hydrophilic and hydrophobic properties,"" explains Adrian Ruff. ""This was the only way to achieve stable films with good catalyst loading.""The electrodes constructed in this way were still permeable to gas. The tests also showed that the polymer matrix also functions as an oxygen shield for porous three-dimensional electrodes. The scientists used the system to achieve a current density of eight milliamperes per square centimetre. Earlier bioanodes with polymer and hydrogenase only reached one milliampere per square centimetre.Functional biofuel cellThe team combined the bioanode described above with a biocathode and showed that a functional fuel cell can be produced in this way. It achieved a power density of up to 3.6 milliwatts per square centimetre and an open circuit voltage of 1.13 volts, which is just below the theoretical maximum of 1.23 volts.Story Source:Materials provided by Ruhr-University Bochum. ",0.072584,0.228127,0.121191,0.1364,0.123402,0.119689,0.149307,0,0
252,Doubly-excited electrons reach new energy states,"Positrons are short-lived subatomic particle with the same mass as electrons and a positive charge. They are used in medicine, e.g. in positron emission tomography (PET), a diagnostic imaging method for metabolic disorders. Positrons also exist as negatively charged ions, called positronium ions (Ps-), which are essentially a three-particle system consisting of two electrons bound to a positron.Now, commercially available lasers are capable of producing photons that carry enough energy to bring the electrons of negatively charge ions, like Ps?, to doubly-excited states, referred to as D-wave resonance. Positronium ions are, however, very difficult to observe because they are unstable and often disappear before physicists get a chance to analyse them.Sabyasachi Kar from the Harbin Institute of Technology, China, and Yew Kam Ho from the Academia Sinica, Taipei, Taiwan, have now characterised these higher energy levels reached by electrons in resonance in these three-particle systems, which are too complex to be described using simple equations. This theoretical model, recently published in EPJ D, is intended to offer guidance for experimentalists interested in observing these resonant structures. This model of a three-particle system can be adapted to problems in atomic physics, nuclear physics, and semiconductor quantum dots, as well as antimatter physics and cosmology.In this study, the authors first test the validity of their theoretical approach by showing that the resonance parameters for negatively charged hydrogen ions (H-) -- modelled as a three-particle system made up of two electrons and one proton -- are in agreement with previous studies. The authors then calculate, for the first time, new resonance states associated with the positronium ion (Ps-) in higher energy regions by modelling it as a three-particle system. In turn, they elaborate on seven modes of resonance for the electrons that have never before been reported.Story Source:Materials provided by Springer. ",0.126495,0.173253,0.153548,0.178566,0.167282,0.197568,0.171251,0,0
253,Microorganisms help production,"Oil is still the most economically attractive resource for fuels and basic chemicals that can be used to manufacture everyday products such as plastic bottles and detergent. New biotechnological processes aim to simplify the use of renewable biomass as an alternative to the fossil raw material and make it more cost-effective. Researchers at KIT are focusing on plant biomass such as wood and straw which is not used as food or feed.e Oil is profitable but its use is detrimental to the climate and environment. Besides, supplies of the fossil raw material are dwindling. Processes applied so far to win basic chemicals such as ethanol from renewable materials are expensive. What is more, they use plants such as maize, sugar beet and rape which also serve as food for humans and animals. ''To achieve sustainable and environmentally friendly energy and raw material supply, we need to develop innovative technologies which make the use of renewable biomass also attractive from an economic point of view,'' says Professor Christoph Syldatk, Head of the Institute of Process Engineering in Life Sciences II / Technical Biology at KIT. His research group is examining how raw materials that do not compete with food or feed can be processed biotechnologically -- for example straw, green waste and sawdust. These second-generation, non-edible, bio-based raw materials consist to a large extent of lignocellulose which forms the cell walls of woody plants. To be able to use lignocellulose, however, it first needs to be broken down into its components (fractions). This process has so far been time-consuming and expensive. To reduce production costs and establish lignocellulose as a raw material, researchers at KIT are examining, among other things, how -- on the basis of lignocellulose fractions -- new types of biosurfactants can be produced using microbial or enzymatic synthesis.The aim is to convert the woody biomass into basic components for the production of chemicals and materials such as bioplastics. Bacteria, yeasts and molds are among the microorganisms, the metabolism of which is used by researchers in the lab for such innovative product syntheses and chemical changes. Some industry partners are already implementing KIT's application-oriented research on a large scale. Products can be manufactured using a bio-based process. Their molecules and properties are identical to those of petrochemical components. ''On top of that, there are more options to modify the molecular structure,''explains Syldatk. For example, plastics can be equipped with a higher melting point or greater gas permeability, and surfactants with modified foam properties. ''We are trying to play around with bacteria in fundamental research to find out which functions the respective structures have, and if possible to produce tailor-made compounds,'' says the biotechnologist.Process optimization is also involved in the use of microorganisms for further processing of synthesis gases which are produced by pyrolysis from straw or wood waste in the bioliq pilot plant at KIT. ''A major advantage of using synthesis gas is that it provides the same starting conditions, no matter what type of biomass was used as a raw material,'' says the researcher. Flue gas can now also be converted with the help of microorganisms, ''because they tolerate sulfur compounds or even use them for their metabolism. For chemical processing, the combustion gases would first need to be cleaned from these toxic compounds,'' explains Syldatk. In its bio-economy research program, the state of Baden-Werttemberg supports the KIT-driven development of innovative methods for the microbial use of lignocellulose.Story Source:Materials provided by Karlsruher Institut fer Technologie (KIT). ",0.0654569,0.148133,0.150408,0.148045,0.0968466,0.121415,0.149637,0,0
254,"Renewable energy cooperatives, an opportunity for energy transition ","The transition from fossil to renewable energy sources is a necessary condition for the sustainability of human societies. There are movements in societies seeking to make this transition ""not merely technological but also designed to exploit the capacity for social transformation they offer, given the possibility of their modular use and capacity for producing energy on a local level,"" said elvaro Campos-Celador, researcher in the Department of Thermal Engines and Machines of the UPV/EHU's Faculty of Engineering -- Bilbao and author of the study, together with Jon Teres-Zubiaga of the same department, and Ieigo Capellen-Perez of the University of Valladolid.Renewable energy cooperatives are good candidates for this; they are made up of groups of citizens and undertake to generate and market renewable energy. ""Models of this type could replace the current energy model, which is virtually in the hands of five major companies, and make it more democratic, non-profit making, and one in which citizens can play an active role in energy, which is something so important and central to their lives,"" pointed out Jon Teres.Yet ""renewable energies are not enjoying the best of times; even though they have had very strong institutional support since 2004, a series of regulatory changes took place around 2011 and almost completely hampered the possibility of implementing renewables in an economically viable way, and led to the bankruptcy of enterprises and small investors,"" explained Campos.The researcher describes the reasons that led to this change as complex. ""Before the economic crisis, forecasts had been made indicating that consumption was expected to rise in the population and investments were made in all kinds of power stations, both renewable ones and ones based on fossil fuels. What happened is that from 2008 onwards consumption not only failed to continue to rise, it began to decrease, so many investments ended up jeopardised, above all in the free-market scenario,"" explained Campos.n the study they analysed, in that situation, the ownership of the various sources of energy existing across Spain. ""Over 90% of the ownership and production of fossil energies corresponds to the major electricity companies that enjoy, let us say, excellent relations with that various governments there have been, and yet renewable energies correspond more to other investors,"" specified the researcher. This conflict could have been responsible for what happened.""What is more, during the same period ""a story appeared in the press saying that renewables were expensive and were making the price of electricity higher and that we were all paying for this. When examining the references of the people who were evaluating this phenomenon, we saw that it was possible to demonstrate the opposite, in other words, that renewables were in fact cutting the price of electricity,"" he said.Faced with all this hostility, ""the cooperatives found themselves forced to adapt to the new context and have managed to forge ahead with new projects by means of innovative funding methods,"" reasoned Teres. For example, the initiative ""Recupera el Sol"" (Recover the Sun) aims, by means of small contributions by citizens, to ""socialize"" photovoltaic plants, which many owners have found themselves forced to renounce because they could not secure a return that was sufficient to offset the financial costs of their investment.Right now, the number of contracts that the cooperatives have across Spain as a whole exceeds 70,000. ""Although this does not account for more than 0.3% of all the electricity contracts in Spain, it is true that the number is growing significantly, and in particular over the last 3 to 4 years,"" said Campos.""I believe that the mission of the cooperatives right now is to demonstrate that renewable energy and this way of managing energy is viable, and even cheaper in terms of the price of electricity. There is no doubt that much remains to be done, but in actual fact there is a very strong potential. And a more favourable regulatory framework would undoubtedly encourage the expansion of renewables,"" concluded Campos.Story Source:Materials provided by University of the Basque Country. ",0.0675207,0.0849738,0.124844,0.165468,0.0914304,0.150664,0.127245,0,0
255,Making wind farms more efficient,"With energy demands rising, researchers at Penn State Behrend and the University of Tabriz, Iran, have completed an algorithm -- or approach -- to design more efficient wind farms, helping to generate more revenue for builders and more renewable energy for their customers.Wind energy is on the rise, and not just in the US,"" said Mohammad Rasouli, assistant professor of electrical engineering at Penn State Erie, the Behrend College. ""The efficiency of solar panels is less than 25 percent, and is still a subject of current research. Wind turbines, on the other hand, are much more efficient and convert over 45 percent of the wind energy to electricity.""Though wind turbines are efficient, wind farm layouts can reduce this efficiency if not properly designed. Builders do not always put turbines in the places with the highest wind speeds, where they will generate the most power, said Rasouli. Turbine spacing is also important -- because turbines create drag that lowers wind speed, the first turbines to catch the wind will generate more power than those that come after.To build more efficient wind farms, designers must take these factors into account wind speed and turbine spacing, as well as land size, geography, number of turbines, amount of vegetation, meteorological conditions, building costs, and other considerations, according to the researchers.Balancing all of these factors to find an optimum layout is difficult, even with the assistance of mathematical models.""This is a multi-objective approach,"" said Rasouli. ""We have a function and we want to optimize it while taking into account various constraints.""The researchers focused on one approach, called ""biogeographical-based optimization."" Created in 2008 and inspired by nature, BBO is based on how animals naturally distribute themselves to make the best use of their environment based on their needs. By creating a mathematical model from animal behavior, it is then possible for the researchers to calculate the optimal distribution of objects in other scenarios, such as turbines on a wind farm.""Analytical methods require a lot of computation,"" said Rasouli. ""This BBO method minimizes computation and gives better results, finding the optimum solution at less computational cost.""Other researchers used simplified versions of the BBO approach in 2017 and 2018 to calculate more efficient wind farm layouts, but these simplified versions did not take into account all factors affecting the optimum layout.The researchers from Penn State and the University of Tabriz completed the approach by incorporating additional variables, including real market data, the roughness of the surface -- which affects how much power is in the wind -- and how much wind each turbine receives.The research team also improved the BBO approach by incorporating a more realistic model for calculating wakes -- areas with slower wing speeds created after the wind blows past a turbine, similar to the wake behind a boat -- and testing how sensitive the model was to other factors such as interest rates, financial incentives, and differences in energy production costs. They report their results online in the Journal of Cleaner Production, which will be published in November.""This is a more realistic optimization approach compared to some of the simplifying methods that are out there,"" said Rasouli. ""This would be better to customers, to manufacturers, and to grid-style, larger-size wind farms.""By incorporating more data, such as updated meteorological records and manufacturer information, the researchers will be able to use the BBO approach to optimize wind farm layouts in many different locations, helping wind farm designers across the world make better use of their land and generate more energy to meet future energy demands from consumers.""There is an end time for fossil fuels,"" said Rasouli. ""With this and upcoming methods or better optimization approaches, we can make better use of wind energy.""Story Source:Materials provided by Penn State. ",0.118359,0.0975077,0.137144,0.193249,0.110544,0.183076,0.146032,0,0
256,Nano-scale process may speed arrival of cheaper hi-tech products,"An inexpensive way to make products incorporating nanoparticles -- such as high-performance energy devices or sophisticated diagnostic tests -- has been developed by researchers.The process could speed the commercial development of devices, materials and technologies that exploit the physical properties of nanoparticles, which are thousands of times thinner than a human hair.The particles' small size means they behave differently compared with conventional materials, and their unusual properties are inspiring research towards new applications.Engineers demonstrated their manufacturing technique, known as electrospinning, by building a fuel cell -- a device that converts fuels into electrical power without combustion.Their device was produced featuring strands of nanoscale fibres incorporating nanoparticles on the surface. It offers a high contact area between the fuel cell components and the oxygen in the air, making it more efficient.Researchers at the University of Edinburgh and California Institute of Technology built their fuel cell using a nozzle-free electrospinning device -- a rotating drum in a bath of liquid under high voltage and temperature.Nanofibres are produced from the liquid on the surface of the drum, which are spun onto an adjacent hot surface. As the fibres cool to form a fuel cell component, nanocrystals emerge on their surface, creating a large surface area.Tests showed the nanofibre fuel cell performed better than conventional components. Such devices are very difficult to manufacture by other techniques, researchers say. The study, published in Nature Communications, was funded by the US Department of Energy.Dr Norbert Radacsi, of the University of Edinburgh's School of Engineering, who led the study, said: ""Our approach of electrospinning offers a quick and inexpensive way to form nanomaterials with high surface area. This could lead to products with improved performance, such as fuel cells, on an industrial scale.""Story Source:Materials provided by University of Edinburgh. ",0.090221,0.184882,0.119271,0.142981,0.131875,0.134684,0.148481,0,0
257,Harvesting renewable energy from the sun and outer space at the same time,"Scientists at Stanford University have demonstrated for the first time that heat from the sun and coldness from outer space can be collected simultaneously with a single device. Their research, published November 8 in the journal Joule, suggests that devices for harvesting solar and space energy will not compete for land space and can actually help each other function more efficiently.Renewable energy is increasingly popular as an economical and efficient alternative to fossil fuels, with solar energy topping charts as the worldwide favorite. But there is another powerful energy source overhead that can perform just the opposite function -- outer space.""It is widely recognized that the sun is a perfect heat source nature offers human beings on Earth,"" says Zhen Chen, the first author of the study, who is a former postdoctoral research associate at Stanford in the group of Shanhui Fan and is currently a professor at the Southeast University of China. ""It is less widely recognized that nature also offers human beings outer space as a perfect heat sink.""Objects give off heat as infrared radiation -- a form of light invisible to the human eye. Most of this radiation is reflected back to Earth by particles in the atmosphere, but some of it escapes into space, allowing surfaces that emit enough radiation within the infrared range to drop below the temperature of their surroundings. Radiative cooling technology reflects copious amounts of infrared light, providing an air conditioning alternative that doesn't emit greenhouse gases. It may also help improve solar cell efficiency, which decreases the hotter solar cells become -- if only the two technologies can coexist peacefully on one rooftop.Chen and his colleagues developed a device combining radiative cooling with solar absorption technology. The device consists of a germanium solar absorber on top of a radiative cooler with silicon nitride, silicon, and aluminum layers enclosed in a vacuum to minimize unwanted heat loss. Both the solar absorber and the atmosphere are transparent in the mid-infrared range of 8-13 microns, offering a channel for infrared radiation from the radiative cooler to pass through to outer space. The team demonstrated that the combined device can simultaneously provide 24?C in solar heating and 29?C in radiative cooling, with the solar absorber improving the radiative cooler's performance by blocking heat from the sun.""On a rooftop, we imagine a photovoltaic cell can supply electricity while the radiative cooler can cool down the house on hot summer days,"" says Chen.While this technology appears promising, Chen believes there is still plenty of work to do before it can be scaled up for commercial use. While the vacuum enveloping the device could be scaled up with relative ease, the infrared-transparent window made from zinc selenide is still too costly, and the solar absorber and radiative cooler could be designed from cheaper high-performing materials as well. Chen thinks it is also important to test the use of photovoltaic cells in the place of a solar absorber -- an idea which has yet to be demonstrated. But in spite of all these practical challenges, the team believes this research demonstrates that renewable energy has even more rooftop potential than previously thought.""I think this technology could potentially revolutionize the current solar cell technology,"" says Chen. ""If our concept is demonstrated and scaled up, the future solar cell will have two functions in one: electricity and cooling.""Story Source:Materials provided by Cell Press. ",0.113402,0.138559,0.132479,0.176094,0.156853,0.158187,0.181615,0,0
258,"Metallic nanocatalysts imitate the structure of enzymes In catalysis, nature is sometimes more efficient than artificial systems; researchers have copied one of the tricks","An international team of researchers has transferred certain structural characteristics of natural enzymes, which ensure particularly high catalytic activity, to metallic nanoparticles. The desired chemical reaction thus did not take place at the particle surface as usual, but in channels inside the metal particles -- and with three times higher catalytic activity. A team from the University of New South Wales, Australia, and Ruhr-Universitet Bochum, Germany, reported on these nanozymes in the Journal of the American Chemical Society, published online on 23 September 2018.Active centres in channelsIn the case of enzymes, the active centres, where the chemical reaction takes place, are located inside. The reacting substances have to pass through a channel from the surrounding solution to the active centre, where the spatial structure provides particularly favourable reaction conditions. ""It is assumed, for example, that a locally altered pH value prevails in the channels and that the electronic environment in the active centres is also responsible for the efficiency of natural enzymes,"" says Professor Wolfgang Schuhmann, head of the Bochum Center for Electrochemical Sciences.Channels produced in nickel-platinum particlesIn order to artificially imitate the enzyme structures, the researchers produced particles of nickel and platinum about ten nanometres in diameter. They then removed the nickel by means of chemical etching, whereby channels were formed. In the final step, they deactivated the active centres on the particle surface. ""This enabled us to ensure that only the active centres in the channels participated in the reaction,"" explains Patrick Wilde, a doctoral candidate at the Center for Electrochemical Sciences. The researchers compared the catalytic activity of the particles produced in this way with the activity of conventional particles with active centres on the surface.Three times greater activityFor the test, the team used the oxygen reduction reaction, which, among other things, forms the basis of the operation of fuel cells. Active centres at the end of the channels catalysed the reaction three times more efficiently than active centres on the particle surface.""The results show the enormous potential of nanozymes,"" sums up Dr. Corina Andronescu, a group leader at the Center for Electrochemical Sciences. The researchers now want to extend the concept to other reactions, such as electrocatalytic CO2 reduction, and investigate the principles of increased activity in more detail. ""We would like to be able to imitate the way enzymes work even better in the future,"" adds Schuhmann. ""Ultimately, we hope that the concept will contribute to industrial applications in order to make energy conversion processes more efficient using electricity generated from renewable sources.""Story Source:Materials provided by Ruhr-University Bochum. ",0.0999736,0.186366,0.168295,0.155747,0.150477,0.147462,0.162131,0,0
259,Images of photosynthetic protein complex splitting water,"In a new article published in Nature an international research team presents high-resolution images of photosystem II, the protein complex that splits water into hydrogen ions and oxygen during photosynthesis. The images will help researchers better understand this complex mechanism, possibly opening up the door to developing cheap and efficient solar fuel devices.When Earth was formed, the atmosphere was rich in carbon dioxide and no free oxygen molecules were present. Early life forms, comparable to present day microorganisms, satisfied their energy needs initially by 'eating' small energy-rich molecules. Subsequently, some 'discovered' how to harvest solar energy and store it in energy-rich molecules such as sugars, but it was not until these early life forms could extract electrons and protons from water molecules that evolution took a huge turn and allowed the development of life as we know it. This tremendous explosion of biological diversity was possible because of the abundance of water and solar energy. The by-product oxygen in turn enabled the evolution of complex animals after accumulating in the atmosphere.It is now nearly 50 years since Bessel Kok established that biological water oxidation in photosystem II involves a five-step reaction cycle of a catalyst that accumulates four oxidising equivalents before water oxidation proceeds in a fast concerted reaction. Although high-resolution structures of the dark-stable state of photosystem II have been obtained in recent years, the structural changes that occur during the five-step reaction cycle remained largely unknown.Using ultrashort (femtosecond) X-ray laser pulses delivered by the X-ray free electron laser near Stanford, USA, an international team of researchers has now managed to obtain high-resolution images of photosystem II and its remarkable water-splitting catalyst of all four stable states of the reaction cycle, as well as snapshots of reaction steps between some stable states. The research group of Johannes Messinger, chair of Molecular Biomimetics at the Department of Chemistry at the engstrem Laboratory at Uppsala University in Sweden was a part of the team.""I have been working for 30 years now to understand the mechanism of water oxidation in photosynthesis. This result is a dream come true! These new images will facilitate understanding this complex reaction on a level of detail previously thought impossible,"" says Johannes Messinger.Researchers expect that understanding how photosystem II can activate the cheap and abundant metal ions calcium and manganese to form one of the best water-oxidation catalysts available to date, will allow chemists to do the same. This would open the door to developing cheap and efficient solar fuel devices that store solar energy in the bonds of molecular hydrogen or other solar fuels obtained by carbon dioxide or nitrogen reduction.""Solar fuels are carbon-free or carbon-neutral. They will be needed in addition to batteries to turn the present fossil fuel based energy system into a renewable energy economy. The need of solar fuels is obvious if one realises that world-wide 80 per cent of present day energy consumption is fuel based. Even in Sweden more than 50 percent of the energy is used in form of fuels and only 34 percent as electricity,"" says Johannes Messinger.Story Source:Materials provided by Uppsala University. ",0.0571619,0.145743,0.118701,0.135356,0.0859806,0.11586,0.135522,0,0
260,Graphene takes a step towards renewable fuel,"Researchers at Linkeping University, Sweden, are working to develop a method to convert water and carbon dioxide to the renewable energy of the future, using the energy from the sun and graphene applied to the surface of cubic silicon carbide. They have now taken an important step towards this goal, and developed a method that makes it possible to produce graphene with several layers in a tightly controlled process.The research group has also shown that graphene acts as a superconductor in certain conditions. Their results have been published in the scientific journals Carbon and Nano Letters.Carbon, oxygen and hydrogen. These are the three elements you would get if you took apart molecules of carbon dioxide and water. The same elements are the building blocks of chemical substances that we use for fuel, such as ethanol and methane. The conversion of carbon dioxide and water to renewable fuel, if possible, would provide an alternative to fossil fuels, and contribute to reducing our emission of carbon dioxide to the atmosphere. Jianwu Sun, senior lecturer at Linkeping University, is trying to find a way to do just that.The first step is to develop the material they plan to use. Researchers at Linkeping University have previously developed a world-leading method to produce cubic silicon carbide, which consists of silicon and carbon. The cubic form has the ability to capture energy from the sun and create charge carriers. This is, however, not sufficient. Graphene, one of the thinnest materials ever produced, plays a key role in the project. The material comprises a single layer of carbon atoms bound to each other in a hexagonal lattice. Graphene has a high ability to conduct an electric current, a property that would be useful for solar energy conversion. It also has several unique properties, and possible uses of graphene are being extensively studied all over the world.In recent years, the researchers have attempted to improve the process by which graphene grows on a surface in order to control the properties of the graphene. Their recent progress is described in an article in the scientific journal Carbon.""It is relatively easy to grow one layer of graphene on silicon carbide. But it's a greater challenge to grow large-area uniform graphene that consists of several layers on top of each other. We have now shown that it is possible to grow uniform graphene that consists of up to four layers in a controlled manner,"" says Jianwu Sun, of the Department of Physics, Chemistry and Biology at Linkeping University.One of the difficulties posed by multilayer graphene is that the surface becomes uneven when different numbers of layers grow at different locations. The edge when one layer ends has the form of a tiny, nanoscale, staircase. For the researchers, who want large flat areas, these steps are a problem. It is particularly problematic when steps collect in one location, like a wrongly built staircase in which several steps have been united to form one large step. The researchers have now found a way to remove these united large steps by growing the graphene at a carefully controlled temperature. Furthermore, the researchers have shown that their method makes it possible to control how many layers the graphene will contain. This is the first key step in an ongoing research project whose goal is to make fuel from water and carbon dioxide.In a closely related article in the journal Nano Letters, the researchers describe investigations into the electronic properties of multilayer graphene grown on cubic silicon carbide.""We discovered that multilayer graphene has extremely promising electrical properties that enable the material to be used as a superconductor, a material that conducts electrical current with zero electrical resistance. This special property arises solely when the graphene layers are arranged in a special way relative to each other,"" says Jianwu Sun.Theoretical calculations had predicted that multilayer graphene would have superconductive properties, provided that the layers are arranged in a particular way. In the new study, the researchers demonstrate experimentally for the first time that this is the case. Superconducting materials are used in, among other things, superconducting magnets -- extremely powerful magnets found in magnet resonance cameras for medical investigations, and in particle accelerators for research. There are many potential areas of application for superconductors, such as electrical supply lines with zero energy loss, and high-speed trains that float on a magnetic field. Their use is currently limited by the inability to produce superconductors that function at room temperature: currently available superconductors function only at extremely low temperatures.Story Source:Materials provided by Linkeping University. ",0.080483,0.16115,0.127338,0.175528,0.12778,0.145365,0.159848,0,0
261,Filtering liquids with liquids saves electricity Liquid-gated membrane filtration system improves industrial wastewater purification and saves energy,"Filtering and treating water, both for human consumption and to clean industrial and municipal wastewater, accounts for about 13% of all electricity consumed in the US every year and releases about 290 million metric tons of CO2 into the atmosphere annually -- roughly equivalent to the combined weight of every human on Earth.One of the most common methods of processing water is passing it through a membrane with pores that are sized to filter out particles that are larger than water molecules. However, these membranes are susceptible to ""fouling,"" or clogging by the very materials they are designed to filter out, necessitating more electricity to force the water through a partially clogged membrane and frequent membrane replacement, both of which increase water treatment costs.New research from the Wyss Institute for Biologically Inspired Engineering at Harvard University and collaborators at Northeastern University and the University of Waterloo demonstrates that the Wyss' liquid-gated membranes (LGMs) filter nanoclay particles out of water with twofold higher efficiency, nearly threefold longer time-to-foul, and a reduction in the pressure required for filtration over conventional membranes, offering a solution that could reduce the cost and electricity consumption of high-impact industrial processes such as oil and gas drilling. The study is reported in APL Materials.""This is the first study to demonstrate that LGMs can achieve sustained filtration in settings similar to those found in heavy industry, and it provides insight into how LGMs resist different types of fouling, which could lead to their use in a variety of water processing settings,"" said first author Jack Alvarenga, a Research Scientist at the Wyss Institute.LGMs mimic nature's use of liquid-filled pores to control the movement of liquids, gases and particles through biological filters using the lowest possible amount of energy, much like the small stomata openings in plants' leaves allow gases to pass through. Each LGM is coated with a liquid that acts as a reversible gate, filling and sealing its pores in the ""closed"" state. When pressure is applied to the membrane, the liquid inside the pores is pulled to the sides, creating open, liquid-lined pores that can be tuned to allow the passage of specific liquids or gases, and resist fouling due to the liquid layer's slippery surface. The use of fluid-lined pores also enables the separation of a target compound from a mixture of different substances, which is common in industrial liquid processing.The research team decided to test their LGMs on a suspension of bentonite clay in water, as such ""nanoclay"" solutions mimic the wastewater produced by drilling activities in the oil and gas industry. They infused 25-mm discs of a standard filter membrane with perfluoropolyether, a type of liquid lubricant that has been used in the aerospace industry for over 30 years, to convert them into LGMs. They then placed the membranes under pressure to draw water through the pores but leave the nanoclay particles behind, and compared the performance of untreated membranes to LGMs.The untreated membranes displayed signs of nanoclay fouling much more quickly than the LGMs, and the LGMs were able to filter water three times longer than the standard membranes before requiring a ""backwash"" procedure to remove particles that had accumulated on the membrane. Less frequent backwashing could translate to a reduction in the use of cleaning chemicals and energy required to pump backwash water, and improve the filtration rate in industrial water treatment settings.While the LGMs did eventually experience fouling, they displayed a 60% reduction in the amount of nanoclay that accumulated within their structure during filtration, which is known as ""irreversible fouling"" because it is not removed by backwashing. This advantage gives LGMs a longer lifespan and makes more of the filtrate recoverable for alternate uses. Additionally, the LGMs required 16% less pressure to initiate the filtration process, reflecting further energy savings.""LGMs have the potential for use in industries as diverse as food and beverage processing, biopharmaceutical manufacturing, textiles, paper, pulp, chemical, and petrochemical, and could offer improvements in energy use and efficiency across a wide swath of industrial applications,"" said corresponding author Joanna Aizenberg, Ph.D., who is a Founding Core Faculty member of the Wyss Institute and the Amy Smith Berylson Professor of Material Sciences at Harvard's John A. Paulson School of Engineering and Applied Sciences (SEAS).The team's next steps for the research include larger-scale pilot studies with industry partners, longer-term operation of the LGMs, and filtering even more complex mixtures of substances. These studies will provide insight into the commercial viability of LGMs for different applications, and how long they would last in a number of use cases.""The concept of using a liquid to help filter other liquids, while perhaps not obvious to us, is prevalent in nature. It's wonderful to see how leveraging nature's innovation in this manner can potentially lead to huge energy savings,"" said Wyss Founding Director Donald Ingber, M.D., Ph.D., who is also the Judah Folkman Professor of Vascular Biology at Harvard Medical School and the Vascular Biology Program at Boston Children's Hospital, as well as Professor of Bioengineering at SEAS.Story Source:Materials provided by Wyss Institute for Biologically Inspired Engineering at Harvard. ",0.0571928,0.11248,0.120601,0.116883,0.0828256,0.115738,0.121497,0,0
262,New electro-optic laser pulses 100 times faster than usual ultrafast light,"Physicists at the National Institute of Standards and Technology (NIST) have used common electronics to build a laser that pulses 100 times more often than conventional ultrafast lasers. The advance could extend the benefits of ultrafast science to new applications such as imaging of biological materials in real time.The technology for making electrooptic lasers has been around for five decades, and the idea seems alluringly simple. But until now researchers have been unable to electronically switch light to make ultrafast pulses and eliminate electronic noise, or interference.As described in the Sept. 28 issue of Science, NIST scientists developed a filtering method to reduce the heatinduced interference that otherwise would ruin the consistency of electronically synthesized light.""We tamed the light with an aluminum can,"" project leader Scott Papp said, referring to the ""cavity"" in which the electronic signals are stabilized and filtered. As the signals bounce back and forth inside something like a soda can, fixed waves emerge at the strongest frequencies and block or filter out other frequencies.Ultrafast refers to events lasting picoseconds (trillionths of a second) to femtoseconds (quadrillionths of a second). This is faster than the nanoscale regime, introduced to the cultural lexicon some years ago with the field of nanotechnology (nanoseconds are billionths of a second).The conventional source of ultrafast light is an optical frequency comb, a precise ""ruler"" for light. Combs are usually made with sophisticated ""mode-locked"" lasers, which form pulses from many different colors of light waves that overlap, creating links between optical and microwave frequencies. Interoperation of optical and microwave signals powers the latest advances in communications, timekeeping and quantum sensing systems.In contrast, NIST's new electro-optic laser imposes microwave electronic vibrations on a continuouswave laser operating at optical frequencies, effectively carving pulses into the light.""In any ultrafast laser, each pulse lasts for, say, 20 femtoseconds,"" lead author David Carlson said. ""In mode-locked lasers, the pulses come out every 10 nanoseconds. In our electro-optic laser, the pulses come out every 100 picoseconds. So that's the speedup here -- ultrafast pulses that arrive 100 times faster or more.""""Chemical and biological imaging is a good example of the applications for this type of laser,"" Papp said. ""Probing biological samples with ultrafast pulses provides both imaging and chemical makeup information. Using our technology, this kind of imaging could happen dramatically faster. So, hyperspectral imaging that currently takes a minute could happen in real time.""To make the electro-optic laser, NIST researchers start with an infrared continuous-wave laser and create pulses with an oscillator stabilized by the cavity, which provides the equivalent of a memory to ensure all the pulses are identical. The laser produces optical pulses at a microwave rate, and each pulse is directed through a microchip waveguide structure to generate many more colors in the frequency comb.The electro-optic laser offers unprecedented speed combined with accuracy and stability that are comparable to that of a mode-locked laser, Papp said. The laser was constructed using commercial telecommunications and microwave components, making the system very reliable. The combination of reliability and accuracy makes electro-optic combs attractive for long-term measurements of optical clock networks or communications or sensor systems in which data needs to be acquired faster than is currently possible.Story Source:Materials provided by National Institute of Standards and Technology (NIST). ",0.131827,0.150145,0.160665,0.199897,0.235932,0.176743,0.174418,0,0
263,"Printable tags turn everyday objects into smart, connected devices ","Engineers have developed printable metal tags that could be attached to everyday objects and turn them into ""smart"" Internet of Things devices.The metal tags are made from patterns of copper foil printed onto thin, flexible, paper-like substrates and are made to reflect WiFi signals. The tags work essentially like ""mirrors"" that reflect radio signals from a WiFi router. When a user's finger touches these mirrors, it disturbs the reflected WiFi signals in such a way that can be remotely sensed by a WiFi receiver, like a smartphone.The tags can be tacked onto plain objects that people touch and interact with every day, like water bottles, walls or doors. These plain objects then essentially become smart, connected devices that can signal a WiFi device whenever a user interacts with them. The tags can also be fashioned into thin keypads or smart home control panels that can be used to remotely operate WiFi-connected speakers, smart lights and other Internet of Things appliances.""Our vision is to expand the Internet of Things to go beyond just connecting smartphones, smartwatches and other high-end devices,"" said senior author Xinyu Zhang, a professor of electrical and computer engineering at the UC San Diego Jacobs School of Engineering and member of the Center for Wireless Communications at UC San Diego. ""We're developing low-cost, battery-free, chipless, printable sensors that can include everyday objects as part of the Internet of Things.""Zhang's team named the technology ""LiveTag."" These metal tags are designed to only reflect specific signals within in the WiFi frequency range. By changing the type of material they're made of and the pattern in which they're printed, the researchers can redesign the tags to reflect either Bluetooth, LTE or cellular signals.The tags have no batteries, silicon chips, or any discrete electronic components, so they require hardly any maintenance -- no batteries to change, no circuits to fix.The team presented their work at the recent USENIX Symposium on Networked Systems Design and Implementation Conference.Smart taggingAs a proof of concept, the researchers used LiveTag to create a paper-thin music player controller complete with a play/pause button, next track button and sliding bar for tuning volume. The buttons and sliding bar each consist of at least one metal tag so touching any of them sends signals to a WiFi device. The researchers have so far only tested the LiveTag music player controller to remotely trigger a WiFi receiver, but they envision that it would be able to remotely control WiFi-connected music players or speakers when attached to a wall, couch armrest, clothes, or other ordinary surface.The researchers also adapted LiveTag as a hydration monitor. They attached it to a plastic water bottle and showed that it could be used to track a user's water intake by monitoring the water level in the bottle. The water inside affects the tag's response in the same way a finger touch would -- as long as the bottle is not made of metal, which would block the signal. The tag has multiple resonators that each get detuned at a specific water level. The researchers imagine that the tag could be used to deliver reminders to a user's smartphone to prevent dehydration.Future applicationsOn a broader scope, Zhang envisions using LiveTag technology to track human interaction with everyday objects. For example, LiveTag could potentially be used as an inexpensive way to assess the recovery of patients who have suffered from stroke.""When patients return home, they could use this technology to provide data on their motor activity based on how they interact with everyday objects at home -- whether they are opening or closing doors in a normal way, or if they are able to pick up bottles of water, for example. The amount, intensity and frequency of their activities could be logged and sent to their doctors to evaluate their recovery,"" said Zhang. ""And this can all be done in the comfort of their own homes rather than having to keep going back to the clinic for frequent motor activity testing,"" he added.Another example is tagging products at retail stores and assessing customer interest based on which products they touch. Rather than use cameras, stores could use LiveTag as an alternative that offers customers more privacy, said Zhang.Next stepsThe researchers note several limitations of the technology. LiveTag currently cannot work with a WiFi receiver further than one meter (three feet) away, so researchers are working on improving the tag sensitivity and detection range. Ultimately, the team aims to develop a way to make the tags using normal paper and ink printing, which would make them cheaper to mass produce.Paper title: ""LiveTag: Sensing Human-Object Interaction Through Passive Chipless WiFi Tags."" Co-authors include Chuhan Gao and Yilong Li at University of Wisconsin-Madison.Story Source:Materials provided by University of California - San Diego. ",0.09359,0.0970874,0.135789,0.21481,0.16397,0.142765,0.156507,0,0
264,"Mobile phone radiation may affect memory performance in adolescents, study finds ","Radiofrequency electromagnetic fields may have adverse effects on the development of memory performance of specific brain regions exposed during mobile phone use. These are the findings of a study involving nearly 700 adolescents in Switzerland. The investigation, led by the Swiss Tropical and Public Health Institute (Swiss TPH), will be published on Monday, 23 July 2018 in the peer-reviewed journal Environmental Health Perspectives.The rapid evolution of information and communication technologies (ICT) goes along with an increase in exposure to radiofrequency electromagnetic fields (RF-EMF) in our daily life. The most relevant exposure source to the brain is the use of a mobile phone close to the head. Several studies have been conducted to identify potential health effects related to RF-EMF, though results have remained inconclusive.The research conducted by scientists at the Swiss Tropical and Public Health Institute (Swiss TPH) looked at the relationship between exposure to RF-EMF from wireless communication devices and memory performance in adolescents. The study follows up a report published in the scientific journal Environment International in 2015 with twice the sample size and more recent information on the absorption of RF-EMF in adolescent brains during different types of wireless communication device use. These are the world's first epidemiological studies to estimate cumulative RF-EMF brain dose in adolescents.Media usage and brain exposure in young adults The study to be published on 23 July 2018 found that cumulative RF-EMF brain exposure from mobile phone use over one year may have a negative effect on the development of figural memory performance in adolescents, confirming prior results published in 2015. Figural memory is mainly located in the right brain hemisphere and association with RF-EMF was more pronounced in adolescents using the mobile phone on the right side of the head. ""This may suggest that indeed RF-EMF absorbed by the brain is responsible for the observed associations."" said Martin Reesli, Head of Environmental Exposures and Health at Swiss TPH.The rapid evolution of information and communication technologies (ICT) goes along with an increase in exposure to radiofrequency electromagnetic fields (RF-EMF) in our daily life. The most relevant exposure source to the brain is the use of a mobile phone close to the head. Several studies have been conducted to identify potential health effects related to RF-EMF, though results have remained inconclusive.The research conducted by scientists at the Swiss Tropical and Public Health Institute (Swiss TPH) looked at the relationship between exposure to RF-EMF from wireless communication devices and memory performance in adolescents. The study follows up a report published in the scientific journal Environment International in 2015 with twice the sample size and more recent information on the absorption of RF-EMF in adolescent brains during different types of wireless communication device use. These are the world's first epidemiological studies to estimate cumulative RF-EMF brain dose in adolescents.Media usage and brain exposure in young adults The study to be published on 23 July 2018 found that cumulative RF-EMF brain exposure from mobile phone use over one year may have a negative effect on the development of figural memory performance in adolescents, confirming prior results published in 2015. Figural memory is mainly located in the right brain hemisphere and association with RF-EMF was more pronounced in adolescents using the mobile phone on the right side of the head. ""This may suggest that indeed RF-EMF absorbed by the brain is responsible for the observed associations."" said Martin Reesli, Head of Environmental Exposures and Health at Swiss TPH.Other aspects of wireless communication use, such as sending text messages, playing games or browsing the Internet cause only marginal RF-EMF exposure to the brain and were not associated with the development of memory performance. ""A unique feature of this study is the use of objectively collected mobile phone user data from mobile phone operators."" said Reesli. He emphasised that further research is needed to rule out the influence of other factors. ""For instance, the study results could have been affected by puberty, which affects both mobile phone use and the participant's cognitive and behavioural state.""The data gathered from the Health Effects Related to Mobile phone usE in adolescentS (HERMES) cohort looked at the relationship between exposure to RF-EMF and development of memory performance of almost 700 adolescents over the course of one year. Participants, aged 12 to 17 years, were recruited from 7th to 9th public school grades in urban and rural areas of Swiss-German speaking Switzerland.Minimising the risk of RF-EMF exposure The potential effect of RF-EMF exposure to the brain is a relatively new field of scientific inquiry. ""It is not yet clear how RF-EMF could potentially affect brain processes or how relevant our findings are in the long-term."" said Reesli. ""Potential risks to the brain can be minimised by using headphones or the loud speaker while calling, in particular when network quality is low and the mobile phone is functioning at maximum power.""About the publication The study was conducted by Swiss TPH in collaboration with the European Union project GERoNiMO, which aims to improve the knowledge of whether and to what extent RF-EMF affects health. The work on dose calculations was conducted in collaboration with Belgian scientists. The project was funded by the European Community's Seventh Framework Programme and the Swiss National Science Foundation (SNSF).Story Source:Materials provided by Swiss Tropical and Public Health Institute. ",0.123317,0.171936,0.191017,0.257342,0.209407,0.164135,0.19574,0,0
265,Buried Internet infrastructure at risk as sea levels rise,"Thousands of miles of buried fiber optic cable in densely populated coastal regions of the United States may soon be inundated by rising seas, according to a new study by researchers at the University of Wisconsin-Madison and the University of Oregon.The study, presented July 16, 2018 at a meeting of internet network researchers, portrays critical communications infrastructure that could be submerged by rising seas in as soon as 15 years, according to the study's senior author, Paul Barford, a UW-Madison professor of computer science.""Most of the damage that's going to be done in the next 100 years will be done sooner than later,"" says Barford, an authority on the ""physical internet"" -- the buried fiber optic cables, data centers, traffic exchanges and termination points that are the nerve centers, arteries and hubs of the vast global information network. ""That surprised us. The expectation was that we'd have 50 years to plan for it. We don't have 50 years.""The study, conducted with Barford's former student Ramakrishnan Durairajan, now of the University of Oregon, and Carol Barford, who directs UW-Madison's Center for Sustainability and the Global Environment, is the first assessment of risk of climate change to the internet. It suggests that by the year 2033 more than 4,000 miles of buried fiber optic conduit will be underwater and more than 1,100 traffic hubs will be surrounded by water. The most susceptible U.S. cities, according to the report, are New York, Miami and Seattle, but the effects would not be confined to those areas and would ripple across the internet, says Barford, potentially disrupting global communications.The peer-reviewed study combined data from the Internet Atlas, a comprehensive global map of the internet's physical structure, and projections of sea level incursion from the National Oceanic and Atmospheric Administration (NOAA). The study, which only evaluated risk to infrastructure in the United States, was shared today with academic and industry researchers at the Applied Networking Research Workshop, a meeting of the Association for Computing Machinery, the Internet Society and the Institute of Electrical and Electronics Engineers.Much of this infrastructure is buried and follows long-established rights of way, typically paralleling highways and coastlines, says Barford. ""When it was built 20-25 years ago, no thought was given to climate change.""Many of the conduits at risk are already close to sea level and only a slight rise in ocean levels due to melting polar ice and thermal expansion as climate warms will be needed to expose buried fiber optic cables to sea water. Hints of the problems to come, says Barford, can be seen in the catastrophic storm surges and flooding that accompanied hurricanes Sandy and Katrina.Buried fiber optic cables are designed to be water-resistant, but unlike the marine cables that ferry data from continent to continent under the ocean, they are not waterproof.Risk to the physical internet, says Barford, is coupled to the large population centers that exist on the coasts, which also tend to be the same places where the transoceanic marine cables that underpin global communication networks come ashore. ""The landing points are all going to be underwater in a short period of time,"" he notes.Moreover, much of the data that transits the internet tends to converge on a small number of fiber optic strands that lead to large population centers like New York, one of the more vulnerable cities identified in the study.The impact of mitigation such as sea walls, according to the study, are difficult to predict. ""The first instinct will be to harden the infrastructure,"" Barford says. ""But keeping the sea at bay is hard. We can probably buy a little time, but in the long run it's just not going to be effective.""In addition to looking at the risk to local and long-haul infrastructure in the nation's coastal areas, the study examined the risk to the buried assets of individual internet service providers. It found the networks of CenturyLink, Inteliquent and AT&T to be at highest risk.The findings of the study, argues the Wisconsin computer scientist, serve notice to industry and government. ""This is a wake-up call. We need to be thinking about how to address this issue.""Story Source:Materials provided by University of Wisconsin-Madison. Original written by Terry Devitt. ",0.0860171,0.107578,0.192014,0.182329,0.122094,0.144557,0.144893,0,0
266,"Researchers map light and sound wave interactions in optical fibers New insight into opto-mechanics of optical fibers may now be applied to sensor systems of longer reach, higher spatial resolution, and better precision","Optical fibers make the internet happen. They are fine threads of glass, as thin as a human hair, produced to transmit light. Optical fibers carry thousands of Giga bits of data per second across the world and back. The same fibers also guide ultrasound waves, somewhat similar to those used in medical imaging.These two wave phenomena -- optical and ultrasonic -- possess attributes that are fundamentally different. Fibers are designed to keep propagating light strictly inside an inner core region, since any light that penetrates outside this region represents the loss of a precious signal. In contrast, ultrasonic waves can reach the outer boundaries of fibers, and probe their surroundings.Intuition, and much of the training given in fundamental undergraduate classes in mechanics and optics, instructs to consider light and sound waves as separate and unrelated entities. But this perspective is incomplete. Propagating light can drive the oscillations of ultrasonic waves, as if it were some kind of transducer, due to the basic rules of electro-magnetism. Likewise, the presence of ultrasound can scatter and modify light waves. Light and sound waves can interact/affect one another and aren't necessarily separate and unrelated.The research field of opto-mechanics is dedicated to the study of this interplay. Such studies, especially on fibers, can be very useful and bear surprising results. For example, earlier this year research groups at Bar-Ilan University, Israel and EPFL, Switzerland developed sensing protocols that allow optical fibers to ""listen"" outside an optical fiber where they cannot ""look,"" based on an interplay between light waves and ultrasound. By launching light waves into a single end of a standard telecommunication fiber, the measurement setup could identify and map liquid media over several kilometers. These findings were published in two articles (in the journal Nature Communications). Such methods can serve in oil and gas pipelines, monitoring oceans and lakes, climate studies, desalination plants, process control in chemical industries, and more.The mutual effects of light and sound waves that co-propagate in a fiber continue to draw interest and attention. In a paper just published in the journal Applied Physics Letters -- Photonics, the research group of Prof. Avi Zadok, of Bar-Ilan University's Faculty of Engineering and Institute for Nanotechnology and Advanced Materials, took this study one step further. The group constructed a distributed spectrometer, a measurement protocol that can map local power levels of multiple optical wave components over many kilometers of fiber. ""The measurements unravel how the generation of ultrasonic waves can mix these optical waves together. Rather than propagate independently, the opto-mechanical interactions lead to the amplification of certain optical waves, and to the attenuation of others, in complicated fashion. The observed complex dynamics are fully accounted for, however, by a corresponding model,"" said Zadok.The report by Zadok and doctoral students Yosef London, Hagai Diamandi and Gil Bashan is highlighted in the journal as an ""Editor's Pick."" This new insight into the opto-mechanics of optical fibers may now be applied to sensor systems of longer reach, higher spatial resolution, and better precision to assist, for example, in the detection of leaks in reservoirs, dams and pipelines.Story Source:Materials provided by Bar-Ilan University. ",0.0973843,0.117597,0.138272,0.158088,0.165861,0.131108,0.14396,0,0
267,Security vulnerabilities in terahertz data links,"A new study shows that terahertz data links, which may play a role in ultra-high-speed wireless data networks of the future, aren't as immune to eavesdropping as many researchers have assumed. The research, published in the journal Nature, shows that it is possible for a clever eavesdropper to intercept a signal from a terahertz transmitter without the intrusion being detected at the receiver.""The conventional wisdom in the terahertz community has been that it's virtually impossible to spy on a terahertz data link without the attack being noticed,"" said Daniel Mittleman, a professor in Brown University's School of Engineering and a coauthor of the research. ""But we show that undetected eavesdropping in the terahertz realm is easier than most people had assumed and that we need to be thinking about security issues as we think about designing network architectures.""Because of its higher frequency, terahertz radiation can carry up to 100 times more data than the microwaves used in wireless communication today, which makes terahertz an attractive option for use in future wireless networks. Along with enhanced bandwidth, it has also been generally assumed that the way in which high-frequency waves propagate would naturally enhance security. Unlike microwaves, which propagate in wide-angle broadcasts, terahertz waves travel in narrow, very directional beams.""In microwave communications, an eavesdropper can put an antenna just about anywhere in the broadcast cone and pick up the signal without interfering with the intended receiver,"" Mittleman said. ""Assuming that the attacker can decode that signal, they can then eavesdrop without being detected. But in terahertz networks, the narrow beams would mean that an eavesdropper would have to place the antenna between the transmitter and receiver. The thought was that there would be no way to do that without blocking some or all of the signal, which would make an eavesdropping attempt easily detectable by the intended receiver.""Mittleman and colleagues from Brown, Rice University and the University at Buffalo set out to test that notion. They set up a direct line-of-site terahertz data link between a transmitter and receiver, and experimented with devices capable of intercepting signal. They were able show several strategies that could steal signal without being detected -- even when the data-carrying beam is very directional, with a cone angle of less than 2 degrees (in contrast to microwave transmission, where the angle is often as large as 120 degrees).One set of strategies involves placing objects at the very edge of a beam that is capable of scattering a tiny portion of the beam. In order for a data link to be reliable, the diameter of the beam must be slightly larger than the aperture of the receiver. That leaves a sliver of signal for an attacker to work with without casting a detectable shadow on the receiver.The researchers showed that a flat piece of metal could redirect a portion of the beam to a secondary receiver operated by an attacker. The researchers were able to acquire a usable signal at the second receiver with no significant loss of power at the primary receiver.The team showed an even more flexible approach (from the attacker's perspective) by using a metal cylinder in the beam instead of a flat plate.""Cylinders have the advantage that they scatter light in all directions, giving an attacker more options in setting up a receiver,"" said Josep Jornet, an assistant professor of engineering at Buffalo and a study co-author. ""And given the physics of terahertz wave propagation, even a very small cylinder can significantly scatter the signal without blocking the line-of-sight path.""The researchers went on to demonstrate another type of attack involving a lossless beam splitter that would also be difficult, if not impossible, to detect. The beam splitter placed in front of a transmitter would enable an attacker to steal just enough to be useful, yet not so much that it would set off alarm bells among network administrators.The bottom line, the researchers say, is that while there are inherent security enhancements associated with terahertz links in comparison with lower frequencies, these security improvements are still far from foolproof.""Securing wireless transmission from eavesdroppers has been a challenge since the days of Marconi,"" said Edward Knightly, professor of electrical and computer engineering at Rice University and a study coauthor. ""While terahertz bands take a huge leap in this direction, we unfortunately found that a determined adversary can still be effective in intercepting the signal.""Story Source:Materials provided by Brown University. ",0.0836587,0.0877619,0.124959,0.187639,0.150883,0.117166,0.129396,0,0
268,Regional seismic data help locate September 2017 North Korean nuclear test,"The epicenter of the Sept. 3, 2017 nuclear test explosion in North Korea occurred about 3.6 kilometers northwest of the country's first nuclear test in October 2006, according to a new high-precision analysis of the explosion and its aftermath.The study published in Seismological Research Letters by Lian-Feng Zhao of the Chinese Academy of Sciences and colleagues used regional seismic data collected from a number of sources to locate the 2017 test, and to confirm that subsequent seismic events were not also nuclear explosions.Their paper is published as part of the journal's special focus section on the September 2017 North Korean explosion. The body-wave magnitude 6.1 underground test by the Democratic People's Republic of Korea (DPRK) is the largest such test in more than 20 years, and is the sixth declared nuclear test by the DPRK since 2006. The September explosion is an order of magnitude larger than the next largest test by the country, which occurred in September 2016.Zhao and colleagues used seismic wave data from 255 seismograph stations in the China National Digital Seismic Network, Global Seismic Network, International Federation of Digital Seismograph Networks and Full Range Seismograph Network in Japan to investigate the explosion and three other seismic events that occurred in the minutes and days after.Although global seismic networks may pick up the signal of underground nuclear tests, the signals they detect are often too weak to be to be used in the kind of location analysis performed by Zhao and colleagues. ""The closer to the sources the better,"" said Zhao. ""However, seismometers cannot be deployed in the North Korean test site due to political issues. Thus, seismologists have developed methods that can be applied to regional seismic data to investigate seismic characteristics of the underground nuclear explosions.""The researchers used seismic data from the first DPRK nuclear test as the ""master event"" to calibrate their location analysis, since the epicenter of that small explosion could be visually located using satellite images of localized ground surface damage. The much larger September 2017 explosion produced surface damage over an area about nine square kilometers, however, in ground that was already disturbed by previous nuclear tests. ""For example, after the sixth North Korean nuclear test, large displacements occurred on the west and south flank [of the test site] and debris flows were localized in pre-existing channels,"" Zhao explained. ""These spatially distributing phenomena made it difficult for us to directly determine the epicenter of the explosion.""Zhao and colleagues used regional seismic data instead to calculate that the epicenter of the September 2017 explosion was at 41.3018eN and 129.0696eE. A seismic event that took place about eight minutes after the explosion occurred very close to the explosion epicenter -- less than 200 meters away -- and probably represents the seismic signature of the collapse of a cavity left by the underground explosion, the researchers suggested.Two subsequent seismic events, one on 23 September and one on 12 October, were located about eight kilometers northeast of the nuclear test site. Zhao and colleagues said that the seismic signatures of these two events indicate that they are not explosions, but may have resulted from mechanisms such as landslide or ground collapse. They may also be very shallow natural earthquakes that were triggered by the explosion, they noted, a possibility that will require more research on the pre- and post-explosion stresses on the faults where the events occurred.Story Source:Materials provided by Seismological Society of America. ",0.134707,0.138498,0.240375,0.229926,0.161567,0.19781,0.190015,0,0
269,Photonic chips harness sound waves to speed up local networks,"It used to be known as the information superhighway -- the fibre-optic infrastructure on which our gigabytes and petabytes of data whizz around the world at (nearly) the speed of light.And like any highway system, increased traffic has created slowdowns, especially at the junctions where data jumps on or off the system.Local and access networks especially, such as financial trading systems, city-wide mobile phone networks and cloud computing warehouses, are therefore not as fast as they could be.This is because increasingly complex digital signal processing and laser-based 'local oscillator' systems are needed to unpack the photonic, or optical, information and transfer it into the electronic information that computers can process.Now, scientists at the University of Sydney have for the first time developed a chip-based information recovery technique that eliminates the need for a separate laser-based local oscillator and complex digital signal processing system.""Our technique uses the interaction of photons and acoustic waves to enable an increase in signal capacity and therefore speed,"" said Dr Elias Giacoumidis, joint lead author of a new study. ""This allows for the successful extraction and regeneration of the signal for electronic processing at very-high speed.""The incoming photonic signal is processed in a filter on a chip made from a glass known as chalcogenide. This material has acoustic properties that allows a photonic pulse to 'capture' the incoming information and transport it on the chip to be processed into electronic information.This removes the need for complicated laser oscillators and complex digital signal processing.""This will increase processing speed by microseconds, reducing latency or what is referred to as 'lag' in the gaming community,"" said Dr Amol Choudhary from the University of Sydney Nano Institute and School of Physics. ""While this doesn't sound a lot, it will make a huge difference in high-speed services, such as the financial sector and emerging e-health applications.""The photonic-acoustic interaction harnesses what is known as stimulated Brillouin scattering, a effect used by the Sydney team to develop photonic chips for information processing.""Our demonstration device using stimulated Brillouin scattering has produced a record-breaking narrowband of about 265 megahertz bandwidth for carrier signal extraction and regeneration. This narrow bandwidth increases the overall spectral efficiency and therefore overall capacity of the system,"" Dr Choudhary said.Group research leader and Director of Sydney Nano, Professor Ben Eggleton, said: ""The fact that this system is lower in complexity and includes extraction speedup means it has huge potential benefit in a wide range of local and access systems such as metropolitan 5G networks, financial trading, cloud computing and the Internet-of-Things.""The study is published today in Optica.Dr Choudhary said the research team's next steps will be to construct prototype receiver chips for further testing.The study was a collaboration with Monash University and the Australian National University.Story Source:Materials provided by University of Sydney. ",0.08483,0.106111,0.117621,0.204804,0.173361,0.106895,0.141014,0,0
270,Plasma thruster: New space debris removal technology,"The Earth is currently surrounded by debris launched into space over several decades. This space junk can collide with satellites and not only cause damage to spacecraft but also result in further debris being created.To preserve a secure space environment, the active removal or de-orbiting of space debris is an emergent technological challenge. If remedial action is not taken in the near future it will be difficult to sustain human space activities. To overcome this issue, several methods for the removal and de-orbiting of debris have been proposed so far; classified as either contact (e.g., robotic arm, tether net, electrodynamic tether) or contactless methods (e.g., laser, ion beam shepherd), with the contactless methods proving to be more secure.The ion beam shepherd contactless method uses a plasma beam ejected from the satellite to impart a force to the debris thereby decelerating it, which results in it falling to a lower altitude, re-entering the Earth's atmosphere and burning up naturally. However, ejecting the plasma beam toward the debris accelerates the satellite in the opposite direction, which makes it difficult to maintain a consistent distance between debris and the satellite.To safely and effectively remove debris, two propulsion systems have to be mounted on the satellite to eject bi-directional plasma beams. This interferes with a satellite system integration requiring the reduction of a satellite's weight and size.""If the debris removal can be performed by a single high-power propulsion system, it will be of significant use for future space activity,"" said Associate Professor Kazunori Takahashi from Tohoku University in Japan, who is leading research on new technology to remove space debris in collaboration with colleagues at the Australian National University.The Japanese and Australian research group has demonstrated that a helicon plasma thruster can yield the space debris removal operation using a single propulsion system. In the laboratory experiment, the bi-directional ejection of plasma plumes from the single plasma thruster was precisely controlled with a magnetic field and gas injection; then the decelerating force imparted to an object simulating debris was measured whilst maintaining the zero-net force to the thruster (and satellite). The system, having the single plasma thruster, can be operational in three operational modes: acceleration of the satellite; deceleration of the satellite; and debris removal.""The helicon plasma thruster is an electrodeless system, which allows it to undertake long operations performed at a high-power level."" says Takahashi, ""This discovery is considerably different to existing solutions and will make a substantial contribution to future sustainable human activity in space.""Story Source:Materials provided by Tohoku University. ",0.204815,0.180163,0.215596,0.211524,0.180377,0.247941,0.207513,0,0
271,"Smaller, faster and more efficient modulator sets to revolutionize optoelectronic industry ","A research team comprising members from City University of Hong Kong (CityU), Harvard University and renowned information technologies laboratory has successfully fabricated a tiny on-chip lithium niobate modulator, an essential component for the optoelectronic industry. The modulator is smaller, more efficient with faster data transmission and costs less. The technology is set to revolutionise the industry.The electro-optic modulator produced in this breakthrough research is only 1 to 2 cm long and its surface area is about 100 times smaller than traditional ones. It is also highly efficient -- higher data transmission speed with data bandwidth tripling from 35 GHz to 100 GHz, but with less energy consumption and ultra-low optical losses. The invention will pave the way for future high-speed, low power and cost-effective communication networks as well as quantum photonic computation.The research project is titled ""Integrated lithium niobate electro-optic modulators operating at CMOS-compatible voltages"" and was published in the latest issue of the  journal Nature.Electro-optic modulators are critical components in modern communications. They convert high-speed electronic signals in computational devices such as computers to optical signals before transmitting them through optical fibres. But the existing and commonly used lithium niobate modulators require a high drive voltage of 3 to 5V, which is significantly higher than 1V, a voltage provided by a typical CMOS (complementary metal-oxide-semiconductor) circuitry. Hence an electrical amplifier that makes the whole device bulky, expensive and high energy-consuming is needed.Dr Wang Cheng, Assistant Professor in the Department of Electronic Engineering at CityU and co-first author of the paper, and the research teams at Harvard University and Nokia Bell Labs have developed a new way to fabricate lithium niobate modulator that can be operated at ultra-high electro-optic bandwidths with a voltage compatible with CMOS.""In the future, we will be able to put the CMOS right next to the modulator, so they can be more integrated, with less power consumption. The electrical amplifier will no longer be needed,"" said Dr Wang.Thanks to the advanced nano fabrication approaches developed by the team, this modulator can be tiny in size while transmitting data at rates up to 210 Gbit/second, with about 10 times lower optical losses than existing modulators.""The electrical and optical properties of lithium niobate make it the best material for modulator. But it is very difficult to fabricate in nanoscale, which limits the reduction of modulator size,"" Dr Wang explains. ""Since lithium niobate is chemically inert, conventional chemical etching does not work well with it. While people generally think physical etching cannot produce smooth surfaces, which is essential for optical transmission, we have proved otherwise with our novel nano fabrication techniques.""With optical fibres becoming ever more common globally, the size, the performance, the power consumption and the costs of lithium niobate modulators are becoming a bigger factor to consider, especially at a time when the data centres in the information and communications technology (ICT) industry are forecast to be one of the largest electricity users in the world.This revolutionary invention is now on its way to commercialisation. Dr Wang believes that those who look for modulators with the best performance to transmit data over long distances will be among the first to get in touch with this infrastructure for photonics.Dr Wang began this research in 2013 when he joined Harvard University as a PhD student at Harvard's John A. Paulson School of Engineering and Applied Sciences. He recently joined CityU and is looking into its application for the coming 5G communication together with the research team at the State Key Laboratory of Terahertz and Millimeter Waves at CityU.""Millimetre wave will be used to transmit data in free space, but to and from and within base stations, for example, it can be done in optics, which will be less expensive and less lossy,"" he explains. He believes the invention can enable applications in quantum photonics, too.Story Source:Materials provided by City University of Hong Kong. ",0.093461,0.122394,0.119619,0.189667,0.17656,0.137449,0.140408,0,0
272,Enabling 'internet of photonic things' with miniature sensors,"A team of researchers at Washington University in St. Louis is the first to successfully record environmental data using a wireless photonic sensor resonator with a whispering-gallery-mode (WGM) architecture.The photonic sensors recorded data during the spring of 2017 under two scenarios: one was a real-time measurement of air temperature over 12 hours, and the other was an aerial mapping of temperature distribution with a sensor mounted on a drone in a St. Louis city park. Both measurements were accompanied by a commercial thermometer with a Bluetooth connection for comparison purposes. The data from the two compared very favorably.In the grand world of the ""internet of things"" (IoT), there are vast numbers of spatially distributed wireless sensors predominately based on electronics. These devices often are hampered by electromagnetic interference, such as disturbed audio or visual signals caused by a low-flying airplane and a kitchen grinder causing unwanted noise on a radio.But optical sensors are ""immune to electromagnetical interference and can provide a significant advantage in harsh environments,"" said Lan Yang, the Edwin H. & Florence G. Skinner Professor of Electrical & Systems Engineering in the School of Engineering & Applied Science, who led the study from which the findings were published Sept. 5 in Light: Science and Applications.""Optical sensors based on resonators show small footprints, extreme sensitivity and a number of functionalities, all of which lend capability and flexibility to wireless sensors,"" Yang said. ""Our work could pave the way to large-scale application of WGM sensors throughout the internet.""Yang's sensor belongs to a category called whispering gallery mode resonators, so named because they work like the famous whispering gallery in St. Paul's Cathedral in London, where someone on the one side of the dome can hear a message spoken to the wall by someone on the other side. Unlike the dome, which has resonances or sweet spots in the audible range, the sensor resonates at light frequencies and also at vibrational or mechanical frequencies, as Yang and her collaborators recently showed.""In contrast to existing table-sized lab equipment, the mainboard of the WGM sensor is a mere 127 millimeters by 67 millimeters -- roughly 5 inches by 2.5 inches -- and integrates the entire architecture of the sensor system,"" said Xiangyi Xu, the paper's first author and a graduate student in Yang's lab. ""The sensor itself is made of glass and is the size of just one human hair; it is connected to the mainboard by a single optical fiber. A laser light is used to probe a WGM sensor. Light coupled out of the sensor is sent to a photodetector with a transmission amplifier. A processor controls peripherals such as the laser current drive, monitoring circuit, thermo-electric cooler and Wi-Fi unit,"" Xu said.In her WGM, light propagates along the circular rim of a structure by constant internal reflection. Inside the circular rim, light rotates 1 million times. Over that space, light waves detect environmental changes, such as temperature and humidity, for example. The sensor node is monitored by a customized operating systems app that controls the remote system and collects and analyzes sensing signals.Wireless sensors, whether electronic or photonic (light-based), can monitor such environmental factors as humidity, temperature and air pressure. Applications for wireless sensors encompass environmental and health-care monitoring, precision agricultural practices and smart cities' data-gathering, among other possibilities. Smart cities are connected cities driven by internet data-harvesting. Precision agriculture uses digitized geographic information systems for precision agricultural practices such as soil mapping, which enables precise fertilizer and chemical applications and choice of seed selection for more efficient and profitable farming.Yang and her colleagues had to address stability issues, which were handled by the customized operation systems app they developed, and miniaturization of bulky laboratory measurement systems.""We developed a smartphone app to control the sensing system over WiFi,"" Yang said. ""By connecting the sensor system to the internet, we can realize real-time remote control of the system.""In June 2017, Yang and her group mounted the whole system on the outside wall of a building and accumulated a plot of the frequency shift of the resonance. They compared their data with the commercial thermometer.""Thanks to their small size, the capability and flexibility of wireless photonic sensors can be improved by making them mobile,"" Yang said.The researchers also mounted their system on an unmanned drone in May 2017 alongside the commercial thermometer. When the drone flew from one measurement location to others, the resonance frequency of the WGM shifted in response to temperature variations.""The measurements matched well with results from the commercial thermometer,"" she said. ""The successful demonstrations show the potential applications of our wireless WGM sensor in the IoT. There are numerous promising sensing applications possible with WGM technology, including magnetic, acoustic, environmental and medical sensing.""The miniaturization of resonator sensing systems represents an exciting opportunity for IoT, as it will enable IoT to exploit a new class of photonic sensors with unprecedented sensitivity and capabilities,"" said Chenyang Lu, the Fullgraf Professor in the Department of Computer Science & Engineering and a co-author of the paper.Story Source:Materials provided by Washington University in St. Louis. Original written by Tony Fitzpatrick. ",0.0684261,0.0830849,0.124035,0.146969,0.148858,0.110034,0.126351,0,0
273,Understanding urban issues through credit cards,"Digital traces from credit card and mobile phone usage can be used to map urban lifestyles and understand human mobility, according to a report led by UCL, MIT and UC Berkeley.Credit Card Records (CCRs) are currently used to measure similarities in purchasing activity, but for the first time researchers have used the data along with Call Detailed Records (CDRs) to understand the daily rhythms of human mobility and communication.Combining both reveals patterns in citizens' socio-economic behaviours.For the study, published today in Nature Communications, researchers used anonymous and aggregated credit card data from a major city, with the results allowing them to group the urban population into six clusters.Older women dominated the 'Homemaker' cluster and tended to have the least expenditure and mobility, with their core transaction being grocery shops. The 'Commuters' cluster was mainly men who lived the farthest from the city centre.Young people can be split into two groups, with the younger having taxis as their core transaction. The slightly older group also has computer networks and information services, with a higher than average expenditure and operating mainly within the city centre.The research, conducted in collaboration with Grandata and UN Global Pulse, is part of a wider project funded by the United Nations Foundation and the Gates Foundation to investigate the economic, social and health status of women and girls in developing countries.Lead researcher Dr Riccardo Di Clemente (UCL Centre for Advance Spatial Analysis), said: ""Our approach brings together human mobility behavioural dynamics with socio-economic and demographic information.""This may enable policymakers to make more informed decisions about resource allocations to address socio-economic inequality, economic growth and improve social cohesion.""Principal investigator Professor Marta C. Gonzalez (UC Berkeley College of Environmental Design), said: ""Mobile phone data is already used for transport planning. Through this research we found that credit card data can allow us to understand how groups of citizens move and communicate, as well as revealing their spending patterns.""The method gives us a lot of information from data passively collected worldwide, uncovering purchase sequences of uses by type and their role in their space and social network.""The team found that analysing CCRs together with CDRs reveals how women cope with stressors such as recessions and macroeconomic policy shifts, and could be used when planning systems to enable more women to use mobile money rather than carry cash.The method and results can be used when designing and managing effective social protection systems in developing countries and could be particularly valuable for urban development, such as planning infrastructure and mapping neighbourhoods.Story Source:Materials provided by University College London. ",0.070044,0.102802,0.173633,0.255107,0.131663,0.11904,0.171856,0,0
274,"Common Wifi can detect weapons, bombs and chemicals in bags ","Ordinary WiFi can easily detect weapons, bombs and explosive chemicals in bags at museums, stadiums, theme parks, schools and other public venues, according to a Rutgers University-New Brunswick-led study.The researchers' suspicious object detection system is easy to set up, reduces security screening costs and avoids invading privacy such as when screeners open and inspect bags, backpacks and luggage. Traditional screening typically requires high staffing levels and costly specialized equipment.""This could have a great impact in protecting the public from dangerous objects,"" said Yingying (Jennifer) Chen, study co-author and a professor in the Department of Electrical and Computer Engineering in Rutgers-New Brunswick's School of Engineering. ""There's a growing need for that now.""The peer-reviewed study received a best paper award at the 2018 IEEE Conference on Communications and Network Security on cybersecurity. The study -- led by researchers at the Wireless Information Network Laboratory (WINLAB) in the School of Engineering -- included engineers at Indiana University-Purdue University Indianapolis (IUPUI) and Binghamton University.WiFi, or wireless, signals in most public places can penetrate bags to get the dimensions of dangerous metal objects and identify them, including weapons, aluminum cans, laptops and batteries for bombs. WiFi can also be used to estimate the volume of liquids such as water, acid, alcohol and other chemicals for explosives, according to the researchers.This low-cost system requires a WiFi device with two to three antennas and can be integrated into existing WiFi networks. The system analyzes what happens when wireless signals penetrate and bounce off objects and materials.Experiments with 15 types of objects and six types of bags demonstrated detection accuracy rates of 99 percent for dangerous objects, 98 percent for metal and 95 percent for liquid. For typical backpacks, the accuracy rate exceeds 95 percent and drops to about 90 percent when objects inside bags are wrapped, Chen said.""In large public areas, it's hard to set up expensive screening infrastructure like what's in airports,"" Chen said. ""Manpower is always needed to check bags and we wanted to develop a complementary method to try to reduce manpower.""Next steps include trying to boost accuracy in identifying objects by imaging their shapes and estimating liquid volumes, she said.Story Source:Materials provided by Rutgers University. ",0.104325,0.139259,0.179627,0.2198,0.171711,0.153461,0.188346,0,0
275,"Machine learning technique reconstructs images passing through a multimode fiber Approach could improve medical diagnostics, telecommunications","Through innovative use of a neural network that mimics image processing by the human brain, a research team reports accurate reconstruction of images transmitted over optical fibers for distances of up to a kilometer.In The Optical Society's journal for high-impact research, Optica, the researchers report teaching a type of machine learning algorithm known as a deep neural network to recognize images of numbers from the pattern of speckles they create when transmitted to the far end of a fiber. The work could improve endoscopic imaging for medical diagnosis, boost the amount of information carried over fiber-optic telecommunication networks, or increase the optical power delivered by fibers.""We use modern deep neural network architectures to retrieve the input images from the scrambled output of the fiber,"" said Demetri Psaltis, Swiss Federal Institute of Technology, Lausanne, who led the research in collaboration with colleague Christophe Moser. ""We demonstrate that this is possible for fibers up to 1 kilometer long"" he added, calling the work ""an important milestone.""Deciphering the blurOptical fibers transmit information with light. Multimode fibers have much greater information-carrying capacity than single-mode fibers. Their many channels -- known as spatial modes because they have different spatial shapes -- can transmit different streams of information simultaneously.While multimode fibers are well suited for carrying light-based signals, transmitting images is problematic. Light from the image travels through all of the channels and what comes out the other end is a pattern of speckles that the human eye cannot decode.To tackle this problem, Psaltis and his team turned to a deep neural network, a type of machine learning algorithm that functions much the way the brain does. Deep neural networks can give computers the ability to identify objects in photographs and help improve speech recognition systems. Input is processed through several layers of artificial neurons, each of which performs a small calculation and passes the result on to the next layer. The machine learns to identify the input by recognizing the patterns of output associated with it.""If we think about the origin of neural networks, which is our very own brain, the process is simple,"" explains Eirini Kakkava, a doctoral student working on the project. ""When a person stares at an object, neurons in the brain are activated, indicating recognition of a familiar object. Our brain can do this because it gets trained throughout our life with images or signals of the same category of objects, which changes the strength of the connections between the neurons."" To train an artificial neural network, researchers follow essentially the same process, teaching the network to recognize certain images (in this case, handwritten digits) until it is able to recognize images in the same category as the training images that it has not seen before.Learning by the numbersTo train their system, the researchers turned to a database containing 20,000 samples of handwritten numbers, 0 through 9. They selected 16,000 to be used as training data, and kept aside 2,000 to validate the training and another 2,000 for testing the validated system. They used a laser to illuminate each digit and sent the light beam through an optical fiber, which had approximately 4,500 channels, to a camera on the far end. A computer measured how the intensity of the output light varied across the captured image, and they collected a series of examples for each digit.Although the speckle patterns collected for each digit looked the same to the human eye, the neural network was able to discern differences and recognize patterns of intensity associated with each digit. Testing with the set-aside images showed that the algorithm achieved 97.6 percent accuracy for images transmitted through a 0.1 meter long fiber and 90 percent accuracy with a 1 kilometer length of fiber.A simpler methodNavid Borhani, a research-team member, says this machine learning approach is much simpler than other methods to reconstruct images passed through optical fibers, which require making a holographic measurement of the output. The neural network was also able to cope with distortions caused by environmental disturbances to the fiber such as temperature fluctuations or movements caused by air currents that can add noise to the image -- a situation that gets worse with fiber length.""The remarkable ability of deep neural networks to retrieve information transmitted through multimode fibers is expected to benefit medical procedures like endoscopy and communications applications,"" Psaltis said. Telecommunication signals often have to travel through many kilometers of fiber and can suffer distortions, which this method could correct. Doctors could use ultrathin fiber probes to collect images of the tracts and arteries inside the human body without needing complex holographic recorders or worrying about movement. ""Slight movements because of breathing or circulation can distort the images transmitted through a multimode fiber,"" Psaltis said. The deep neural networks are a promising solution for dealing with that noise.Psaltis and his team plan to try the technique with biological samples, to see if that works as well as reading handwritten numbers. They hope to conduct a series of studies using different categories of images to explore the possibilities and limits of their technique.Story Source:Materials provided by The Optical Society. ",0.0611951,0.0711704,0.124513,0.319932,0.137405,0.0964675,0.159145,0,0
276,Moving closer to completely optical artificial neural network Optical training of neural networks could lead to more efficient artificial intelligence,"Researchers have shown that it is possible to train artificial neural networks directly on an optical chip. The significant breakthrough demonstrates that an optical circuit can perform a critical function of an electronics-based artificial neural network and could lead to less expensive, faster and more energy efficient ways to perform complex tasks such as speech or image recognition.""Using an optical chip to perform neural network computations more efficiently than is possible with digital computers could allow more complex problems to be solved,"" said research team leader Shanhui Fan of Stanford University. ""This would enhance the capability of artificial neural networks to perform tasks required for self-driving cars or to formulate an appropriate response to a spoken question, for example. It could also improve our lives in ways we can't imagine now.""An artificial neural network is a type of artificial intelligence that uses connected units to process information in a manner similar to the way the brain processes information. Using these networks to perform a complex task, for instance voice recognition, requires the critical step of training the algorithms to categorize inputs, such as different words.Although optical artificial neural networks were recently demonstrated experimentally, the training step was performed using a model on a traditional digital computer and the final settings were then imported into the optical circuit. In Optica, The Optical Society's journal for high impact research, Stanford University researchers report a method for training these networks directly in the device by implementing an optical analogue of the 'backpropagation' algorithm, which is the standard way to train conventional neural networks.""Using a physical device rather than a computer model for training makes the process more accurate,"" said Tyler W. Hughes, first author of the paper. ""Also, because the training step is a very computationally expensive part of the implementation of the neural network, performing this step optically is key to improving the computational efficiency, speed and power consumption of artificial networks.""A light-based networkAlthough neural network processing is typically performed using a traditional computer, there are significant efforts to design hardware optimized specifically for neural network computing. Optics-based devices are of great interest because they can perform computations in parallel while using less energy than electronic devices.In the new work, the researchers overcame a significant challenge to implementing an all-optical neural network by designing an optical chip that replicates the way that conventional computers train neural networks.An artificial neural network can be thought of as a black box with a number of knobs. During the training step, these knobs are each turned a little and then the system is tested to see if the performance of the algorithms improved.""Our method not only helps predict which direction to turn the knobs but also how much you should turn each knob to get you closer to the desired performance,"" said Hughes. ""Our approach speeds up training significantly, especially for large networks, because we get information about each knob in parallel.""On-chip trainingThe new training protocol operates on optical circuits with tunable beam splitters that are adjusted by changing the settings of optical phase shifters. Laser beams encoding information to be processed are fired into the optical circuit and carried by optical waveguides through the beam splitters, which are adjusted like knobs to train the neural network algorithms.In the new training protocol, the laser is first fed through the optical circuit. Upon exiting the device, the difference from the expected outcome is calculated. This information is then used to generate a new light signal, which is sent back through the optical network in the opposite direction. By measuring the optical intensity around each beam splitter during this process, the researchers showed how to detect, in parallel, how the neural network performance will change with respect to each beam splitter's setting. The phase shifter settings can be changed based on this information, and the process may be repeated until the neural network produces the desired outcome.The researchers tested their training technique with optical simulations by teaching an algorithm to perform complicated functions, such as picking out complex features within a set of points. They found that the optical implementation performed similarly to a conventional computer.""Our work demonstrates that you can use the laws of physics to implement computer science algorithms,"" said Fan. ""By training these networks in the optical domain, it shows that optical neural network systems could be built to carry out certain functionalities using optics alone.""The researchers plan to further optimize the system and want to use it to implement a practical application of a neural network task. The general approach they designed could be used with various neural network architectures and for other applications such as reconfigurable optics.Story Source:Materials provided by The Optical Society. ",0.0652279,0.0808367,0.109874,0.349915,0.132284,0.126471,0.177679,0,0
277,Using light for next-generation data storage,"Tiny, nano-sized crystals of salt encoded with data using light from a laser could be the next data storage technology of choice, following research by Australian scientists.The researchers from the University of South Australia and University of Adelaide, in collaboration with the University of New South Wales, have demonstrated a novel and energy-efficient approach to storing data using light.""With the use of data in society increasing dramatically due to the likes of social media, cloud computing and increased smart phone adoption, existing data storage technologies such as hard drive disks and solid-state storage are fast approaching their limits,"" says project leader Dr Nick Riesen, a Research Fellow at the University of South Australia.""We have entered an age where new technologies are required to meet the demands of 100s of terabyte (1000 gigabytes) or even petabyte (one million gigabytes) storage. One of the most promising techniques of achieving this is optical data storage.""Dr Riesen and University of Adelaide PhD student Xuanzhao Pan developed technology based on nanocrystals with light-emitting properties that can be efficiently switched on and off in patterns that represent digital information. The researchers used lasers to alter the electronic states, and therefore the fluorescence properties, of the crystals.Their research shows that these fluorescent nanocrystals could represent a promising alternative to traditional magnetic (hard drive disk) and solid-state (solid state drive) data storage or blu-ray discs. They demonstrated rewritable data storage in crystals that are 100s of times smaller than that visible with the human eye.""What makes this technique for storing information using light interesting is that several bits can be stored simultaneously. And, unlike most other optical data storage techniques, the data is rewritable,"" says Dr Riesen.This 'multilevel data storage' -- storing several bits on a single crystal -- opens the way for much higher storage densities. The technology also allows for very low-power lasers to be used, increasing its energy efficiency and being more practical for consumer applications.""The low energy requirement also makes this system ideal for optical data storage on integrated electronic circuits,"" says Professor Hans Riesen from the University of New South Wales.""These results showcase the benefits of establishing complementary research capabilities and infrastructure at collaborating universities -- this has been a deliberate strategy in the photonics domain that is bearing fruit across a number of projects,"" says Professor Tanya Monro, DVC-R at the University of South Australia.The technology also has the potential to push forward the boundaries of how much digital data can be stored through the development of 3D data storage.""We think it's possible to extend this data storage platform to 3D technologies in which the nanocrystals would be embedded into a glass or polymer, making use of the glass-processing capabilities we have at IPAS,"" says Professor Heike Ebendorff-Heidepriem, University of Adelaide. ""This project shows the far-reaching applications that can be achieved through transdisciplinary research into new materials.""Dr Riesen says: ""3D optical data storage could potentially allow for up to petabyte level data storage in small data cubes. To put that in perspective, it is believed that the human brain can store about 2.5 petabytes. This new technology could be a viable solution to the great challenge of overcoming the bottleneck in data storage.""The research is published in the open access journal Optics Express.Story Source:Materials provided by University of South Australia. ",0.0775696,0.112107,0.128639,0.224896,0.137109,0.13612,0.163418,0,0
278,Implanting diamonds with flaws offers key technology for quantum communications,"Diamonds are prized for their purity, but their flaws might hold the key to a new type of highly secure communications.Princeton University researchers are using diamonds to help create a communication network that relies on a property of subatomic particles known as their quantum state. Researchers believe such quantum information networks would be extremely secure and could also allow new quantum computers to work together to complete problems that are currently unsolvable. But scientists currently designing these networks face several challenges, including how to preserve fragile quantum information over long distances.Now, researchers have arrived at a possible solution using synthetic diamonds.In an article published this week in the journal Science, the researchers describe how they were able to store and transmit bits of quantum information, known as qubits, using a diamond in which they had replaced two carbon atoms with one silicon atom.In standard communications networks, devices called repeaters briefly store and re-transmit signals to allow them to travel greater distances. Nathalie de Leon, an assistant professor of electrical engineering at Princeton University and the lead researcher, said the diamonds could serve as quantum repeaters for networks based on qubits.The idea of a quantum repeater has been around for a long time, ""but nobody knew how to build them,"" de Leon said. ""We were trying to find something that would act as the main component of a quantum repeater.""The key challenge in creating quantum repeaters has been finding a material that could both store and transmit qubits. So far, the best way to transmit qubits is to encode them in particles of light, called photons. Optical fibers currently used across much of the network already transmit information via photons. However, qubits in an optical fiber can travel only short distances before their special quantum properties are lost and the information is scrambled. It is difficult to trap and store a photon, which by definition moves at the speed of light.Instead, researchers have looked to solids such as crystals to provide the storage. In a crystal, such as a diamond, qubits could theoretically be transferred from photons to electrons, which are easier to store. The key place to carry out such a transfer would be flaws within the diamond, locations where elements other than carbon are trapped in the diamond's carbon lattice. Jewelers have known for centuries that impurities in diamonds produce different colors. To de Leon's team, these color centers, as the impurities are called, represent an opportunity to manipulate light and create a quantum repeater.Previous researchers first tried using defects called nitrogen vacancies -- where a nitrogen atom takes the place of one of the carbon atoms -- but found that although these defects store information, they don't have the correct optical properties. Others then decided to look at silicon vacancies -- the substitution of a carbon atom with a silicon atom. But silicon vacancies, while they could transfer the information to photons, lacked long coherence times.""We asked, 'What do we know about what causes the limitations of these two color centers?',"" de Leon said. ""Can we just design something else from scratch, something that addresses all these problems?""The Princeton-led team and their collaborators decided to experiment with the electrical charge of the defect. Silicon vacancies in theory should be electrically neutral, but it turns out other nearby impurities can contribute electrical charges to the defect. The team thought there might be a connection between the charge state and the ability to keep electron spins in the proper orientation to store qubits.The researchers partnered with Element Six, an industrial diamond manufacturing company, to construct electrically neutral silicon vacancies. Element Six started by laying down layers of carbon atoms to form the crystal. During the process, they added boron atoms, which have the effect of crowding out other impurities that could spoil the neutral charge.""We have to do this delicate dance of charge compensation between things that can add charges or take away charges,"" de Leon said. ""We control the distribution of charge from the background defects in the diamonds, and that allows us to control the charge state of the defects that we care about.""Next, the researchers implanted silicon ions into the diamond, and then heated the diamonds to high temperatures to remove other impurities that could also donate charges. Through several iterations of materials engineering, plus analyses performed in collaboration with scientists at the Gemological Institute of America, the team produced neutral silicon vacancies in diamonds.The neutral silicon vacancy is good at both transmitting quantum information using photons and storing quantum information using electrons, which are key ingredients in creating the essential quantum property known as entanglement, which describes how pairs of particles stay correlated even if they become separated. Entanglement is the key to quantum information's security: recipients can compare measurements of their entangled pair to see if an eavesdropper has corrupted one of the messages.The next step in the research is to build an interface between the neutral silicon vacancy and the photonic circuits to bring the photons from the network into and out of the color center.Ania Bleszynski Jayich, a physics professor at the University of California, Santa Barbara, said the researchers had successfully met a longstanding challenge of finding a diamond flaw with characteristics favorable to working with quantum properties of both photons and electrons.""The success of the authors' materials-engineering approach to identifying promising solid-state defect-based quantum platforms highlights the versatility of solid-state defects and is likely to inspire a more comprehensive and extensive search across a larger cross-section of material and defect candidates,"" said Jayich, who was not involved in the research.The Princeton team included Brendon Rose, a postdoctoral research associate, and graduate students Ding Huang and Zi-Huai Zhang, who are members of de Leon's laboratory. The de Leon team also included postdoctoral research associates Paul Stevenson, Sorawis Sangtawesin, and Srikanth Srinivasan, a former postdoctoral researcher now at IBM. Additional contributions came from staff researcher Alexei Tyryshkin and Professor of Electrical Engineering Stephen Lyon. The team collaborated with Lorne Loudin at the Gemological Institute of America and Matthew Markham, Andrew Edmonds and Daniel Twitchen at Element Six.This work was supported by the National Science Foundation under the EFRI ACQUIRE program (grant No. 1640959) and through the Princeton Center for Complex Materials, a Materials Research Science and Engineering Center (DMR-1420541). This material is also based upon work supported by the Air Force Office of Scientific Research under award number FA9550-17-0158. D.H. acknowledges support from a National Science Scholarship from Agency for Science, Technology, and Research (A*STAR) of Singapore.Story Source:Materials provided by Princeton University, Engineering School. Original written by Catherine Zandonella. ",0.0531176,0.108608,0.104359,0.170579,0.117237,0.0954542,0.128385,0,0
279,"Spectral cloaking could make objects invisible under realistic conditions New approach to invisibility cloaking could be used to secure data transmissions and advance sensing, telecommunications and other applications","Researchers and engineers have long sought ways to conceal objects by manipulating how light interacts with them. A new study offers the first demonstration of invisibility cloaking based on the manipulation of the frequency (color) of light waves as they pass through an object, a fundamentally new approach that overcomes critical shortcomings of existing cloaking technologies.The approach could be applicable to securing data transmitted over fiber optic lines and also help improve technologies for sensing, telecommunications and information processing, researchers say. The concept, theoretically, could be extended to make 3D objects invisible from all directions; a significant step in the development of practical invisibility cloaking technologies.Most current cloaking devices can fully conceal the object of interest only when the object is illuminated with just one color of light. However, sunlight and most other light sources are broadband, meaning that they contain many colors. The new device, called a spectral invisibility cloak, is designed to completely hide arbitrary objects under broadband illumination.The spectral cloak operates by selectively transferring energy from certain colors of the light wave to other colors. After the wave has passed through the object, the device restores the light to its original state. Researchers demonstrate the new approach in Optica, The Optical Society's journal for high impact research.""Our work represents a breakthrough in the quest for invisibility cloaking,"" said Jose Azaea, National Institute of Scientific Research (INRS), Montreal, Canada. ""We have made a target object fully invisible to observation under realistic broadband illumination by propagating the illumination wave through the object with no detectable distortion, exactly as if the object and cloak were not present.""Overcoming previous hurdlesWhen viewing an object, what you are really seeing is the way in which the object modifies the energy of the light waves that interact with it. Most solutions for invisibility cloaking involve altering the paths that light follows so that waves propagate around, rather than through, an object. Other approaches, called ""temporal cloaking,"" tamper with the propagation speed of the light such that the object is temporarily concealed as it passes through the light beam during a prescribed length of time.In either approach, different colors of an incoming light wave must follow different paths as they travel through the cloaking device, thus taking different amounts of time to reach their destination. This alteration of the wave's temporal profile can make it apparent to observers that something is not as it should be.""Conventional cloaking solutions rely on altering the propagation path of the illumination around the object to be concealed; this way, different colors take different amounts of time to traverse the cloak, resulting in easily detectable distortion that gives away the presence of the cloak,"" said Luis Romero Cortes, National Institute of Scientific Research (INRS). ""Our proposed solution avoids this problem by allowing the wave to propagate through the target object, rather than around it, while still avoiding any interaction between the wave and the object.""Rearranging colorsAzaea and his team accomplished this by developing a method to rearrange different colors of broadband light so that the light wave propagates through the object without actually ""seeing"" it. To do this, the cloaking device first shifts the colors toward regions of the spectrum that will not be affected by propagation through the object. For example, if the object reflects green light, then light in the green portion of the spectrum might be shifted to blue so that there would be no green light for it to reflect. Then, once the wave has cleared the object, the cloaking device reverses the shift, reconstructing the wave in its original state.The team demonstrated their approach by concealing an optical filter, which is a device that absorbs light in a prescribed set of colors while allowing other colors of light to pass through, that they illuminated with a short pulse of laser light.The cloaking device was constructed from two pairs of two commercially available electro-optical components. The first component is a dispersive optical fiber, which forces the different colors of a broadband wave to travel at different speeds. The second is a temporal phase modulator, which modifies the optical frequency of light depending on when the wave passes through the device. One pair of these components was placed in front of the optical filter while the other pair was placed behind it.The experiment confirmed that the device was able to transform the light waves in the range of frequencies that would have been absorbed by the optical filter, then completely reverse the process as the light wave exited the filter on the other side, making it look as though the laser pulse had propagated through a non-absorbing medium.Putting cloaking to useWhile the new design would need further development before it could be translated into a Harry Potter-style, wearable invisibility cloak, the demonstrated spectral cloaking device could be useful for a range of security goals. For example, current telecommunication systems use broadband waves as data signals to transfer and process information. Spectral cloaking could be used to selectively determine which operations are applied to a light wave and which are ""made invisible"" to it over certain periods of time. This could prevent an eavesdropper from gathering information by probing a fiber optic network with broadband light.The overall concept of reversible, user-defined spectral energy redistribution could also find applications beyond invisibility cloaking. For example, selectively removing and subsequently reinstating colors in the broadband waves that are used as telecommunication data signals could allow more data to be transmitted over a given link, helping to alleviate logjams as data demands continue to grow. Or, the technique could be used to minimize some key problems in today's broadband telecommunication links, for example by reorganizing the signal energy spectrum to make it less vulnerable to dispersion, nonlinear phenomena and other undesired effects that impair data signals.While the researchers demonstrated spectral cloaking when the object was illuminated from only one spatial direction, Azaea said it should be possible to extend the concept to make an object invisible under illumination from every direction. The team plans to continue their research toward this goal. In the meantime, the team is also working to advance practical applications for single-direction spectral cloaking in one-dimensional wave systems, such as for fiber optics based applications.Story Source:Materials provided by The Optical Society. ",0.0914126,0.0975179,0.117815,0.168608,0.162653,0.127347,0.134719,0,0
280,Multiple lasers could be replaced by a single microcomb,"Every time we send an e-mail, a tweet, or stream a video, we rely on laser light to transfer digital information over a complex network of optical fibers. Dozens of high-performance lasers are needed to fill up the bandwidth and to squeeze in an increasing amount of digital data. Researchers have now shown that all these lasers can be replaced by a single device called a microcomb.A microcomb is an optical device that generates very sharp and equidistant frequency lines in a tiny microphotonic chip. This technology was developed about a decade ago and is now reaching a maturity level that enables new applications, including lidar, sensing, timekeeping and of course optical communications.The soul of a microcomb is a tiny optical cavity that confines laser light in space. Therefore, this technology provides a fantastic playground to explore new nonlinear physical phenomena. These conditions have now been utilised by researchers at Chalmers University of Technology, Sweden, in cooperation with researchers at Purdue University, USA. Victor Torres Company, Associate Professor at Chalmers, is one of the authors of a paper that was recently published in the journal Nature Communications.""We observed that the optical frequencies of the microcomb interfered destructively over a short period of time, thus providing the formation of a wave inside the cavity that resembled a 'hole' of light. The interesting aspect of this waveform is that it yielded a sufficient amount of power per frequency line, which was essential to achieve these high-performance experiments in fiber communication systems,"" says Victor Torres Company.The physical formation of these ""dark"" pulses of light is far from being fully understood, but the researchers believe that their unique properties will enable novel applications in fiber-optic communication systems and spectroscopy.""I will be able to explore these aspects thanks to the financial support of the European Research Council (ERC),"" says Victor Torres Company. ""This is a bright start to better understand the formation of dark pulses in microresonators and their potential use in optical communications. The research could lead to faster and more power-efficient optical communication links in the future.""The results are the fruit of a collaborative effort between researchers at the School of Electrical and Computer Engineering at Purdue University, who fabricated the samples, and the group of Professor Peter Andrekson at the Photonics Laboratory at Chalmers, which hosts world-class experimental facilities for fiber-optic communications research.""Our findings do not represent the first demonstration of a microcomb in fiber communications, but it is the first time that the microcomb has achieved a performance compatible with the strong demands of future communication systems,"" says Peter Andrekson, who is also one of the co-authors of the paper.The main author is Attila Felep, who defended his doctoral thesis ""Fiber-optic communications with microresonator frequency combs"" at the Photonics Laboratory in April.""Working with the microcomb and this experiment has been a great experience. This proof-of-concept demonstration has allowed us to explore the requirements for future chip-scale data transmitters while at the same time proving the potential of this very exciting dark pulse comb technology,"" he says.Story Source:Materials provided by Chalmers University of Technology. ",0.0794147,0.107193,0.123971,0.187211,0.149905,0.128797,0.145272,0,0
281,New 28-GHz transceiver paves the way for future 5G devices,"Scientists at Tokyo Institute of Technology have designed and fabricated a tiny, but incredibly fast, reliable, and accurate 28-GHz transceiver meant for stable high-speed 5G communications. The fabricated transceiver trumps previous designs in various regards by taking a new approach for beam steering.The importance of wireless communications is evident in modern societies, and thus, a lot of work has been done on 5G communications as it is the upcoming big step in mobile networks. The new standard for mobile networks promises data rates and speeds at least an order of magnitude higher than those of 4G (LTE), while even allowing for smaller antennas and radio frequency (RF) transceivers because of the higher frequencies used.Most state-of-the-art transceivers designed for 5G employ RF phase shifters. Accurate phase shifting is important because it allows the transceiver to guide the main lobe of the radiation pattern of the antenna array; in other words, it is used to ""point"" the antenna array towards a specific direction so that both communicating ends (transmitter and receiver) exchange signals with the highest power possible. However, using RF phase shifters brings about certain complications and does not quite make the cut for 5G.Motivated by this, a team of scientists at Tokyo Institute of Technology, led by Associate Professor Kenichi Okada, developed a 28-GHz transceiver employing a local oscillator (LO) phase shifting approach. Instead of using multiple RF phase shifters, they designed a circuit that allows the transceiver to shift the phase of a local oscillator in steps of 0.04e with minimal error. In turn, this allows for a beam-steering resolution of 0.1e, which represents an improvement of an order of magnitude compared with previous designs (meaning that antenna array can be made to precisely point towards the desired direction).What's more, the proposed LO phase shifting approach solves another problem of using multiple RF phase shifters: calibration complexity. RF phase shifters require precise and complex calibration so that their gain remains invariant during phase tuning, which is a very important requirement for the correct operation of the device. The situation becomes worse as the array increases in size. On the other hand, the proposed phase shifting approach results in a gain variation that is very close to zero over the entire 360e range.Amazingly, the transceiver that the research team designed was implemented in a circuit board measuring only 4 mm e 3 mm using minimal components. They compared the performance of their device with that of other state-of-the-art transceivers for 5G. The data rate they achieved was approximately 10 Gb/s higher than that achieved with other methods, while maintaining a phase error and gain variations an order of magnitude lower.The results of this study are being presented at the 2018 IEEE Radio Frequency Integrated Circuits Symposium in the RMo2A session. The proposed LO phase shifting approach will hopefully help to bring forth the much-anticipated deployment of 5G mobile networks and the development of more reliable and speedy wireless communications.Story Source:Materials provided by Tokyo Institute of Technology. ",0.09818,0.100291,0.122371,0.197199,0.206796,0.127908,0.137768,0,0
282,Options to optimize profit in broadband satellite constellations,"Several large telecommunications companies have proposed plans to provide global broadband services by launching hundreds and even thousands of satellites into orbit. Although broadband for everyone sounds like a great idea, it also carries great financial risk, resulting in bankruptcy for some who've tried it. Recent research at the University of Illinois suggests a more cost-effective strategy using regional coverage and staged deployment.""It's actually very easy to determine how many satellites you need to get global coverage. You can do the math by hand,"" said Koki Ho, assistant professor in the Department of Aerospace Engineering at U of I. ""But total global coverage isn't necessarily what a company wants or needs.It might not be the best way for the company to maximize their profit. We looked at different ways to design a satellite constellation that first provides regional coverage and then can be expanded stage by stage, keeping areas with uncertain demand in consideration to maximize profit.""For the study, Ho and his graduate students, Hang Woon lee and Pauline Jakob, divided regions of the Earth into a grid, with each unit measuring 4 degrees latitude by 4 degrees longitude to calculate whether full coverage was being achieved.""Satellites are continually moving in orbit so there isn't one satellite serving a specific unit,"" Ho explained. ""One satellite might move out of the range and another one moves in. As the satellite remains in orbit, the orbit slowing decays and the satellite falls lower so we also calculated for the propulsion needed to return the satellite to its proper orbit and, when needed, to decommission it into outer space.""Ho said that broadband companies in the United States may be looking only at global coverage, but smaller countries, such as Japan, may want to start their business by serving only their own country and having success there before adding more countries or going global.""There is a level of uncertainty in knowing what the next area of interest will be in the market, so we created a multi-staged plan,"" Ho said. ""We started from zero, as if there were no satellites in space. The first stage, might just serve one country. The second stage could add Europe, India, or the 48 contiguous states in the U.S. However, the second stage will take advantage of the first stage and provide more coverage. All of the stages are optimized to minimize the launch and other costs and still provide full coverage to each area of interest.""The financial savings are substantial.One example in the study compares two scenarios: an optimized two-stage deployment strategy with a global coverage constellation and one that deploys all of the satellites in one stage. The expected lifecycle cost for the optimal two-stage configuration is shown to be 28.9 percent and 19.5 percent less than the optimal global-coverage constellation for unique and same satellite design cases, respectively.Story Source:Materials provided by University of Illinois College of Engineering. ",0.166086,0.132011,0.170882,0.186498,0.152668,0.209153,0.153144,0,0
283,Framework to stop cyber attacks on internet-connected cars,"A new study by Maanak Gupta, doctoral candidate at The University of Texas at San Antonio, and Ravi Sandhu, Lutcher Brown Endowed Professor of computer science and founding executive director of the UTSA Institute for Cyber Security (ICS), examines the cybersecurity risks for new generations of smart which includes both autonomous and internet connected cars.""Driverless and connected cars are increasingly becoming a part of our world, where cybersecurity threats are already a reality,"" Sandhu said. ""It's imperative that we support research that addresses these concerns and presents a strong, innovative solution.""Cars with internet connectivity, also known as ""connected cars,"" offer potential for many conveniences and innovations. They could allow for real-time and location-sensitive communication between drivers or even pedestrians, which could help make the roads safer for both. The connectivity could also allow the cars to capture safety and environmental conditions around the vehicle, including road obstructions, accidents, which also enables real-time vehicle-to-vehicle interaction on road.""Connected cars have almost infinite possibilities for creative technological applications,"" Gupta said. ""Companies could even take advantage of the connectivity to implement location-based marketing tactics, providing drivers with nearby sales and offers.""However, the researchers caution that as soon as cars are exposed to internet supported functionality, they are also open to the same cybersecurity threats that loom over other electronic devices, such as computers and cell phones. For this reason, Gupta and Sandhu created an authorization framework for connected cars which provides a conceptual overview of various access control decision and enforcement points needed for dynamic and short-lived interaction in smart cars ecosystem.""There are vulnerabilities in every machine,"" said Gupta. ""We're working to make sure someone doesn't take advantage of those vulnerabilities and turn them into threats. The questions of 'who do I trust?' and 'how do I trust?' are still to be answered in smart cars.""Gupta and Sandhu framework discussed an access control oriented architecture for connected cars and proposed authorization framework, which is a key to determine what and where vulnerabilities can be exploited. They further discuss several approaches to mitigate cyber threats in this ecosystem.Using this framework, the team at ICS is trying to create and use security authorization policies in different access control decision points to prevent cyber attacks and unauthorized access to sensors and data in smart cars.""There are infinite opportunities in this new IoT domain but at the same time cyber threats will have serious implications in smart cars. Can you imagine if someone controls your car steering remotely, or shuts down the engine in the middle of the road?"" Gupta said. ""There should not be absolutely any open end to orchestrate attacks on these cars.""According to Gupta, the authorization framework can also be applied to driverless cars, noting that these vehicles may be even more vulnerable to cyber threats.""If we're going to open the world to cars driven by machines, we must be absolutely certain that they aren't able to be compromised by a malicious attack,"" he said. ""That it what this framework is for.""Story Source:Materials provided by University of Texas at San Antonio. Original written by Joanna Carver. ",0.108484,0.109942,0.157358,0.298737,0.149376,0.179746,0.193801,0,0
284,High-sensitivity microsensors on the horizon Design breakthrough will improve data gathering from hard-to-monitor environments,"Wireless microsensors have enabled new ways to monitor our environment by allowing users to measure spaces previously off limits to research, such as toxic areas, vehicle components, or remote areas in the human body. Researchers, however, have been stymied by limited improvements in the quality of data and sensitivity of these devices stemming from challenges associated with the environments they operate in and the need for sensors with extremely small footprints.A new paper published today in Nature Electronics by researchers at the Advanced Science Research Center (ASRC) at The Graduate Center of The City University of New York, Wayne State University, and Michigan Technological University, explains how new devices with capabilities far beyond those of conventional sensors can be built by borrowing concepts from quantum mechanics.The team, led by Andrea Ale, director of the ASRC's Photonics Initiative and Einstein Professor of Physics at The Graduate Center, and Pai-Yen Chen, professor at Wayne State University, developed a new technique for designing microsensors that allows for significantly enhanced sensitivity and a very small footprint. Their method involves using isospectral parity-time-reciprocal scaling, or PTX symmetry, to design the electronic circuits. A 'reader' is paired with a passive microsensor that meets this PTX symmetry. The pair achieves highly sensitive radio-frequency readings.""In the push to miniaturize the sensors to improve their resolution and enable large-scale networks of sensing devices, improving the sensitivity of microsensors is crucial,"" Ale said. ""Our approach addresses this need by introducing a generalized symmetry condition that enables high-quality readings in a miniaturized footprint.""The work builds on recent advances in the area of quantum mechanics and optics, which have shown that systems symmetric under space and time inversion, or parity-time (PT) symmetric, may offer advantages for sensor design. The paper generalizes this property to a wider class of devices that satisfy a more general form of symmetry -- PTX-symmetry. This type of symmetry, is particularly well-suited to maintain high sensitivity, while drastically reducing the footprint.The researchers were able to show this phenomenon in a telemetric sensor system based on a radio-frequency electronic circuit, which exhibited drastically improved resolution and sensitivity compared to conventional sensors. The microelectromechanical (MEMS)-based wireless pressure sensors share the sensitivity advantages of previous PT-symmetric devices, but crucially the generalized symmetry condition allows both for device miniaturization and enables an efficient realization at low frequencies within a compact electronic circuit.This new approach may allow researchers to overcome the current challenges in deploying ubiquitous networks of long-lasting, unobtrusive microsensors to monitor large areas. In the age of the internet of things and big data, such networks are useful for wireless health, smart cities, and cyber-physical systems that dynamically gather and store large amounts of information for eventual analysis.""Development of wireless microsensors with high sensitivity is one of the major challenging issues for practical uses in bioimplants, wearable electronics, internet-of-things, and cyber-physical systems,"" Chen said. ""While there has been continuous progress in miniature micro-machined sensors, the basics of telemetric readout technique remains essentially unchanged since its invention. This new telemetry approach will make possible the long-sought goal of successfully detecting tiny physical or chemical actuation from contactless microsensors.""Story Source:Materials provided by Advanced Science Research Center, GC/CUNY. ",0.0835616,0.0963702,0.106032,0.183884,0.153071,0.130338,0.138327,0,0
285,"Engineers on a roll toward smaller, more efficient radio frequency transformers ","The future of electronic devices lies partly within the ""internet of things"" -- the network of devices, vehicles and appliances embedded within electronics to enable connectivity and data exchange. University of Illinois engineers are helping realize this future by minimizing the size of one notoriously large element of integrated circuits used for wireless communication -- the transformer.Three-dimensional rolled-up radio frequency transformers take 10 to 100 times less space, perform better when the power transfer ratio increases and have a simpler fabrication process than their 2-D progenitors, according to a paper detailing their design and performance in the journal Nature Electronics.""Transformers are one of the largest and heaviest elements on any circuit board,"" said principal investigator Xiuling Li, a professor of electrical and computer engineering. ""When you pick up an LED light bulb, it feels heavy for its size and that is in part because of the bulky transformer inside. The size of these transformers may become a key obstacle to overcome in the future for wireless communication and IoT.""Transformers use coiled wires to convert input signals to specific output signals for use in devices like microchips. Previous researchers have developed some radio frequency transformers using a stacked conducting material to solve the space problem, but these have limited performance potential. This limited performance is due to inefficient magnetic coupling between coils when they have a high turns ratio, meaning that the primary coil is much longer than the secondary coil, or vice versa, Li said. These stacked transformers need to be made using special materials and are difficult to fabricate, bulky and unbendable -- things that are far from ideal for internet of things devices.The new transformer design uses techniques Li's group previously developed for making rolled inductors. ""We are making 3-D structures using 2-D processing,"" Li said. The team deposits carefully patterned metal wires onto stretched 2-D thin films. Once they release the tension, the 2-D films self-roll into tiny tubes, allowing the primary and secondary wires to coil and nest perfectly inside each other into a much smaller area for optimum magnetic induction and coupling.The nested 3-D architecture leads to high turns ratio coils, Li said. ""A high turns ratio transformer can be used as an impedance transformer to improve the sensitivity of extremely low power receivers, which are expected to be a key enabler for IoT wireless front ends,"" said electrical and computer engineering professor and co-author Songbin Gong.Rolled transformers can also receive and process higher frequency signals than the larger devices.""Wireless communication will be faster and use higher-frequency signals in the future. The current generation of radio frequency transformers simply cannot keep up with the miniaturization requirements and high-frequency operation of the future,"" said lead author and postdoctoral researcher Wen Huang. ""Smaller transformers with more turns allow for better reception of faster, high-frequency wireless signals, as well as high-level integration in IoT applications.""The new transformers have a robust fabrication process -- stable beyond standard foundry temperatures and compatible with industry-standard materials. This study used gold wire, but the team has successfully demonstrated the fabrication of their rolled devices using industry-standard copper.""The next step will be to use thinner and more-conductive metal such as graphene, allowing these devices to be made even smaller and more flexible. This advancement may make it possible for the devices to be woven into the fabrics of high-tech wearables,"" Li said.Story Source:Materials provided by University of Illinois at Urbana-Champaign. Original written by Lois Yoksoulian. ",0.0860118,0.0992675,0.106683,0.167537,0.16075,0.177997,0.129607,0,0
286,Shrinking the synthesizer,"Only a few decades ago, finding a particular channel on the radio or television meant dialing a knob by hand and then making small adjustments to hone in on the right signal. That's no longer the case, thanks to something called a radio-frequency synthesizer, which generates accurate signal frequencies.While radio frequency control has long since been mastered, optical frequency control still exists in the bygone ""tuner knob"" era. This is because optical frequencies are so much higher (200 million megahertz) than radio frequencies (100 megahertz). Setting the absolute frequency, or color, of light emitted from a laser with precision is difficult because laser frequencies tend to drift as radio stations once did.Optical frequency synthesizers provide unprecedented performance but until now have been large, expensive and power hungry. To address these limitations, the Defense Advanced Research Projects Agency (DARPA) in 2014 launched the Direct On-Chip Digital Optical Synthesizer (DODOS) program.Now, a team of UC Santa Barbara scientists -- including John Bowers, a professor of electrical and computer engineering, his colleague Luke Theogarajanand four UCSB graduate student researchers -- has produced significant advances in chip-based integrated photonics and nonlinear optics that enable miniature, energy-efficient components for an optical synthesizer. Their findings appear in the journal Nature.""We took something that occupied a whole optical bench, weighed 50 pounds and used a kilowatt of power and made it orders of magnitude more efficient by integrating the key elements onto silicon photonic integrated circuits,"" said Bowers, UCSB's Fred Kavli Chair in Nanotechnology and director of the campus's Institute for Energy Efficiency.Optical frequency synthesizers have proven extremely valuable in a variety of scientific endeavors, from searching the skies for far-off planets to detecting chemicals through sensitive laser spectroscopy and enabling high-precision light detection and ranging (LIDAR) by using light as a ruler to measure distance.""The development of optical frequency synthesis has significantly enhanced our ability to accurately and precisely measure time and space,"" said Gordon Keeler,the DARPA program manager leading DODOS. ""However, our ability to leverage the technology has been limited. Through DODOS, we're creating technologies that will enable broader deployment and unlock numerous applications. The goal is to shrink laboratory-grade capabilities down to the size of a sugar cube for use in applications like LIDAR, coherent communications, chemical sensing and precision metrology.""Combining a pair of frequency combs, several miniature lasers and other compact optoelectronic components, the researchers were able to replicate the capabilities of a tabletop optical frequency synthesizer on three microchips, each less than 5 mm x 10 mm in size.Theogarajan and his students designed and developed electrical integrated circuits to control the synthesizer, which can tune over 50 nanometers and deliver a frequency stability of 7 x 10-13 after one second of averaging -- matching that of the input reference clock. A stable clock is important.""The more accurate the clock is, the better you can resolve distance or velocity or do navigation,"" Bowers explained. ""GPS has a certain resolution. Your phone will locate you to within a few meters. But if you had a better, more stable clock, you could triangulate more precisely and get better resolution. It's called PNT, for 'precision navigation and timing.'""The scientists created the two miniaturized frequency combs by circulating laser light generated with single-color ""pump"" lasers around optical racetracks fabricated on silicon chips. Doing so correctly can produce many additional colors, yielding a spectrum that looks like a hair comb where each ""tooth"" is an individual color or frequency. This is a significant departure from the tabletop version of an optical frequency synthesizer, which uses fiber optics, specialized mirrors and large mechanical components built by hand to achieve a similar effect.The DODOS program is entering its final phase, during which researchers will work to integrate the individual components with electronics and fabricate a compactly packaged device suitable for use in future military and commercial optical systems. The synthesizer could have applications in navigation, spectroscopy and astronomy.The goal, Bowers noted, is to make this very precise miniature laser cost almost nothing.Story Source:Materials provided by University of California - Santa Barbara. Original written by James Badham. ",0.0834227,0.0885771,0.110589,0.170242,0.167875,0.122947,0.132457,0,0
287,Laser frequency combs may be the future of Wi-Fi Discovery has the potential of increasing the capacity of wireless communications,"Wi-Fi and cellular data traffic are increasing exponentially but, unless the capacity of wireless links can be increased, all that traffic is bound to lead to unacceptable bottlenecks.Upcoming 5G networks are a temporary fix but not a long-term solution. For that, researchers have focused on terahertz frequencies, the submillimeter wavelengths of the electromagnetic spectrum. Data traveling at terahertz frequencies could move hundreds of times faster than today's wireless.In 2017, researchers at the Harvard John A. Paulson School of Engineering and Applied Sciences (SEAS) discovered that an infrared frequency comb in a quantum cascade laser could offer a new way to generate terahertz frequencies. Now, those researchers have uncovered a new phenomenon of quantum cascade laser frequency combs, which would allow these devices to act as integrated transmitters or receivers that can efficiently encode information.The research is published in Optica.""This work represents a complete paradigm shift for the way a laser can be operated,"" said Federico Capasso, the Robert L. Wallace Professor of Applied Physics and Vinton Hayes Senior Research Fellow in Electrical Engineering and senior author of the paper. ""This new phenomenon transforms a laser -- a device operating at optical frequencies -- into an advanced modulator at microwave frequencies, which has a technological significance for efficient use of bandwidth in communication systems.""Frequency combs are widely-used, high-precision tools for measuring and detecting different frequencies -- a.k.a. colors -- of light. Unlike conventional lasers, which emit a single frequency, these lasers emit multiple frequencies simultaneously, evenly spaced to resemble the teeth of a comb. Today, optical frequency combs are used for everything from measuring the fingerprints of specific molecules to detecting distant exoplanets.This research, however, wasn't interested in the optical output of the laser.""We were interested in what was going on inside the laser, in the laser's electron skeleton,"" said Marco Piccardo, a postdoctoral fellow at SEAS and first author of the paper. ""We showed, for the first time, a laser at optical wavelengths operates as a microwave device.""Inside the laser, the different frequencies of light beat together to generate microwave radiation. The researchers discovered that light inside the cavity of the laser causes electrons to oscillate at microwave frequencies -- which are within the communications spectrum. These oscillations can be externally modulated to encode information onto a carrier signal.""This functionality has never been demonstrated in a laser before,"" said Piccardo. ""We have shown that the laser can act as a so-called quadrature modulator, allowing two different pieces of information to be sent simultaneously through a single frequency channel and successively be retrieved at the other end of a communication link.""""Currently, terahertz sources have serious limitations due to limited bandwidth,"" said Capasso. ""This discovery opens up an entirely new aspect of frequency combs and could lead, in the near future, to a terahertz source for wireless communications.""This paper was co-authored by Dmitry Kazakov (Harvard), Noah A. Rubin (Harvard), Paul Chevalier (Harvard), Yongrui Wang (Texas A&M), Feng Xie (Thorlabs), Kevin Lascola (Thorlabs) and Alexey Belyanin (Texas A&M). The research was supported by the Defense Advanced Research Projects and the National Science Foundation.Story Source:Materials provided by Harvard John A. Paulson School of Engineering and Applied Sciences. Original written by Leah Burrows. ",0.101712,0.117468,0.136092,0.200334,0.218116,0.142641,0.15487,0,0
288,"Scientists achieve direct electrocatalytic reduction of carbon dioxide, raising hopes for smart carbon capture ","Chemists at Tokyo Institute of Technology (Tokyo Tech) propose an innovative way to achieve carbon capture using a rhenium-based electrocatalytic system that is capable of reducing low-concentration CO2 (even 1%) with high selectivity and durability, which is a new potential technology to enable direct utilization of CO2 in exhaust gases from heavy industries.Scientists are closer to finding effective ways to reduce CO2 levels -- a vital part of responding to climate change and energy efficiency challenges.A study led by Osamu Ishitani of the Department of Chemistry, Tokyo Tech now demonstrates the advantages of applying electrocatalysis1 to capture low-concentration CO2.In their study published in Chemical Science, Ishitani and colleagues including Hiromu Kumagai and Tetsuya Nishikawa drew on decades of work on honing the capabilities of a rhenium-based catalyst, and demonstrated its ability to reduce low-concentration CO2 in the presence of a chemical called triethanolamine (TEOA).Compared to many previous studies that have focused on reducing pure CO2, few have explored how to improve direct capture of low-concentration CO2 -- a topic that warrants further investigation, considering that plants harness low concentrations of CO2 (about 400 ppm, that is 0.04% of the atmosphere) and exhaust gases from heavy industries typically contain low levels of CO2 (around 3-13%).By avoiding the need for additional energy-consuming condensation processes, their strategy, if scaled up, could provide a more viable, environmentally friendly solution to CO2 capture in many settings.In a series of experiments to assess electrocatalytic activity, the researchers found that at a CO2 concentration of 1%, the rhenium-based catalyst showed very high selectivity (94%) towards carbon monoxide (CO) formation.A likely reason behind the high performance, the researchers say, is the efficient insertion of CO2 into the rhenium-oxygen bond.The researchers aim to continue systematically investigating promising strategies to help reduce real-world CO2 levels.Story Source:Materials provided by Tokyo Institute of Technology. ",0.109402,0.180882,0.187603,0.216326,0.139602,0.182257,0.204193,0,0
289,Light triggers gold in unexpected way Mechanism to control output of a nanoscale antenna,"Rice University researchers have discovered a fundamentally different form of light-matter interaction in their experiments with gold nanoparticles.They weren't looking for it, but students in the lab of Rice chemist Stephan Link found that exciting the microscopic particles just right produced a near-perfect modulation of the light they scatter. The discovery may become useful in the development of next-generation, ultrasmall optical components for computers and antennas.A paper about the research appears in the American Chemical Society journal ACS Nano.The work springs from the complicated interactions between light and plasmonic metal particles that absorb and scatter light extremely efficiently. Plasmons are quasiparticles, collective excitations that move in waves on the surface of some metals when excited by light.The Rice researchers were studying pinwheel-like plasmonic structures of C-shaped gold nanoparticles to see how they responded to circularly polarized light and its rotating electric field, especially when the handedness, or the direction of rotation of the polarization, was reversed. They then decided to study individual particles.""We stripped it back into the simplest possible system where we only had a single arm of the pinwheel, with a single incident light direction,"" said Lauren McCarthy, a graduate student in the Link lab. ""We weren't expecting to see anything. It was a complete surprise when I put this sample on the microscope and rotated my polarization from left- to right-handed. I was like, 'Are these turning on and off?' That's not supposed to happen.""She and co-lead author Kyle Smith, a recent Rice alumnus, had to go deep to figure out why they saw this ""giant modulation.""At the start, they knew shining polarized light at a particular angle onto the surface of their sample of gold nanoparticles attached to a glass substrate would create an evanescent field, an oscillating electromagnetic wave that rides the surface of the glass and traps the light like parallel mirrors, an effect known as a total internal reflection.They also knew that circularly polarized light is composed of transverse waves. Transverse waves are perpendicular to the direction the light is moving and can be used to control the particle's visible plasmonic output. But when the light is confined, longitudinal waves also occur. Where transverse waves move up and down and side to side, longitudinal waves look something like blobs being pumped through a pipe (as illustrated by shaking a Slinky).They discovered the plasmonic response of the C-shaped gold nanoparticles depends on the out-of-phase interactions between both transverse and longitudinal waves in the evanescent field.For the pinwheel, the researchers found they could change the intensity of the light output by as much as 50 percent by simply changing the handedness of the circularly polarized light input, thus changing the relative phase between the transverse and longitudinal waves.When they broke the experiment down to individual, C-shaped gold nanoparticles, they found the shape was important to the effect. Changing the handedness of the polarized input caused the particles to almost completely turn on and off.Simulations of the effect by Rice physicist Peter Nordlander and his team confirmed the explanation for what the researchers observed.""We knew we had an evanescent field and we knew it could be doing something different, but we didn't know exactly what,"" McCarthy said. ""That didn't become clear to us until we got the simulations done, telling us what the light was actually exciting in the particles, and seeing that it actually matches up with what the evanescent field looks like.""It led to our realization that this can't be explained by how light normally operates,"" she said. ""We had to adjust our understanding of how light can interact with these sorts of structures.""The shape of the nanoparticle triggers the orientation of three dipoles (concentrations of positive and negative charge) on the particles, McCarthy said.""The fact that the half-ring has a 100-nanometer radius of curvature means the entire structure takes up half a wavelength of light,"" she said. ""We think that's important for exciting the dipoles in this particular orientation.""The simulations showed that reversing the incident-polarized light handedness and throwing the waves out of phase reversed the direction of the center dipole, dramatically reducing the ability of the half-ring to scatter light under one-incident handedness. The polarization of the evanescent field then explains the almost complete turning on and off effect of the C-shaped structures.""Interestingly, we have in a way come full circle with this work,"" Link said. ""Flat metal surfaces also support surface plasmons like nanoparticles, but they can only be excited with evanescent waves and do not scatter into the far field. Here we found that the excitation of specifically shaped nanoparticles using evanescent waves produces plasmons with scattering properties that are different from those excited with free-space light.""Story Source:Materials provided by Rice University. ",0.106776,0.136317,0.12753,0.134539,0.136953,0.15671,0.139029,0,0
290,Greenhouse gas 'detergent' recycles itself in atmosphere,"A simple molecule in the atmosphere that acts as a ""detergent"" to break down methane and other greenhouse gases has been found to recycle itself to maintain a steady global presence in the face of rising emissions, according to new NASA research. Understanding its role in the atmosphere is critical for determining the lifetime of methane, a powerful contributor to climate change.The hydroxyl (OH) radical, a molecule made up of one hydrogen atom, one oxygen atom with a free (or unpaired) electron is one of the most reactive gases in the atmosphere and regularly breaks down other gases, effectively ending their lifetimes. In this way OH is the main check on the concentration of methane, a potent greenhouse gas that is second only to carbon dioxide in contributing to increasing global temperatures.With the rise of methane emissions into the atmosphere, scientists historically thought that might cause the amount of hydroxyl radicals to be used up on the global scale and, as a result, extend methane's lifetime, currently estimated to be nine years. However, in addition to looking globally at primary sources of OH and the amount of methane and other gases it breaks down, this new research takes into account secondary OH sources, recycling that happens after OH breaks down methane and reforms in the presence of other gases, which has been observed on regional scales before.""OH concentrations are pretty stable over time,"" said atmospheric chemist and lead author Julie Nicely at NASA's Goddard Space Flight Center in Greenbelt, Maryland. ""When OH reacts with methane it doesn't necessarily go away in the presence of other gases, especially nitrogen oxides (NO and NO2). The break down products of its reaction with methane react with NO or NO2 to reform OH. So OH can recycle back into the atmosphere.""Nitrogen oxides are one set of several gases that contribute to recycling OH back into the atmosphere, according to Nicely's research, published in the Journal of Geophysical Research: Atmospheres. She and her colleagues used a computer model informed by satellite observations of various gases from 1980 to 2015 to simulate the possible sources for OH in the atmosphere. These include reactions with the aforementioned nitrogen oxides, water vapor and ozone. They also tested an unusual potential source of new OH: the enlargement of the tropical regions on Earth.OH in the atmosphere also forms when ultraviolet sunlight reaches the lower atmosphere and reacts with water vapor (H2O) and ozone (O3) to form two OH molecules. Over the tropics, water vapor and ultraviolet sunlight are plentiful. The tropics, which span the region of Earth to either side of the equator, have shown some evidence of widening farther north and south of their current range, possibly due to rising temperatures affecting air circulation patterns. This means that the tropical region primed for creating OH will potentially increase over time, leading to a higher amount of OH in the atmosphere. This tropical widening process is slow, however, expanding only 0.5 to 1 degree in latitude every 10 years. But the small effect may still be important, according to Nicely.She and her team found that, individually, the tropical widening effect and OH recycling through reactions with other gases each comprise a relatively small source of OH, but together they essentially replace the OH used up in the breaking down of methane.""The absence of a trend in global OH is surprising,"" said atmospheric chemist Tom Hanisco at Goddard who was not involved in the research. ""Most models predict a 'feedback effect' between OH and methane. In the reaction of OH with methane, OH is also removed. The increase in NO2 and other sources of OH, such as ozone, cancel out this expected effect."" But since this study looks at the past thirty-five years, it's not guaranteed that as the atmosphere continues to evolve with global climate change that OH levels will continue to recycle in the same way into the future, he said.Ultimately, Nicely views the results as a way to fine-tune and update the assumptions that are made by researchers and climate modelers who describe and predict how OH and methane interact throughout the atmosphere. ""This could add clarification on the question of will methane concentrations continue rising in the future? Or will they level off, or perhaps even decrease? This is a major question regarding future climate that we really don't know the answer to,"" she said.Story Source:Materials provided by NASA/Goddard Space Flight Center. ",0.120048,0.204868,0.201518,0.172995,0.134485,0.182573,0.185266,0,0
291,How microscopic machines can fail in the blink of an eye,"How long can tiny gears and other microscopic moving parts last before they wear out? What are the warning signs that these components are about to fail, which can happen in just a few tenths of a second? Striving to provide clear answers to these questions, researchers at the National Institute of Standards and Technology (NIST) have developed a method for more quickly tracking microelectromechanical systems (MEMS) as they work and, just as importantly, as they stop working.By using this method for microscopic failure analysis, researchers and manufacturers could improve the reliability of the MEMS components that they are developing, ranging from miniature robots and drones to tiny forceps for eye surgery and sensors to detect trace amounts of toxic chemicals.Over the past decade, researchers at the National Institute of Standards and Technology (NIST) have measured the motion and interactions between MEMS components. In their newest work, the scientists succeeded in making these measurements a hundred times faster, on the scale of thousandths, rather than tenths, of a second.The faster time scale enabled the researchers to resolve fine details of the transient and erratic motions that may occur before and during the failure of MEMS. The faster measurements also allowed repetitive testing -- necessary for assessing the durability of the miniature mechanical systems -- to be conducted more quickly. The NIST researchers, including Samuel Stavis and Craig Copeland, described their work in the Journal of Microelectromechanical Systems.As in their previous work, the team labeled the MEMS components with fluorescent particles to track their motion. Using optical microscopes and sensitive cameras to view and image the light-emitting particles, the researchers tracked displacements as small as a few billionths of a meter and rotations as tiny as several millionths of a radian. One microradian is the angle corresponding to an arc of about 10 meters along the circumference of the earth.A faster imaging system and larger fluorescent particles, which emit more light, provided the scientists with the tools to perform their particle-tracking measurements a hundred times more rapidly than before.""If you cannot measure how the components of a MEMS move at the relevant length and time scales, then it is difficult to understand how they work and how to improve them,"" Copeland said.In their test system, Stavis, Copeland and their colleagues tested part of a microelectromechanical motor. The test part snapped back and forth, rotating a gear through a ratchet mechanism. Although this system is one of the more reliable MEMS that transfer motion through parts in sliding contact, it nonetheless can exhibit such problems as erratic performance and untimely failure.The team found that the jostling of contacting parts in the system, whether contact between the parts occurred at only one point or shifted between several points, and wear of the contacting surfaces, could all play a key role in the durability of MEMS.""Our tracking method is broadly applicable to study the motion of microsystems, and we continue to advance it,"" said Stavis.Story Source:Materials provided by National Institute of Standards and Technology (NIST). ",0.0927242,0.105492,0.141304,0.179548,0.133812,0.149907,0.153104,0,0
292,New study reveals common table salt may have been crucial for the origins of life,"One of the most fundamental unexplained questions in modern science is how life began. Scientists generally believe that simple molecules present in early planetary environments were converted to more complex ones that could have helped jumpstart life by the input of energy from the environment. Scientists consider the early Earth was suffused with many kinds of energy, from the high temperatures produced by volcanoes to the ultraviolet radiation beamed down by the sun. One of the most classic studies of how organic compounds could have been made on the early Earth is the Miller-Urey experiment, which showed you electrical discharges simulating lightning can help make a variety of organic compounds, including amino acids, which are basic building blocks of all life. Another important source of energy in planetary environments is high-energy radiation, which has various sources including radioactive decay of naturally occurring chemical elements such as uranium and potassium. Research led by Yi Ruiqin and Albert Fahrenbach from the Earth-Life Science Institute (ELSI) at Tokyo Institute of Technology, Japan, has recently shown that a variety of compounds useful for the synthesis of RNA, are produced when simple compound, combined with sodium chloride, are exposed to gamma rays.This work importantly brings us one step closer to understanding how RNA, which is widely thought to be a candidate molecule for helping start life, could have arisen abiotically on early Earth. Due to its complexity, making RNA ""from scratch"" under primitive solar system conditions is no easy task. Biology is great at it, because it has evolved over billions of years to do the job with amazing efficiency. Before life emerged, there would have been little in the environment that would have assisted in making RNA. These researchers found that sodium chloride -- or common table salt -- can assist in making the necessary building blocks for RNA. Sodium chloride is the chemical compound that makes the sea salty, thus it is highly likely this process could occur on primitive planets, including Earth.The most challenging aspect of this work was figuring out that salt, specifically the chloride component, played a crucial role in these reactions. Typically, chemists ignore chloride in their reactions. When chemists conduct reactions in water, it is highly likely at least some chloride is in there anyway, though most of the time it just sits idly by as a ""spectator."" It often doesn't play a significant role in the reactions chemists are interested in, it's just part of the background a lot of the time. These researchers found out though, that this was not the case in their experiments, and it took them some time to figure that out. What they eventually deduced was that the ionizing radiation they were using as the energy source to drive their reactions causes chloride to lose an electron and become what is known as a ""radical."" As the name suggests, the chloride is then no longer so mild-mannered and becomes much more chemically reactive. Once the chloride is activated by gamma radiation, it is free to help construct other high energy compounds which finally can help build up complex RNA molecules.While these researchers haven't yet coaxed their reactions all the way to RNA, this work shows that there is now nothing in principle which should stop this from occurring. The question now is not so much how to make all the necessary building blocks to make RNA, but how to combine them in a ""warm little pond"" to make the first RNA polymers. One of the major challenges to this is understanding how other molecules, that is, other than those important for making RNA, might affect this process. The authors think this could be pretty ""messy"" chemistry in the sense that a lot of other molecules, which could interfere with this process, would be made at the same time. Whether these other molecules will interfere with RNA synthesis, or even have a beneficial effect, is the future focus of these scholars' research. Understanding very complex mixtures of chemicals is not only a challenge in origins of life research, but a major challenge for organic chemistry in general.Story Source:Materials provided by Tokyo Institute of Technology. ",0.0477885,0.141955,0.102738,0.124572,0.079856,0.0942707,0.114896,0,0
293,Jumpin' droplets! Researchers seek to improve efficiency of condensers Research describes strategy for keeping condensed droplets from coalescing into a film,"Hold a cold drink on a hot day, and watch as small droplets form on the glass, eventually coalescing into a layer of moisture (and prompting you to reach for a coaster).This basic physical process, condensation, is what refrigerators and air conditioners use to remove heat from vapor by turning it into a liquid. Just like the cold glass, the surfaces of metal condensers form thin layers of moisture as they work.And that's a problem. The liquid layer acts as a thermally resistant barrier between the warm vapor and the cold condenser surface, decreasing the condenser's heat transfer efficiency. Ideally, the droplets on the condenser, instead of coalescing, would simply bead up and move away, making way for more vapor to contact the condenser and turn into liquid.Materials scientists at Colorado State University have spent time thinking about this problem. They've published the fundamental physics of a possible solution in the journal Science Advances. Their new strategy could potentially increase the efficiency of condensers, used in many domestic and industrial products.A team led by Arun Kota, assistant professor in mechanical engineering and the School of Biomedical Engineering, has figured out how to keep condensed droplets from coalescing into a film, and to make the droplets jump high enough to move away from the condenser surface.""We believe that our strategy has the potential to enable next-generation condensers with improved efficiency,"" Kota said. ""Our strategy is simple, power-free and scalable."" The experiments and numerical simulations were carried out by the paper's co-first authors: CSU graduate student Hamed Vahabi and postdoctoral researcher Wei Wang.Their solution is a combination of creativity, chemistry and physics, along with Kota's lab's extensive research in ""superomniphobic"" surfaces that repel many different kinds of liquids. The researchers worked out the physics of using a superomniphobic surface with knife-like ridges to form these jumping droplets.When droplets coalesce on these superomniphobic ridges, the ridge architecture causes the new, larger droplet to jump away with significantly higher kinetic energy compared to surfaces with no ridge architecture. The researchers envision that condensers dotted with such superomniphobic ridges can remove condensed droplets more efficiently, leading to higher heat transfer efficiency.Other researchers have demonstrated the ability to make droplets jump this way, but the CSU work is set apart by combining the superomniphobic surface with the specific ridge architecture. Furthermore, they made the jumping-droplet phenomenon work with a wide range of liquids, including those with low surface tensions and high viscosities. They've also shown that the concept works at many sizes, from macroscopic down to micron length scales and potentially even sub-micron length scales.Story Source:Materials provided by Colorado State University. ",0.0992788,0.151365,0.144859,0.175787,0.124293,0.159107,0.177899,0,0
294,A golden age for particle analysis Researchers develop a process to measure nanorods quickly and accurately,"Process engineers at Friedrich-Alexander-Universitet Erlangen-Nernberg (FAU) have developed a method which allows the size and shape of nanoparticles in dispersions to be determined considerably quicker than ever before. Based on gold nanorods, they demonstrated how length and diameter distributions can be measured accurately in just one step instead of the complicated series of electron microscopic images which have been needed up until now. Nanoparticles from precious metals are used, for example, as catalysts and contrast agents for diagnosing cancer.Even in the Middle Ages, gold particles were used to create vibrant red and blue colours, for example to illustrate biblical scenes in stained glass windows. This effect is caused by the interaction between the electromagnetic fields of the incoming light with the electrons in the metal which are made to vibrate collectively. Nowadays, nanoparticles of gold or silver are of interest for applications in biotechnology and as catalysts whilst their optical properties are also still being used, for example for medical imaging purposes, where they act as a contrast agent for diagnosing tumours. The particles are synthesised specially for the various purposes, as their properties depend on their size, shape, surface, inner structure and composition.Monitoring this synthesis process is very complex: whilst it is relatively straightforward to determine the size of the nanoparticles using optical measuring techniques, a great many electron microscopic images have to be analysed in a detailed and time-consuming process before the shape of the particle can be determined. This hinders the development of new manufacturing and processing methods, as time-consuming measurements are needed to keep track of any changes to the size or properties of the particles.Determining size and shape in just one stepTogether with working groups from the field of mathematics led by Dr. Lukas Pflug and Prof. Dr. Michael Stingl, Professorship of Mathematical Optimisation, and physical chemistry, led by Prof. Dr. Carola Kryschi, Chair of Physical Chemistry, process engineers at FAU led by Simon Wawra and Prof. Dr. Wolfgang Peukert, Chair of Particle Technology, have developed a new method for measuring the length and diameter distribution of plasmonic gold nanorods in one single experiment. In a first step, the particles are dispersed in water in an ultrasonic bath and made to sink using centrifugation. At the same time they are targeted with flashes of light and their spectral properties recorded using a detector. 'By combining multi-wavelength absorption optics and analytical ultracentrifugation, we were able to measure the optical and sedimentary properties of the nanorods simultaneously,' explains Prof. Dr. Wolfgang Peukert. The researchers based their analysis method on the fact that both sedimentation speed and strength of light absorption depend on the diameter and length of nanorods. 'The distribution of length, diameter, aspect ratio, surface and volume can be derived directly as a result,' explains Wolfgang Peukert.Method can also be used for particles of other shapesThe method developed at FAU is not only restricted to nanoparticles made of precious metals. It can be used on a number of plasmonically active materials and can also be extended to other geometric shapes. During synthesis, sphere-shaped particles are created at the same time as nanorods, and their distribution and percentage by mass in the sample can also be measured accurately. Peukert: 'Our new method allows a comprehensive and quantitative analysis of these highly interesting particle systems. We believe that our work will contribute to being able to characterise plasmonic nanoparticles rapidly and reliably during synthesis and in a number of applications.'Story Source:Materials provided by University of Erlangen-Nuremberg. ",0.0711441,0.128618,0.129725,0.150942,0.122267,0.11174,0.140478,0,0
295,New device for manipulating and moving tiny objects with light,"When you shine a beam of light on your hand, you don't feel much, except for a little bit of heat generated by the beam. When you shine that same light into a world that is measured on the nano- or micro scale, the light becomes a powerful manipulating tool that you can use to move objects around -- trapped securely in the light.Researchers from the Structured Light group from the School of Physics at the University of the Witwatersrand in Johannesburg, South Africa, have found a way to use the full beam of a laser light, to control and manipulate minute objects such as single cells in a human body, tiny particles in small volume chemistry, or working on future on-chip devices.While the specific technique, called holographic optical trapping and tweezing, is not new, the Wits Researchers found a way to optimally use the full force of the light -- including vector light that was previously unavailable for this application. This forms the first vector holographic trap.""Previously holographic traps were limited to particular classes of light (scalar light), so it is very exciting that we can reveal a holistic device that covers all classes of light, including replicating all previous trapping devices,"" Professor Andrew Forbes, team leader of the collaboration and Distinguished Professor in the School of Physics where he heads up the Wits Structured Light Laboratory.""What we have done is that we have demonstrated the first vector holographic optical trapping and tweezing system. The device allows micrometer sized particles, such as biological cells, to be captured and manipulated only with light.""The final device could trap multiple particles at once and move them around just with vector states of light. The experiments for this study were performed by Nkosi Bhebhe as part of his doctoral studies. The work is published in Nature's on-line journal, Scientific Reports.In conventional optical trapping and tweezing systems, light is focused very tightly into a small volume that contains small particles, such as biological cells. At this small scale (typically micro- or nanometres) the forces that the light can exert are significant, so particles can be trapped by the light and then controlled. As the light is moved, the particles will move with it. This idea won American scientist Arthur Ashkin the 2018 Nobel prize in Physics. Originally the light was controlled mechanically with stages and mirrors, but the idea was later improved on by moving the light around holographically, that is, by using computer generated holograms to control the light without moving parts, thereby controlling the particles. Until now only special classes of laser beams, called scalar beams, could be used in such holographic traps.In their paper titled: A vector holographic optical trap the Wits researchers showed how to create and control any pattern of light holographically, and then used this to form a new optical trapping and tweezing device.""In particular the device could work with both the traditional laser beams (scalar beams) as well as more complex vector beams. Vector beams are highly topical and have found many applications, but no vector holographic trap was possible until now,"" says Forbes.The Wits researchers demonstrate their new trap by holographically controlling both scalar and vector beams in the same device, advancing the state-of-the-art and introducing a new device to the community. The group expects the new device to be useful in controlled experiments in the micro- and nano-worlds, including single cell studies in biology and medicine, small volume chemical reactions, fundamental physics and for future on-chip devices.Having previously shown that it is possible to create hundreds of custom light patterns from one hologram, the research brings together their prior work on holographic control of light with the application of optical trapping and tweezing.How a holographic optical trap worksIn a conventional optical trap, light is focused very tightly so that it can exert forces on matter. The matter, say a small particle, gets trapped in the light. When the light is moved by mirrors or mechanical stages, so the particle moves with it. This is referred to as optical trapping (capturing the particle) and optical tweezing, moving the particle as if with tweezers, but in this case tweezers made of light. To make the control less mechanical, researchers used holograms to control the light. With spatial light modulators one could encode structured light patterns and move those patterns around inside the trap, so that many particles could be trapped and moved simultaneously. This opened many new exciting fields, but the final holographic optical traps (HOTs) were limited to only scalar beams of light, a tiny fraction of what was possible. The other class of optical beams, vector states, were considered not possible to control holographically. With the new vector HOT, all states of light can now be used. Time will time what this means for the community at large.Story Source:Materials provided by University of the Witwatersrand. ",0.100459,0.13464,0.151906,0.191515,0.162195,0.153585,0.17263,0,0
296,Creating rings in natural antibiotic synthesis,"Scientists at the University of Bristol have revealed the secrets of the key ring forming cascade in the biosynthesis of a globally used antibiotic. They hope their findings could lead to the development of antibiotics with improved properties and new biocatalysts for the clean and efficient synthesis of medicinally important molecules.With the widespread problem of increasing resistance to existing antibiotics, there is an urgent need for the discovery and development of new cost-effective ways to combat bacterial infections.Mupirocin is an antibiotic widely used as a topical treatment for bacterial skin infections such as impetigo. It is produced commercially using the microorganism Pseudomonas fluorescens which has developed complex biosynthetic machinery to produce the final molecule assembled on the tetra-substituted 6-membered ring essential for antibiotic activity.Now, scientists at BrisSynBio, a BBSRC and EPSRC-funded research centre at the University of Bristol, have revealed for the first time an enzymatic reaction cascade that generates selectively this 6-membered ring from a complex linear starting material.The multidisciplinary study, described in Nature Catalysis today (26 November), shows that the enzyme MupW is responsible for a chemically challenging transformation to give a new intermediate which a second enzyme, MupZ, then converts to the 6-membered ring. Without MupZ (which itself forms beautiful hexagonal crystals, shown in the image), a 5-membered ring is produced which has no antibiotic activity.This latest work builds on earlier research carried out by Bristol's Professor Tom Simpson FRS and Professor Chris Thomas from the University of Birmingham.Professor Matt Crump from the School of Chemistry was one of several supervisors who worked on the study. He said: ""This work serves as an example of the importance of nuclear magnetic resonance (NMR) spectroscopy in chemical and biochemical studies. Access to the 700 MHz NMR spectrometer, funded by BrisSynBio, enabled the identification of key intermediates in the pathway and opens future opportunities in synthetic biology which could not be achieved without the sensitivity of cutting-edge instrumentation.""This reaction cascade would be difficult (arguably impossible!) to achieve using existing synthetic methodology and the team is now investigating these biocatalysts to prepare more stable analogues of the active component of the antibiotic mupirocin.Professor Chris Willis, from the School of Chemistry who jointly led the study, said: ""This important breakthrough was achieved by an interdisciplinary team effort comprising talented postdoctoral researchers and postgraduates here at Bristol on a journey that included structural biology, synthetic and analytical chemistry in parallel with molecular modelling to unveil an overall transformation not previously reported for this family of enzymes.""Dr Paul Race and Dr Marc van der Kamp from Bristol's School of Biochemistry were also part of the supervisory team carrying out the study, which was funded by the BBSRC and EPSRC via the Bristol Centre for Synthetic Biology.Story Source:Materials provided by University of Bristol. ",0.0993453,0.172373,0.143936,0.187879,0.131949,0.175085,0.171374,0,0
297,"Disordered materials could be hardest, most heat-tolerant carbides Computational simulations predict new class of carbides that could disrupt industries from machinery to aerospace","Materials scientists at Duke University and UC San Diego have discovered a new class of carbides expected to be among the hardest materials with the highest melting points in existence. Made from inexpensive metals, the new materials may soon find use in a wide range of industries from machinery and hardware to aerospace.A carbide is traditionally a compound consisting of carbon and one other element. When paired with a metal such as titanium or tungsten, the resulting material is extremely hard and difficult to melt. This makes carbides ideal for applications such as coating the surface of cutting tools or parts of a space vehicle.A small number of complex carbides containing three or more elements also exist, but are not commonly found outside of the laboratory or in industrial applications. This is mostly due to the difficulties of determining which combinations can form stable structures, let alone have desirable properties.A team of materials scientists at Duke University and UC San Diego have now announced the discovery of a new class of carbides that join carbon with five different metallic elements at once. The results appear online on November 27 in the journal Nature Communications.Achieving stability from the chaotic mixture of their atoms rather than orderly atomic structure, these materials were computationally predicted to exist by the researchers at Duke University and then successfully synthesized at UC San Diego.""These materials are harder and lighter in weight than current carbides,"" said Stefano Curtarolo, professor of mechanical engineering and materials science at Duke. ""They also have very high melting points and are made out of relatively cheap material mixtures. This combination of attributes should make them very useful to a wide range of industries.""When students learn about molecular structures, they're shown crystals like salt, which resembles a 3-D checkerboard. These materials gain their stability and strength through regular, ordered atomic bonds where the atoms fit together like pieces of a jigsaw puzzle.Imperfections in a crystalline structure, however, can often add strength to a material. If cracks start to propagate along a line of molecular bonds, for example, a group of misaligned structures can stop it in its tracks. Hardening solid metals by creating the perfect amount of disorder is achieved through a process of heating and quenching called annealing.The new class of five-metal carbides takes this idea to the next level. Jettisoning any reliance on crystalline structures and bonds for their stability, these materials rely completely on disorder. While a pile of baseballs won't stand on its own, a pile of baseballs, shoes, bats, hats and gloves just might.The difficulty lies in predicting which combination of elements will stand firm. Trying to make new materials is expensive and time-consuming. Computing atomic interactions through first principle simulations is even more so. And with five slots for metallic elements and 91 to choose from, the number of potential recipes quickly becomes daunting.""To figure out which combinations will mix well, you have to do a spectral analysis based on entropy,"" said Pranab Sarker, a postdoctoral associate in Curtarolo's lab and one of the first authors of the paper. ""Entropy is incredibly time-consuming and difficult to calculate by building a model atom-by-atom. So we tried something different.""The team first narrowed the field of ingredients to eight metals known to create carbide compounds with high hardness and melting temperatures. They then calculated how much energy it would take for a potential five-metal carbide to form a large set of random configurations.If the results were spread far apart, it indicated that the combination would likely favor a single configuration and fall apart -- like having too many baseballs in the mix. But if there were many configurations tightly clumped together, it indicated the material would likely form many different structures all at once, providing the disorder needed for structural stability.The group then tested its theory by getting colleague Kenneth Vecchio, professor of NanoEngineering at UC San Diego, to attempt to actually make nine of the compounds. This was done by combining the elements in each recipe in a finely powdered form, pressing them at temperatures up to 4,000 degrees Fahrenheit and running 2000 Amps of current directly through them.""Learning to process these materials was a difficult task,"" said Tyler Harrington, a PhD student in Vecchio's lab and co-first author of the paper. ""They behave differently than any materials that we've ever dealt with, even the traditional carbides.""They chose the three recipes their system deemed most likely to form a stable material, the two least likely, and four random combinations that scored in between. As predicted, the three most likely candidates were successful while the two least likely were not. Three of the four intermediate scorers also formed stable structures. While the new carbides are all likely to have desirable industrial properties, one improbable combination stood out -- a combination of molybdenum, niobium, tantalum, vanadium and tungsten called MoNbTaVWC5 for short.""Getting this set of elements to combine is basically like trying to squeeze together a bunch of squares and hexagons,"" said Cormac Toher, an assistant research professor in Curtarolo's laboratory. ""Going on intuition alone, you'd never think that combination would be feasible. But it turns out that the best candidates are actually counterintuitive.""""We don't know its exact properties yet because it hasn't been fully tested,"" said Curtarolo. ""But once we get it into the laboratory in the next couple of months, I wouldn't be surprised if it turned out to be the hardest material with the highest melting point ever made.""""This collaboration is a team of researchers focused on demonstrating the unique and potentially paradigm-changing implications of this new approach,"" said Vecchio. ""We are using innovative approaches to first-principles modeling combined with state-of-the-art synthesis and characterization tools to provide the integrated 'closed-loop' methodology so necessary for advanced materials discovery.""Story Source:Materials provided by Duke University. ",0.0332436,0.0882128,0.0839899,0.124735,0.0627866,0.0824123,0.109605,0,0
298,Mere sunlight can be used to eradicate pollutants in water Chemists present new process to produce hydrated electrons,"Advances in environmental technology: You don't need complex filters and laser systems to destroy persistent pollutants in water. Chemists at Martin Luther University Halle-Wittenberg (MLU) have developed a new process that works using mere sunlight. The process is so simple that it can even be conducted outdoors under the most basic conditions. The chemists present their research in the journal Chemistry -- a European Journal.The chemists at MLU rely on electrons moving freely in water, so-called hydrated electrons, to degrade dissolved pollutants. ""These electrons are extremely reactive and can be used for a plethora of reactions. They break down even the most recalcitrant pollutants,"" explains MLU-chemist Professor Martin Goez. For this to work, however, the electrons have to be released from the molecular compounds in which they are usually tightly bound. Until now, generating those electrons has required complex and expensive high-power lasers. Goez's research group has been working for years on finding an alternative to this. A few months ago they introduced a new approach which only required a green light-emitting diode as the sole energy source. Vitamin C and traces of a metal complex as the catalyst were used to bring about the desired reaction. However, the catalyst had to be enclosed in tiny containers known as micelles. This reduced the reaction's efficiency, and the micelle molecules themselves were only partially biodegradable.The group therefore looked for a way to avoid these additives. Ultimately, they found the answer in a highly charged anionic catalyst based on a ruthenium-metal complex. By combining this with urate (a salt of uric acid), the researchers were able to effect the desired reaction in water without the need of micelles by exploiting the Coulombic repulsions. Further investigations have revealed that the new process is not only a very efficient way to produce hydrated electrons but also has a wide range of applications. ""Our new approach is so simple that it doesn't even need to take place in a lab,"" says Goez. His group conducted a field trial in a meadow and tested their new approach in water contaminated with chloroacetic acid. The result: the pollutants were eliminated in a small sample of water even when there was only a moderate amount of sunshine. Follow-up studies will examine whether the method developed by the chemists in Halle can also be used for large-scale removal of pollutants.Story Source:Materials provided by Martin-Luther-Universitet Halle-Wittenberg. ",0.0729793,0.177604,0.144937,0.146539,0.123396,0.124933,0.144318,0,0
299,Molecular motors: Chemical carousel rotates in the cold,"Chemists at Ludwig-Maximilians-Universitaet (LMU) in Munich have developed the first molecular motor that can be powered by light alone. Its operation is therefore essentially independent of the temperature.Molecular motors, which rotate unidirectionally in response to an external energy input, constitute an important class of components for future applications in the field of nanotechnology. Molecules whose structure and spatial conformation can be altered by light are particularly promising candidates for this task. However, all light-driven molecular motors so far described are dependent on reactions that require the input of heat and are therefore dependent on a certain minimum environmental temperature. LMU chemist Henry Dube has now achieved a decisive breakthrough in this respect. Together with his student Aaron Gerwien, he has developed the first molecular motor that rotates on exposure to light alone, irrespective of the temperature. Not only is its operation not conditional on a specific minimum temperature -- it actually rotates faster at lower temperatures. This unique characteristic of the new molecule could significantly extend the range of applications available to future nanomachines. The LMU researchers have just reported their findings in the Journal of the American Chemical Society.The essential property that turns a synthetic chemical into a molecular rotary motor is that an external energy source can cause some component of the molecule to rotate unidirectionally. Each 360e rotation takes place in discrete steps like the ticking of a clock hand. The tricky part is to ensure that each forward step does not go into reverse. All of the molecular motors so far described have used what is called a ratchet mechanism to prevent such reversals. The idea is that after each forward step a ratcheting step alters the configuration of the molecule in such a way that the reverse reaction is sterically inhibited. The conformational changes necessary to achieve this are normally induced by heat. As a result, the rate of rotation is dependent on the ambient temperature, and below a certain minimum temperature the movement ceases.Like earlier motor systems developed by Dube and his colleagues, the new motor is based on an organic substance called hemithioindigo. This molecule is made up of two different carbon skeletons, which are connected by a mobile double bond. ""We have succeeded in modifying the molecule such that a complete rotation of one of the structural modules relative to the other requires only three reaction steps,"" says Dube. Each rotational step is activated by visible light and there is no need for intermediate, thermally driven ratchet steps. Indeed, all three steps involved in the full rotation are promoted by a reduction in temperature, so that the rotation rate of the new molecules actually increases at lower temperatures. ""Each rotation step is made up of three different photoreactions, two of which we experimentally demonstrated directly for the first time only this year,"" Dube explains. The researchers are confident that their motor's novel driving mechanism and unique behavior will make it possible in the not too distant future for researchers to synthesize molecular machines which, thanks to their relative insensitivity to the precise environmental temperature, will enable unique applications not possible with hitherto known motors.Story Source:Materials provided by Ludwig-Maximilians-Universitet Menchen. ",0.0724394,0.127605,0.102058,0.133149,0.0993164,0.150609,0.13623,0,0
300,Hairy nanotechnology provides green anti-scaling solution,"A new type of cellulose nanoparticle, invented by McGill University researchers, is at the heart of a more effective and less environmentally damaging solution to one of the biggest challenges facing water-based industries: preventing the buildup of scale.Formed by the accumulation of sparingly soluble minerals, scale can seriously impair the operation of just about any equipment that conducts or stores water -- from household appliances to industrial installations. Most of the anti-scaling agents currently in use are high in phosphorus derivatives, environmental pollutants that can have catastrophic consequences for aquatic ecosystems.In a series of papers published in the Royal Society of Chemistry's Materials Horizons and the American Chemical Society's Applied Materials & Interfaces, a team of McGill chemists and chemical engineers describe how they have developed a phosphorus-free anti-scaling solution based on a nanotechnology breakthrough with an unusual name: hairy nanocellulose.An unlikely candidateLead author Amir Sheikhi, now a postdoctoral fellow in the Department of Bioengineering at the University of California, Los Angeles, says despite its green credentials cellulose was not an obvious place to look for a way to fight scale.""Cellulose is the most abundant biopolymer in the world. It's renewable and biodegradable. But it's probably one of the least attractive options for an anti-scaling agent because it's neutral, it has no charged functional groups,"" he says.While working as a postdoctoral fellow with McGill chemistry professor Ashok Kakkar, Sheikhi developed a number of macromolecular antiscalants that were more effective than products widely used in industry -- but all of his discoveries were phosphonate-based. His desire to push his research further and find a phosphorus-free alternative led him to take a closer look at cellulose.""Nanoengineered hairy cellulose turned out to work even better than the phosphonated molecules,"" he says.The breakthrough came when the research team succeeded in nanoengineering negatively charged carboxyl groups onto cellulose nanoparticles. The result was a particle that was no longer neutral, but instead carried charged functional groups capable of controlling the tendency of positively charged calcium ions to form scale.Hirsute wonder particle a chance discoveryPrevious attempts to functionalize cellulose in this way focused on two earlier forms of nanoparticle -- cellulose nanofibrils and cellulose nanocrystals. But these efforts produced only a minimal amount of useful product. The difference this time was that the McGill team worked with hairy nanocellulose -- a new nanoparticle first discovered in the laboratory of McGill chemistry professor Theo van de Ven.Van de Ven, who also participated in the anti-scaling research, recalls the moment in 2011 when Han Yang, then a doctoral student in his lab, stumbled upon the new form of nanocellulose.""He came into my office with a test tube that looked like it had water in it and he said, 'Sir! My suspension has disappeared!'"" van de Ven says with a grin.""He had a white suspension of kraft fibres and it had turned transparent. When something is transparent, you know immediately it has either dissolved or turned nano. We performed a number of characterizations and we realized he had made a new form of nanocellulose.""Extreme versatilityThe secret to making hairy nanocellulose lies in cutting cellulose nanofibrils -- which are made up of an alternating series of crystalline and amorphous regions -- at precise locations to produce nanoparticles with amorphous regions sprouting from either end like so many unruly strands of hair.""By breaking the nanofibrils up the way we do, you get all these cellulose chains sticking out which are accessible to chemicals,"" van de Ven explains. ""That's why our nanocellulose can be functionalized to a far greater extent than other kinds.""Given the chemical versatility of hairy nanocellulose, the research team sees strong potential for applications beyond anti-scaling, including drug delivery, antimicrobial agents, and fluorescent dyes for medical imaging.""We can link just about any molecule you can think of to hairy nanocellulose,"" van de Ven says.Story Source:Materials provided by McGill University. ",0.0865442,0.156411,0.187823,0.179804,0.123953,0.161211,0.159548,0,0
301,"New 'smart' material with potential biomedical, environmental uses ","Brown University researchers have shown a way to use graphene oxide (GO) to add some backbone to hydrogel materials made from alginate, a natural material derived from seaweed that's currently used in a variety of biomedical applications. In a paper published in the journal Carbon, the researchers describe a 3-D printing method for making intricate and durable alginate-GO structures that are far stiffer and more fracture resistant that alginate alone.""One limiting factor in the use of alginate hydrogels is that they're very fragile -- they tend to fall apart under mechanical load or in low salt solutions,"" said Thomas Valentin, a Ph.D. student in Brown's School of Engineering who led the work. ""What we showed is by including graphene oxide nanosheets, we can make these structures much more robust.""The material is also capable of becoming stiffer or softer in response to different chemical treatments, meaning it could be used to make ""smart"" materials that are able to react to their surroundings in real time, the research shows. In addition, alginate-GO retains alginate's ability to repel oils, giving the new material potential as a sturdy antifouling coating.The 3-D printing method used to make the materials is known as stereolithography. The technique uses an ultraviolet laser controlled by a computer-aided design system to trace patterns across the surface of a photoactive polymer solution. The light causes the polymers to link together, forming solid 3-D structures from the solution. The tracing process is repeated until an entire object is built layer-by-layer from the bottom up. In this case the polymer solution was made using sodium alginate mixed with sheets of graphene oxide, a carbon-based material that forms one-atom-thick nanosheets that are stronger pound-for-pound than steel.One advantage to the technique is that the sodium alginate polymers link through ionic bonds. The bonds are strong enough to hold the material together, but they can be broken by certain chemical treatments. That gives the material the ability to respond dynamically to external stimuli. Previously, the Brown researchers showed that this ""ionic crosslinking"" can be used to create alginate materials that degrade on demand, rapidly dissolving when treated with a chemical that sweeps away ions from the material's internal structure.For this new study, the researchers wanted to see how graphene oxide might change mechanical properties of alginate structures. They showed that alginate-GO could be made twice as stiff as alginate alone, and far more resistant to failure through cracking.""The addition of graphene oxide stabilizes the alginate hydrogel with hydrogen bonding,"" said Ian Y. Wong, an assistant professor of engineering at Brown and the paper's senior author. ""We think the fracture resistance is due to cracks having to detour around the interspersed graphene sheets rather than being able to break right though homogeneous alginate.""The extra stiffness enabled the researchers to print structures that had overhanging parts, which would have been impossible using alginate alone. Moreover, the increased stiffness didn't prevent alginate-GO also from responding to external stimuli like alginate alone can. The researchers showed that by bathing the materials in a chemical that removes its ions, the materials swelled up and became much softer. The materials regained their stiffness when ions were restored through bathing in ionic salts. Experiments showed that the materials' stiffness could be tuned over a factor of 500 by varying their external ionic environment.That ability to change its stiffness could make alginate-GO useful in a variety of applications, the researchers say, including dynamic cell cultures.""You could imagine a scenario where you can image living cells in a stiff environment and then immediately change to a softer environment to see how the same cells might respond,"" Valentin said. That could be useful in studying how cancer cells or immune cells migrate through different organs throughout the body.And because alginate-GO retains the powerful oil-repellant properties of pure alginate, the new material could make an excellent coating to keep oil and other grime from building up on surfaces. In a series of experiments, the researchers showed that a coating of alginate-GO could keep oil from fouling the surface of glass in highly saline conditions. That could make alginate-GO hydrogels useful for coatings and structures used in marine settings, the researchers say.""These composite materials could be used as a sensor in the ocean that can keep taking readings during an oil spill, or as an antifouling coating that helps to keep ship hulls clean,"" Wong said. The extra stiffness afforded by the graphene would make such materials or coatings far more durable than alginate alone.The researchers plan to continue experimenting with the new material, looking for ways to streamline its production and continue to optimize its properties.Story Source:Materials provided by Brown University. ",0.0490336,0.143248,0.139481,0.123912,0.0924487,0.101036,0.132554,0,0
302,Sustainable 'nano-raspberry' to neutralize poisonous carbon monoxide,"Scientists from the Nagoya Institute of Technology (NITech) in Japan have developed a sustainable method to neutralize carbon monoxide, the odorless poison produced by cars and home boilers. Their results were featured on the cover of the September issue of the journal, Nanomaterials.Traditionally, carbon monoxide needs a noble metal -- a rare and expensive ingredient -- to convert into carbon dioxide and readily dissipate into the atmosphere. Although the noble metal ensures structural stability at a variety of temperatures, it's a cost-prohibitive and finite resource and researchers have been anxious to find an alternative.Now, a team led by Dr. Teruaki Fuchigami at the NITech has developed a raspberry-shaped nanoparticle capable of the same oxidation process that makes carbon monoxide gain an extra oxygen atom and lose its most potent toxicity.""We found that the raspberry-shaped particles achieve both high structural stability and high reactivity even in a single nanoscale surface structure,"" said Dr. Fuchigami, an assistant professor in the Department of Life Science and Applied Chemistry at the NITech and first author on the paper.The key, according to Dr. Fuchigami, is ensuring the particles are highly complex but organized. A single, simple particle can oxidize carbon monoxide, but it will naturally join with other simple particles. Those simple particles compact together and lose their oxidation abilities, especially as temperatures rise in an engine or boiler.Catalytic nanoparticles with single nano-scale and complex three-dimensional (3D) structures can achieve both high structural stability and high catalytic activity, however, such nanoparticles are difficult to produce using conventional methods. Dr. Fuchigami and his team worked to control not only the size of the particles, but also how they assembled together. They used cobalt oxide nanoparticles, a noble metal alternative that can oxidize well but eventually presses together and becomes inactive.The researchers applied sulfate ions to formation process of the cobalt oxide particle. The sulfate ions grasp the particles, creating a chemically bonded bridge. Called a ligand, this bridge holds the nanoparticles together while also inhibiting the clumping growth that would lead to a loss of catalytic activity.The resulting particle looked like a raspberry: small cells bound together into something greater than the sum of its parts.""The phenomenon of crosslinking two substances has been formulated in the field of metal-organic framework research, but, as far as we can tell, this is the first report in oxide nanoparticles. The effects of bridging ligands on the formation of oxide nanoparticles, which will be helpful to establish a synthesis theory for complex 3D nanostructures,"" Dr. Fuchigami said of the raspberry-shaped nanostructure.The unique surface nanostructure of the raspberry-shaped particles remained stable even under the harsh catalytic reaction process, improving the low-temperature CO oxidation activity.Dr. Fuchigami and his team will continue to study the bridging ligands with the goal of precisely controlling the design aspect of nanomaterials, such as the size and morphology.Ultimately, they plan to discover the most stable and active configuration for chemical catalysis and other applications.Story Source:Materials provided by Nagoya Institute of Technology. ",0.07414,0.199909,0.15536,0.149459,0.115406,0.131258,0.156125,0,0
303,Making X-ray microscopy 10 times faster,"Microscopes make the invisible visible. And compared to conventional light microscopes, transmission x-ray microscopes (TXM) can see into samples with much higher resolution, revealing extraordinary details. Researchers across a wide range of scientific fields use TXM to see the structural and chemical makeup of their samples -- everything from biological cells to energy storage materials.Now, scientists at the National Synchrotron Light Source II (NSLS-II) -- a U.S. Department of Energy (DOE) Office of Science User Facility at DOE's Brookhaven National Laboratory -- have developed a TXM that can image samples 10 times faster than previously possible. Their research is published in Applied Physics Letters.""We have significantly improved the speed of x-ray microscopy experiments,"" said Wah-Keat Lee, lead scientist at NSLS-II's Full Field X-ray Imaging (FXI) beamline, where the microscope was built. At FXI, Lee and his colleagues reduced the time it takes a TXM to image samples in 3-D from over 10 minutes to just one minute, while still producing images with exceptional 3-D resolution -- below 50 nanometers, or 50 billionths of a meter. ""This breakthrough will enable scientists to visualize their samples much faster at FXI than at similar instruments around the world,"" Lee said.Aside from reducing the time it takes to complete an experiment, a faster TXM can collect more valuable data from samples.""The holy grail of almost all imaging techniques is to be able to see a sample in 3-D and in real time,"" Lee said. ""The speed of these experiments is relevant because we want to observe changes that happen quickly. There are many structural and chemical changes that happen on different time scales, so a faster instrument can see a lot more. For example, we have the ability to track how corrosion happens in a material, or how well various parts of a battery are performing.""To offer these capabilities at FXI, the team needed to build a TXM using the latest developments in ultrafast nano-positioning (a method of moving a sample while limiting vibrations), sensing (a method of tracking sample movement), and control. The new microscope was developed in-house at Brookhaven Lab through a collaborative effort between the engineers, beamline staff, and research and development teams at NSLS-II.The researchers said developing superfast capabilities at FXI also strongly depended on the advanced design of NSLS-II.""Our ability to make FXI more than 10 times faster than any other instrument in the world is also due to the powerful x-ray source at NSLS-II,"" Lee said. ""At NSLS-II, we have devices called damping wigglers, which are used to achieve the very small electron beams for the facility. Fortunately for us, these devices also produce a very large number of x-rays. The amount of these powerful x-rays directly relates to the speed of our experiments.""Using the new capabilities at FXI, the researchers imaged the growth of silver dendrites on a sliver of copper. In a single minute, the beamline captured 1060 2-D images of the sample and reconstructed them to form a 3-D snapshot of the reaction. Repeating this, the researchers were able to form a minute-by-minute, 3-D animation of the chemical reaction.""We chose to image this reaction because it demonstrates the power of FXI,"" said Mingyuan Ge, lead author of the research and a scientist at NSLS-II. ""The reaction is well-known, but it has never been visualized in 3-D with such a fast acquisition time. In addition, our spatial resolution is 30 to 50 times finer than optical microscopy used in the past.""With the completion of this research, FXI has begun its general user operations, welcoming researchers from around the world to use the beamline's advanced capabilities.Story Source:Materials provided by DOE/Brookhaven National Laboratory. ",0.065703,0.118387,0.10685,0.164332,0.110476,0.108695,0.129339,0,0
304,Treated superalloys demonstrate unprecedented heat resistance,"Researchers at Idaho National Laboratory have discovered how to make ""superalloys"" even more super, extending useful life by thousands of hours. The discovery could improve materials performance for electrical generators and nuclear reactors. The key is to heat and cool the superalloy in a specific way. That creates a microstructure within the material that can withstand high heat more than six times longer than an untreated counterpart.""We came up with a way to make a superalloy that is much more resistant to heat-related failures. This could be useful in electricity generators and elsewhere,"" said Subhashish Meher, an INL materials scientist. He was lead author of a new Science Advances paper describing the research.Alloys are combinations of two or more metallic elements. Superalloys are exceptionally strong and offer other significantly improved characteristics due to the addition of trace amounts of cobalt, ruthenium, rhenium or other elements to a base metal. Understanding how to build an improved superalloy is important for making the metallic mixture better for a particular purpose.INL scientists have been studying nickel-based superalloys. Since these superalloys can withstand high heat and extreme mechanical forces, they are useful for electricity-generating turbines and high-temperature nuclear reactor components. Previous research had shown that performance can be improved if the material structure of the superalloy repeats in some way from very small sizes to very large, like a box within a box within a box.This is called a hierarchical microstructure. In a superalloy, it consists of a metallic matrix with precipitates, regions where the composition of the mixture differs from the rest of the metal. Embedded within the precipitates are still finer-scale particles that are the same composition as the matrix outside the precipitates -- conceptually like nested boxes.Meher and his coauthors studied how these precipitates formed within a superalloy. They also investigated how this structure stood up to heat and other treatments.They found that with the right recipe of heating and cooling, they could make the precipitates two or more times larger than would be the case otherwise, thereby creating the desired microstructure. These larger precipitates lasted longer when subjected to extreme heat. Moreover, computer simulation studies suggest that the superalloy can resist heat-induced failure for 20,000 hours, compared to about 3,000 hours normally.One application could be electrical generators that last much longer because the superalloy that they are constructed of would be tougher. What's more, INL scientists may now be able to come up with a procedure that can be applied to other superalloys. So, it may be possible to adjust a superalloy's strength, heat tolerance or other properties to enhance its use in a particular application.""We are now better able to dial in properties and improve material performance,"" Meher said.Story Source:Materials provided by DOE/Idaho National Laboratory. ",0.109465,0.165019,0.156447,0.184164,0.130475,0.214713,0.178366,0,0
305,Metallic nanoparticles light up another path towards eco-friendly catalysts Platinum clusters consisting 19 atoms perform 50 times higher catalytic activity,"Scientists at Tokyo Institute of Technology produced subnano-sized metallic particles that are very effective as catalysts for the oxidation of hydrocarbons. These catalysts can be as much as 50 times more effective than well-known Au-Pd bimetallic nanocatalysts.The oxidation of aromatic hydrocarbons is critically important for producing a great variety of useful organic compounds that are used throughout all types of industries. These oxidation processes require the use of catalysts and solvents, which are usually environmentally hazardous. Thus, finding a solvent-free oxidation process using nanosized catalytic particles has attracted considerable attention. Interestingly, sub-nanoscale catalytic particles (subnanocatalysts, or SNCs) composed of noble metals are even better at their job because their increased surface area and unique electronic state results in favorable effects for oxidizing hydrocarbons and also prevents them from getting oxidized themselves. This makes them cost-effective because the amount of metal required for SNCs is lower than for nano-sized catalysts.A team including Dr. Miftakhul Huda, Keigo Minamisawa, Dr. Takamasa Tsukamoto, and Dr. Makoto Tanabe at Tokyo Institute of Technology (Tokyo Tech), led by Prof. Kimihisa Yamamoto, created multiple types of SNCs by using dendrimers, which are tree-like spherical molecules that can be used as a template to contain the desired catalysts. ""Dendrimer is expected to provide internal nanospaces that could be suitable for catalytic conversion in the presence of metal particles,"" explains Yamamoto .The team created various catalysts of different sizes, depending on the noble metal used and the number of atoms of each catalytic particle. They compared their performance to find the best noble metal for making SNCs and then tried to determine the mechanism behind their high catalytic activity. Smaller SNCs were found to be better, while less oxophilic metals (such as platinum) were superior. The team postulated that the surface of platinum SNCs does not oxidize easily, which makes them reusable and results in the highest catalytic performance of the Pt19 SNC that can be as high as 50 times more effective than the common Au-Pd nanocatalysts. The team will continue working to shed light on these catalytic phenomena. ""The development of a more detailed mechanism including theoretical considerations is currently in progress,"" says Tanabe. The applications of such catalysts could greatly contribute for reducing pollution and enhancing our effective use of Earth's metal resources.Story Source:Materials provided by Tokyo Institute of Technology. ",0.112109,0.231506,0.169263,0.185267,0.156078,0.162153,0.184768,0,0
306,"Arming drug hunters, chemists design new reaction for drug discovery The reaction uses phosphorus to stitch together molecular rings called pyridines","If pharmaceutical chemists are the drug hunters who discover new medicines, scientists like Andrew McNally and Robert Paton are the armorers -- the deft creators who arm drug hunters with the sharpest tools.The pair of Colorado State University organic chemists have forged a powerful new such tool for drug hunters -- a simple, elegantly designed chemical reaction that could fling open an underexplored wing of biologically relevant chemistry. Their contribution, detailed in the journal Science Nov. 16, could be a shot in the arm for the discovery of new drugs.Assistant Professor McNally, a synthetic chemist, and Associate Professor Paton, an expert in computational chemical design, joined forces to create a new carbon-carbon bond reaction that's fundamental to how small-molecule drugs are made and discovered. The reaction uses phosphorus, rather than a commonly used transition metal, to stitch together molecular rings called pyridines. The lack of an accessible chemical reaction for coupling pyridine rings had been a deficiency in the field of drug discovery.The new reaction, created in McNally's lab, is analogous to the well-known palladium-catalyzed cross-coupling reaction, which makes carbon-carbon bonds using the transition metal palladium as a point of contact. Palladium-catalyzed reactions, which were the subject of the 2010 Nobel Prize in Chemistry, have been used for 30-plus years in pharmaceutical labs as the workhorse chemistry for coupling benzene rings. Benzene coupling is a foundational reaction in many pharmaceutically active compounds, from which thousands of drugs today -- painkillers, antimalarials, contraceptives -- were first synthesized in laboratories.But the palladium-catalyzed reaction, for which the late CSU chemist John Stille was a major innovator in the 1970 and 80s, does not work as well for coupling pyridine rings. Coupled pyridine rings are a potentially valuable pharmacophore, or chemical part known to interact with a biological system -- the basis for how drugs interact with the body. McNally's creation thus allows for easy construction of traditionally difficult-to-make chemical compounds that are known biological targets. They offer potential for discovery of drugs for diseases old and new -- a new arsenal of tools previously out of reach.""A major goal for our lab has always been for anyone in a pharmaceutical setting to go into the lab and try out our chemistry,"" McNally said. ""If people can pick this up and start to use it to discover drug leads, that would be an incredible win. We have used the transition metal chemistry for many years, but to get a new approach in there has been quite hard. We've tried to make this as easy as possible.""Collaborating with Paton's lab was integral to the discovery of the new reaction, McNally said, because experimentation alone could not have yielded their resulting model. Paton specializes in quantum chemistry, using it to rationally design new chemical structures to carry out specific tasks. Through these methods, Paton and his team validated the use of phosphorus, and followed the mechanism by which the challenging pyridine coupling is orchestrated.""This is the first study we know of that gives us a complete understanding of how these bonds are made,"" McNally said. ""People had viewed these phosphorus-mediated reactions as somewhat esoteric, with no practical significance. The model we developed has also allowed us to develop other reactions that will be valuable to the pharmaceutical industry that are ongoing in our laboratory.""Paton says he hopes medicinal chemists use this new chemistry to develop libraries of compounds with phosphorus-catalyzed pyridine couplings, and that these libraries could open doors for new drug treatments.""We want to give people reliable methods they can use every day to make important molecules,"" McNally said.Story Source:Materials provided by Colorado State University. ",0.0754986,0.190777,0.128248,0.202904,0.120854,0.13954,0.180646,0,0
307,Solving a 75-year-old mystery might provide a new source of farm fertilizer,"The solution to a 75-year-old materials mystery might one day allow farmers in developing nations to produce their own fertilizer on demand, using sunlight and nitrogen from the air.Thanks to a specialized X-ray source at Lawrence Berkeley National Laboratory, researchers at the Georgia Institute of Technology have confirmed the existence of a long-hypothesized interaction between nitrogen and titanium dioxide (TiO2) -- a common photoactive material also known as titania -- in the presence of light. The catalytic reaction is believed to use carbon atoms found as contaminants on the titania.If the nitrogen-fixing reaction can be scaled up, it might one day help power clean farm-scale fertilizer production that could reduce dependence on capital-intensive centralized production facilities and costly distribution systems that drive up costs for farmers in isolated areas of the world. Most of the world's fertilizer is now made using ammonia produced by the Haber-Bosch process, which requires large amounts of natural gas.""In the United States, we have an excellent production and distribution system for fertilizer. However, many countries are not able to afford to build Haber-Bosch plants, and may not even have adequate transportation infrastructure to import fertilizers. For these regions, photocatalytic nitrogen fixation might be useful for on-demand fertilizer production,"" said Marta Hatzell, an assistant professor in Georgia Tech's Woodruff School of Mechanical Engineering. ""Ultimately, this might be a low-cost process that could make fertilizer-based nutrients available to a broader array of farmers.""Hatzell and collaborator Andrew Medford, an assistant professor in Georgia Tech's School of Chemical and Biomolecular Engineering, are working with scientists at the International Fertilizer Development Center (IFDC) to study the potential impacts of the reaction process. The research was reported October 29 in the Journal of the American Chemical Society.The research began more than two years ago when Hatzell and Medford began collaborating on a materials mystery that originated with a 1941 paper published by Seshacharyulu Dhar, an Indian soil scientist who reported observing an increase in ammonia emitted from compost subjected to light. Dhar suggested that a photocatalytic reaction with minerals in the compost could be responsible for the ammonia.Since that paper, other researchers have reported nitrogen fixation on titania and ammonia production, but the results have not been consistently confirmed experimentally.Medford, a theoretician, worked with graduate research assistant Benjamin Comer to model the chemical pathways that would be needed to fix nitrogen on titania to potentially create ammonia using additional reactions. The calculations suggested the proposed process was highly unlikely on pure titania, and the researchers failed to win a grant they had proposed to use to study the mysterious process. However, they were awarded experimental time on the Advanced Light Source at the U.S. Department of Energy's Lawrence Berkeley National Laboratory, which allowed them to finally test a key component of the hypothesis.Specialized equipment at the lab allowed Hatzell and graduate student Yu-Hsuan Liu to use X-ray photoelectron spectroscopy (XPS) to examine the surface of titania as nitrogen, water and oxygen interacted with the surfaces under near-ambient pressure in the dark and in the light. At first, the researchers saw no photochemical nitrogen fixation, but as the experiments continued, they observed a unique interaction between nitrogen and titania when light was directed at the minerals surface.What accounted for the initial lack of results? Hatzell and Medford believe that surface contamination with carbon -- likely from a hydrocarbon -- is a necessary part of the catalytic process for nitrogen reduction on the titania. ""Prior to testing, the samples are cleaned to remove nearly all the trace carbon from the surface, however during experiments carbon from various sources (gases and the vacuum chamber) can introduce trace amount of carbon back onto the sample,"" Hatzell explained. ""What we observed was that reduced nitrogen species only were detected if there was a degree of carbon on the sample.""The hydrocarbon contamination hypothesis would explain why earlier research had provided inconsistent results. Carbon is always present at trace levels on titania, but getting the right amount and type may be key to making the hypothesized reaction work.""We think this explains the puzzling results that had been reported in the literature, and we hope it gives insights into how to engineer new catalysts using this 75-year-old mystery,"" Medford said. ""Often the best catalysts are materials that are very pristine and made in a clean room. Here you have just the opposite -- this reaction actually needs the impurities, which could be beneficial for sustainable applications in farming.""The researchers hope to experimentally confirm the role of carbon with upcoming tests at Pacific Northwest National Laboratory (PNNL), which will allow them to directly probe the carbon during the photocatalytic nitrogen fixation process. They also hope to learn more about the catalytic mechanism so that they can better control the reaction to improve efficiency, which is currently less than one percent.The research reported in the journal did not measure ammonia, but Hatzell and her students have since detected it in lab scale tests. Because the ammonia is currently produced at such low levels, the researchers had to take precautions to avoid ammonia-based contamination. ""Even tape used on equipment can create small quantities of ammonia that can affect the measurements,"" Medford added.Though the amounts of ammonia produced by the reaction are currently low, Hatzell and Medford believe that with process improvements, the advantages of on-site fertilizer production under benign conditions could overcome that limitation.""While this may sound ridiculous from a practical perspective at first, if you actually look at the needs of the problem and the fact that sunlight and nitrogen from the air are free, on a cost basis it starts to look more interesting,"" Medford said. ""If you could operate a small-scale ammonia production facility with enough capacity for one farm, you have immediately made a difference.""Hatzell credits cutting-edge surface science with finally providing an explanation to the mystery.""Since earlier investigators looked at this, there have been significant advances made in the area of measurement and surface science,"" she said. ""Most surface science measurements require the use of ultra-high vacuum conditions which do not mimic the catalytic environment you aim to investigate. The near ambient pressure XPS at Lawrence Berkeley National lab, allowed us to take a step closer to observing this reaction in its native environment.""Story Source:Materials provided by Georgia Institute of Technology. ",0.0480101,0.0960778,0.139282,0.115276,0.0750055,0.0977899,0.10574,0,0
308,New research could fine-tune the gene scissors CRISPR,"The introduction of the tool for gene editing, the so-called gene scissors CRISPR, in 2007 was a revolution within medical science and cell biology. But even though the perspectives are great, the launch of CRISPR has been followed by debate, especially focussing on ethical issues and the technology's degree of accuracy and side effects.However, in a new study published in the scientific journal Cell researchers from the Novo Nordisk Foundation Center for Protein Research have described how one of the CRISPR technologies, the so-called Cas12a, works -- all the way down to the molecular level. This makes it possible to fine-tune the gene-editing process to only achieve the desired effects.'If we compare CRISPR to a car engine, what we have done is make a complete 3D map of the engine and thus gained an understanding of how it works. This knowledge will enable us to fine-tune the CRISPR engine and make it work in various ways -- as a Formula 1 racer as well as an off-road truck', says Professor Guillermo Montoya from the Novo Nordisk Foundation Center for Protein Research.Molecular Film The researchers have used a so-called cryo-electron microscope to map the technology. The recently inaugurated cryoEM facility at the University of Copenhagen has established the state-of-the-art technology enabling the researchers to take photographs of the different shapes of the molecule when CRISPR-Cas12a cuts up the DNA strand.They combined it with a fluorescent microscopy technique called 'single molecule FRET' that directly observes the motions of the molecules and the sequence of events for each individual protein.Among other things, this sequence of events revealed to the researchers that three ""pieces"" of the CRISPR tools must change form for the DNA to be cut properly.'Our new study shows the precise series of events in the genome leading to gene editing. These three ""pieces"" that change, work like airport security checks. You have to complete all checks and in the right order to proceed', says Associate Professor Nikos Hatzakis from the Department of Chemistry and the Nano-Science center.More Editing than Desired According to the researchers, their new findings can explain why CRISPR technology can have side effects on the genome. Once the DNA strand has been cut, the three 'security checks' remain open.This can cause the process to last longer than wanted, because the machinery behind gene editing continues to run and can cause genetic changes.However, now the researchers expect their new knowledge to put an end to this. They believe it can be used to fine-tune the gene-editing technology right away.Story Source:Materials provided by University of Copenhagen The Faculty of Health and Medical Sciences. ",0.0917746,0.143897,0.138406,0.226067,0.12057,0.15087,0.194153,0,0
309,Simulations suggest graphene can stretch to be a tunable ion filter,"Researchers at the National Institute of Standards and Technology (NIST) have conducted simulations suggesting that graphene, in addition to its many other useful features, can be modified with special pores to act as a tunable filter or strainer for ions (charged atoms) in a liquid.The concept, which may also work with other membrane materials, could have applications such as nanoscale mechanical sensors, drug delivery, water purification and sieves or pumps for ion mixtures similar to biological ion channels, which are critical to the function of living cells. The research is described in the November 26 issue of Nature Materials.""Imagine something like a fine-mesh kitchen strainer with sugar flowing through it,"" project leader Alex Smolyanitsky said. ""You stretch that strainer in such a way that every hole in the mesh becomes 1-2 percent larger. You'd expect that the flow through that mesh will be increased by roughly the same amount. Well, here it actually increases 1,000 percent. I think that's pretty cool, with tons of applications.""If it can be achieved experimentally, this graphene sieve would be the first artificial ion channel offering an exponential increase in ion flow when stretched, offering possibilities for fast ion separations or pumps or precise salinity control. Collaborators plan laboratory studies of these systems, Smolyanitsky said.Graphene is a layer of carbon atoms arranged in hexagons, similar in shape to chicken wire, that conducts electricity. The NIST molecular dynamics simulations focused on a graphene sheet 5.5 by 6.4 nanometers (nm) in size and featuring small holes lined with oxygen atoms. These pores are crown ethers -- electrically neutral circular molecules known to trap metal ions. A previous NIST simulation study showed this type of graphene membrane might be used for nanofluidic computing.In the simulations, the graphene was suspended in water containing potassium chloride, a salt that splits into potassium and chlorine ions. The crown ether pores can trap potassium ions, which have a positive charge. The trapping and release rates can be controlled electrically. An electric field of various strengths was applied to drive the ion current flowing through the membrane.Researchers then simulated tugging on the membrane with various degrees of force to stretch and dilate the pores, greatly increasing the flow of potassium ions through the membrane. Stretching in all directions had the biggest effect, but even tugging in just one direction had a partial effect.Researchers found that the unexpectedly large increase in ion flow was due to a subtle interplay of a number of factors, including the thinness of graphene; interactions between ions and the surrounding liquid; and the ion-pore interactions, which weaken when pores are slightly stretched. There is a very sensitive balance between ions and their surroundings, Smolyanitsky said.Story Source:Materials provided by National Institute of Standards and Technology (NIST). ",0.0946066,0.208976,0.169041,0.167734,0.156924,0.151053,0.172317,0,0
310,Stretchy solar cells a step closer Engineers give organic photovoltaics new properties,"Organic solar cells that can be painted or printed on surfaces are increasingly efficient, and now show promise for incorporation into applications like clothing that also require them to be flexible.The Rice University lab of chemical and biomolecular engineer Rafael Verduzco has developed flexible organic photovoltaics that could be useful where constant, low-power generation is sufficient.Verduzco and his team incorporate a network of elastic additives that make the electrically active material less brittle with little to no loss of current flow.The research appears in the American Chemical Society journal Chemistry of Materials.Organic solar cells rely on carbon-based materials including polymers, as opposed to hard, inorganic materials like silicon, to capture sunlight and translate it into current. Organics are also thin, lightweight, semitransparent and inexpensive. While middle-of-the-road, commercial, silicon-based solar cells perform at about 22 percent efficiency -- the amount of sunlight converted into electricity -- organics top out at around 15 percent.""The field has been obsessed with the efficiency chart for a long time,"" Verduzco said. ""There's been an increase in efficiency of these devices, but mechanical properties are also really important, and that part's been neglected.""If you stretch or bend things, you get cracks in the active layer and the device fails.""Verduzco said one approach to fixing the brittle problem would be to find polymers or other organic semiconductors that are flexible by nature, but his lab took another tack. ""Our idea was to stick with the materials that have been carefully developed over 20 years and that we know work, and find a way to improve their mechanical properties,"" he said.Rather than make a mesh and pour in the semiconducting polymers, the Rice researchers mixed in sulfur-based thiol-ene reagents. The molecules blend with the polymers and then crosslink with each other to provide flexibility.The process is not without cost, because too little thiol-ene leaves the crystalline polymers prone to cracking under stress, while too much dampens the material's efficiency.Testing helped the lab find its Goldilocks Zone. ""If we replaced 50 percent of the active layer with this mesh, the material would get 50 percent less light and the current would drop,"" Verduzco said. ""At some point, it's not practical. Even after we confirmed the network was forming, we needed to determine how much thiol-ene we needed to suppress fracture and the maximum we could put in without making it worthless as an electronic device.""At about 20 percent thiol-ene, they found that cells retained their efficiency and gained flexibility. ""They're small molecules and don't disrupt the morphology much,"" Verduzco said. ""We can shine ultraviolet light or apply heat or just wait, and with time the network will form. The chemistry is mild, fast and efficient.""The next step was to stretch the material. ""Pure P3HT (the active polythiophene-based layer) started cracking at about 6 percent strain,"" Verduzco said. ""When we added 10 percent thiol-ene, we could strain it up to 14 percent. At around 16 percent strain we started seeing cracks throughout the material.""At strains higher than 30 percent, the material flexed just fine but became useless as a solar cell. ""We found there's essentially no loss in our photocurrent up to about 20 percent,"" he said. ""That seems to be the sweet spot.""Damage under strain affected the material even when the strain was released. ""The strain impacts how these crystal domains pack and translates to microscopic breaks in the device,"" Verduzco said. ""The holes and electrons still need paths to get to the opposite electrodes.""He said the lab expects to try different organic photovoltaic materials while working to make them more stretchable with less additive for larger test cells.Rice graduate student Jorge Wu Mok is lead author of the paper. Co-authors are Rice graduate student Zhiqi Hu, undergraduate Changxu Sun and alumnus Isaiah Barth; summer researchers Rodrigo Mueoz and Joshua Jackson of Rice and Houston Community College; research scientist Tanguy Terlier of Rice's Shared Equipment Authority; and Kevin Yager of Brookhaven National Laboratory, Upton, N.Y.The research was supported by the National Science Foundation, the Welch Foundation for Chemical Research and the Department of Energy Office of Science Facilities.Story Source:Materials provided by Rice University. ",0.0558986,0.119168,0.12071,0.132474,0.108303,0.0957896,0.134019,0,0
311,Chemists develop new method for selective binding of proteins,"A new method of selectively binding proteins to nanoparticles has been described by a team of German and Chinese researchers headed by Prof. Bart Jan Ravoo, a chemist at the ""Center for Soft Nanoscience"" at the University of Menster (Germany). The nanoparticles automatically recognize specific peptides, i.e. small proteins, and enter into highly selective binding with them. Among the model peptides which the researchers examined were amyloids. Deposits of amyloids, for example, play a major role in Alzheimer's disease, so the researchers are hoping that the mechanism they have discovered might provide a new approach to treating diseases in which such deposits occur. The study has been published in the latest issue of the Nature Chemistry journal.Producing exact contact surfacesThe interaction between protein molecules or between them and other biomolecules plays a major role in very many physiological processes. In this context, molecular recognition entails the binding of proteins through multiple nanoscale points of contact on the surface of the protein. Typically, very many of these points are involved, resulting in a unique, exact and complementary contact surface. The mechanism these protein molecules bind is therefore described as a ""key-and-lock principle."" The researchers have now described a method for producing such contact surfaces on nanoparticles, so that selected proteins can be targeted and bound.Co-assembly and ""heteromultivalency""What is new about the method is that it is based on the principles of co-assembly and so-called heteromultivalency. Co-assembly means that the nanoparticles are not -- as is otherwise usual -- produced as a result of complex and tailor-made chemical synthesis. Instead, the researchers produce them using a relatively simple method in which two lipid-type (""fatty"") components are mixed in water and spontaneously form the nanoparticles required. These particles are adaptive, i.e. they change their inner structure and thus achieve optimum binding to the target protein. ""Heteromultivalency"" means that the nanoparticles form a very large number of different points of contact which, at the same time, interact with the protein. Following the example of natural physiological processes, a particularly high selectivity thus occurs.Potential for diagnostics, imaging and active ingredients in medicines""So far, this principle of heteromultivalency in particular has scarcely been examined in detail or exploited,"" says Bart Jan Ravoo. ""We describe an entirely new approach which will make it possible to develop further synthetic protein binders. This could be considered for use for example in diagnostics or imaging -- or as potential active ingredients in medicines."" The team of researchers has shown that the nanoparticles bind with amyloids and, as a result, dissolve aggregates of these peptides. The occurrence of amyloid aggregates is closely connected with the development of Alzheimer's. This is why the new method might provide an approach for developing new types of treatment. It has not yet been clarified, however, whether the amyloid aggregates do indeed cause Alzheimer's or are, rather, a result of the disease. Moreover, the new method for dissolving the aggregates has only been tried out in a test tube. Further studies are therefore necessary in order to assess more clearly its potential as an approach to treatment.The methodThe researchers used amphiphilic, water-soluble cyclodextrin and calixarene molecules as co-assembling components for the nano-contact points. They examined the binding of the resultant nanoparticles to the peptides, using for example fluorescence spectroscopy. They used electron microscopy to demonstrate binding to the amyloids.Story Source:Materials provided by University of Menster. ",0.082286,0.182644,0.153116,0.167741,0.124453,0.129119,0.157788,0,0
312,Mmore effective hydrogel for healing wounds,"Researchers at the University of New Hampshire have created an easy-to-make, low-cost injectable hydrogel that could help wounds heal faster, especially for patients with compromised health issues.Wound healing can be complex and challenging, especially when a patient has other health obstacles that seriously impede the process. Often injectable hydrogels are applied to irregular shaped wounds, like diabetic ulcers, to help form a temporary matrix, or structure, to keep the wound stable while cells rejuvenate. The caveat is that current hydrogels are not porous enough and do not allow neighboring cells to pass through toward the wound to help it mend.""While valuable for helping patients, current hydrogels have limited clinical efficacy,"" said Kyung Jae Jeong, assistant professor of chemical engineering at UNH. ""We discovered a simple solution to make the hydrogels more porous and therefore help to speed up the healing.""In the study, recently published in the journal of ACS Applied Bio Materials, the researchers outline how they made a macroporous hydrogel by combining readily available gelatin microgels -- hydrogels that are a few hundred microns in diameter -- with an inexpensive enzyme called microbial transglutaminase (mTG). Gelatin was used because it is a natural protein derived from collagen, a protein found in connective tissue in the body such as skin. Assembling these tiny microgels with mTG helped create a hydrogel with large enough pores for the neighboring cells to move into the wound for repair. In addition, this new injectable formulation allows for the slow release of protein drugs to aid wound healing, such as platelet-derived growth factor (PDGF). The researchers compared conventional nonporous hydrogels with the new macroporous hydrogels, and found a notable increase in the migration of tissue cells inside the hydrogel, which is the hallmark of wound healing.Along with diabetic ulcers, the macroporous hydrogel could help with other forms of healing on the skin, cornea, internal organs during surgery and even has military implications.Story Source:Materials provided by University of New Hampshire. ",0.130848,0.186158,0.194038,0.235632,0.196899,0.193174,0.215031,0,0
313,"Freeze-frame microscopy captures molecule's 'lock-and-load' on DNA With cryo-EM, scientists obtain clearer picture of key machinery that transcribes DNA","Pushing the limits of cryo-electron microscopy, University of California, Berkeley, scientists have captured freeze-frames of the changing shape of a huge molecule, one of the body's key molecular machines, as it locks onto DNA and loads the machinery for reading the genetic code.The molecule, called transcription factor IID, is critical to transcribing genes into RNA that will later be used as blueprints to make proteins. Because of its many moving parts and large size, however, TFIID's 3D structure has been hard to capture: the moving parts become a blur.Cryo-EM, an imaging technique whose discoverers garnered the 2017 Nobel Prize in Chemistry, is the only way to obtain a snapshot of bulky, floppy structures like this. High-resolution structural information is essential for understanding how TFIID translates the operating instructions in our genome and how it sometimes goes haywire.The new, more detailed snapshots of the molecule's moving parts could help drug designers create drugs that interfere with the molecule's structural changes in order to tweak the expression of a gene that is causing disease.""These structures give you the potential for rationally designing small molecules that will disturb the normal function, because now we don't have just a single structure, we have many structures, which is even more powerful because we can target the motion that we are seeing right now,"" said Eva Nogales, a UC Berkeley professor of molecular and cell biology and a faculty scientist at Lawrence Berkeley National Laboratory.Nogales and her colleagues, most prominently UC Berkeley graduate students Avinash Patel and Robert Louder, posted their findings online this week in advance of print publication in the journal Science.""Where you stick that drug and how you make that drug operate is highly dependent on the transient nature of these structures, and that is something we have come to realize only very recently,"" said Robert Tjian, a UC Berkeley professor of molecular and cell biology who discovered TFIID and has worked on the molecule for much of his career, though he was not a co-author of the new paper. ""Because these molecules are moving around and their structures are so complicated, conventional drug discovery would never reveal what is going on. Eva's structures are going to change that. This has the potential to open up the universe of druggable targets.""Freezing moleculesTFIID is an agglomeration of more than a dozen distinct proteins that homes in on a promoter -- a region of DNA that controls the transcription of a nearby gene -- and tests the sequence to make sure it has landed at the right spot. Once this is confirmed, it opens up to recruit dozens of other proteins that then start ratcheting along the gene, using the DNA sequence as a template to create a complementary sequence of RNA, called messenger RNA. This then wends its way out of the nucleus into the body of the cell, where it is translated by other molecular machines into protein.""TFIID is probably the hardest structure anybody has tried to solve, because it is massive and it is highly flexible,"" said Tjian. ""The only way you can see these very flexible structures is by cryo-EM, and Eva can now freeze all these different flexible states and describe the movement.""Cryo-EM involves freezing a drop containing millions of copies of a molecule, in every imaginable orientation, and using an electron microscope to determine the structure by combining images to define the 3D shape. Because TFIID has many moving parts as it binds to DNA and gets ready to transcribe a gene, averaging all the frozen positions produces a blurred image.Nogales compares previous efforts at imaging TFIID, including her own attempts going back nearly 20 years, to taking a photograph of a soccer team on the field and averaging the motions of all the players. The result is a generic human torso with blurry head and limbs.Imaging the soccer fieldNow, thanks to more than two years of intensive work by Patel and Louder, it's possible to capture higher resolution images that are akin to distinguishing a striker's leg kick from a goalie's hand block and a fullback's header.""Imagine you have an image of the 22 soccer players on the field, and you were to combine them into a single unit that you would call the 'generic soccer player,'"" she said. ""It will look like a blurry picture -- you could barely see that it's a humanoid form and that there is some kind of motion, but you wouldn't realize that a difference exists between the players.""The improved pictures are a result of better detectors developed originally by colleagues at Berkeley Lab, and steadily improving computer algorithms to analyze the huge amounts of data collected by the detectors. This helped Nogales and her team to define five distinct structures of the TFIID molecule.""They span the whole binding sequence: before binding to the DNA, initial binding to the promoter, subsequent binding after it double checks that this is the right place, and the final state,"" Nogales said.She and her colleagues continue to push the limits of cryo-EM, hoping to determine the 3D structure of TFIID after other transcription proteins land on it to complete the transcription process.Story Source:Materials provided by University of California - Berkeley. Original written by Robert Sanders. ",0.0570003,0.102232,0.118412,0.189343,0.10014,0.111621,0.140191,0,0
314,Nanotubes built from protein crystals: Breakthrough in biomolecular engineering,"Researchers at Tokyo Institute of Technology (Tokyo Tech) have succeeded in constructing protein nanotubes from tiny scaffolds made by cross-linking of engineered protein crystals. The achievement could accelerate the development of artificial enzymes, nano-sized carriers and delivery systems for a host of biomedical and biotechnological applications.An innovative way for assembly of proteins into well-ordered nanotubes has been developed by a group led by Takafumi Ueno at Tokyo Tech's Department of Biomolecular Engineering .Tailor-made protein nanostructures are of intense research interest, as they could be used to develop highly specific and powerful catalysts, targeted drug and vaccine delivery systems, and for the design of many other promising biomaterials.Scientists have faced challenges in constructing protein assemblies in aqueous solution due to the disorganized ways in which proteins interact freely under varying conditions such as pH and temperature.The new method, reported in the journal Chemical Science, overcomes these problems by using protein crystals, which serve as a promising scaffold for proteins to self-assemble into desired structures. The method has four steps, as illustrated in Construction of nanotubes from protein crystals:The crystal system, composed of the ordered arrangement of assembled structures, makes it easy to control precise chemical interactions of interest by cross-linking to stabilize the assembly structure -- an accomplishment that cannot be achieved from cross-linking of proteins in solution.The researchers chose a naturally occurring protein called RubisCO as a building block for construction of nanotube. Due to its high stability, RubisCO could keep its shape, and its crystal structure from previous research had recommended it for this study.Using Transmission Electron Microscopy (TEM) imaging at Tokyo Tech's Suzukakedai Biomaterials Analysis Division, the team successfully confirmed the formation of the protein nanotubes.The study also demonstrated that the protein nanotubes could retain their enzymatic ability.""Our cross-linking method can facilitate the formation of the crystal scaffold efficiently at the desired position (specific cysteine sites) within each tubes of the crystal,"" says Ueno. ""At present, since more than 100,000 protein crystal structures have been deposited in Protein data bank, our method can be applied to other protein crystals for construction of supramolecular protein assemblies, such as cages, tubes, sheets.""The nanotube in this study can be utilized for various applications. The tube provides the environment for accumulation of the exogenous molecules which can be used as platforms of delivery in pharmaceutical related fields. The tube can also be potential for catalysis because the protein building block has the enzymatic activity in nature.Story Source:Materials provided by Tokyo Institute of Technology. ",0.112642,0.202497,0.222404,0.20566,0.183162,0.166166,0.200305,0,0
315,Moths and magnets could save lives Experimental herapy could repair mutations that cause genetic diseases,"A new technology that relies on a moth-infecting virus and nanomagnets could be used to edit defective genes that give rise to diseases like sickle cell, muscular dystrophy and cystic fibrosis.Rice University bioengineer Gang Bao has combined magnetic nanoparticles with a viral container drawn from a particular species of moth to deliver CRISPR/Cas9 payloads that modify genes in a specific tissue or organ with spatial control.Because magnetic fields are simple to manipulate and, unlike light, pass easily through tissue, Bao and his colleagues want to use them to control the expression of viral payloads in target tissues by activating the virus that is otherwise inactivated in blood.The research appears in Nature Biomedical Engineering. In nature, CRISPR/Cas9 bolsters microbes' immune systems by recording the DNA of invaders. That gives microbes the ability to recognize and attack returning invaders, but scientists have been racing to adapt CRISPR/Cas9 to repair mutations that cause genetic diseases and to manipulate DNA in laboratory experiments.CRISPR/Cas9 has the potential to halt hereditary disease -- if scientists can get the genome-editing machinery to the right cells inside the body. But roadblocks remain, especially in delivering the gene-editing payloads with high efficiency.Bao said it will be necessary to edit cells in the body to treat many diseases. ""But efficiently delivering genome-editing machinery into target tissue in the body with spatial control remains a major challenge,"" Bao said. ""Even if you inject the viral vector locally, it can leak to other tissues and organs, and that could be dangerous.""The delivery vehicle developed by Bao's group is based on a virus that infects Autographa californica, aka the alfalfa looper, a moth native to North America. The cylindrical baculovirus vector (BV), the payload-carrying part of the virus, is considered large at up to 60 nanometers in diameter and 200-300 nanometers in length. That's big enough to transport more than 38,000 base pairs of DNA, which is enough to supply multiple gene-editing units to a target cell, Bao said.He said the inspiration to combine BV and magnetic nanoparticles came from discussions with Rice postdoctoral researcher and co-lead author Haibao Zhu, who learned about the virus during a postdoctoral stint in Singapore but knew nothing about magnetic nanoparticles until he joined the Bao lab. The Rice team had previous experience using iron oxide nanoparticles and an applied magnetic field to open blood vessel walls just enough to let large-molecule drugs pass through.""We really didn't know if this would work for gene editing or not, but we thought, 'worth a shot,'"" Bao said.The researchers use the magnetic nanoparticles to activate BV and deliver gene-editing payloads only where they're needed. To do this, they take advantage of an immune-system protein called C3 that normally inactivates baculoviruses.""If we combine BV with magnetic nanoparticles, we can overcome this deactivation by applying the magnetic field,"" Bao said. ""The beauty is that when we deliver it, gene editing occurs only at the tissue, or the part of the tissue, where we apply the magnetic field.""Application of the magnetic field allows BV transduction, the payload-delivery process that introduces gene-editing cargo into the target cell. The payload is also DNA, which encodes both a reporter gene and the CRISPR/Cas9 system.In tests, the BV was loaded with green fluorescent proteins or firefly luciferase. Cells with the protein glowed brightly under a microscope, and experiments showed the magnets were highly effective at targeted delivery of BV cargoes in both cell cultures and lab animals.Bao noted his and other labs are working on the delivery of CRISPR/Cas9 with adeno-associated viruses (AAV), but he said BV's capacity for therapeutic cargo is roughly eight times larger. ""However, it is necessary to make BV transduction into target cells more efficient,"" he said.Story Source:Materials provided by Rice University. Original written by Mike Williams. ",0.133727,0.192364,0.169712,0.231836,0.166478,0.18909,0.220216,0,0
316,Tailoring the surface of carbon may hold the key to monitoring patient blood in real-time,"Thanks to the development of a new computational model, Postdoctoral Researcher Miguel Caro is spearheading work on tailor-made carbon surfaces by researchers at Aalto University, who work in partnership with Professor Gabor Csanyi and Dr Volker Deringer from Cambridge University.'For the first time, we can identify the chemical properties of carbon surfaces and better understand how we can prepare them for specific purposes,' explains Aalto University's Professor Tomi Laurila.The potential applications for tailor-made carbon surfaces are wide and include protective coatings, car parts, biomedical coatings and biosensors. Yet for these developments to be realised, detailed atomic level knowledge is still needed on how carbon surfaces are structured and how they can be modified.The local environment of every atom in amorphous carbons, also called diamond-like carbons, is slightly different. This means that the number of neighbouring atoms, as well as the distances and angles between them, varies, posing a big challenge in the search to customize these surfaces.The new computational model has finally allowed researchers to identify a wide variety of local atomic environments and classify them according to their properties. The research team has also calculated the varying degrees of strength with which different groups--hydrogen, alcohol (hydroxyl), and oxygen--will attach to surface sites. Some bonds are, naturally, stronger than others. Because new information about the surface structures can be incorporated to 'retrain' and improve the model, the properties of still unknown surfaces can be predicted based on previous results.'Through computations, we can now not only explore what material surfaces look like at the atomic level but also see how they interact with other substances under analysis, as well as understand the kinds of chemical groups formed on these surfaces because of this interaction. We are also investigating what kinds of surfaces are needed to optimise the interaction with molecules that we would like to be able to detect, such as hydrogen peroxide,' explains Laurila.In other words, these simulation models based on density functional theory and machine learning tell us what kinds of structures can be developed--and how those structures may be optimised for specific applications.'In the future we will be able to produce tailored carbon surfaces, for example, for medical sensors, which could be used to monitor the concentration of a particular medication in a patient's blood in real-time. Tracking changes in specific biomarkers in patients may be the key to improving therapeutic treatments currently used, or help us identify the risk of outbreaks of many common diseases earlier than ever before,' Laurila says.Story Source:Materials provided by Aalto University. ",0.063447,0.119618,0.133125,0.183454,0.0961533,0.110099,0.152146,0,0
317,Fully identified: The pathway of protons,"The question how certain algal enzymes accomplish the high proton transfer rate for hydrogen production had in the past been subject to speculation. Dr. Martin Winkler, Dr. Jifu Duan, Professor Eckhard Hofmann and Professor Thomas Happe from Ruhr-Universitet Bochum (RUB), together with colleagues from Freie Universitet Berlin, traced the pathway of protons all the way into the active center of [FeFe]-hydrogenases. Their findings might enable scientists to create stable chemical reproductions of such efficient, yet fragile biocatalysts. The researchers published their report in the journal Nature Communications from 9 November 2018.Unique efficiency due to transfer pathwayIn their catalytic center, hydrogenases manufacture molecular hydrogen (H2) from two protons and two electrons. They extract the protons required for this process from the surrounding water and transfer them -- via a transport chain -- into their catalytic core. The exact proton pathway through the hydrogenase had as yet not been understood. ""This transfer pathway is a jigsaw piece, crucial for understanding the interplay of cofactor and protein which is the reason why biocatalysts are so much more efficient than hydrogen-producing chemical complexes,"" explains Dr. Martin Winkler, one of the authors of this study from the Photobiotechnology research group at RUB.Structures of enzyme variants decodedIn order to figure out which of the hydrogenase building blocks are involved in proton transfer, the researchers substituted them individually. They replaced each either by an amino acid with a similar function or by a dysfunctional amino acid. Thus, 22 variants of two different hydrogenases were created. Subsequently, the researchers compared those variants with regard to different aspects, including their spectroscopic properties and their enzyme activity. ""The molecular structures of twelve protein variants, which were solved using X-ray structure analysis, proved particularly informative,"" says Winkler.Amino acids with no function shut down hydrogenasesDepending on where and how the researchers had changed the hydrogenase, hydrogen production became less efficient or stopped altogether. ""Thus, we ascertained why some variants are severely impaired in terms of enzyme activity and why others are hardly impaired at all -- against all expectations,"" says Martin Winkler.The closer to the catalytic centre the replaced amino acids were located, the less able the hydrogenase was to compensate for these modifications. If building blocks with no function were embedded in sensitive locations, hydrogen production was shut down. ""The thus generated state resembles an oversaturation due to proton stress where protons as well as hydrogen are simultaneously introduced into the hydrogenase,"" elaborates Martin Winkler. ""In the course of our project, we were for the first time able to stabilise and analyse this highly transient state that we had already encountered in experiments.""Valuable baseline informationThis study has made it possible to assign the functions of individual amino acids to the proton transfer pathway for the enzyme group of [FeFe] hydrogenases. ""Moreover, it provides valuable information on the molecular mechanism of proton transfer by redox-active proteins and the structural requirements thereof,"" concludes Thomas Happe.FundingThe project was funded by the Volkswagen Foundation, the China Scholarship Council, and the German Research Foundation under the umbrella of the Resolv Cluster of Excellence (EXC1069).Story Source:Materials provided by Ruhr-University Bochum. Original written by Meike Drieeen; translated by Donata Zuber. ",0.0893814,0.197907,0.151588,0.178949,0.139466,0.145854,0.163971,0,0
318,"Intense tests reveal elusive, complex form of nitrogen ","An unusually complex form of one of the most abundant chemical elements on Earth has been revealed in the lab for the first time.Researchers created a crystallised version of nitrogen -- which at normal conditions is the main constituent of air -- by subjecting it to extreme pressures and temperatures.The study shows for the first time that simple molecular elements can have complex structures at high pressures.It could inform similar studies in other elements, researchers say.An international team of scientists led by the University of Edinburgh used a high-pressure diamond-tipped anvil to squeeze tiny amounts of nitrogen at pressures half a million times that of Earth's atmosphere, while heating it to about 500 Celsius.They then used specialist X-ray technology to capture an image of the resulting crystals, and were surprised to find that the nitrogen had formed a complicated arrangement made up of dozens of molecules.The team had expected to uncover a much simpler structure.Their findings resolve speculation over the structure of this form of nitrogen, known as ?-N2. It was discovered 15 years ago but its structure was unknown until now.Computer simulations of the new structure have given valuable insights, finding it to be surprisingly stable.The study, published in Nature Communications, was carried out in collaboration with the European Synchrotron Radiation Facility in France, and with researchers in China. It was supported by the Engineering and Physical Sciences Research Council.Robin Turnbull, of the University of Edinburgh's School of Physics and Astronomy, who led the study, said: ""We hope that these results will prompt further investigations into why relatively simple elements should form such complex structures -- it's important that we keep searching for promising new lines of scientific investigation.""Story Source:Materials provided by University of Edinburgh. ",0.0840202,0.131698,0.167474,0.180474,0.120066,0.133268,0.155921,0,0
319,Healing kidneys with nanotechnology,"Each year, there are some 13.3 million new cases of acute kidney injury (AKI), a serious affliction. Formerly known as acute renal failure, the ailment produces a rapid buildup of nitrogenous wastes and decreases urine output, usually within hours or days of disease onset. Severe complications often ensue.AKI is responsible for 1.7 million deaths annually. Protecting healthy kidneys from harm and treating those already injured remains a significant challenge for modern medicine.In new research appearing in the journal Nature Biomedical Engineering, Hao Yan and his colleagues at the University of Wisconsin-Madison and in China describe a new method for treating and preventing AKI. Their technique involves the use of tiny, self-assembling forms measuring just billionths of a meter in diameter.Yan directs the Biodesign Center for Molecular Design and Biomimetics and is the Martin D. Glick Distinguished Professor in the School of Molecular Sciences at ASU.The triangular, tubular and rectangular shapes are designed and built using a method known as DNA origami. Here, the base pairing properties of DNA's four nucleotides are used to engineer and fabricate DNA origami nanostructures (DONs), which self-assemble and preferentially accumulate in kidneys.""The interdisciplinary collaboration between nanomedicine and the in-vivo imaging team led by professor Weibo Cai at the University of Wisconsin-Madison and the DNA nanotechnology team has led to a novel application -- applying DNA origami nanostructures to treat acute kidney injury,"" Yan says. ""This represents a new horizon for DNA nanotechnology research.""Experiments described in the new study -- conducted in mice as well as human embryonic kidney cells -- suggest that DONs act as a rapid and active kidney protectant and may also alleviate symptoms of AKI. The distribution of DONs was examined with positron emission tomography (PET). Results showed that the rectangular nanostructures were particularly successful, protecting the kidneys from harm as effectively as the leading drug therapy and alleviating a leading source of AKI known as oxidative stress.The study is the first to explore the distribution of DNA nanostructures in a living system by means of quantitative imaging with PET and paves the way for a host of new therapeutic approaches for the treatment of AKI as well as other renal diseases.""This is an excellent example of team science, with multidisciplinary and multinational collaboration,"" Cai said. ""The four research groups are located in different countries, but they communicate regularly and have synergistic expertise. The three equally-contributing first authors (Dawei Jiang, Zhilei Ge, Hyung-Jun Im) also have very different backgrounds, one in radiolabeling and imaging, one in DNA nanostructures, and the other in clinical nuclear medicine. Together, they drove the project forward.""Vital organKidneys perform essential roles in body, removing waste and extra water from the blood to form urine. Urine then flows from the kidneys to the bladder through the ureters. Wastes in the blood are produced from the normal breakdown of active muscle and from foods, which the body requires for energy and self-repair.Acute kidney injury can range considerably in severity. In advanced AKI, kidney transplantation is required as well as supportive therapies including rehydration and dialysis. Contrast-induced AKI, a common form of the illness, is caused by contrast agents sometimes used to improve the clarity of medical imaging. An anti-oxidant drug known as N-acetylcysteine (NAC) is used clinically to protect the kidneys from toxic assault during such procedures, but poor bioavailability of the drug in the kidneys can limit its effectiveness. (Currently, there is no known cure for AKI.)Nanomedicine -- the engineering of atoms or molecules at the nanoscale for biomedical applications -- represents a new and exciting avenue of medical exploration and therapy. Recent research in the field has driven advances leading to improved imaging and therapeutics for a range of diseases, including AKI, though the use of nanomaterials within living systems in order to treat kidney disease has thus far been limited.The base-pairing properties of nucleic acids like DNA and RNA enable the design of tiny programmable structures of predictable shape and size, capable of performing a multitude of tasks. Further, these nanoarchitectures are desirable for use in living systems due to their stability, low toxicity, and low immunogenicity.New designsThe current study marks the first investigation of DNA origami nanostructures within living organisms, using quantitative imaging to track their behavior. The PET imaging used in the study allowed for a quantitative and reliable real-time method to study the circulation of DONs in a living organism and to assess their physiological distribution. Rectangular DONs were identified as the most effective therapeutic to treat AKI in mice, based on the PET analysis.Yan and his colleagues prepared a series of DONs and used radio labeling to study their behavior in mouse kidney, using PET imaging. The PET scans showed that the DONs had preferentially accumulated in the kidneys of healthy mice as well as those with induced AKI. Of the three shapes used in the experiments, the rectangular DONs provided the greatest benefit in terms of protection and therapy and were comparable in their effect to the drug NAC, considered the gold standard treatment for AKI.Patients with kidney disease often have accompanying maladies, including a high incidence of cardiovascular disease and malignancy. Acute kidney illness may be induced through the process of oxidative stress, which results from an increase in oxygen-containing waste products known as reactive oxygen species, that cause damage to lipids, proteins and DNA. This can occur when the delicate balance of free radicals and anti-oxidant defenses becomes destabilized, causing inflammation and accelerating the progression of renal disease. (Foods and supplements rich in antioxidants act to protect cells from the harmful effects of reactive oxygen species.)Safeguarding kidneys with DNA geometryThe protective and therapeutic effects of the DONs described in the new study are due to the ability of the nanostructures to scavenge reactive oxygen species, thereby insulating vulnerable cells from damage due to oxidative stress. This effect was studied in human embryonic kidney cell lines as well as in living mice. The accumulation of the nanostructures in both healthy and diseased kidneys provided an increased therapeutic effect compared with traditional AKI therapy. (DONs alleviated oxidative stress within 2 hours of incubation with affected kidney cells.)Improvement in AKI kidney function -- comparable with high-dose administration of the drug NAC -- was observed following the introduction of nanostructures. Examination of stained tissue samples further confirmed the beneficial effects of the DONs in the kidney.The authors propose several mechanisms to account for the persistence in the kidneys of properly folded origami nanostructures, including their resistance to digestive enzymes, avoidance of immune surveillance and low protein absorption.Levels of serum creatinine and blood urea nitrogen (BUN) were used to assess renal function in mice. AKI mice treated with rectangular DONs displayed improved kidney excretory function comparable to mice receiving treatment using the mainline drug NAC.Further, the team established the safety of rectangular DONs, evaluating their potential to elicit an immune response in mice by examining blood levels of interleukin-6 and tumor necrosis factor alpha. Results showed the DONs were non-immunogenetic and tissue staining of heart, liver, spleen lungs and kidney revealed their low toxicity in primary organs, making them attractive candidates for clinical use in humans.Based on the effective scavenging of reactive oxygen species by DONs in both human kidney cell culture and living mouse kidney, the study concludes that the approach may indeed provide localized protection for kidneys from AKI and may even offer effective treatment for AKI-damaged kidneys or other renal disorders.The successful proof-of-concept study expands the potential for a new breed of therapeutic programmable nanostructures, engineered to address far-flung medical challenges, from smart drug delivery to precisely targeted organ and tissue repair.Story Source:Materials provided by Arizona State University. Original written by Richard Harth. ",0.0588622,0.139395,0.125428,0.142792,0.101711,0.110298,0.152538,0,0
320,"Self-assembling protein filaments designed and built from scratch Scientists' newfound ability to create these components could lead to constructing entirely novel, useful materials unlike any found in nature","For the first time, scientists have created, from scratch, self-assembling protein filaments.These were constructed from identical protein subunits that snap together spontaneously to form long, helical, thread-like structures.In the natural world, protein filaments are essential components of several structural and moving parts in living cells, as well as many body tissues.These include the cytoskeletons that give cells their shape, the cellular microtubules that orchestrate cell division, and the most common protein in our bodies, collagen, which gives both strength and flexibility to our cartilage, skin and other tissues.""Being able to create protein filaments from scratch -- or de novo -- will help us better understand the structure and mechanics of naturally occurring protein filaments and will also allow us to create entirely novel materials unlike any found in nature,"" said David Baker, a professor of biochemistry at the University of Washington School of Medicine, and director of the UW Institute for Protein Design, who directed the project. He is also a Howard Hughes Medical Institute investigator,Such materials might include human-made fibers that equal or surpass the strength of spider silk, which by weight is stronger than steel, Baker said. He also mentioned the possibility of nano-scale wire circuitry.To design the filaments, the researchers used a computer program developed in the Baker laboratory, called Rosetta, that can predict the shape of a protein from its amino acid sequence.To function properly, proteins must fold into a precise shape. This folding is driven by properties of the individual amino acids and how they interact with each other and the surrounding fluid environment. The forces of attraction and repulsion drive the protein to come to rest in a shape that has the lowest energy level.By calculating which shape would balance out these forces of attraction and repulsion to yield the lowest total energy level, Rosetta can predict, with a high degree of accuracy, the shape a protein will assume in nature.Using Rosetta, the researchers set out to design small proteins that had amino acids on their surface that would cause them to latch onto each other. This allowed them to assemble into a helix by aligning like steps in a winding staircase. For the helix to be stable, the designed protein binds other copies positioned above and below it as the helix winds around, tier on tier.""We were eventually able to design proteins that would snap together like Legos,"" said Hao Shen, a Ph.D. candidate at the UW Molecular Engineering & Sciences Institute. He and Jorge Fallas, an acting instructor in biochemistry at the UW School of Medicine, are lead authors of a paper describing the approach.This paper will be published online by the journal Science on Thursday, Nov. 8, 2018.Fallas said that the designed proteins are relatively small. They are made up of about only 180 to 200 amino acids and measure only about one nanometer in length, but assemble into stable filaments more than 10,000 nanometers long. A nanometer is 1 billionth of a meter, or about the width of 10 hydrogen atoms lined up side by side.The researchers also showed that that, by tinkering with the designed protein's concentration in solution and by adding caps that inhibited the design's ability to bind, they could drive the filaments to grow or to disassemble.""The ability to program the dynamics of filament formation will give us insights into how filament assembly and disassembly is regulated in nature,"" said Baker. ""The stability of these proteins suggest they could serve as easily modifiable scaffolds for a range of applications ranging from new diagnostic tests to nano-electronics.""Story Source:Materials provided by University of Washington Health Sciences/UW Medicine. ",0.0860967,0.150479,0.170417,0.168053,0.126685,0.146129,0.173436,0,0
321,Transforming carbon dioxide into industrial fuels,"Imagine a day when -- rather than being spewed into the atmosphere -- the gases coming from power plants and heavy industry are instead captured and fed into catalytic reactors that chemically transform greenhouse gases like carbon dioxide into industrial fuels or chemicals and that emit only oxygen.It's a future that Haotian Wang says may be closer than many realize.A Fellow at the Rowland Institute at Harvard, Wang and colleagues have developed an improved system to use renewable electricity to reduce carbon dioxide into carbon monoxide -- a key commodity used in a number of industrial processes. The system is described in a November 8 paper published in Joule, a newly launched sister journal of Cell press.""The most promising idea may be to connect these devices with coal-fired power plants or other industry that produces a lot of CO2,"" Wang said. ""About 20 percent of those gases are CO2, so if you can pump them into this cell...and combine it with clean electricity, then we can potentially produce useful chemicals out of these wastes in a sustainable way, and even close part of that CO2 cycle.""The new system, Wang said, represents a dramatic step forward from the one he and colleagues first described in a 2017 paper in Chem.Where that old system was barely the size of a cell phone and relied on two electrolyte-filled chambers, each of which held an electrode, the new system is cheaper and relies on high concentrations of CO2 gas and water vapor to operate more efficiently -- just one 10-by-10-centimeter cell, Wang said, can produce as much as four liters of CO per hour.The new system, Wang said, addresses the two main challenges -- cost and scalability -- that were seen as limiting the initial approach.""In that earlier work, we had discovered the single nickel-atom catalysts which are very selective for reducing CO2 to CO...but one of the challenges we faced was that the materials were expensive to synthesize,"" Wang said. ""The support we were using to anchor single nickel atoms was based on graphene, which made it very difficult to scale up if you wanted to produce it at gram or even kilogram scale for practical use in the future.""To address that problem, he said, his team turned to a commercial product that's thousands of times cheaper than graphene as an alternative support -- carbon black.Using a process similar to electrostatic attraction, Wang and colleagues are able to absorb single nickel atoms (positively charged) into defects (negatively charged) in carbon black nanoparticles, with the resulting material being both low-cost and highly selective for CO2 reduction.""Right now, the best we can produce is grams, but previously we could only produce milligrams per batch,"" Wang said. ""But this is only limited by the synthesis equipment we have; if you had a larger tank, you could make kilograms or even tons of this catalyst.""The other challenge Wang and colleagues had to overcome was tied to the fact that the original system only worked in a liquid solution.The initial system worked by using an electrode in one chamber to split water molecules into oxygen and protons. As the oxygen bubbled away, protons conducted through the liquid solution would move into the second chamber, where -- with the help of the nickel catalyst -- they would bind with CO2 and break the molecule apart, leaving CO and water. That water could then be fed back into the first chamber, where it would again be split, and the process would start again.""The problem was that, the CO2 we can reduce in that system are only those dissolved in water; most of the molecules surrounding the catalyst were water,"" he said. ""There was only a trace amount of CO2, so it was pretty inefficient.""While it may be tempting to simply increase the voltage applied on the catalyst to increase the reaction rate, that can have the unintended consequence of splitting water, not reducing CO2, Wang said.""If you deplete the CO2 that's close to the electrode, other molecules have to diffuse to the electrode, and that takes time,"" Wang said. ""But if you're increasing the voltage, it's more likely that the surrounding water will take that opportunity to react and split into hydrogen and oxygen.""The solution proved to be relatively simple -- to avoid splitting water, the team took the catalyst out of solution.""We replaced that liquid water with water vapor, and feed in high-concentration CO2 gas,"" he said. ""So if the old system was more than 99 percent water and less than 1 percent CO2, now we can completely reverse that, and pump 97 percent CO2 gas and only 3 percent water vapor into this system. Before those liquid water also functions as ion conductors in the system, and now we use ion exchange membranes instead to help ions move around without liquid water.""The impact is that we can deliver an order of magnitude higher current density,"" he continued. ""Previously, we were operating at about ten milliamps-per-centimeter squared, but today we can easily ramp up to 100 milliamps.""Going forward, Wang said, the system still has challenges to overcome -- particularly related to stability.""If you want to use this to make an economic or environmental impact, it needs to have a continuous operations of thousands of hours,"" he said. ""Right now, we can do this for tens of hours, so there's still a big gap, but I believe those problems can be addressed with more detailed analysis of both the CO2 reduction catalyst and the water oxidation catalyst.""Ultimately, Wang said, the day may come when industry will be able to capture the CO2 that is now released into the atmosphere and transform it into useful products.""Carbon monoxide is not a particularly high value chemical product,"" Wang said. ""To explore more possibilities, my group has also developed several copper-based catalysts that can further reduce CO2 into products that are much more valuable.""Wang credited the freedom he enjoyed at the Rowland Institute for helping lead to breakthroughs like the new system.""Rowland has provided me, as an early career researcher, a great platform for independent research, which initiates a large portion of the research directions my group will continue to push forward,"" said Wang, who recently accepted a position at Rice University. ""I will definitely miss my days here.""Story Source:Materials provided by Harvard University. Original written by Peter Reuell. ",0.0465539,0.126515,0.0868629,0.107897,0.0730421,0.0976684,0.104614,0,0
322,"New material, manufacturing process use sun's heat for cheaper renewable electricity ","Solar power accounts for less than 2 percent of U.S. electricity but could make up more than that if the cost of electricity generation and energy storage for use on cloudy days and at nighttime were cheaper.A Purdue University-led team developed a new material and manufacturing process that would make one way to use solar power -- as heat energy -- more efficient in generating electricity.The innovation is an important step for putting solar heat-to-electricity generation in direct cost competition with fossil fuels, which generate more than 60 percent of electricity in the U.S.""Storing solar energy as heat can already be cheaper than storing energy via batteries, so the next step is reducing the cost of generating electricity from the sun's heat with the added benefit of zero greenhouse gas emissions,"" said Kenneth Sandhage, Purdue's Reilly Professor of Materials Engineering.The research, which was done at Purdue in collaboration with the Georgia Institute of Technology, the University of Wisconsin-Madison and Oak Ridge National Laboratory, published in the journal Nature.This work aligns with Purdue's Giant Leaps celebration, acknowledging the university's global advancements made for a sustainable economy and planet as part of Purdue's 150th anniversary. This is one of the four themes of the yearlong celebration's Ideas Festival, designed to showcase Purdue as an intellectual center solving real-world issues.Solar power doesn't only generate electricity via panels in farms or on rooftops. Another option is concentrated power plants that run on heat energy.Concentrated solar power plants convert solar energy into electricity by using mirrors or lenses to concentrate a lot of light onto a small area, which generates heat that is transferred to a molten salt. Heat from the molten salt is then transferred to a ""working"" fluid, supercritical carbon dioxide, that expands and works to spin a turbine for generating electricity.To make solar-powered electricity cheaper, the turbine engine would need to generate even more electricity for the same amount of heat, which means the engine needs to run hotter.The problem is that heat exchangers, which transfer heat from the hot molten salt to the working fluid, are currently made of stainless steel or nickel-based alloys that get too soft at the desired higher temperatures and at the elevated pressure of supercritical carbon dioxide.Inspired by the materials his group had previously combined to make ""composite"" materials that can handle high heat and pressure for applications like solid-fuel rocket nozzles, Sandhage worked with Asegun Henry, now at the Massachusetts Institute of Technology, to conceive of a similar composite for more robust heat exchangers.Two materials showed promise together as a composite: The ceramic zirconium carbide, and the metal tungsten.Purdue researchers created plates of the ceramic-metal composite. The plates host customizable channels for tailoring the exchange of heat, based on simulations of the channels conducted at Georgia Tech by Devesh Ranjan's team.Mechanical tests by Edgar Lara-Curzio's team at Oak Ridge National Laboratory and corrosion tests by Mark Anderson's team at Wisconsin-Madison helped show that this new composite material could be tailored to successfully withstand the higher temperature, high-pressure supercritical carbon dioxide needed for generating electricity more efficiently than today's heat exchangers.An economic analysis by Georgia Tech and Purdue researchers also showed that the scaled-up manufacturing of these heat exchangers could be conducted at comparable or lower cost than for stainless steel or nickel alloy-based ones.""Ultimately, with continued development, this technology would allow for large-scale penetration of renewable solar energy into the electricity grid,"" Sandhage said. ""This would mean dramatic reductions in human-made carbon dioxide emissions from electricity production.""A YouTube video is available at https://youtu.be/PMC3EE19ouw.Story Source:Materials provided by Purdue University. Original written by Kayla Wiles. ",0.0729729,0.145131,0.126377,0.152005,0.102179,0.156519,0.161673,0,0
323,Large-scale US wind power would cause warming that would take roughly a century to offset,"All large-scale energy systems have environmental impacts, and the ability to compare the impacts of renewable energy sources is an important step in planning a future without coal or gas power. Extracting energy from the wind causes climatic impacts that are small compared to current projections of 21st century warming, but large compared to the effect of reducing US electricity emissions to zero with solar. Research publishing in the journal Joule on October 4 reports the most accurate modelling yet of how increasing wind power would affect climate, finding that large-scale wind power generation would warm the Continental United States 0.24 degrees Celsius because wind turbines redistribute heat in the atmosphere.""Wind beats coal by any environmental measure, but that doesn't mean that its impacts are negligible,"" says senior author David Keith, an engineering and public policy professor at Harvard University. ""We must quickly transition away from fossil fuels to stop carbon emissions. In doing so, we must make choices between various low-carbon technologies, all of which have some social and environmental impacts.""""Wind turbines generate electricity but also alter the atmospheric flow,"" says first author Lee Miller. ""Those effects redistribute heat and moisture in the atmosphere, which impacts climate. We attempted to model these effects on a continental scale.""To compare the impacts of wind and solar, Keith and Miller started by establishing a baseline for the 2012-2014 US climate using a standard weather forecasting model. Then they added in the effect on the atmosphere of covering one third of the Continental US with enough wind turbines to meet present-day US electricity demand. This is a relevant scenario if wind power plays a major role in decarbonizing the energy system in the latter half of this century. This scenario would warm the surface temperature of the Continental US by 0.24 degrees Celsius.Their analysis focused on the comparison of climate impacts and benefits. They found that it would take about a century to offset that effect with wind-related reductions in greenhouse gas concentrations. This timescale was roughly independent of the specific choice of total wind power generation in their scenarios.""The direct climate impacts of wind power are instant, while the benefits accumulate slowly,"" says Keith. ""If your perspective is the next 10 years, wind power actually has -- in some respects -- more climate impact than coal or gas. If your perspective is the next thousand years, then wind power is enormously cleaner than coal or gas.""More than ten previous studies have now observed local warming caused by US wind farms. Keith and Miller compared their simulated warming to observations and found rough consistency between the observations and model.They also compared wind power's impacts with previous projections of solar power's influence on the climate. They found that, for the same energy generation rate, solar power's impacts would be about 10 times smaller than wind. But both sources of energy have their pros and cons.""In terms of temperature difference per unit of energy generation, solar power has about 10 times less impact than wind,"" says Miller. ""But there are other considerations. For example, solar farms are dense, whereas the land between wind turbines can be co-utilized for agriculture."" The density of wind turbines and the time of day during which they operate can also influence the climatic impacts.Keith and Miller's simulations do not consider any impacts on global-scale meteorology, so it remains somewhat uncertain how such a deployment of wind power may affect the climate in other countries.""The work should not be seen as a fundamental critique of wind power. Some of wind's climate impacts may be beneficial. So rather, the work should be seen as a first step in getting more serious about assessing these impacts,"" says Keith. ""Our hope is that our study, combined with the recent direct observations, marks a turning point where wind power's climatic impacts begin to receive serious consideration in strategic decisions about decarbonizing the energy system.""Keith and Miller also have a related paper, ""Observation-based solar and wind power capacity factors and power densities,"" being published in Environmental Research Letters on October 4, which validates the generation rates per unit area simulated here using observations.Story Source:Materials provided by Cell Press. ",0.131233,0.138252,0.180867,0.150374,0.119039,0.212469,0.151835,0,0
324,Device could provide refrigeration for off-grid locations,"MIT researchers have devised a new way of providing cooling on a hot sunny day, using inexpensive materials and requiring no fossil fuel-generated power. The passive system, which could be used to supplement other cooling systems to preserve food and medications in hot, off-grid locations, is essentially a high-tech version of a parasol.The system allows emission of heat at mid-infrared range of light that can pass straight out through the atmosphere and radiate into the cold of outer space, punching right through the gases that act like a greenhouse. To prevent heating in the direct sunlight, a small strip of metal suspended above the device blocks the sun's direct rays.The new system is described this week in the journal Nature Communications in a paper by research scientist Bikram Bhatia, graduate student Arny Leroy, professor of mechanical engineering and department head Evelyn Wang, professor of physics Marin Soljacic, and six others at MIT.In theory, the system they designed could provide cooling of as much as 20 degrees Celsius (36 degrees Fahrenheit) below the ambient temperature in a location like Boston, the researchers say. So far, in their initial proof-of-concept testing, they have achieved a cooling of 6 C (about 11 F). For applications that require even more cooling, the remainder could be achieved through conventional refrigeration systems or thermoelectric cooling.Other groups have attempted to design passive cooling systems that radiate heat in the form of mid-infrared wavelengths of light, but these systems have been based on complex engineered photonic devices that can be expensive to make and not readily available for widespread use, the researchers say. The devices are complex because they are designed to reflect all wavelengths of sunlight almost perfectly, and only to emit radiation in the mid-infrared range, for the most part. That combination of selective reflectivity and emissivity requires a multilayer material where the thicknesses of the layers are controlled to nanometer precision.But it turns out that similar selectivity can be achieved by simply blocking the direct sunlight with a narrow strip placed at just the right angle to cover the sun's path across the sky, requiring no active tracking by the device. Then, a simple device built from a combination of inexpensive plastic film, polished aluminum, white paint, and insulation can allow for the necessary emission of heat through mid-infrared radiation, which is how most natural objects cool off, while preventing the device from being heated by the direct sunlight. In fact, simple radiative cooling systems have been used since ancient times to achieve nighttime cooling; the problem was that such systems didn't work in the daytime because the heating effect of the sunlight was at least 10 times stronger than the maximum achievable cooling effect.But the sun's heating rays travel in straight lines and are easily blocked -- as we experience, for example, by stepping into the shadow of a tree on a hot day. By shading the device by essentially putting an umbrella over it, and supplementing that with insulation around the device to protect it from the ambient air temperature, the researchers made passive cooling more viable.""We built the setup and did outdoors experiments on an MIT rooftop,"" Bhatia says. ""It was done using very simple materials"" and clearly showed the effectiveness of the system.""It's kind of deceptively simple,"" Wang says. ""By having a separate shade and an emitter to the atmosphere -- two separate components that can be relatively low-cost -- the system doesn't require a special ability to emit and absorb selectively. We're using angular selectivity to allow blocking the direct sun, as we continue to emit the heat-carrying wavelengths to the sky.""This project ""inspired us to rethink about the usage of 'shade,'"" says Yichen Shen, a research affiliate and co-author of the paper. ""In the past, people have only been thinking about using it to reduce heating. But now, we know if the shade is used smartly together with some supportive light filtering, it can actually be used to cool the object down,"" he says.One limiting factor for the system is humidity in the atmosphere, Leroy says, which can block some of the infrared emission through the air. In a place like Boston, close to the ocean and relatively humid, this constrains the total amount of cooling that can be achieved, limiting it to about 20 degrees Celsius. But in drier environments, such as the southwestern U.S. or many desert or arid environments around the world, the maximum achievable cooling could actually be much greater, he points out, potentially as much as 40 C (72 F).While most research on radiative cooling has focused on larger systems that might be applied to cooling entire rooms or buildings, this approach is more localized, Wang says: ""This would be useful for refrigeration applications, such as food storage or vaccines."" Indeed, protecting vaccines and other medicines from spoilage in hot, tropical conditions has been a major ongoing challenge that this technology could be well-positioned to address.Even if the system wasn't sufficient to bring down the temperature all the way to needed levels, ""it could at least reduce the loads"" on the electrical refrigeration systems, to provide just the final bit of cooling, Wang says.The system might also be useful for some kinds of concentrated photovoltaic systems, where mirrors are used to focus sunlight on a solar cell to increase its efficiency. But such systems can easily overheat and generally require active thermal management with fluids and pumps. Instead, the backside of such concentrating systems could be fitted with the mid-infrared emissive surfaces used in the passive cooling system, and could control the heating without any active intervention.As they continue to work on improving the system, the biggest challenge is finding ways to improve the insulation of the device, to prevent it from heating up too much from the surrounding air, while not blocking its ability to radiate heat. ""The main challenge is finding insulating material that would be infrared-transparent,"" Leroy says.The team has applied for patents on the invention and hope that it can begin to find real-world applications quite rapidly.Story Source:Materials provided by Massachusetts Institute of Technology. ",0.0572675,0.0812304,0.0857205,0.118306,0.0916025,0.0962849,0.117418,0,0
325,"Electrical cable triggers lightweight, fire-resistant cladding discovery ","A University of Melbourne researcher has led the successful development of an organic, non-combustible and lightweight cladding core -- a product that was previously thought to be impossible to create.Typically, lightweight cladding is made from organic, carbon-based, composite materials like plastic, but these materials by their nature are combustible. Non-combustible materials like steel, ceramic tiles or concrete are much heavier and more expensive to produce and install.University of Melbourne Fire Engineering Group research leader Kate Nguyen has discovered that the plastic insulation around electrical cables uses tiny ceramic particles that activate and chemically interact with each other, forming and spreading a heat resistant network through the material.In partnership with construction materials company Envirosip, who commissioned the research, Dr Nguyen began experimenting with different ceramic particles at the University's testing furnace at Creswick, north west of Melbourne.After several false starts, Dr Nguyen provided expert guidance in formulating a material that could withstand heat of 750 degrees Celsius.""When it passed our first test I was excited, but even after the fifth time I still couldn't quite believe it,"" Dr Nguyen said.The development comes in the wake of the 2017 Grenfell Tower blaze in London that cost the lives of 72 people and as the building industry globally works to create a lightweight cladding material that does not catch fire.The material itself is lightweight and feels like a compressed powder. A pale grey colour, the tiny ceramic particles appear as dark specks. At high temperatures these particles blend with the rest of the material, turning it a dark grey colour and rendering it non-combustible.The material has been tested by an independent testing facility approved by the National Association of Testing Authorities and has achieved Australian and International Standards on combustibility of construction materials.Envirosip and the University will now work to commericalise the development, which has been carried out as part of the Australian Research Council's Centre for Advanced Manufacturing of Prefabricated Housing.""When you are doing research, not all ideas will be successful. To go from success to commercialisation is another big step, but we believe we have developed something special that will be significant for the industry,"" Dr Nguyen said.This research was presented at the 2nd Edition Fire Safety and Cladding Summit in Sydney on Tuesday, 20 November.Story Source:Materials provided by University of Melbourne. ",0.104379,0.169958,0.18703,0.17139,0.138495,0.152872,0.170588,0,0
326,Revealing the inner working of magnetic materials,"Chromium nitride, CrN, is a magnetic material used in industry as, among other things, a hard surface coating. It is also of interest to researchers, since it is a poor conductor of heat at high temperatures, which makes it suitable for use in, for example, thermoelectric systems. In such systems, the material is to conduct current without conducting heat.The behaviour of chromium nitride, however, is somewhat remarkable at slightly higher temperatures. Nitrides are compounds that contain nitrogen, N, together with one other element. The ability of most nitrides to conduct heat falls slowly but surely as the temperature increases. The heat conduction of chromium nitride, in contrast, falls precipitously after a moderate temperature rise, and then remains at a constant low level, even as the material is heated to 600 eC. The mechanisms behind this behaviour have left researchers perplexed for many years.The past decade has seen major breakthroughs in theoretical research in materials science. Researchers have determined which calculation methods are most accurate, and have gained access to sufficiently powerful supercomputers to be able to carry out the calculations.""There has been a major hole in our knowledge in the particular case of how magnetic materials function at high temperatures,"" says Bjern Alling, researcher in theoretical physics at LiU.It was nearly four years ago, at the end of 2014, that he was awarded a major research grant from the Swedish Research Council to attempt to fill this hole, in collaboration with researchers at the Max-Planck-Institut fer Eisenforschung in Desseldorf. Bjern Alling spent two years at the institute, a world-leader in magnetic materials research.The collaboration has been successful and resulted in an article in the journal Physical Review Letters, where the group describes a new method that has enabled it to calculate exactly what happens in chromium nitride when it is heated. At last we have theoretical calculations that agree with the behaviour of the material.""We want to understand the materials, independently of their temperature, pressure and composition, and be able to describe them accurately. The theoretical calculations and the methods we have developed give a stable basis to stand on when developing industrial applications. It would have been impossible to determine this basis by experiments,"" says Bjern Alling.The method they have developed gives results of high accuracy, and this means that the calculations are highly demanding.In solid materials, the atoms are arranged in a well-organised crystal structure, at definite distances from each other. As the material is heated, the atoms start to vibrate.Each atom in a magnetic material contains what can be thought of as a tiny compass needle, a dipole with a positive and a negative end. In classical magnetic materials, such as iron, the needles all point in the same direction, which gives the material its typical magnetic properties. As the material is heated, however, the compass needles start to rotate in an unpredictable manner.Methods are available to calculate and simulate the vibrations and rotations with high accuracy separately, but they predict that the ability to conduct heat will decrease gradually. This is not what happens for chromium nitride.""We have now developed a method in which we describe how the atomic vibrations change on a femtosecond timescale, calculating the forces in the atoms using quantum mechanical methods. To this we add calculations of spin dynamics -- how much the magnetism in the atom rotates in a femtosecond. We then put this calculation back into the dynamic model of how atoms vibrate,"" Bjern Alling explains.The method was successful.""Chromium nitride is remarkable for its low heat conduction at slightly elevated temperatures. We have now been able to show why, and our simulations predict the behaviour accurately. No one has managed to do this previously.""The calculation and simulation of what happens in the material during 30 picoseconds requires more than a month of processor time for the resources available to the researchers at the National Supercomputer Centre at LiU and in Desseldorf""We have been able to combine a deep understanding of the fundamental physical and quantum phenomena, and we have had access to sufficient computer power. It will be some time before the method is widely used in science, since the calculations are so accurate and demanding, but we must use this method to make progress,"" says Bjern Alling.The next step will be to apply the method to iron and its alloys. This is one of the oldest materials used throughout human history, but we still don't have a deep understanding of it.""This is theoretical research with huge practical applications, not least in the steel industry,"" says Bjern Alling.Story Source:Materials provided by Linkeping University. ",0.0520022,0.0900417,0.108414,0.13385,0.0857746,0.112705,0.126179,0,0
327,"See-through film rejects 70 percent of incoming solar heat Material could be used to coat windows, save on air-conditioning costs","To battle the summer heat, office and residential buildings tend to crank up the air conditioning, sending energy bills soaring. Indeed, it's estimated that air conditioners use about 6 percent of all the electricity produced in the United States, at an annual cost of $29 billion dollars -- an expense that's sure to grow as the global thermostat climbs.Now MIT engineers have developed a heat-rejecting film that could be applied to a building's windows to reflect up to 70 percent of the sun's incoming heat. The film is able to remain highly transparent below 32 degrees Celsius, or 89 degrees Fahrenheit. Above this temperature, the researchers say, the film acts as an ""autonomous system"" to reject heat. They estimate that if every exterior-facing window in a building were covered in this film, the building's air conditioning and energy costs could drop by 10 percent.The film is similar to transparent plastic wrap, and its heat-rejecting properties come from tiny microparticles embedded within it. These microparticles are made from a type of phase-changing material that shrinks when exposed to temperatures of 85 degrees Fahrenheit or higher. In their more compact configurations, the microparticles give the normally transparent film a more translucent or frosted look.Applied to windows in the summer, the film could passively cool a building while still letting in a good amount of light. Nicholas Fang, a professor of mechanical engineering at MIT, says the material provides an affordable and energy-efficient alternative to existing smart window technologies.""Smart windows on the market currently are either not very efficient in rejecting heat from the sun, or, like some electrochromic windows, they may need more power to drive them, so you would be paying to basically turn windows opaque,"" Fang says. ""We thought there might be room for new optical materials and coatings, to provide better smart window options.""Fang and his colleagues, including researchers from the University of Hong Kong, have published their results in the journal Joule.""A fishnet in water""Just over a year ago, Fang began collaborating with researchers at the University of Hong Kong, who were keen on finding ways to reduce the energy usage of buildings in the city, particularly in the summer months, when the region grows notoriously hot and air-conditioning usage is at its peak.""Meeting this challenge is critical for a metropolitan area like Hong Kong, where they are under a strict deadline for energy savings,"" says Fang, referring to Hong Kong's commitment to reduce its energy use by 40 percent by the year 2025.After some quick calculations, Fang's students found that a significant portion of a building's heat comes through windows, in the form of sunlight.""It turns out that for every square meter, about 500 watts of energy in the form of heat are brought in by sunlight through a window,"" Fang says. ""That's equivalent to about five light bulbs.""Fang, whose group studies the light-scattering properties of exotic, phase-changing materials, wondered whether such optical materials could be fashioned for windows, to passively reflect a significant portion of a building's incoming heat.The researchers looked through the literature for ""thermochromic"" materials -- temperature-sensitive materials that temporarily change phase, or color, in response to heat. They eventually landed on a material made from poly (N-isopropylacrylamide)-2-Aminoethylmethacrylate hydrochloride microparticles. These microparticles resemble tiny, transparent, fiber-webbed spheres and are filled with water. At temperatures of 85 F or higher, the spheres essentially squeeze out all their water and shrink into tight bundles of fibers that reflect light in a different way, turning the material translucent.""It's like a fishnet in water,"" Fang says. ""Each of those fibers making the net, by themselves, reflects a certain amount of light. But because there's a lot of water embedded in the fishnet, each fiber is harder to see. But once you squeeze the water out, the fibers become visible.""In previous experiments, other groups had found that while the shrunken particles could reject light relatively well, they were less successful in shielding against heat. Fang and his colleagues realized that this limitation came down to the particle size: The particles used previously shrank to a diameter of about 100 nanometers -- smaller than the wavelength of infrared light -- making it easy for heat to pass right through.Instead, Fang and his colleagues expanded the molecular chain of each microparticle, so that when it shrank in response to heat, the particle's diameter was about 500 nanometers, which Fang says is ""more compatible to the infrared spectrum of solar light.""A comfort distinctionThe researchers created a solution of the heat-shielding microparticles, which they applied between two sheets of 12-by-12-inch glass to create a film-coated window. They shone light from a solar simulator onto the window to mimic incoming sunlight, and found that the film turned frosty in response to the heat. When they measured the solar irradiance transmitted through the other side of the window, the researchers found the film was able to reject 70 percent of the heat produced by the lamp.The team also lined a small calorimetric chamber with the heat-rejecting film and measured the temperature inside the chamber as they shone light from a solar simulator through the film. Without the film, the inner temperature heated to about 102 F -- ""about the temperature of a high fever,"" Fang notes. With the film, the inner chamber stayed at a more tolerable 93 F.""That's a big difference,"" Fang says. ""You could make a big distinction in comfort.""Going forward, the team plans to conduct more tests of the film to see whether tweaking its formula and applying it in other ways might improve its heat-shielding properties.This research was funded, in part, by the HKUST-MIT Consortium.Story Source:Materials provided by Massachusetts Institute of Technology. Original written by Jennifer Chu. ",0.0775517,0.105029,0.12187,0.151758,0.114102,0.12984,0.149208,0,0
328,Plasma-based system provides radical new path for water purification,"Many of today's methods of purifying water rely on filters and chemicals that need regular replenishing or maintenance. Millions of people, however, live in areas with limited access to such materials, leading the research community to explore new options of purifying water in using plasmas. Many plasma-based approaches are expensive, but a new class of plasma devices may change that.Researchers at the University of Alabama in Huntsville have been studying a new type of plasma generator for water purification. The new generator pulses voltage signals to ionize gas at atmospheric pressure and produce many useful byproducts, including hydroxyl radicals, which cause a cascade of reactions that lead to purer water samples.""We're finding ways to speed up the purification process,"" said Ryan Gott, a doctoral candidate in aerospace engineering at UAH who will present the research next week at the American Physical Society 71st Annual Gaseous Electronics Conference and the 60th Annual meeting of the APS Division of Plasma Physics, which will take place Nov. 5-9 at the Oregon Convention Center in Portland.""In theory, if this technology can be developed in a real-world, practical system, it would be able to purify water at lower costs than current methods can,"" Gott said.While the term ""plasma"" conjures images of superhot solar jets traveling through space, most plasma-based water purification approaches work through plasma's ability to generate reactive free radicals, rendering many compounds in the water inert. The plasma and ensuing chemical reactions release energy and chemical species that can kill even tough microcystin bacteria, one culprit in algal blooms that lurk in our water supply.""The pulses are so fast that it doesn't change the temperature of the water,"" Gott said. ""You can touch our plasma jet with your hand.""Unlike more common, ozone-producing plasma purifiers, the new device relies on the production of hydroxyl radicals. This method will hopefully sidestep some of the hurdles that have hampered ozone-based counterparts, namely high power consumption and challenges keeping excessive heat in check.Using optical emission spectroscopy, the UAH researchers have been able to compare how different factors play a role in producing more hydroxyl radicals from their plasma device. Increasing voltage, for example, appears to have the biggest effect on output, followed by increasing the frequency of the pulses.Right now, the device is limited to 10 kilovolts, but the researchers are hoping to see what higher voltage could mean down the road.Gott said after the group continues to better understand the mechanisms behind how the plasma interacts with water, he hopes to scale up the technology for point-of-use applications.""The end goal is to develop something that can be mass-produced and distributed to places that need it the most,"" he said.Story Source:Materials provided by American Physical Society. ",0.100077,0.135527,0.144738,0.158025,0.124238,0.161797,0.153243,0,0
329,Peak performance: New stellarator experiments show promising results Wendelstein 7-X superconducting stellarator successfully completes first operational phase,"Imagine building a machine so advanced and precise you need a supercomputer to help design it. That's exactly what scientists and engineers in Germany did when building the Wendelstein 7-X experiment. The device, funded by the German federal and state governments and the European Union, is a type of fusion device called a stellarator. The new experiment's goal is to contain a super-heated gas, called plasma, in a donut-shaped vessel using magnets that twist their way around the donut.The team completed construction of Wendelstein 7-X, the world's most advanced superconducting stellarator, in 2015 and, since then, scientists have been busy studying its performance.""The advantage of stellarators over other types of fusion machines is that the plasmas produced are extremely stable and very high densities are possible,"" said Dr. Novimir Pablant, a U.S. physicist from the Princeton Plasma Physics Laboratory, who works alongside a multinational team of scientists and engineers from Europe, Australia, Japan, and the United States (the U.S. collaboration is funded by the Department of Energy).Using a tool called an X-ray spectrometer, Pablant studied the light given off by the plasma to answer an important question: Did the design of Wendelstein 7-X's twisted magnetic field work? His results indicate that, indeed, the plasma temperatures and electric fields are already in the range required for peak performance. He will present his work at the American Physical Society Division of Plasma Physics conference in Portland, Ore.If the scientists working on Wendelstein 7-X are successful in optimizing the machine performance, the plasma contained in the donut will become even hotter than the sun. Atoms making up the plasma will fuse together, yielding safe, clean energy to be used for power. This achievement is a major milestone as it shows that it is possible to achieve temperatures of more than 10 million degrees in high-density plasmas using only microwaves to heat the electrons in the plasma. This achievement takes us one step closer to making fusion power a reality.Story Source:Materials provided by American Physical Society. ",0.124584,0.138862,0.141618,0.18276,0.135506,0.183006,0.171143,0,0
330,Taming plasmas: Improving fusion using microwaves Scientists use microwaves to keep high-energy particles in place,"We all know microwaves are good for cooking popcorn, but scientists have recently shown they can also prevent dangerous waves in plasmas and help produce clean, nearly limitless energy with fusion. Fusion takes place when fast moving atomic particles slam into each other and stick together. The particles need to be so hot that atoms break down, leaving a gas of charged particles called a plasma. The energy given off when plasma particles fuse can be harnessed to make electricity.To harness fusion energy, the fast particles can be confined by a strong magnetic field, which guides the particles along a closed path. If the particles get knocked off their closed path, they make fusion less efficient and can even damage the fusion device. Scientists, therefore, are looking for ways to prevent the energetic particles from veering off course.One way a particle can be kicked out of the fusion device is by interacting with waves. Just as a boat in a lake can be jostled by waves passing by, a particle in a plasma can get a boost of energy from waves moving along the magnetic field used to confine the plasma. In the work here, the waves are called Alfven waves named after the Nobel Prize winner Hannes Alfven who discovered them. Such waves are problematic for future tokamak fusion reactors because they make it more challenging to keep the plasma hot and undergoing fusion.""Controlling these waves helps us hold onto the fast particles that heat fusion plasmas,"" said Dr. Michael Van Zeeland from General Atomics, who led the research. The international team that carried out this research was composed of more than 30 scientists from across the globe who worked together to develop an approach to keep the waves in check.The research, which was conducted at the DIII-D National Fusion Facility in San Diego, California, and the ASDEX-Upgrade facility in Germany, will be presented at the American Physical Society Division of Plasma Physics meeting in Portland, Oregon. The scientists used a specific kind of microwaves, electron cyclotron waves, which they precisely directed near the location of the waves on the magnetic field. The microwaves were found to modify the wave activity significantly-in some cases completely removing them.This research yielded insight into these particular microwaves and how they interact with waves on the magnetic fields. The researchers believe the results can lead to the development of approaches to control or reduce the presence of waves on the magnetic fields and could help chart a path to more efficient fusion energy.Story Source:Materials provided by American Physical Society. ",0.138077,0.152914,0.192113,0.202147,0.159498,0.20413,0.206174,0,0
331,Inside job: A new technique to cool a fusion reactor A potentially game-changing approach could safely keep a fusion reactor from getting too hot,"Fusion offers the potential of near limitless energy by heating a gas trapped in a magnetic field to incredibly high temperatures where atoms are so energetic that they fuse together when they collide. But if that hot gas, called a plasma, breaks free from the magnetic field, it must be safely put back in place to avoid damaging the fusion device -- this problem has been one of the great challenges of magnetically confined fusion.During these so-called disruptions, the rapid release of the energy in the plasma can damage the fusion device: Intense heat can vaporize or melt the walls, large electrical currents can generate damaging forces, and high-energy ""runaway"" electron beams can cause intense localized damage.Making disruptions less disruptive involves injecting material into the plasma that evenly radiates away the plasma energy. One challenge is that the material has difficulty reaching the middle of the plasma before a disruption occurs. Researchers hope that getting material into the middle can provide ""inside-out"" cooling of the plasma, preventing the disruption and the production of runaway electrons.Researchers at the DIII-D National Fusion Facility have demonstrated a revolutionary new technique to achieve this ""inside-out"" cooling before a disruption occurs. A thin-walled diamond-shelled pellet carries a payload of boron dust deep into the plasma. The experiments show that shell pellets fired into the core at around 450 miles per hour can deposit boron dust deep in the plasma where it is most effective. The diamond shells gradually disintegrate in the plasma before releasing the dust near the center of the plasma.The new approach transforms prospects for fusion energy by potentially solving three major problems -- efficiently radiating away the plasma's heat, reducing forces by the plasma on the fusion device, and preventing the formation of energetic electron beams.As DIII-D Science Director, Richard Buttery, comments, ""Shell pellets offer the potential of dealing with all three facets of the challenge, eliminating risk of device harm.""Future work is aimed at creating more sophisticated shell designs that can carry larger payloads and penetrate reactor-class plasmas.Another technique being explored at DIII-D is known as shattered pellet injection. In this approach, solid frozen pellets made of a heavy isotope of hydrogen and neon or argon are fired toward the plasma at high speed. They shatter into small fragments before hitting the edge of the plasma. Researchers performed experiments and extrapolated the results to the large fusion device, ITER, being developed in France. They believe this technique will be effective in ITER.""The best way to reliably prevent disruptions remains an open question,"" said researcher Nick Eidietis, who works at the DIII-D fusion device in San Diego and will be presenting his research at the American Physical Society Division of Plasma Physics meeting in Portland, Oregon. ""But we are making significant progress in developing the understanding and techniques necessary to achieve fusion power. If this new shell technique fulfills its initial promise, it will transform prospects for reliable fusion power plant operation.""Story Source:Materials provided by American Physical Society. ",0.123687,0.16252,0.174959,0.205882,0.144491,0.189871,0.214715,0,0
332,Researchers turn plastic bottle waste into ultralight supermaterial,"Researchers from the the National University of Singapore (NUS) have made a significant contribution towards resolving the global issue of plastic waste, by creating a way to convert plastic bottle waste into aerogels for many useful applications.Plastic bottles are commonly made from polyethylene terephthalate (PET), which is the most recycled plastic in the world. The PET aerogels developed by the NUS-led research team using plastic bottle waste -- a world's first -- are soft, flexible, durable, extremely light and easy to handle. They also demonstrate superior thermal insulation and strong absorption capacity. These properties make them attractive for a wide range of applications, such as for heat and sound insulation in buildings, oil spill cleaning, and also as a lightweight lining for firefighter coats and carbon dioxide absorption masks that could be used during fire rescue operations and fire escape.This pioneering work was achieved by a research team led by Associate Professor Hai Minh Duong and Professor Nhan Phan-Thien from the Department of Mechanical Engineering at NUS Faculty of Engineering. The technology to produce PET aerogels was developed in collaboration with Dr Xiwen Zhang from the Singapore Institute of Manufacturing Technology (SIMTech) under the Agency for Science, Technology and Research (A*STAR).Recycling plastic bottle wastePlastic waste is toxic and non-biodegradable. Such waste often ends up in oceans and landfills, affecting marine life and causing problems such as groundwater contamination and land scarcity. Globally, the annual consumption of plastic bottles has been rising steadily, and it is expected to exceed half a trillion tons per year by 2021.""Plastic bottle waste is one of the most common type of plastic waste and has detrimental effects on the environment. Our team has developed a simple, cost-effective and green method to convert plastic bottle waste into PET aerogels for many exciting uses. One plastic bottle can be recycled to produce an A4-sized PET aerogel sheet. The fabrication technology is also easily scalable for mass production. In this way, we can help cut down the harmful environmental damage caused by plastic waste,"" said Assoc Prof Duong.Versatile PET aerogelsThe research team took two years (from August 2016 to August 2018) to develop the technology to fabricate PET aerogels. This work was published in the scientific journal Colloids and Surfaces A in August 2018.""Our PET aerogels are very versatile. We can give them different surface treatments to customise them for different applications. For instance, when incorporated with various methyl groups, the PET aerogels can absorb large amounts of oil very quickly. Based on our experiments, they perform up to seven times better than existing commercial sorbents, and are highly suitable for oil spill cleaning,"" added Prof Nhan.Lighter and safer firefighter coatsAnother novel application is to harness the heat insulation property of the PET aerogels for fire safety applications.Existing firefighter coats are bulky and they are often used with other breathing and safety equipment. This could take a toll on firefighters, especially during extended operations.When coated with fire retardant chemicals, the novel lightweight PET aerogel demonstrates superior thermal resistance and stability. It can withstand temperatures of up to 620 degree Celsius -- this is seven times higher than the thermal lining used in conventional firefighter coats, but weighs only about 10 per cent of the weight of conventional thermal lining. The soft and flexible nature of the PET aerogel also provides greater comfort.Prof Nhan explained, ""By adopting PET aerogels that are coated with fire retardants as a lining material, firefighter coats can be made much lighter, safer and cheaper. It is also possible to produce low-cost heat-resistant jackets for personal use.""2-in-1 mask that absorbs harmful carbon dioxide and dust particlesWhen coated with an amine group, the PET aerogel can quickly absorb carbon dioxide from the environment. Its absorption capacity is comparable to materials used in gas masks, which are costly and bulky. To illustrate this application, the team embedded a thin layer of PET aerogel into a commercial fine particle mask to create a prototype mask that can absorb both dust particles and carbon dioxide effectively.Prof Nhan said, ""In highly urbanised countries like Singapore, the carbon dioxide absorption masks and heat-resistant jackets made using PET aerogels can be placed alongside fire extinguishers in high-rise buildings to provide added protection to civilians when they escape from a fire.""""Masks lined with amine-reinforced PET aerogels can also benefit people living in countries such as China, where air pollution and carbon emission are major concerns. Such masks can be easily produced, and can also potentially be made reusable,"" added Assoc Prof Duong.NUS researchers are also looking into making simple surface modification to the PET aerogels for absorption of toxic gases such as carbon monoxide, which is the deadliest component of smoke.In their earlier work, the research team had successfully converted paper and fashion waste into cellulose and cotton aerogels respectively. Coupled with this latest innovation involving the recycling of plastic bottle waste into aerogels, the NUS team was recently awarded first place in the Sustainable Technologies category of the 2018 Create the Future Design Contest by Tech Briefs.Next stepsThe research team has filed a patent for its novel PET aerogel technology, and will continue to enhance the performance of the PET aerogels and explore new applications. The NUS researchers are also keen to work with companies to bring the technology to market.Story Source:Materials provided by National University of Singapore. ",0.0684685,0.117792,0.143206,0.137658,0.114645,0.114648,0.135265,0,0
333,Wearable heating pad with nanowires to conduct heat,"Sometimes nothing feels better on stiff, aching joints than a little heat. But many heating pads and wraps are rigid and provide uneven warmth, especially when the person is moving around. Researchers have now made a wearable heater by modifying woven Kevlare fabric with nanowires that conduct and retain heat. They report their results in ACS' journal Nano Letters.Even at rest, the human body produces a lot of heat, but most of this warmth dissipates to the air and is wasted. Cold-weather clothing is often made from materials that keep heat close to the body, offering thermal insulation. For even more warmth, scientists have tried coating textiles with metallic nanowires that can be heated with a small battery. However, researchers are still searching for a material that provides good thermal conductivity and insulation while being safe, inexpensive, durable and flexible. Hyung Wook Park and colleagues wondered if they could make a wearable heating device by incorporating metallic nanowires into Kevlare, the famous bullet-proof fiber used in many types of body armor.To make their wearable heater, the team grew copper-nickel nanowires between two Kevlare sheets. They filled in the spaces between the nanowires with a resin containing reduced graphene oxide to encourage uniform heating. Applying a low voltage (1.5 volts) to the composite material caused a rapid and uniform increase in surface temperature to 158 F -- a typical ""high"" setting on a heating pad. In another experiment, the team showed that the material acted as a thermal insulator by reflecting infrared radiation emitted from a hot plate set at human body temperature. The fabric was strong, flexible, breathable and washable, while still absorbing impacts similar to regular Kevlare. In addition to wearable heat therapy, the new material could be used to make heated body armor for police and military personnel in cold climates, the researchers say.Story Source:Materials provided by American Chemical Society. ",0.11887,0.165891,0.165984,0.171083,0.181963,0.146413,0.184055,0,0
334,New composite material that can cool itself down under extreme temperatures,"A cutting-edge material, inspired by nature, that can regulate its own temperature and could equally be used to treat burns and help space capsules withstand atmospheric forces is under development at the University of Nottingham.The research paper, Temperature -- dependent polymer absorber as a switchable state NIR reactor, is published in the journal Scientific Reports today (Friday 26 October).""A major challenge in material science is to work out how to regulate human-made material temperature as the human body can do in relationship to its environment,"" explains lead author Dr Mark Alston, Assistant Professor in Environmental Design, from the Faculty of Engineering.The research used a network of multiple microchannels with active flowing fluids (fluidics) as a method and proof of concept to develop a thermally-functional material made of a synthetic polymer. The material is enhanced with precise control measures that can switch conductive states to manage its own temperature in relationship to its environment.""This bio-inspired engineering approach advances the structural assembly of polymers for use in advanced materials. Nature uses fluidics to regulate and manage temperature in mammals and in plants to absorb solar radiation though photosynthesis and this research used a leaf-like model to mimic this function in the polymer.""Dr Alston adds: ""This approach will result in an advanced material that can absorb high solar radiation, as the human body can do, to cool itself autonomously whatever the environment it is placed in. A thermally-functional material could be used as a heat regulation system for burn injuries to cool skin surface temperature and monitor and improve healing.""This kind of heat flow management could also prove invaluable in space flight where high solar loads can cause thermal stresses on the structural integrity of space capsules.By regulation of the structural material temperature of the vehicle, this will not only advance structural properties but could also generate useful power. This thermal energy could be removed from the re-circulated fluid system to be stored in a reservoir tank on board the capsule. Once captured, the energy could be converted into electrical energy or to heat water for use by the crew.The experimental side of this research is laboratory-based and has been developed in collaboration with UK Government research institute: Scientific Research Facilities Council (SRFC). The next steps for the research are to secure funding for a demonstrator scale-up to present to aerospace manufacturing and to identify an industrial partner.Story Source:Materials provided by University of Nottingham. ",0.0777769,0.115717,0.133201,0.138254,0.106862,0.122909,0.152834,0,0
335,"Low cost, energy-saving radiative cooling system ready for real-world applications ","University of Colorado Boulder and University of Wyoming engineers have successfully scaled up an innovative water-cooling system capable of providing continuous day-and-night radiative cooling for structures. The advance could increase the efficiency of power generation plants in summer and lead to more efficient, environmentally-friendly temperature control for homes, businesses, utilities and industries.The new research demonstrates how the low-cost hybrid organic-inorganic radiative cooling metamaterial, which debuted in 2017, can be scaled into a roughly 140-square-foot array -- small enough to fit on most rooftops -- and act as a kind of natural air conditioner with almost no consumption of electricity.""You could place these panels on the roof of a single-family home and satisfy its cooling requirements,"" said Dongliang Zhao, lead author of the study and a postdoctoral researcher in CU Boulder's Department of Mechanical Engineering.The findings are described today in the journal Joule and take advantage of natural radiative cooling principles.""As Earth's temperature warms due to the absorbed heat from the sunlight during the day, it continuously emits infrared light to the cold universe all the time,"" said Professor Ronggui Yang of Mechanical Engineering and lead author of the study. ""During the night, Earth cools down due to the emission without the sunshine.""The researchers' film-like material reflects incoming almost all sunlight while still allowing an object's stored heat to escape as much as possible, keeping it cooler than ambient air even in the midday sun.""The material, which we can now produce at low cost using the current roll-to-roll manufacturing techniques, offers significant advantages."" said Associate Professor Xiaobo Yin of Mechanical Engineering and CU Boulder's Materials Science and Engineering Program.""We can now apply these materials on building roof tops, and even build large-scale water cooling systems like this one with significant advantages over the conventional air conditioning systems, which require high amounts of electricity to function,"" said Associate Professor Gang Tan of the University of Wyoming's Department of Civil and Architectural Engineering.The researchers tested their system outdoors in a variety of weather conditions, including wind, precipitation and humidity. In experiments conducted in August and September 2017, their proprietary RadiCold module kept a container of water covered by the metamaterial 20 degrees Fahrenheit cooler than the ambient air between 12:30 p.m. and 3 p.m., the most intense summer sunlight of the day.The researchers also introduced an element of dynamic scheduling to their technology, anticipating that structures such as offices may have limited or no cooling demand at night. In a building-integrated system, however, a cold storage unit could be added to capture the cold through heat transfer fluid such as water in this system and allow it to be retrieved during the subsequent day to reduce the cooling strain during peak demand periods.""We have built a module that performs in real-world, practical situations,"" said Yang. ""We have moved quite far and fast from a materials level to a system level.""The RadiCold module could become a viable solution for supplemental cooling for single-family homes, businesses, power plants, municipal utilities and data center facilities among other potential applications, Yang said.Additional co-authors of the study include CU Boulder graduate students Ablimit Aili and Yao Zhai as well as senior undergraduate students Jiatao Lu and Dillon Kidd of Mechanical Engineering. The U.S. Department of Energy's Advanced Research Projects Agency -- Energy (ARPA-E) provided funding for the research. The technology has been licensed to Radi-Cool Inc.Story Source:Materials provided by University of Colorado at Boulder. Original written by Trent Knoss. ",0.0680285,0.0977478,0.109888,0.132528,0.0859887,0.129379,0.137376,0,0
336,Ancient enzymes the catalysts for new discoveries,"University of Queensland-led research recreating 450 million-year-old enzymes has resulted in a biochemical engineering 'hack' which could lead to new drugs, flavours, fragrances and biofuels.Professor Elizabeth Gillam from UQ's School of Chemistry and Molecular Biosciences said the study showed ancient enzymes could survive high temperatures and that this could help create chemicals cheaply and at scale.""We looked at how we could use a biological agent, like enzymes, to accelerate chemical reactions, as an alternative to current commercial processes,"" Professor Gillam said.""It's often very difficult to make precise changes to complex chemicals, but this is essential in many industries, the pharmaceutical industry being a prime example.""These methods often attack multiple sites on a chemical, so one ends up with a mixture of by-products, while often requiring a lot of energy and creating harmful waste.""The team found enzymes that were more effective at higher temperatures and could be better, quicker and cheaper catalysts, using less energy and avoiding toxic chemicals.""Naturally occurring enzymes do not survive for long enough to make this alternative competitive -- so we came up with a hack,"" Professor Gillam said.""These enzymes' pre-Cambrian-era-ancestors were able to survive great heat, when temperatures on Earth were around 60 degrees Celsius.""We obtained all the gene sequences we could for a particular set of ancient enzymes, worked out their genetic evolutionary history and determined the most likely sequence of their common ancestor that would have existed in the earliest vertebrate animals.""Then we recreated this gene, put it into a bacterium and tested the properties of the enzyme it encoded.""The team found the ancestral enzyme could deal with high temperatures and lasted about 100 times longer at ambient temperatures.""This means more 'bang for your buck' in a commercial process, but also improves environmental sustainability, and widens our understanding and use of enzymes in synthetic biology.""The breadth of commercial applications is only limited by the imagination.""For example, this discovery could advance fields like gene therapy or help remediate polluted environments -- there's a lot of work to do.""Story Source:Materials provided by University of Queensland. ",0.10046,0.181414,0.171991,0.198318,0.140551,0.166364,0.178272,0,0
337,A new path to solving a longstanding fusion challenge Novel design could help shed excess heat in next-generation fusion power plants,"A class exercise at MIT, aided by industry researchers, has led to an innovative solution to one of the longstanding challenges facing the development of practical fusion power plants: how to get rid of excess heat that would cause structural damage to the plant.The new solution was made possible by an innovative approach to compact fusion reactors, using high-temperature superconducting magnets. This method formed the basis for a massive new research program launched this year at MIT and the creation of an independent startup company to develop the concept. The new design, unlike that of typical fusion plants, would make it possible to open the device's internal chamber and replace critical components; this capability is essential for the newly proposed heat-draining mechanism.The new approach is detailed in a paper in the journal Fusion Engineering and Design, authored by Adam Kuang, a graduate student from that class, along with 14 other MIT students, engineers from Mitsubishi Electric Research Laboratories and Commonwealth Fusion Systems, and Professor Dennis Whyte, director of MIT's Plasma Science and Fusion Center, who taught the class.In essence, Whyte explains, the shedding of heat from inside a fusion plant can be compared to the exhaust system in a car. In the new design, the ""exhaust pipe"" is much longer and wider than is possible in any of today's fusion designs, making it much more effective at shedding the unwanted heat. But the engineering needed to make that possible required a great deal of complex analysis and the evaluation of many dozens of possible design alternatives.Taming fusion plasmaFusion harnesses the reaction that powers the sun itself, holding the promise of eventually producing clean, abundant electricity using a fuel derived from seawater -- deuterium, a heavy form of hydrogen, and lithium -- so the fuel supply is essentially limitless. But decades of research toward such power-producing plants have still not led to a device that produces as much power as it consumes, much less one that actually produces a net energy output.Earlier this year, however, MIT's proposal for a new kind of fusion plant -- along with several other innovative designs being explored by others -- finally made the goal of practical fusion power seem within reach. But several design challenges remain to be solved, including an effective way of shedding the internal heat from the super-hot, electrically charged material, called plasma, confined inside the device.Most of the energy produced inside a fusion reactor is emitted in the form of neutrons, which heat a material surrounding the fusing plasma, called a blanket. In a power-producing plant, that heated blanket would in turn be used to drive a generating turbine. But about 20 percent of the energy is produced in the form of heat in the plasma itself, which somehow must be dissipated to prevent it from melting the materials that form the chamber.No material is strong enough to withstand the heat of the plasma inside a fusion device, which reaches temperatures of millions of degrees, so the plasma is held in place by powerful magnets that prevent it from ever coming into direct contact with the interior walls of the donut-shaped fusion chamber. In typical fusion designs, a separate set of magnets is used to create a sort of side chamber to drain off excess heat, but these so-called divertors are insufficient for the high heat in the new, compact plant.One of the desirable features of the ARC design is that it would produce power in a much smaller device than would be required from a conventional reactor of the same output. But that means more power confined in a smaller space, and thus more heat to get rid of.""If we didn't do anything about the heat exhaust, the mechanism would tear itself apart,"" says Kuang, who is the lead author of the paper, describing the challenge the team addressed -- and ultimately solved.Inside jobIn conventional fusion reactor designs, the secondary magnetic coils that create the divertor lie outside the primary ones, because there is simply no way to put these coils inside the solid primary coils. That means the secondary coils need to be large and powerful, to make their fields penetrate the chamber, and as a result they are not very precise in how they control the plasma shape.But the new MIT-originated design, known as ARC (for advanced, robust, and compact) features magnets built in sections so they can be removed for service. This makes it possible to access the entire interior and place the secondary magnets inside the main coils instead of outside. With this new arrangement, ""just by moving them closer [to the plasma] they can be significantly reduced in size,"" says Kuang.In the one-semester graduate class 22.63 (Principles of Fusion Engineering), students were divided into teams to address different aspects of the heat rejection challenge. Each team began by doing a thorough literature search to see what concepts had already been tried, then they brainstormed to come up with multiple concepts and gradually eliminated those that didn't pan out. Those that had promise were subjected to detailed calculations and simulations, based, in part, on data from decades of research on research fusion devices such as MIT's Alcator C-Mod, which was retired two years ago. C-Mod scientist Brian LaBombard also shared insights on new kinds of divertors, and two engineers from Mitsubishi worked with the team as well. Several of the students continued working on the project after the class ended, ultimately leading to the solution described in this new paper. The simulations demonstrated the effectiveness of the new design they settled on.""It was really exciting, what we discovered,"" Whyte says. The result is divertors that are longer and larger, and that keep the plasma more precisely controlled. As a result, they can handle the expected intense heat loads.""You want to make the 'exhaust pipe' as large as possible,"" Whyte says, explaining that the placement of the secondary magnets inside the primary ones makes that possible. ""It's really a revolution for a power plant design,"" he says. Not only do the high-temperature superconductors used in the ARC design's magnets enable a compact, high-powered power plant, he says, ""but they also provide a lot of options"" for optimizing the design in different ways -- including, it turns out, this new divertor design.Going forward, now that the basic concept has been developed, there is plenty of room for further development and optimization, including the exact shape and placement of these secondary magnets, the team says. The researchers are working on further developing the details of the design.""This is opening up new paths in thinking about divertors and heat management in a fusion device,"" Whyte says.Story Source:Materials provided by Massachusetts Institute of Technology. Original written by David L. Chandler. ",0.0659667,0.0839037,0.106521,0.14346,0.0795456,0.140283,0.13466,0,0
338,Major step toward printed anisotropic magnets,"The U.S. Department of Energy's Critical Materials Institute has taken a major step toward printed, aligned anisotropic magnets via additive manufacturing processes.The Energy Innovation Hub manufactured hybrid nylon bonded neodymium-iron-boron and samarium-iron-nitrogen magnets using the Big Area Additive Manufacturing (BAAM) located at Oak Ridge National Laboratory.""The application of additive manufacturing to magnet production is relatively new, and there are challenges to overcome between the nature of the process and the end properties of the product,"" said Ikenna Nlebedim, a scientist at the CMI.A post-printing alignment process with applied electromagnetic fields and heat allows the researchers to tune the magnetic properties of the magnet without deforming its printed shape.""For 3D printed anisotropic bonded magnets, a one-step print and align process is the ultimate goal but still needs work to be successful,"" said Nlebedim. ""We continue to pursue that goal.""By applying magnetic alignment, the researchers were able to improve magnetic performance of the already dysprosium-free composite bonded magnet without using more critical materials. ""This means more economical use of expensive and critical rare earth materials,"" said Nlebedim.The research is discussed in the paper, ""Additive Manufacturing of anisotropic hybrid NdFeB-SmFeN nylon composite bonded magnets,"" co-authored by Kinjal Ghandha, Ling Li, I.C. Nlebedim, Brian K. Post, Vlastimil Kunc, Brian C. Sales, James Bell, and M. Parans Paranthaman; and published in the Journal of Magnetism and Magnetic Materials.""The application of additive manufacturing to magnet production is relatively new, and there are challenges to overcome between the nature of the process and the end properties of the product,"" said Ikenna Nlebedim, a scientist at the CMI.A post-printing alignment process with applied electromagnetic fields and heat allows the researchers to tune the magnetic properties of the magnet without deforming its printed shape.""For 3D printed anisotropic bonded magnets, a one-step print and align process is the ultimate goal but still needs work to be successful,"" said Nlebedim. ""We continue to pursue that goal.""By applying magnetic alignment, the researchers were able to improve magnetic performance of the already dysprosium-free composite bonded magnet without using more critical materials. ""This means more economical use of expensive and critical rare earth materials,"" said Nlebedim.The research is discussed in the paper, ""Additive Manufacturing of anisotropic hybrid NdFeB-SmFeN nylon composite bonded magnets,"" co-authored by Kinjal Ghandha, Ling Li, I.C. Nlebedim, Brian K. Post, Vlastimil Kunc, Brian C. Sales, James Bell, and M. Parans Paranthaman; and published in the Journal of Magnetism and Magnetic Materials.The Critical Materials Institute is a Department of Energy Innovation Hub led by the U.S. Department of Energy's Ames Laboratory and supported by the Office of Energy Efficiency and Renewable Energy's Advanced Manufacturing Office, which supports early-stage research to advance innovation in U.S. manufacturing and promote American economic growth and energy security. CMI seeks ways to eliminate and reduce reliance on rare-earth metals and other materials critical to the success of clean energy technologies.Story Source:Materials provided by DOE/Ames Laboratory. ",0.113163,0.153026,0.16712,0.188346,0.14257,0.217269,0.179265,0,0
339,New concept to cool boiling surface may help prevent nuclear power plant accidents,"A new University of Hawaii at Manoa study has produced a new technique involving heat that could help prevent nuclear power plant accidents.Boiling is usually associated with heating, however, in many industrial applications associated with extremely hot components, such as nuclear power plants and metal casting, boiling is used as an effective cooling mechanism. This is due to ""latent heat,"" the heat absorbed to change water into vapor, which removes a huge amount of heat from a hot surface.There is a limit to the amount of heat that can be eliminated through boiling. Increasing this tolerable heat limit is important for many reasons, but especially for safety.Sangwoo Shin, an assistant professor in mechanical engineering at the UH Manoa College of Engineering, has demonstrated a novel concept that overcomes the tolerable heat limit or what's known as the critical heat flux (CHF). He leads a research team that has come up with a new method that increased the CHF by 10 percent compared to approaches used in the past.According to Shin, this is important because, if the surface is extremely hot, the water near the surface will quickly change into vapor, leaving no liquid to use for cooling the surface.""The result of this failure of cooling leads to a meltdown of the heated surface, as witnessed from the disaster at the Fukushima nuclear power plant in 2011,"" explained Shin. The incident was prompted by the Tohoku earthquake that struck eastern Japan, which generated a tsunami and disabled the power and cooling systems of the plant's reactors. ""In this regard, extensive efforts have been put into increasing the CHF,"" he said.To date, one of the most effective ways of enhancing the CHF is by roughening the surface with nanostructures, specifically, nanowires. High surface roughness leads to an increased number of sites at which the bubbling occurs, thus resulting in enhanced CHF.The study found that boiling heat transfer was much more favorable with a new concept that involves coating the hot surface using nanoscale bimorphs, a piece of long metal that can bend when exposed to heat due to thermal expansion.The hot surface causes the bimorphs to deform spontaneously, which makes the surface condition to be more favorable for boiling.Shin says future studies to further CHF enhancement can be expected by choosing the right geometry and material for the nano-bimorphs, which may contribute to developing energy-efficient technologies for extremely hot systems.Story Source:Materials provided by University of Hawaii at Manoa. ",0.098562,0.156251,0.161374,0.17177,0.125181,0.162658,0.204039,0,0
340,Polymer coating cools down buildings,"With temperatures rising and heat-waves disrupting lives around the world, cooling solutions are becoming ever more essential. This is a critical issue especially in developing countries, where summer heat can be extreme and is projected to intensify. But common cooling methods such as air conditioners are expensive, consume significant amounts of energy, require ready access to electricity, and often require coolants that deplete ozone or have a strong greenhouse effect.An alternative to these energy-intensive cooling methods is passive daytime radiative cooling (PDRC), a phenomenon where a surface spontaneously cools by reflecting sunlight and radiating heat to the colder atmosphere. PDRC is most effective if a surface has a high solar reflectance (R) that minimizes solar heat gain, and a high, thermal emittance (?) that maximizes radiative heat loss to the sky. If R and ? are sufficiently high, a net heat loss can occur, even under sunlight.Developing practical PDRC designs has been challenging: many recent design proposals are complex or costly, and cannot be widely implemented or applied on rooftops and buildings, which have different shapes and textures. Up to now, white paints, which are inexpensive and easy to apply, have been the benchmark for PDRC. White paints, however, usually have pigments that absorb UV light, and do not reflect longer solar wavelengths very well, so their performance is only modest at best.Researchers at Columbia Engineering have invented a high-performance exterior PDRC polymer coating with nano-to-microscale air voids that acts as a spontaneous air cooler and can be fabricated, dyed, and applied like paint on rooftops, buildings, water tanks, vehicles, even spacecraft -- anything that can be painted. They used a solution-based phase-inversion technique that gives the polymer a porous foam-like structure. The air voids in the porous polymer scatter and reflect sunlight, due to the difference in the refractive index between the air voids and the surrounding polymer. The polymer turns white and thus avoids solar heating, while its intrinsic emittance causes it to efficiently lose heat to the sky. The study is published online today in Science.The team -- Yuan Yang, assistant professor of materials science and engineering; Nanfang Yu, associate professor of applied physics; and Jyotirmoy Mandal, lead author of the study and a doctoral student in Yang's group (all department of applied physics and applied mathematics) -- built upon earlier work that demonstrated that simple plastics and polymers, including acrylic, silicone, and PET, are excellent heat radiators and could be used for PDRC. The challenges were how to get these normally transparent polymers to reflect sunlight without using silver mirrors as reflectors and how to make them easily deployable.They decided to use phase-inversion because it is a simple, solution-based method for making light-scattering air-voids in polymers. Polymers and solvents are already used in paints, and the Columbia Engineering method essentially replaces the pigments in white paint with air voids that reflect all wavelengths of sunlight, from UV to infrared.""This simple but fundamental modification yields exceptional reflectance and emittance that equal or surpass those of state-of-the-art PDRC designs, but with a convenience that is almost paint-like,"" says Mandal.The researchers found their polymer coating's high solar reflectance (R > 96%) and high thermal emittance (? ~ 97%) kept it significantly cooler than its environment under widely different skies, e.g. by 6?C in the warm, arid desert in Arizona and 3?C in the foggy, tropical environment of Bangladesh. ""The fact that cooling is achieved in both desert and tropical climates, without any thermal protection or shielding, demonstrates the utility of our design wherever cooling is required,"" Yang notes.The team also created colored polymer coatings with cooling capabilities by adding dyes. ""Achieving a superior balance between color and cooling performance over current paints is one of the most important aspects of our work,"" Yu notes. ""For exterior coatings, the choice of color is often subjective, and paint manufacturers have been trying to make colored coatings, like those for roofs, for decades.""The group took environmental and operational issues, such as recyclability, bio-compatibility, and high-temperature operability, into consideration, and showed that their technique can be generalized to a range of polymers to achieve these functionalities. ""Polymers are an amazingly diverse class of materials, and because this technique is generic, additional desirable properties can be conveniently integrated into our PDRC coatings, if suitable polymers are available,"" Mandal adds.""Nature offers many ways for heating and cooling, some of which are extremely well known and widely studied and others that are poorly known. Radiative cooling -- by using the sky as a heat sink -- belongs to the latter group, and its potential has been strangely overlooked by materials scientists until a few years ago,"" says Uppsala University Physics Professor Claes-Geran Granqvist, a pioneer in the field of radiative cooling, who was not involved with the study. ""The publication by Mandal et al. highlights the importance of radiative cooling and represents an important breakthrough by demonstrating that hierarchically porous polymer coatings, which can be prepared cheaply and conveniently, give excellent cooling even in full sunlight.""Yang, Yu, and Mandal are refining their design in terms of applicability, while exploring possibilities such as the use of completely biocompatible polymers and solvents. They are in talks with industry about next steps.""Now is a critical time to develop promising solutions for sustainable humanity,"" Yang notes. ""This year, we witnessed heat waves and record-breaking temperatures in North America, Europe, Asia, and Australia. It is essential that we find solutions to this climate challenge, and we are very excited to be working on this new technology that addresses it.""Yu adds that he used to think that white was the most unattainable color: ""When I studied watercolor painting years ago, white paints were the most expensive. Cremnitz white or lead white was the choice of great masters, including Rembrandt and Lucian Freud. We have now demonstrated that white is in fact the most achievable color. It can be made using nothing more than properly sized air voids embedded in a transparent medium. Air voids are what make snow white and Saharan silver ants silvery.""Story Source:Materials provided by Columbia University School of Engineering and Applied Science. ",0.0598969,0.102773,0.113595,0.12427,0.089434,0.101207,0.133305,0,0
341,Metal that withstands ultra-high temperature and pressure identified,"Japanese scientists have identified a metal able to stand up to constant forces in ultrahigh temperature, offering promising applications including in aircraft jet engines and gas turbines for electric power generation.The first-of-its-kind study, published in Nature's open access journal Scientific Reports in July 2018, describes a titanium carbide (TiC)-reinforced, molybdenum-silicon-boron (Mo-Si-B)-based alloy, or MoSiBTiC, whose high-temperature strength was identified under constant forces in the temperature ranges of 1400oC-1600oC.""Our experiments show that the MoSiBTiC alloy is extremely strong compared with cutting-edge Nickel-based single crystal superalloys, which are commonly used in hot sections of heat engines such as jet engines of aircrafts and gas turbines for electric power generation,"" said lead author Professor Kyosuke Yoshimi of Tohoku University's Graduate School of Engineering.""This work suggests that the MoSiBTiC, as ultrahigh temperature materials beyond Nickel-based superalloys, is one promising candidate for those applications,"" added Yoshimi.Yoshimi and colleagues report several parameters that highlight the alloy's favorable ability to withstand disruptive forces under ultrahigh temperatures without deforming. They also observed the alloy's behavior when exposed to increasing forces and when cavities within MoSiBTiC formed and grew, resulting in to microcracks and final rupturing.The performance of heat engines is key to future harvest of energy from fossil fuel and the subsequent conversion to electric power and propulsion force. The enhancement of their functionality may determine how efficient they are at energy conversion. Creep behavior -- or the material's ability to withstand forces under ultrahigh temperatures -- is an important factor since increased temperatures and pressures lead to creep deformation. Understanding the material's creep can help engineers construct efficient heat engines that can withstand the extreme temperature environments.The researchers assessed the alloy's creep in a stress range of 100-300 MPa for 400 hours. (MPa, or megapascal, is a unit used to measure extremely high pressure. One MPa equals approximately 145psi, or pound per square inch).All experiments were performed in a computer-controlled test rig under vacuum in order to prevent the material from oxidizing, or reacting with the any potential air moisture, which could ultimately result in rust formation.Furthermore, the study reports that, contrary to previous studies, the alloy experiences larger elongation with decreasing forces. This behavior, they write, has so far only been observed with superplastic materials that are capable of withstanding against unexpected premature failure.These findings are an important indicator for MoSiBTiC's applicability in systems that function at extremely high temperatures, such as energy conversion systems in automotive applications, power plants, and propulsion systems in aircraft engines and rockets. The researchers say that several additional microstructural analyses are needed in order to fully understand the alloy's mechanics and its ability to recover from exposure of high stresses such as large forces under high temperatures.They hope to keep refining their findings in their future endeavors. ""Our ultimate goal is to invent a novel ultrahigh temperature material superior to Nickel-based superalloys and replace high-pressure turbine blades made of Nickel-based superalloys with new turbine blades of our ultrahigh temperature material,"" said Yoshimi. ""To go there, as the next step, the oxidation resistance of the MoSiBTiC must be improved by alloy design without deteriorating its excellent mechanical properties. But it is really challenging!""Story Source:Materials provided by Tohoku University. ",0.0737323,0.101586,0.120493,0.118816,0.0880338,0.151059,0.148332,0,0
342,Engineers develop first method for controlling nanomotors,"In a breakthrough for nanotechnology, engineers at The University of Texas at Austin have developed the first method for selecting and switching the mechanical motion of nanomotors among multiple modes with simple visible light as the stimulus.The capability of mechanical reconfiguration could lead to a new class of controllable nanoelectromechanical and nanorobotic devices for a variety of fields including drug delivery, optical sensing, communication, molecule release, detection, nanoparticle separation and microfluidic automation.The finding, made by Donglei (Emma) Fan, associate professor at the Cockrell School of Engineering's Department of Mechanical Engineering, and Ph.D. candidate Zexi Liang, demonstrates how, depending on the intensity, light can instantly increase, stop and even reverse the rotation orientation of silicon nanomotors in an electric field. This effect and the underlying physical principles have been unveiled for the first time. It switches mechanical motion of rotary nanomotors among various modes instantaneously and effectively.The researchers published their findings in the Sept. 14 issue of Science Advances.Nanomotors, which are nanoscale devices capable of converting energy into movement at the cellular and molecular levels, have the potential to be used in everything from drug delivery to nanoparticle separation.Using light from a laser or light projector at strengths varying from visible to infrared, the UT researchers' novel technique for reconfiguring the motion of nanomotors is efficient and simple in its function. Nanomotors with tunable speed have already been researched as drug delivery vessels, but using light to adjust the mechanical motions has far wider implications for nanomotors and nanotechnology research more generally.""The ability to alter the behavior of nanodevices in this way -- from passive to active -- opens the door to the design of autonomous and intelligent machines at the nanoscale,"" Fan said.Fan describes the working principle of reconfigurable electric nanomotors as a mechanical analogy of electric transistors, the basic building blocks of microchips in cellphones, computers, laptops and other electronic devices that switch on demand to external stimuli.""We successfully tested our hypothesis based on the newly discovered effect through a practical application,"" Fan added.""We were able to distinguish semiconductor and metal nanomaterials just by observing their different mechanical motions in response to light with a conventional optical microscope. This distinction was made in a noncontact and nondestructive manner compared to the prevailing destructive contact-based electric measurements.""The discovery of light acting as a switch for adjusting the mechanical behaviors of nanomotors was based on examinations of the interactions of light, an electric field and semiconductor nanoparticles at play in a water-based solution.This is Fan and her team's latest breakthrough in this area. In 2014, they developed the smallest, fastest and longest-running rotary nanomotors ever designed.The research was funded by Fan's National Science Foundation Faculty Early Career Development Award and the Welch Foundation.Story Source:Materials provided by University of Texas at Austin. ",0.0796715,0.104446,0.101132,0.156355,0.121943,0.135869,0.142959,0,0
343,Origami inspires highly efficient solar steam generator,"Water covers most of the globe, yet many regions still suffer from a lack of clean drinking water. If scientists could efficiently and sustainably turn seawater into clean water, a looming global water crisis might be averted. Now, inspired by origami, the Japanese art of paper folding, researchers have devised a solar steam generator that approaches 100 percent efficiency for the production of clean water. They report their results in ACS Applied Materials & Interfaces.Solar steam generators produce clean water by converting energy from the sun into heat, which evaporates seawater, leaving salts and other impurities behind. Then, the steam is collected and condensed into clean water. Existing solar steam generators contain a flat photothermal material, which produces heat from absorbed light. Although these devices are fairly efficient, they still lose energy by heat dissipation from the material into the air. Peng Wang and colleagues wondered if they could improve energy efficiency by designing a three-dimensional photothermal material. They based their structure on the Miura fold of origami, which consists of interlocking parallelograms that form ""mountains"" and ""valleys"" within the 3D structure.The researchers made their solar steam generator by depositing a light-absorbing nanocarbon composite onto a cellulose membrane that was patterned with the Miura fold. They found that their 3D device had a 50 percent higher evaporation rate than a flat 2D device. In addition, the efficiency of the 3D structure approached 100 percent, compared with 71 percent for the 2D material. The researchers say that, compared to a flat surface, origami ""valleys"" capture the sunlight better so that less is lost to reflection. In addition, heat can flow from the valleys toward the cooler ""mountains,"" evaporating water along the way instead of being lost to the air.Story Source:Materials provided by American Chemical Society. ",0.118843,0.16762,0.209111,0.183625,0.14419,0.205406,0.195437,0,0
344,Atomic clocks now keep time well enough to improve models of Earth,"Experimental atomic clocks at the National Institute of Standards and Technology (NIST) have achieved three new performance records, now ticking precisely enough to not only improve timekeeping and navigation, but also detect faint signals from gravity, the early universe and perhaps even dark matter.The clocks each trap a thousand ytterbium atoms in optical lattices, grids made of laser beams. The atoms tick by vibrating or switching between two energy levels. By comparing two independent clocks, NIST physicists achieved record performance in three important measures: systematic uncertainty, stability and reproducibility.Published online Nov. 28 in the journal Nature, the new NIST clock records are:""Systematic uncertainty, stability, and reproducibility can be considered the 'royal flush' of performance for these clocks,"" project leader Andrew Ludlow said. ""The agreement of the two clocks at this unprecedented level, which we call reproducibility, is perhaps the single most important result, because it essentially requires and substantiates the other two results.""""This is especially true because the demonstrated reproducibility shows that the clocks' total error drops below our general ability to account for gravity's effect on time here on Earth. Hence, as we envision clocks like these being used around the country or world, their relative performance would be, for the first time, limited by Earth's gravitational effects.""Einstein's theory of relativity predicts that an atomic clock's ticking, that is, the frequency of the atoms' vibrations, is reduced -- shifted toward the red end of the electromagnetic spectrum -- when operated in stronger gravity. That is, time passes more slowly at lower elevations.While these so-called redshifts degrade a clock's timekeeping, this same sensitivity can be turned on its head to exquisitely measure gravity. Super-sensitive clocks can map the gravitational distortion of space-time more precisely than ever. Applications include relativistic geodesy, which measures the Earth's gravitational shape, and detecting signals from the early universe such as gravitational waves and perhaps even as-yet-unexplained ""dark matter.""NIST's ytterbium clocks now exceed the conventional capability to measure the geoid, or the shape of the Earth based on tidal gauge surveys of sea level. Comparisons of such clocks located far apart such as on different continents could resolve geodetic measurements to within 1 centimeter, better than the current state of the art of several centimeters.In the past decade of new clock performance records announced by NIST and other labs around the world, this latest paper showcases reproducibility at a high level, the researchers say. Furthermore, the comparison of two clocks is the traditional method of evaluating performance.Among the improvements in NIST's latest ytterbium clocks was the inclusion of thermal and electric shielding, which surround the atoms to protect them from stray electric fields and enable researchers to better characterize and correct for frequency shifts caused by heat radiation.The ytterbium atom is among potential candidates for the future redefinition of the second -- the international unit of time -- in terms of optical frequencies. NIST's new clock records meet one of the international redefinition roadmap's requirements, a 100-fold improvement in validated accuracy over the best clocks based on the current standard, the cesium atom, which vibrates at lower microwave frequencies.NIST is building a portable ytterbium lattice clock with state-of-the-art performance that could be transported to other labs around the world for clock comparisons and to other locations to explore relativistic geodesy techniques.The work is supported by NIST, the National Aeronautics and Space Administration and the Defense Advanced Research Projects Agency.Story Source:Materials provided by National Institute of Standards and Technology (NIST). ",0.109343,0.115433,0.158912,0.210821,0.140448,0.18704,0.170785,0,0
345,Insight into swimming fish could lead to robotics advances,"The constant movement of fish that seems random is actually precisely deployed to provide them at any moment with the best sensory feedback they need to navigate the world, Johns Hopkins University researchers found.The finding, published today in the journal Current Biology, enhances our understanding of active sensing behaviors performed by all animals including humans, such as whisking, touching and sniffing. It also demonstrates how robots built with better sensors could interact with their environment more effectively.""There's a saying in biology that when the world is still, you stop being able to sense it,"" says senior author Noah Cowan, a mechanical engineer and roboticist at Johns Hopkins. ""You have to actively move to perceive your world. But what we found that wasn't known before is that animals constantly regulate these movements to optimize sensory input.""For humans, active sensing includes feeling around in the dark for the bathroom light switch, or bobbling an object up and down in our hands to figure out how much it weighs. We do these things almost unconsciously, and scientists have known little about how and why we adjust our movements in response to the sensory feedback we get from them.To answer the question, Cowan and his colleagues studied fish that generate a weak electric field around their bodies to help them with communication and navigation. The team created an augmented reality for the fish so they could observe how fish movements changed as feedback from the environment changed.Inside the tank, the weakly electric fish hovered within a tube where they wiggled back and forth constantly to maintain a steady level of sensory input about their surroundings. The researchers first changed the environment by moving the tube in a way that was synchronized with the fish's movement, making it harder for the fish to extract the same amount of information they had been receiving. Next, the researchers made the tube move in the direction opposite the fish's movement, making it easier for the fish. In each case, the fish immediately increased or decreased their swimming to make sure they were getting the same amount of information. They swam farther when the tube's movement gave them less sensory feedback and they swam less when they could get could get more feedback from with less effort. The findings were even more pronounced in the dark, when the fish had to lean even more on their electrosense.""Their actions to perceive their world is under constant regulation,"" said Eric Fortune from the New Jersey Institute of Technology, a co-author on the study. ""We think that's also true for humans.""Because Cowan is a roboticist and most of the authors on this team are engineers, they hope to use the biological insight to build robots with smarter sensors. Sensors are rarely a key part of robot design now, but these findings made Cowan realize they perhaps should be.""Surprisingly, engineers don't typically design systems to operate this way,"" says Debojyoti Biswas, a graduate student at Johns Hopkins and the lead author of the study. ""Knowing more about how these tiny movements work might offer new design strategies for our smart devices to sense the world.""Video: https://www.youtube.com/watch?time_continue=10&v=GqPtNTwxsqcStory Source:Materials provided by Johns Hopkins University. ",0.0902243,0.113418,0.141901,0.207179,0.126136,0.154405,0.187699,0,0
346,It's not a shock: Better bandage promotes powerful healing,"A new, low-cost wound dressing developed by University of Wisconsin-Madison engineers could dramatically speed up healing in a surprising way.The method leverages energy generated from a patient's own body motions to apply gentle electrical pulses at the site of an injury.In rodent tests, the dressings reduced healing times to a mere three days compared to nearly two weeks for the normal healing process.""We were surprised to see such a fast recovery rate,"" says Xudong Wang, a professor of materials science and engineering at UW-Madison. ""We suspected that the devices would produce some effect, but the magnitude was much more than we expected.""Wang and collaborators described their wound dressing method today (Nov. 29, 2018) in the journal ACS Nano.Researchers have known for several decades that electricity can be beneficial for skin healing, but most electrotherapy units in use today require bulky electrical equipment and complicated wiring to deliver powerful jolts of electricity.""Acute and chronic wounds represent a substantial burden in healthcare worldwide,"" says collaborator Angela Gibson, professor of surgery at UW-Madison and a burn surgeon and director of wound healing services at UW Health. ""The use of electrical stimulation in wound healing is uncommon.""In contrast with existing methods, the new dressing is much more straightforward.""Our device is as convenient as a bandage you put on your skin,"" says Wang.The new dressings consist of small electrodes for the injury site that are linked to a band holding energy-harvesting units called nanogenerators, which are looped around a wearer's torso. The natural expansion and contraction of the wearer's ribcage during breathing powers the nanogenerators, which deliver low-intensity electric pulses.""The nature of these electrical pulses is similar to the way the body generates an internal electric field,"" says Wang.And, those low-power pulses won't harm healthy tissue like traditional, high-power electrotherapy devices might.In fact, the researchers showed that exposing cells to high-energy electrical pulses caused them to produce almost five times more reactive oxygen species -- major risk factors for cancer and cellular aging -- than did cells that were exposed to the nanogenerators.Also a boon to healing: They determined that the low-power pulses boosted viability for a type of skin cell called fibroblasts, and exposure to the nanogenerator's pulses encouraged fibroblasts to line up (a crucial step in wound healing) and produce more biochemical substances that promote tissue growth.""These findings are very exciting,"" says collaborator Weibo Cai, a professor of radiology at UW-Madison. ""The detailed mechanisms will still need to be elucidated in future work.""In that vein, the researchers aim to tease out precisely how the gentle pulses aid in healing. The scientists also plan to test the devices on pig skin, which closely mimics human tissue.And, they are working to give the nanogenerators additional capabilities -- tweaking their structure to allow for energy harvesting from small imperceptible twitches in the skin or the thrumming pulse of a heartbeat.""The impressive results in this study represent an exciting new spin on electrical stimulation for many different wound types, given the simplicity of the design,"" says Gibson, who will collaborate with the team to confirm the reproducibility of these results in human skin models.If the team is successful, the devices could help solve a major challenge for modern medicine.""We think our nanogenerator could be the most effective electrical stimulation approach for many therapeutic purposes,"" says Wang.And because the nanogenerators consist of relatively common materials, price won't be an issue.""I don't think the cost will be much more than a regular bandage,"" says Wang. ""The device in itself is very simple and convenient to fabricate.""This research was supported by grants from the National Institutes of Health (R01EB021336 and P30CA014520).Story Source:Materials provided by University of Wisconsin-Madison. Original written by Sam Million-Weaver. ",0.108835,0.13568,0.143754,0.200054,0.147934,0.199767,0.170102,0,0
347,Artificial joint restores wrist-like movements to forearm amputees,"A new artificial joint restores important wrist-like movements to forearm amputees, something which could dramatically improve their quality of life. A group of researchers led by Max Ortiz Catalan, Associate Professor at Chalmers University of Technology, Sweden, have published their research in the journal IEEE Transactions on Neural Systems & Rehabilitation Engineering.For patients missing a hand, one of the biggest challenges to regaining a high level of function is the inability to rotate one's wrist, or to 'pronate' and 'supinate'. When you lay your hand flat on a table, palm down, it is fully pronated. Turn your wrist 180 degrees, so the hand is palm up, and it is fully supinated.Most of us probably take it for granted, but this is an essential movement that we use every day. Consider using a door handle, a screwdriver, a knob on a cooker, or simply turning over a piece of paper. For those missing their hand, these are much more awkward and uncomfortable tasks, and current prosthetic technologies offer only limited relief to this problem.""A person with forearm amputation can use a motorised wrist rotator controlled by electric signals from the remaining muscles. However, those same signals are also used to control the prosthetic hand,"" explains Max Ortiz Catalan, Associate Professor at the Department for Electrical Engineering at Chalmers.""This results in a very cumbersome and unnatural control scheme, in which patients can only activate either the prosthetic wrist or the hand at one time and have to switch back and forth. Furthermore, patients get no sensory feedback, so they have no sensation of the hand's position or movement.""The new artificial joint works instead with an osseointegrated implant system developed by the Sweden-based company, Integrum AB -- one of the partners in this project. An implant is placed into each of the two bones of the forearm -- the ulna and radius -- and then a wrist-like artificial joint acts as an interface between these two implants and the prosthetic hand. Together, this allows for much more naturalistic movements, with intuitive natural control and sensory feedback.Patients who have lost their hand and wrist often still preserve enough musculature to allow them to rotate the radius over the ulnar -- the crucial movement in wrist rotation. A conventional socket prosthesis, which is attached to the body by compressing the stump, locks the bones in place, preventing any potential wrist rotation, and thus wastes this useful movement.""Depending on the level of amputation, you could still have most of the biological actuators and sensors left for wrist rotation. These allow you to feel, for example, when you are turning a key to start a car. You don't look behind the wheel to see how far to turn -- you just feel it. Our new innovation means you don't have to sacrifice this useful movement because of a poor technological solution, such as a socket prosthesis. You can continue to do it in a natural way,"" says Max Ortiz Catalan.Biomedical Engineers Irene Boni and Jason Millenaar were at Chalmers as visiting international students. They worked with Dr. Ortiz Catalan at his Biomechatronics and Neurorehabilitation Lab at Chalmers, and with Integrum AB on this project.""In tests designed to measure manual dexterity, we have shown that a patient fitted with our artificial joint scored far higher compared to when using conventional socket technology,"" explains Jason Millenaar.""Our new device offers a much more natural range of movement, minimising the need for compensatory movements of the shoulder or torso, which could dramatically improve the day to day lives of many forearm amputees,"" says Irene Boni.See a video of the joint in action here. Story Source:Materials provided by Chalmers University of Technology. ",0.096739,0.114339,0.137499,0.224521,0.135816,0.164369,0.20256,0,0
348,"Photonic radiation sensors survive huge doses undamaged New tests can hasten use of small precision devices in medicine, food safety, spacecraft","Researchers at the National Institute of Standards and Technology (NIST) have published landmark test results that suggest a promising class of sensors can be used in high-radiation environments and to advance important medical, industrial and research applications.Photonic sensors convey information with light instead of electric currents in wires. They can measure, transmit and manipulate streams of photons, typically through optical fibers, and are used to gauge pressure, temperature, distance, magnetic fields, environmental conditions and more.They are attractive because of their small size, low power consumption and tolerance of environmental variables such as mechanical vibration. But the general consensus has been that high levels of radiation would modify the optical properties of their silicon, leading to incorrect readings.So NIST, long a world leader in many areas of photonics research, launched a program to answer those questions. The test results indicate the sensors could be customized for measuring radiation dose in both industrial applications and clinical radiotherapy. The results of its first round of testing is reported in Nature Scientific Reports.Specifically, the NIST results suggest the sensors could be used to track levels of ionizing radiation (with energy high enough to alter the structure of atoms) used in food irradiation to destroy microbes and in medical device sterilization -- estimated to be a $7 billion annual market in the U.S. alone. The sensors also have potential applications in medical imaging and therapy, which together are projected to total nearly $50 billion in annual value worldwide by 2022.""When we looked at publications on the subject, different labs were getting dramatically different results,"" said project scientist Zeeshan Ahmed, who is part of NIST's Photonic Dosimetry Project and leader of NIST's cutting-edge Photonic Thermometry Project. ""That was our main motivation for doing our experiment.""""Another motivation was the growing interest in deploying photonic sensors that can function accurately in very harsh environments, such as close to nuclear reactors, where radiation damage is a major concern,"" Ahmed said. ""In addition, the space industry needs to know how these devices would function in high-radiation environments,"" said project scientist Ronald Tosh. ""Are they going to get damaged or not? What this study shows is that for a certain class of devices and radiation, the damage is negligible.""""We found that oxide-coated silicon photonic devices can withstand radiation exposure up to 1 million gray,"" said Photonic Dosimetry project leader Ryan Fitzgerald, using the SI unit for absorbed radiation. One gray represents one joule of energy absorbed by one kilogram of mass, and 1 gray corresponds to 10,000 chest X-rays. This is roughly what a sensor would receive at a nuclear power plant.""It's the upper limit of what our calibrations customers care about,"" Fitzgerald said. ""So the devices can be assumed to work reliably at industrial or medical radiation levels that are hundreds or thousands of times lower."" Food irradiation, for example, ranges from a few hundred to a few thousand gray, and is typically monitored by its effects on pellets of alanine, an amino acid that changes its atomic properties when exposed to ionizing radiation.To determine the effects of radiation, the NIST researchers exposed two kinds of silicon photonic sensors to hours of gamma radiation from cobalt-60, a radioactive isotope. In both types of sensors, small variations in their physical properties change the wavelength of the light that travels through them. By measuring those changes, the devices can be used as highly sensitive thermometers or strain gauges. This remains true in extreme environments like space flight or nuclear reactors, only if they continue to function properly under exposure to ionizing radiation.""Our results show that these photonic devices are robust in even extreme radiation environments, which suggests they could be also used to measure radiation via its effects on physical properties of irradiated devices,"" Fitzgerald said. ""That should come as good news for U.S. manufacturing, which is anxious to serve the large and growing market for precise delivery of radiation at very small length scales. Photonic sensors could then be developed to measure low-energy electron and X-ray beams used in medical device sterilization and food irradiation.""They will also be of great interest to clinical medicine, in which physicians strive to treat cancers and other conditions with the lowest effective levels of radiation focused on the smallest dimensions to avoid affecting healthy tissue, including electron, proton and ion beams. Reaching that goal demands radiation sensors with extraordinarily high sensitivity and spatial resolution. ""Eventually, we hope to develop chip-scale devices for industrial and medical applications that can determine absorbed dose gradients over distances in the range of micrometers and thus provide unprecedented detail in measurements,"" said project scientist Nikolai Klimov. A micrometer is a millionth of a meter. A human hair is about 100 micrometers wide.The team's results may have large implications for new medical therapies that employ extremely narrow beams of protons or carbon ions and medical sterilization processes that use low-energy beams of electrons. ""Our sensors are naturally small and chip-scale,"" Fitzgerald said. ""Current dosimeters are on the order of millimeters to centimeters, which can give erroneous readings for fields that vary over those dimensions.""In the next stage of the research, the team will test arrays of sensors simultaneously in identical conditions to see if variations in dose over small distances can be resolved.Story Source:Materials provided by National Institute of Standards and Technology (NIST). ",0.0641444,0.0918207,0.120458,0.142981,0.115734,0.11055,0.128648,0,0
349,Making it crystal clear: Crystallinity reduces resistance in all-solid-state batteries,"Scientists at Tokyo Institute of Technology examined the mechanisms behind the resistance at the electrode-electrolyte interface of all-solid-state batteries. Their findings will aid in the development of much better Li-ion batteries with very fast charge/discharge rates.Designing and improving lithium-ion (Li-ion) batteries is crucial for extending the limits of modern electronic devices and electric vehicles because Li-ion batteries are virtually ubiquitous. Scientists at Tokyo Institute of Technology (Tokyo Tech), led by Prof. Taro Hitosugi, had previously reported a new type of all-solid-state battery, also based on lithium ions, which overcame one of the major problems of those batteries: high resistance at the interface between the electrodes and the electrolytes that limits fast charging/discharging.Although the devices they produced were very promising and were much better than conventional Li-ion batteries in some regards, the mechanism behind the reduced interface resistance was unclear. It has been difficult to analyze the buried interfaces in all-solid-state batteries without damaging their layers. Therefore, Hitosugi and his team of researchers again investigated all-solid-state batteries to shed light on this topic. They suspected that crystallinity (which indicates how well-ordered and periodic a solid is) at the electrode-electrolyte interface played a key role in defining interface resistance.To prove this, they fabricated two different all-solid-state batteries composed of electrode and electrolyte layers using a pulsed laser deposition technique. One of these batteries had presumably high crystallinity at the electrode-electrolyte interface, whereas the other one did not. Confirming this was possible by using a novel technique called X-ray crystal truncation-rod scattering analysis. ""X-rays can reach the buried interfaces without destroying the structures,"" explains Hitosugi.Based on their results, the team concluded that a highly crystalline electrode-electrolyte interface resulted in low interface resistance, yielding a high-performance battery. By analyzing the microscopic structure of the interfaces of their batteries, they proposed a plausible explanation for the increased resistance of batteries with less crystalline interfaces. Lithium ions are stuck at the less crystalline interfaces, hindering ion conductivity. ""Controlled fabrication of the electrolyte/electrode interface is crucial to obtain low interface resistance,"" explains Hitosugi. The development of theories and simulations to further understand the migration of Li ions will be crucial for finally achieving useful and improved batteries for all kinds of devices based on electrochemistry.Story Source:Materials provided by Tokyo Institute of Technology. ",0.113925,0.23517,0.177473,0.212395,0.195089,0.17328,0.207071,0,0
350,Explaining the plummeting cost of solar power Photovoltaic module costs have dropped by 99 percent over 4 decades,"The dramatic drop in the cost of solar photovoltaic (PV) modules, which has fallen by 99 percent over the last four decades, is often touted as a major success story for renewable energy technology. But one question has never been fully addressed: What exactly accounts for that stunning drop?A new analysis by MIT researchers has pinpointed what caused the savings, including the policies and technology changes that mattered most. For example, they found that government policy to help grow markets played a critical role in reducing this technology's costs. At the device level, the dominant factor was an increase in ""conversion efficiency,"" or the amount of power generated from a given amount of sunlight.The insights can help to inform future policies and evaluate whether similar improvements can be achieved in other technologies. The findings are being reported today in the journal Energy Policy, in a paper by MIT Associate Professor Jessika Trancik, postdoc Goksin Kavlak, and research scientist James McNerney.The team looked at the technology-level (""low-level"") factors that have affected cost by changing the modules and manufacturing process. Solar cell technology has improved greatly; for example, the cells have become much more efficient at converting sunlight to electricity. Factors like this, Trancik explains, fall in a category of low-level mechanisms that deal with the physical products themselves.The team also estimated the cost impacts of ""high-level"" mechanisms, including learning by doing, research and development, and economies of scale. Examples include the way improved production processes have cut the number of defective cells produced and thus improved yields, and the fact that much larger factories have led to significant economies of scale.The study, which covered the years 1980 to 2012 (during which module costs fell by 97 percent), found that there were six low-level factors that accounted for more than 10 percent each of the overall drop in costs, and four of those factors accounted for at least 15 percent each. The results point to ""the importance of having many different 'knobs' to turn, to achieve a steady decline in cost,"" Trancik says. The more different opportunities there are to reduce costs, the less likely it is that they will be exhausted quickly.The relative importance of the factors has changed over time, the study shows. In earlier years, research and development was the dominant cost-reducing high-level mechanism, through improvements to the devices themselves and to manufacturing methods. For about the last decade, however, the largest single high-level factor in the continuing cost decline has been economies of scale, as solar-cell and module manufacturing plants have become ever larger.""This raises the question of which factors can help continue the cost decline,"" Trancik says. ""What are the limits to the size of the plants?""In terms of government policy, Trancik says, policies that stimulated market growth accounted for about 60 percent of the overall cost decline, so ""that played an important part in reducing costs."" Policies stimulating market growth included measures such as renewable portfolio standards, feed-in tariffs, and a variety of subsidies. Government-funded R&D accounted for the other 40 percent -- although public R&D played a larger part in the earlier years, she says.This is important information, she adds, because ""for a long time there has been a debate about whether these policies work -- were they really driving technological improvement? Now, we can not only answer that question, we can say by how much.""This finding, which is based on modeling device-level mechanisms rather than purely correlational analysis, provides strong evidence of a ""virtuous cycle"" that can be created between technology innovation and policies to reduce emissions, Trancik says. As emissions policies are implemented, low-carbon technology markets grow, technologies improve, and the costs of future emissions reductions can decline. ""This analysis helps us understand why this happens, and how strong the feedbacks can be.""Trancik and her co-workers plan to apply similar methodology to analyzing other technologies, such as nuclear power, as well as the other parts of solar installations -- the so-called balance of systems, including the mounting structures and power controllers needed for the solar modules -- which were not included in this study. ""The method we developed can be used as a tool to assess costs of different technologies, both retrospectively and prospectively,"" Kavlak says.""This opens up a different way of modeling technological change, from the device level all the way up to policy measures, and everything in between,"" Trancik says. ""We're opening up the black box of technological innovation.""""Going forward, we can improve our intuition about what factors in general make technologies improve quickly. The application of this tool to solar PV is just the beginning of what we can do,"" McNerney says.While the study focused on past performance, the factors it identified suggest that ""it does look like there are opportunities for further cost improvements with this technology."" The findings also suggest that researchers should continue working on alternative technologies to crystalline silicon, which is the dominant form of solar photovoltaic technology today, but many other varieties are being actively explored with potentially higher efficiencies or lower materials costs.The study also highlights the importance of continuing the progress in improving the efficiency of the manufacturing systems, whose role in driving down costs has been important. ""There are likely more gains to be had in this direction,"" Trancik says.Story Source:Materials provided by Massachusetts Institute of Technology. Original written by David L. Chandler. ",0.0487851,0.0781096,0.1001,0.146224,0.082089,0.113979,0.112557,0,0
351,Infinite-dimensional symmetry opens up possibility of a new physics -- and new particles,"The symmetries that govern the world of elementary particles at the most elementary level could be radically different from what has so far been thought. This surprising conclusion emerges from new work published by theoreticians from Warsaw and Potsdam. The scheme they posit unifies all the forces of nature in a way that is consistent with existing observations and anticipates the existence of new particles with unusual properties, which may even be present in our close environs.For half a century, physicists have been trying to construct a theory that brings together all four fundamental forces of nature, describes the known elementary particles and predicts the existence of new ones. So far these attempts have not found experimental confirmation and the Standard Model -- an old and surely incomplete, but still surprisingly effective theoretical construct -- has successfully remained in use for years as our best description of the quantum world. In a recent paper in Physical Review Letters, Prof. Krzysztof Meissner from the Institute of Theoretical Physics, Faculty of Physics, University of Warsaw, and Prof. Hermann Nicolai from the Max-Planck-Institut fer Gravitationsphysik in Potsdam have presented a new scheme generalizing the Standard Model that incorporates gravitation into the description. The shortcomings of previous attempts were overcome through the application of a kind of symmetry not previously used in the description of elementary particles.In physics, symmetries are understood somewhat differently than in the colloquial sense of the word. For instance, note that whether we drop a ball from the same spot now or one minute from now, it will still fall in the same way. That is a manifestation of a certain symmetry: the laws of physics remain unchanged with respect to shifts in time. Similarly, we can drop the ball while standing and facing once in a southward direction, once westward, or we can drop it from the same height in one location, then another. The ball will still fall in the same way in both cases, which means that the laws of physics are symmetrical also with respect to the operations of rotation and spatial displacement, respectively.""Symmetries play a huge role in physics because they are related to principles of conservation. For instance, the principle of the conservation of energy involves symmetry with respect to shifts in time, the principle of the conservation of momentum relates to symmetry of spatial displacement, and the principle of the conservation of angular momentum relates to rotational symmetry,"" explains Prof. Meissner.Work striving to devise a supersymmetric theory, meant to describe the symmetries between fermions and bosons, began back in the 1970s. The fermions are elementary particles whose spin -- a quantum property related to rotation -- is expressed in odd multiples of the fraction 1/2, and they include both quarks and leptons. Among the latter are electrons, muons, tauons, and their associated neutrinos (as well as their antiparticles). Protons and neutrons, non-elementary particles so common in our world, are also fermions. Bosons, in turn, are particles with integer spin values. They include the particles responsible for forces (photons, carriers of the electromagnetic force; gluons, carrying the strong nuclear force; W and Z bosons, carrying the weak nuclear force), as well as the Higgs boson.""The first supersymmetric theories tried to combine the forces typical of elementary particles, in other words the electromagnetic force with a symmetry known as U(1), the weak force with symmetry SU(2) and the strong force with symmetry SU(3). Gravity was still missing,"" Prof. Meissner says. ""The symmetry between the bosons and fermions was here still global, which means the same at every point in space. Soon thereafter, theories were posited where symmetry was local, meaning it could manifest differently at each point in space. Ensuring such symmetry in the theory required for gravitation to be included, and such theories became known as supergravities.""It was quickly noticed that in supergravity theories in four spatiotemporal dimensions, there cannot be more than eight different supersymmetric rotations. Each such theory has a strictly defined set of fields (degrees of freedom) with different spins (0, 1/2, 1, 3/2 and 2), known respectively as the fields of scalars, fermions, bosons, gravitinos and gravitons. For supergravity N=8, which has the maximal number of rotations, there are 48 fermions (with spin 1/2), which is precisely the number of degrees of freedom required to account for the six types of quarks and six types of leptons observed in nature! There was therefore every indication that supergravity N=8 is exceptional in many respects. However, it was not ideal.One of the problems in incorporating the Standard Model into N=8 supergravity was posed by the electrical charges of quarks and leptons. All the charges turned out to be shifted by 1/6 with respect to those observed in nature: the electron had a charge of -5/6 instead of -1, the neutrino had 1/6 instead of 0, etc. This problem, first observed by Murray Gell-Mann more than 30 years ago, was not resolved until 2015, when Professors Meissner and Nicolai presented the respective mechanism for modifying the U(1) symmetry.""After making this adjustment we obtained a structure with the symmetries U(1) and SU(3) known from the Standard Model. The approach proved to be very different from all other attempts at generalizing the symmetries of the Standard Model. The motivation was strengthened by the fact that the LHC accelerator failed to produce anything beyond the Standard Model and N=8 supergravity fermion content is compatible with this observation. What was missing was to add the SU(2) group, responsible for the weak nuclear force. In our recent paper we show how this can be done. That would explain why all previous attempts at detecting new particles, motivated by theories that treated the SU(2) symmetry as spontaneously violated for low energies, but as holding in the range of high energies, had to be unsuccessful. In our view, SU(2) is just an approximation for both low and high energies!"" Prof. Meissner explains.Both the mechanism reconciling the electric charges of the particles, and the improvement incorporating the weak force, proved to belong to a symmetry group known as E10. Unlike the symmetry groups previously used in unification theories, E10 is an infinite group, very poorly studied even in the purely mathematical sense. Prof. Nicolai with Thibault Damour and Marc Henneaux had worked on this group before, because it appeared as a symmetry in N=8 supergravity under conditions similar to those during the first moments after the Big Bang, when only one dimension was significant: time.""For the first time we have a scheme that precisely anticipates the composition of the fermions in the Standard Model -- quarks and leptons -- and does so with the proper electric charges! At the same time it includes gravity into the description. It is a huge surprise that the proper symmetry is the staggeringly huge symmetry group E10, virtually unknown mathematically. If further work confirms the role of this group, that will mean a radical change in our knowledge of the symmetries of nature,"" Prof. Meissner points out.Although the dynamics is not yet understood, the scheme proposed by Professors Meissner and Nicolai makes specific predictions. It keeps the number of spin 1/2 fermions as in the Standard Model but on the other hand suggests the existence of new particles with very unusual properties. Importantly, at least some of them could be present in our immediate surroundings, and their detection should be within the possibilities of modern detection equipment. But that is a topic for a separate story.Story Source:Materials provided by Faculty of Physics University of Warsaw. ",0.0682791,0.0903969,0.101716,0.157274,0.104443,0.123843,0.133814,0,0
352,Transition metal complexes: Mixed works better,"Transition-metal complexes -- that is a cumbersome word for a class of molecules with important properties: An element from the group of transition metals sits in the centre. The outer electrons of the transition-metal atom are located in cloverleaf-like extended d-orbitals that can be easily influenced by external excitation. Some transition-metal complexes act as catalysts to accelerate certain chemical reactions, and others can even convert sunlight into electricity. The well-known dye solar cell developed by Michael Graetzel (EPFL) in the 1990s is based on a ruthenium complex.Why not Iron?However, it has not yet been possible to replace the rare and expensive transition metal ruthenium with a less expensive element, such as iron. This is astonishing, because the same number of electrons is found on extended outer d-orbitals of iron. However, excitation with light from the visible region does not release long-lived charge carriers in most of the iron complex compounds investigated so far.Insights by RIXS at BESSY IIA team at BESSY II has now investigated this question in more detail. The group headed by Prof. Alexander Fehlisch has systematically irradiated different iron-complex compounds in solution using soft X-ray light. They were able to measure how much energy of this light was absorbed by the molecules using a method named resonant inelastic X-ray scattering, or RIXS. They investigated complexes in which the iron atom was surrounded either by bipyridine molecules or cyan groups (CN), as well as mixed forms in which the iron centre is bound to one bipyridine and four cyan groups each.Result: mixed forms could workThe team members worked in shifts for two weeks in order to obtain the necessary data. The measurements showed that the mixed forms, which had hardly been investigated so far, are particularly interesting: in the case where iron is surrounded by three bipyridine molecules or six cyan groups (CN), optical excitation leads to only short-term release of charge carriers, or to none at all. The situation changes only once two of the cyano groups are replaced by a bipyridine molecule. ""Then we can see with the soft X-ray excitation how the iron 3d-orbitals delocalize onto the cyan groups, while at the same time the bipyridine molecule can take up the charge carrier,"" explains Raphael Jay, first author of the study and whose doctoral work is in this field.The results show that inexpensive transition-metal complexes could also be suitable for use in solar cells -- if they are surrounded by suitable molecule groups. So there is still a rich field here for material development.Story Source:Materials provided by Helmholtz-Zentrum Berlin fer Materialien und Energie. ",0.103284,0.192709,0.135903,0.165607,0.146067,0.163168,0.166596,0,0
353,When electric fields make spins swirl First example of ferroelectrically tunable skyrmions brings new hope for next-generation magnetic memory devices,"We are reaching the limits of silicon capabilities in terms of data storage density and speed of memory devices. One of the potential next-generation data storage elements is the magnetic skyrmion. A team at the Center for Correlated Electron Systems, within the Institute for Basic Science (IBS, South Korea), in collaboration with the University of Science and Technology of China, have reported the discovery of small and ferroelectrically tunable skyrmions. Published in Nature Materials, this work introduces new compelling advantages that bring skyrmion research a step closer to application.It is envisioned that storing memory on skyrmions -- stable magnetic perturbations of whirling spins (magnetic moments) -- would be faster to read and write, consume less energy, and generate less heat than the currently used magnetic tunnel junctions. In future memory and logic devices, 1 and 0 bits would correspond to the existence and non-existence of a magnetic skyrmion, respectively. Although numerous skyrmion systems have been discovered in laboratories, it is still very challenging to produce controllable, nanometer-sized skyrmions for our technology needs.In this study, the researchers found out that skyrmions with a diameter smaller than 100 nanometers spontaneously form in ultrathin material, consisting of a layer of barium titanate (BaTiO3) and a layer of strontium ruthenate (SrRuO3). Below 160 Kelvin (-113 Celsius), SrRuO3 is ferromagnetic, meaning that its spins are aligned uniformly in a parallel fashion. When the two layers are overlaid, however, a special magnetic interaction swirls SrRuO3's spins, generating magnetic skyrmions. Such peculiar magnetic structure was detected below 80 Kelvin (-193 Celsius) by using magnetic force microscopy and Hall measurements.In addition, by manipulating the ferroelectric polarization of the BaTiO3 layer, the team was able to change the skyrmions' density and thermodynamic stability. The modulation is non-volatile (it persists when the power is turned off), reversible, and nanoscale.""Magnetic skyrmions and ferroelectricity are two important research topics in condensed matter physics. They are usually studied separately, but we brought them together,"" explains Lingfei Wang, first author of the study. ""This correlation provides an ideal opportunity to integrate the high tunability of well-established ferroelectric devices with the superior advantages of skyrmions into next-generation memory and logic devices.""Story Source:Materials provided by Institute for Basic Science. ",0.0959802,0.117776,0.104238,0.186607,0.128737,0.185151,0.146051,0,0
354,"Sugar-powered sensor developed to detect, prevent disease Cell runs on glucose from body fluids","Researchers at Washington State University have developed an implantable, biofuel-powered sensor that runs on sugar and can monitor a body's biological signals to detect, prevent and diagnose diseases.A cross-disciplinary research team led by Subhanshu Gupta, assistant professor in WSU's School of Electrical Engineering and Computer Science, developed the unique sensor, which, enabled by the biofuel cell, harvests glucose from body fluids to run.The research team has demonstrated a unique integration of the biofuel cell with electronics to process physiological and biochemical signals with high sensitivity.Their work recently was published in the IEEE Transactions of Circuits and Systems journal.Professors Su Ha and Alla Kostyukova from the Gene and Linda School of Chemical Engineering and Bioengineering, led design of the biofuel cell.Many popular sensors for disease detection are either watches, which need to be recharged, or patches that are worn on the skin, which are superficial and can't be embedded. The sensor developed by the WSU team could also remove the need to prick a finger for testing of certain diseases, such as diabetes.""The human body carries a lot of fuel in its bodily fluids through blood glucose or lactate around the skin and mouth,"" said Gupta. ""Using a biofuel cell opens the door to using the body as potential fuel.""The electronics in the sensor use state-of-the-art design and fabrication to consume only a few microwatts of power while being highly sensitive. Coupling these electronics with the biofuel cell makes it more efficient than traditional battery-powered devices, said Gupta. Since it relies on body glucose, the sensor's electronics can be powered indefinitely. So, for instance, the sensor could run on sugar produced just under the skin.Unlike commonly used lithium-ion batteries, the biofuel cell is also completely non-toxic, making it more promising as an implant for people, he said. It is also more stable and sensitive than conventional biofuel cells.The researchers say their sensor could be manufactured cheaply through mass production, by leveraging economies of scale.While the sensors have been tested in the lab, the researchers are hoping to test and demonstrate them in blood capillaries, which will require regulatory approval. The researchers are also working on further improving and increasing the power output of their biofuel cell.""This brings together the technology for making a biofuel cell with our sophisticated electronics,"" said Gupta. ""It's a very good marriage that could work for many future applications.""Story Source:Materials provided by Washington State University. ",0.104228,0.185051,0.12525,0.172473,0.172057,0.148581,0.179002,0,0
355,New battery gobbles up carbon dioxide Lithium-based battery could make use of greenhouse gas before it ever gets into the atmosphere,"A new type of battery developed by researchers at MIT could be made partly from carbon dioxide captured from power plants. Rather than attempting to convert carbon dioxide to specialized chemicals using metal catalysts, which is currently highly challenging, this battery could continuously convert carbon dioxide into a solid mineral carbonate as it discharges.While still based on early-stage research and far from commercial deployment, the new battery formulation could open up new avenues for tailoring electrochemical carbon dioxide conversion reactions, which may ultimately help reduce the emission of the greenhouse gas to the atmosphere.The battery is made from lithium metal, carbon, and an electrolyte that the researchers designed. The findings are described today in the journal Joule, in a paper by assistant professor of mechanical engineering Betar Gallant, doctoral student Aliza Khurram, and postdoc Mingfu He.Currently, power plants equipped with carbon capture systems generally use up to 30 percent of the electricity they generate just to power the capture, release, and storage of carbon dioxide. Anything that can reduce the cost of that capture process, or that can result in an end product that has value, could significantly change the economics of such systems, the researchers say.However, ""carbon dioxide is not very reactive,"" Gallant explains, so ""trying to find new reaction pathways is important."" Generally, the only way to get carbon dioxide to exhibit significant activity under electrochemical conditions is with large energy inputs in the form of high voltages, which can be an expensive and inefficient process. Ideally, the gas would undergo reactions that produce something worthwhile, such as a useful chemical or a fuel. However, efforts at electrochemical conversion, usually conducted in water, remain hindered by high energy inputs and poor selectivity of the chemicals produced.Gallant and her co-workers, whose expertise has to do with nonaqueous (not water-based) electrochemical reactions such as those that underlie lithium-based batteries, looked into whether carbon-dioxide-capture chemistry could be put to use to make carbon-dioxide-loaded electrolytes -- one of the three essential parts of a battery -- where the captured gas could then be used during the discharge of the battery to provide a power output.This approach is different from releasing the carbon dioxide back to the gas phase for long-term storage, as is now used in carbon capture and sequestration, or CCS. That field generally looks at ways of capturing carbon dioxide from a power plant through a chemical absorption process and then either storing it in underground formations or chemically altering it into a fuel or a chemical feedstock.Instead, this team developed a new approach that could potentially be used right in the power plant waste stream to make material for one of the main components of a battery.While interest has grown recently in the development of lithium-carbon-dioxide batteries, which use the gas as a reactant during discharge, the low reactivity of carbon dioxide has typically required the use of metal catalysts. Not only are these expensive, but their function remains poorly understood, and reactions are difficult to control.By incorporating the gas in a liquid state, however, Gallant and her co-workers found a way to achieve electrochemical carbon dioxide conversion using only a carbon electrode. The key is to preactivate the carbon dioxide by incorporating it into an amine solution.""What we've shown for the first time is that this technique activates the carbon dioxide for more facile electrochemistry,"" Gallant says. ""These two chemistries -- aqueous amines and nonaqueous battery electrolytes -- are not normally used together, but we found that their combination imparts new and interesting behaviors that can increase the discharge voltage and allow for sustained conversion of carbon dioxide.""They showed through a series of experiments that this approach does work, and can produce a lithium-carbon dioxide battery with voltage and capacity that are competitive with that of state-of-the-art lithium-gas batteries. Moreover, the amine acts as a molecular promoter that is not consumed in the reaction.The key was developing the right electrolyte system, Khurram explains. In this initial proof-of-concept study, they decided to use a nonaqueous electrolyte because it would limit the available reaction pathways and therefore make it easier to characterize the reaction and determine its viability. The amine material they chose is currently used for CCS applications, but had not previously been applied to batteries.This early system has not yet been optimized and will require further development, the researchers say. For one thing, the cycle life of the battery is limited to 10 charge-discharge cycles, so more research is needed to improve rechargeability and prevent degradation of the cell components. ""Lithium-carbon dioxide batteries are years away"" as a viable product, Gallant says, as this research covers just one of several needed advances to make them practical.But the concept offers great potential, according to Gallant. Carbon capture is widely considered essential to meeting worldwide goals for reducing greenhouse gas emissions, but there are not yet proven, long-term ways of disposing of or using all the resulting carbon dioxide. Underground geological disposal is still the leading contender, but this approach remains somewhat unproven and may be limited in how much it can accommodate. It also requires extra energy for drilling and pumping.The researchers are also investigating the possibility of developing a continuous-operation version of the process, which would use a steady stream of carbon dioxide under pressure with the amine material, rather than a preloaded supply the material, thus allowing it to deliver a steady power output as long as the battery is supplied with carbon dioxide. Ultimately, they hope to make this into an integrated system that will carry out both the capture of carbon dioxide from a power plant's emissions stream, and its conversion into an electrochemical material that could then be used in batteries. ""It's one way to sequester it as a useful product,"" Gallant says.Story Source:Materials provided by Massachusetts Institute of Technology. Original written by David L. Chandler. ",0.0583057,0.168078,0.114262,0.129234,0.085706,0.130559,0.138075,0,0
356,Chemists make breakthrough on road to creating a rechargeable lithium-oxygen battery,"Chemists from the University of Waterloo have successfully resolved two of the most challenging issues surrounding lithium-oxygen batteries, and in the process created a working battery with near 100 per cent coulombic efficiency.The new work, which appears this week in Science, proves that four-electron conversion for lithium-oxygen electrochemistry is highly reversible. The team is the first to achieve four-electron conversion, which doubles the electron storage of lithium-oxygen, also known as lithium-air, batteries.""There are limitations based on thermodynamics,"" said Linda Nazar, Canada Research Chair of Solid State Energy Materials and senior author on the project. ""Nevertheless, our work has addressed fundamental issues that people have been trying to resolve for a long time.""The high theoretical-energy density of lithium-oxygen (Li-O2) batteries and their relatively light weight have made them the Holy Grail of rechargeable battery systems. But long-standing issues with the battery's chemistry and stability have kept them a purely academic curiosity.Two of the more serious issues involve the intermediate of the cell chemistry (superoxide, LiO2) and the peroxide product (Li2O2) reacting with the porous carbon cathode, degrading the cell from within. In addition, the superoxide consumes the organic electrolyte in the process, which greatly limits the cycle life.Nazar and her colleagues switched the organic electrolyte to a more stable inorganic molten salt and the porous carbon cathode to a bifunctional metal oxide catalyst. Then by operating the battery at 150 C, they found that the more stable product Li2O is formed instead of Li2O2. This results in a highly reversible Li-oxygen battery with coulombic efficiency approaching 100 per cent.By storing O2 as lithium oxide (Li2O) instead of lithium peroxide (Li2O2), the battery not only maintained excellent charging characteristics, it achieved the maximum four-electron transfer in the system, thereby increasing the theoretical energy storage by 50 per cent.""By swapping out the electrolyte and the electrode host and raising the temperature, we show the system performs remarkably well,"" said Nazar, who is also a University Research Professor in the Department of Chemistry at Waterloo.Story Source:Materials provided by University of Waterloo. ",0.114635,0.256904,0.150215,0.195508,0.166127,0.181786,0.209565,0,0
357,These lithium-ion batteries can't catch fire because they harden on impact,"Lithium-ion batteries commonly used in consumer electronics are notorious for bursting into flame when damaged or improperly packaged. These incidents occasionally have grave consequences, including burns, house fires and at least one plane crash. Inspired by the weird behavior of some liquids that solidify on impact, researchers have developed a practical and inexpensive way to help prevent these fires.They will present their results today at the 256th National Meeting & Exposition of the American Chemical Society (ACS).""In a lithium-ion battery, a thin piece of plastic separates the two electrodes,"" Gabriel Veith, Ph.D., says. ""If the battery is damaged and the plastic layer fails, the electrodes can come into contact and cause the battery's liquid electrolyte to catch fire.""To make these batteries safer, some researchers instead use a nonflammable, solid electrolyte. But these solid-state batteries require significant retooling of the current production process, Veith says. As an alternative, his team mixes an additive into the conventional electrolyte to create an impact-resistant electrolyte. It solidifies when hit, preventing the electrodes from touching if the battery is damaged during a fall or crash. If the electrodes don't touch each other, the battery doesn't catch fire. Even better, incorporating the additive would require only minor adjustments to the conventional battery manufacturing process.The project's eureka moment came when Veith and his kids were playing with a mix of cornstarch and water known as oobleck. ""If you put the mixture on a cookie tray, it flows like a liquid until you start poking it, and then it becomes a solid,"" says Veith, who is based at Oak Ridge National Laboratory and is the project's principal investigator. After the pressure is removed, the substance liquefies again. Veith realized he could exploit this reversible ""shear thickening"" behavior to make batteries safer.This characteristic depends on a colloid, which is a suspension of tiny, solid particles in a liquid. In the case of oobleck, the colloid consists of cornstarch particles suspended in water. For the battery colloid, Veith and his colleagues at Oak Ridge and the University of Rochester used silica suspended in common liquid electrolytes for lithium-ion batteries. On impact, the silica particles clump together and block the flow of fluids and ions, he explains. The researchers used perfectly spherical, 200-nanometer-diameter particles of silica, or essentially a superfine sand. ""If you have that very uniform particle size, the particles disperse homogeneously in the electrolyte, and it works wonderfully,"" Veith says. ""If they're not homogenously sized, then the liquid becomes less viscous on impact, and that's bad.""A few other labs have been studying shear thickening to make batteries safer. One team previously reported on research with ""fumed"" silica, which consists of tiny irregular particles of silica. Another group recently reported on the effect of using rod-shaped silica particles. Veith thinks his spherical particles might be easier to make than the rod-shaped silica and have a faster response and more stopping power on impact than fumed silica.One of Veith's major advances involves the production process for the batteries. During manufacture of traditional lithium-ion batteries, an electrolyte is squirted into the battery case at the end of the production process, and then the battery is sealed. ""You can't do that with a shear-thickening electrolyte because the minute you try to inject it, it solidifies,"" he says. The researchers solved this by putting the silica in place before adding the electrolyte. They are seeking a patent on their technique.In the future, Veith plans to enhance the system so the part of the battery that's damaged in a crash would remain solid, while the rest of the battery would go on working. The team is initially aiming for applications such as drone batteries, but they would eventually like to enter the automotive market. They also plan to make a bigger version of the battery, which would be capable of stopping a bullet. That could benefit soldiers, who often carry 20 pounds of body armor and 20 pounds of batteries when they're on a mission, Veith says. ""The battery would function as their armor, and that would lighten the average soldier by about 20 pounds.""Story Source:Materials provided by American Chemical Society. ",0.0878132,0.182932,0.138749,0.142156,0.122779,0.152773,0.153328,0,0
358,Eye-tracking glasses provide a new vision for the future of augmented reality,"Battery-free eye-tracking glasses developed at Dartmouth College could create an even more realistic experience for augmented reality enthusiasts. The new technology improves player controls for gaming and allows for more accurate image displays.High power consumption and cost have kept eye trackers out of current augmented reality systems. By using near-infrared lights and photodiodes, Dartmouth's DartNets Lab has created an energy-efficient, wearable system that tracks rapid eye movements and allows hands-free input of system commands.The glasses, which can also help monitor human health, are being introduced at MobiCom 2018 taking place from October 29-November 2 in New Delhi, India.""This is an exciting advancement for gamers, developers and other users of smart glasses,"" said Xia Zhou, an associate professor of computer science at Dartmouth and project lead, ""it's the first-ever eye tracker that can fit into your everyday glasses and run without batteries.""Using the eyes as effective input devices in human-to-computer interaction systems like video games requires precision tracking of rapid eye movements at the sub-millimeter level. Ideal tracking devices should be portable and consume low levels of power to eliminate frequent charging. Up until now, no such system has existed.According to the Dartmouth research team, existing wearable eye trackers fall short mainly because of the inability to match high tracking performance with low energy consumption. Most trackers use cameras to capture eye images, requiring intensive image processing and resulting in high costs and the need for clunky external battery packs.""We took a minimalist approach that really pays off in power use and form factor,"" said Tianxing Li, a PhD student at Dartmouth and author of the research paper. ""The new system opens a wide range of uses for eye-tracking applications.""To make the Dartmouth system work, researchers needed to detect the trajectory, velocity, and acceleration of the eye's pupil without cameras. Near-infrared lights are used to illuminate the eye from various directions while photodiodes sense patterns of reflected light. Those reflections are used to infer the pupil's position and diameter in real time through a lightweight algorithm based on supervised learning.The prototype was built with off-the-shelf hardware components and integrated into a regular pair of glasses that track four stages of eye movement known as fixation, smooth pursuit, saccade and blinking. Experiments showed that the system achieves super high accuracy with low error for pupil tracking.""By detecting the type of eye movement, the system can adapt sensing and computation. Some movements have predictable trajectories, allowing the system to infer subsequent pupil position and minimizing energy use."" said Li.With power consumption that is hundreds of times lower than current systems, the Dartmouth eye tracker can be powered by energy harvested from indoor lighting, meaning that no batteries are required. The battery-free eye-tracker system is also easier to integrate into a regular pair of glasses.According to the paper: ""Although solar energy harvesting has been studied extensively in the literature, to the best of our knowledge, there have been no systematic measurements with setups similar to ours."" The layout of the Dartmouth eye-tracker system is made unique by the placement of solar cells vertically on the side of the arms of the glasses to harvest energy from indoor lighting under various user activities.The low-cost system can be used for augmented reality game and display systems. By allowing for a more precise measurements of eye position, the system can one day eliminate the need for hand controllers and can result in more efficient rendering of images by display systems, meaning higher quality images.The study was conducted exclusively indoors because strong infrared light outdoors can saturate light sensors in the current prototype. Future research includes adapting the light sensor gain in the system for outdoor use and improving detection of certain rapid eye movements.With more advancement, continuous eye tracking can also be used to identify health issues like mental disorders, to detect cognitive states like fatigue, and to assess the effectiveness of clinical treatments. Future models will also be optimized for a more miniaturized look and even better integration into regular glasses with various shapes.Story Source:Materials provided by Dartmouth College. ",0.101989,0.104118,0.124504,0.210351,0.145965,0.153246,0.163704,0,0
359,"Nanotubes may give the world better batteries Scientists' method quenches lithium metal dendrites in batteries that charge faster, last longer","Rice University scientists are counting on films of carbon nanotubes to make high-powered, fast-charging lithium metal batteries a logical replacement for common lithium-ion batteries.The Rice lab of chemist James Tour showed thin nanotube films effectively stop dendrites that grow naturally from unprotected lithium metal anodes in batteries. Over time, these tentacle-like dendrites can pierce the battery's electrolyte core and reach the cathode, causing the battery to fail.That problem has both dampened the use of lithium metal in commercial applications and encouraged researchers worldwide to solve it.Lithium metal charges much faster and holds about 10 times more energy by volume than the lithium-ion electrodes found in just about every electronic device, including cellphones and electric cars.""One of the ways to slow dendrites in lithium-ion batteries is to limit how fast they charge,"" Tour said. ""People don't like that. They want to be able to charge their batteries quickly.""The Rice team's answer, detailed in Advanced Materials, is simple, inexpensive and highly effective at stopping dendrite growth, Tour said.""What we've done turns out to be really easy,"" he said. ""You just coat a lithium metal foil with a multiwalled carbon nanotube film. The lithium dopes the nanotube film, which turns from black to red, and the film in turn diffuses the lithium ions.""""Physical contact with lithium metal reduces the nanotube film, but balances it by adding lithium ions,"" said Rice postdoctoral researcher Rodrigo Salvatierra, co-lead author of the paper with graduate student Gladys Lepez-Silva. ""The ions distribute themselves throughout the nanotube film.""When the battery is in use, the film discharges stored ions and the underlying lithium anode refills it, maintaining the film's ability to stop dendrite growth.The tangled-nanotube film effectively quenched dendrites over 580 charge/discharge cycles of a test battery with a sulfurized-carbon cathode the lab developed in previous experiments. The researchers reported the full lithium metal cells retained 99.8 percent of their coulombic efficiency, the measure of how well electrons move within an electrochemical system.Story Source:Materials provided by Rice University. Original written by Mike Williams. ",0.154713,0.291414,0.188117,0.229245,0.20289,0.217964,0.223154,0,0
360,"Whiskers, surface growth and dendrites in lithium batteries Producing smaller, longer-lasting lithium batteries wasn't one problem -- it was three problems","As our love of gadgets grows, so do demands for longer lasting batteries. But there's a problem.To make a longer-lasting battery, it needs to be bigger, and bigger isn't better when it comes to cell phones or electric cars -- not to mention pacemakers.Lithium ion batteries already have a less-than-stellar reputation: think exploding cell phones or fires on airplanes. Beyond these existing problems, when researchers attempt to shrink these batteries without compromising the performance, the results are even more unstable and prone to short-circuiting; engineers have not been able to move past these issues.Researchers at Washington University in St. Louis have new insights into the cause -- or causes -- of these issues, paving the way for smaller, safer, more energy-dense batteries. The result of their work has recently been published online in the journal Joule.Peng Bai, assistant professor in the School of Engineering & Applied Science, has identified three key current boundaries when it comes to these energy-dense lithium metal batteries. It turns out, engineers had been looking for one solution to what turns out to be three problems.A lithium ion battery is made of three layers: one layer of low-voltage material (graphite) called the anode; one of high-voltage material (lithium cobalt oxide) called the cathode; and a layer of porous plastic which separates the two.The separator is wetted by a liquid called an electrolyte. When the battery discharges, lithium ions empty out of the anode, passing through the liquid electrolyte, and move into the cathode. The process is reversed as the battery charges.""With half of the lithium-ion-hosting electrode materials empty at all times,"" Bai said, ""you are wasting half of your space.""Engineers have known that they could build a more energy-dense battery (a smaller battery with a similar output capabilities) by discarding some of the dead weight that comes with half of the host materials always being empty. They have been minimally successful by removing the graphite anode, then reducing the lithium ions with electrons during recharge, a process which forms a thin plating of lithium metal.""The problem is that the lithium metal plating is not uniform,"" Bai said. ""It can grow 'fingers.' ""Researchers have referred to these fingers as ""dendrites."" As they spread from the lithium metal plating, they can penetrate the separator in the battery, leading to a short circuit.But not all ""fingers"" are the same. ""If you call them all dendrites, you're looking for one solution to solve actually three problems, which is impossible,"" Bai said. ""That's why after so many years this problem has never been solved.""His team has identified three distinct types of fingers, or growth modes, in these lithium metal anodes. They also outline at which current each growth mode appears.""If you use very high current, it builds at the tip to produce a treelike structure,"" Bai said. Those are ""true dendrites.""Below the lower limit you have whiskers growing from the root.And within those two limits there exists the dynamic transition from whiskers to dendrites, which Bai calls ""surface growth.""These growths are all related to the competing reactions in the region between the liquid electrolyte and the metal deposits.The study found that a nanoporous ceramic separator can block whiskers up to a certain current density, after which surface growths can slowly penetrate the separator. With a strong enough current, ""true dendrites"" form, which can easily and very quickly penetrate the separator to short the battery.At this point, Bai said, ""our unique transparent cell revealed that the voltage of battery could look quite normal, even though the separator has been penetrated by a lithium metal filament. Without seeing what is happening inside, you could be easily fooled by the seemingly reasonable voltage, but, really, your battery has already failed.""In order to build a safe, efficient, reliable battery with a lithium metal anode, the three growth modes need to be controlled by three different methods.This will be a challenge considering consumers want batteries that can store more energy, and at the same time want them to be charged more quickly. The combination of these two inevitably yields a higher and higher charging current, which may exceed one of the critical currents identified by Bai's team.And, batteries can degrade. When they do, the critical currents identified for the fresh battery no longer apply; the threshold becomes lower. At that point, given the same fast charge current, there's a higher likelihood that the battery will short.""Battery operation is highly dynamic, in a very wide range of currents. Yet its disposition varies dramatically along the cycle life"" Bai said. ""That is why this becomes necessary.""Story Source:Materials provided by Washington University in St. Louis. ",0.0764674,0.16975,0.117559,0.145244,0.12442,0.140332,0.140936,0,0
361,Surprise finding: Discovering a previously unknown role for a source of magnetic fields,"Magnetic forces ripple throughout the universe, from the fields surrounding planets to the gasses filling galaxies, and can be launched by a phenomenon called the Biermann battery effect. Now scientists at the U.S. Department of Energy's (DOE) Princeton Plasma Physics Laboratory (PPPL) have found that this phenomenon may not only generate magnetic fields, but can sever them to trigger magnetic reconnection -- a remarkable and surprising discovery.The Biermann battery effect, a possible seed for the magnetic fields pervading our universe, arises in plasmas -- the state of matter composed of free electrons and atomic nuclei -- when the plasma temperature and density are misaligned. The tops of such plasmas might be hotter than the bottoms, and the density might be greater on the left side than on the right. This misalignment gives rise to an electromotive force that generates current that leads to magnetic fields. The process is named for Ludwig Biermann, a German astrophysicist who discovered it in 1950.Revealed through computer simulationsThe new findings reveal through computer simulations a previously unknown role for the Biermann effect that could improve understanding of reconnection -- the snapping apart and violent reconnection of magnetic field lines in plasmas that gives rise to northern lights, solar flares and geomagnetic space storms that can disrupt cell-phone service and electric grids on Earth.The results ""provide a new platform for replicating in the laboratory the reconnection observed in astrophysical plasmas,"" said Jackson Matteucci, a graduate student in the Program in Plasma Physics at PPPL and lead author of a description of the process in Physical Review Letters. Coauthors of the paper include his thesis advisers, Will Fox of PPPL and Amitava Bhattacharjee, head of the PPPL Theory Department, and researchers from other laboratories.The simulations modeled published results of experiments in China that studied high-energy-density (HED) plasma -- matter under extreme pressure such as exists in the core of the Earth. The experiments, in which PPPL played no part, used lasers to blast a pair of plasma bubbles from a solid metal target. Simulations of the three-dimensional plasma traced the expansion of the bubbles and the magnetic fields that the Biermann effect created, and tracked the collision of the fields to produce magnetic reconnection.The simulations showed that temperature spiked in the reconnecting field lines and reversed the role of the Biermann effect that originated the lines. Because of the spike, the Biermann effect destroyed the magnetic field lines it had created, cutting them like a pair of scissors cutting a rubber band. The sliced fields then reconnected downstream, away from the original reconnection point. ""This is the first simulation to show Biermann battery-mediated magnetic reconnection,"" Matteucci said. ""This process had never been known before.""Tracking billions of ions and electronsModeling the HED experiments required tracking billions of ions and electrons interacting with one another and with the electric and magnetic fields that their motion created, in what are called 3D kinetic simulations. Researchers carried out these simulations on the Titan supercomputer at the DOE Oak Ridge Leadership Computing Facility (OLCF) at Oak Ridge National Laboratory.The scientists have since modeled a British experiment and are working on simulations of experiments performed at the Laboratory for Laser Energetics (LLE) at the University of Rochester and the National Ignition Facility at Lawrence Livermore National Laboratory.Story Source:Materials provided by DOE/Princeton Plasma Physics Laboratory. ",0.109997,0.134711,0.120161,0.15699,0.129747,0.190539,0.154364,0,0
362,A stabilizing influence enables lithium-sulfur battery evolution Stable cathode and five-second fabrication technique could boost adoption of Li-S batteries,"In late July of 2008 a British solar plane set an unofficial flight-endurance record by remaining aloft for more than three days straight. Lithium-sulfur batteries emerged as one of the great technological advances that enabled the flight -powering the plane overnight with efficiency unmatched by the top batteries of the day. Ten years later, the world is still awaiting the commercial arrival of ""Li-S"" batteries. But a breakthrough by researchers at Drexel University has just removed a significant barrier that has been blocking their viability.Technology companies have known for some time that the evolution of their products, whether they're laptops, cell phones or electric cars, depends on the steady improvement of batteries. Technology is only ""mobile"" for as long as the battery allows it to be, and Lithium-ion batteries -- considered the best on the market -- are reaching their limit for improvement.With battery performance approaching a plateau, companies are trying to squeeze every last volt into, and out of, the storage devices by reducing the size of some of the internal components that do not contribute to energy storage. Some unfortunate side-effects of these structural changes are the malfunctions and meltdowns that occurred in a number of Samsung tablets in 2016.Researchers and the technology industry are looking at Li-S batteries to eventually replace Li-ion because this new chemistry theoretically allows more energy to be packed into a single battery -- a measure called ""energy density"" in battery research and development. This improved capacity, on the order of 5-10 times that of Li-ion batteries, equates to a longer run time for batteries between charges.The problem is, Li-S batteries haven't been able to maintain their superior capacity after the first few recharges. It turns out that the sulfur, which is the key ingredient for improved energy density, migrates away from the electrode in the form of intermediate products called polysulfides, leading to loss of this key ingredient and performance fade during recharges.For years scientists have been trying to stabilize the reaction inside Li-S battery to physically contain these polysulfides, but most attempts have created other complications, such as adding weight or expensive materials to the battery or adding several complicated processing steps.But a new approach, reported by researchers in Drexel's College of Engineering in a recent edition of the American Chemical Society journal Applied Materials and Interfaces, entitled ""TiO Phase Stabilized into Free-Standing Nanofibers as Strong Polysulfide Immobilizer in Li-S Batteries: Evidence for Lewis Acid-Base Interactions,"" shows that it can hold polysulfides in place, maintaining the battery's impressive stamina, while reducing the overall weight and the time required to produce them.""We have created freestanding porous titanium monoxide nanofiber mat as a cathode host material in lithium-sulfur batteries,"" said Vibha Kalra, PhD, an assistant professor in the College of Engineering and lead author of the research. ""This is a significant development because we have found that our titanium monoxide-sulfur cathode is both highly conductive and able to bind polysulfides via strong chemical interactions, which means it can augment the battery's specific capacity while preserving its impressive performance through hundreds of cycles. We can also demonstrate the complete elimination of binders and current collector on the cathode side that account for 30-50 percent of the electrode weight -- and our method takes just seconds to create the sulfur cathode, when the current standard can take nearly half a day.""Their findings suggest that the nanofiber mat, which at the microscopic level resembles a bird's nest, is an excellent platform for the sulfur cathode because it attracts and traps the polysulfides that arise when the battery is being used. Keeping the polysulfides in the cathode structure prevents ""shuttling,"" a performance-sapping phenomenon that occurs when they dissolve in the electrolyte solution that separates cathode from anode in a battery. This cathode design can not only help Li-S battery maintain its energy density, but also do it without additional materials that increase weight and cost of production, according to Kalra.To achieve these dual goals, the group has closely studied the reaction mechanisms and formation of polysulfides to better understand how an electrode host material could help contain them.""This research shows that the presence of a strong Lewis acid-base interaction between the titanium monoxide and sulfur in the cathode prevents polysulfides from making their way into the electrolyte, which is the primary cause of the battery's diminished performance,"" said Arvinder Singh, PhD, a postdoctoral researcher in Kalra's lab who was an author of the paper.This means their cathode design can help a Li-S battery maintain its energy density -- and do it without additional materials that increase weight and cost of production, according to Kalra.Kalra's previous work with nanofiber electrodes has shown that they provide a variety of advantages over current battery components. They have a greater surface area than current electrodes, which means they can accommodate expansion during charging, which can boost the storage capacity of the battery. By filling them with an electrolyte gel, they can eliminate flammable components from devices minimizing their susceptibility to leaks, fires and explosions. They are created through an electrospinning process, that looks something like making cotton candy, this means they have an advantage over the standard powder-based electrodes which require the use of insulating and performance deteriorating ""binder"" chemicals in their production.In tandem with its work to produce binder-free, freestanding cathode platforms to improve the performance of batteries, Kalra's lab developed a rapid sulfur deposition technique that takes just five seconds to get the sulfur into its substrate. The procedure melts sulfur into the nanofiber mats in a slightly pressurized, 140-degree Celsius environment -- eliminating the need for time-consuming processing that uses a mix of toxic chemicals, while improving the cathode's ability to hold a charge after long periods of use.""Our Li-S electrodes provide the right architecture and chemistry to minimize capacity fade during battery cycling, a key impediment in commercialization of Li-S batteries,"" Kalra said. ""Our research shows that these electrodes exhibit a sustained effective capacity that is four-times higher than the current Li-ion batteries. And our novel, low-cost method for sulfurizing the cathode in just seconds removes a significant impediment for manufacturing.""Since Zephyr-6's record-setting flight in 2008, many companies have invested in the development of Li-S batteries in hopes of increasing the range of electric cars, making mobile devices last longer between charges, and even helping the energy grid accommodate wind and solar power sources. Kalra's work now provides a path for this battery technology to move past a number of impediments that have slowed its progress.The group will continue to develop its Li-S cathodes with the goals of further improving cycle life, reducing the formation of polysulfides and decreasing cost.This research was supported by Drexel Ventures Innovation Fund and National Science Foundation (CBET-1150528).Story Source:Materials provided by Drexel University. ",0.0610535,0.140418,0.10017,0.124372,0.09232,0.124346,0.125883,0,0
363,AI tool automatically reveals how to write apps that drain less battery,"To send a text message, there's not only ""an app for that,"" there are dozens of apps for that.So why does sending a message through Skype drain over three times more battery than WhatsApp? Developers simply haven't had a way of knowing when and how to make their apps more energy-efficient.Purdue University researchers have created a new tool, called ""DiffProf,"" that uses artificial intelligence to automatically decide for the developer if a feature should be improved to drain less battery and how to make that improvement.""What if a feature of an app needs to consume 70 percent of the phone's battery? Is there room for improvement, or should that feature be left the way it is?"" said Y. Charlie Hu, the Michael and Katherine Birck Professor of Electrical and Computer Engineering and the CEO and co-founder of Mobile Enerlytics, LLC.The tool, which was announced on Oct. 8 at the 13th USENIX Symposium on Operating Systems Design and Implementation, aligns with Purdue's Giant Leaps celebration, acknowledging the university's global advancements made in AI, algorithms and automation as part of Purdue's 150th anniversary. This is one of the four themes of the yearlong celebration's Ideas Festival, designed to showcase Purdue as an intellectual center solving real-world issues.In 2012, Hu's lab was the first to develop a tool for developers to identify hot spots in source code that are responsible for an app's battery drain.""Before this point, trying to figure out how much battery an app is draining was like looking at a black box,"" Hu said. ""It was a big step forward, but it still isn't enough, because developers often wouldn't know what to do with information about the source of a battery drain.""How code runs can dramatically differ between two apps, even if the developers are implementing the same task. DiffProf catches these differences in the ""call trees"" of similar tasks, to show why the messaging feature of one messaging app consumes more energy than another messaging app. DiffProf then reveals how to rewrite the app to drain less battery.""Ultimately, in order for this technique to make a big difference for an entire smartphone, all developers would need to make their apps more energy-efficient,"" said Abhilash Jindal, fellow co-founder of Mobile Enerlytics and a former Ph.D. student in computer science at Purdue.""The impact also depends on how intensively someone uses certain apps. Someone who uses messaging apps a lot might experience longer battery life, but someone who doesn't use their messaging apps at all might not,"" he said.So far, the DiffProf prototype has only been tested for the Android mobile operating system.The work was supported in part by the National Science Foundation (Grant CSR-1718854).Story Source:Materials provided by Purdue University. ",0.114401,0.153053,0.202525,0.29683,0.193279,0.184758,0.219297,0,0
364,"Copper ions flow like liquid through crystalline structures Atomic insights into superionic crystals could lead to safer, more efficient rechargeable batteries","Materials scientists have sussed out the physical phenomenon underlying the promising electrical properties of a class of materials called superionic crystals. A better understanding of such materials could lead to safer and more efficient rechargeable batteries than the current standard-bearer of lithium ion.Becoming a popular topic of study only within the past five years, superionic crystals are a cross between a liquid and a solid. While some of their molecular components retain a rigid crystalline structure, others become liquid-like above a certain temperature, and are able to flow through the solid scaffold.In a new study, scientists from Duke University, Oak Ridge National Laboratory (ORNL) and Argonne National Laboratory (ANL) probed one such superionic crystal containing copper, chromium and selenium (CuCrSe2) with neutrons and X-rays to determine how the material's copper ions achieve their liquid-like properties. The results appear online on Oct. 8 in the journal Nature Physics.""When CuCrSe2 is heated above 190 degrees Fahrenheit, its copper ions fly around inside the layers of chromium and selenium about as fast as liquid water molecules move,"" said Olivier Delaire, associate professor of mechanical engineering and materials science at Duke and senior author on the study. ""And yet, it's still a solid that you could hold in your hand. We wanted to understand the molecular physics behind this phenomenon.""To probe the copper ions' behavior, Delaire and his colleagues turned to two world-class facilities: the Spallation Neutron Source at Oak Ridge and the Advanced Photon Source at Argonne. Each machine provided a unique piece of the puzzle.By pinging a large sample of powdered CuCrSe2 made at Oak Ridge with powerful neutrons, the researchers got a wide-scale view of the material's atomic structure and dynamics, revealing both the vibrations of the stiff scaffold of chromium and selenium atoms as well as the random jumps of copper ions within.For a narrower but more detailed look at vibration modes, the researchers bombarded a tiny single grain of CuCrSe2 crystal with high-resolution X-rays. This allowed them to examine how the rays scattered off of its atoms and how scaffold vibrations enabled shear waves to propagate, a hallmark of solid behavior.With both sets of information in hand, Delaire's group ran quantum simulations of the material's atomic behavior at the National Energy Research Scientific Computing Center to explain their findings. Below the phase transition temperature of 190 degrees Fahrenheit, the copper atoms vibrate around isolated sites, trapped in pockets of the material's scaffold structure. But above that temperature, they are able to hop randomly between multiple available sites. This allows the copper ions to flow throughout the otherwise solid crystal.While more work is needed to understand how the copper atoms interact with one another once both sites become occupied, the findings offer clues as to how to use similar materials in future electronic applications.""Most commercial lithium ion batteries use a liquid electrolyte to transfer ions between the positive and negative terminals of the battery,"" Delaire said. ""While efficient, this liquid can be dangerously flammable, as many laptop and smartphone owners have unfortunately discovered.""""There are variants of superionic crystals that contain ions like lithium or sodium that behave like the copper in CuCrSe2,"" Delaire said. ""If we can understand how superionic crystals work through this study and future research, we could perhaps find a better, solid solution for transporting ions in rechargeable batteries.""Story Source:Materials provided by Duke University. ",0.0696361,0.164435,0.119337,0.139595,0.13524,0.114976,0.141492,0,0
365,Device that integrates solar cell and battery could store electricity outside the grid,"Scientists in the United States and Saudi Arabia have harnessed the abilities of both a solar cell and a battery in one device -- a ""solar flow battery"" that soaks up sunlight and efficiently stores it as chemical energy for later on-demand use. Their research, published September 27 in the journal Chem, could make electricity more accessible in remote regions of the world.While sunlight has increasingly gained appeal as a clean and abundant energy source, it has one obvious limitation -- there is only so much sunlight per day, and some days are a lot sunnier than others. In order to keep solar energy practical, this means that after sunlight is converted to electrical energy, it must be stored. Normally this takes two devices -- a solar cell and a battery -- but the solar flow battery is designed to perform like both.""Compared with separated solar energy conversion and electrochemical energy storage devices, combining the functions of separated devices into a single, integrated device could be a more efficient, scalable, compact, and cost-effective approach to utilizing solar energy,"" says Song Jin, a professor of chemistry at the University of Wisconsin-Madison. Jin and his team developed the device in collaboration with Jr-Hau He, a professor of electrical engineering at King Abdullah University of Science and Technology (KAUST) in Saudi Arabia.The solar flow battery has three different modes. If energy is needed right away, it can act like a solar cell and immediately convert sunlight to electricity. Otherwise, the device can soak up solar energy by day and store it as chemical energy to deliver it later as electricity when night falls or the sky grows cloudy. The device can also be charged by electrical energy if needed, just like a typical battery. The team's most recent solar flow battery model is able to store and deliver electricity from solar energy more efficiently than any other integrated device currently in existence.Jin believes the solar flow battery could help transcend the limitations of the electrical grid by making electricity more readily available to people living in rural areas and providing an alternative source of energy when traditional electrical systems fail.""These integrated solar flow batteries will be especially suitable as distributed and stand-alone solar energy conversion and storage systems in remote locations and enable practical off-grid electrification,"" says Jin.Manufacturing current solar flow batteries is still too expensive for real-world markets, but Jin believes simpler designs, cheaper solar cell materials, and technological advances could help cut costs in the future. And while the current model is comparatively quite efficient, the team has plans to further improve its design. Some of the current device's voltage is still going to waste -- meaning the scientists may need to tweak the redox species and photoelectrode materials that work in tandem to convert solar energy input into electrical output. But Jin believes that, with further research, solar flow batteries may soon be practical.""We believe we could eventually get to 25% efficiency using emerging solar materials and new electrochemistry,"" says Jin. ""At this efficiency range, without using the expensive solar cells, it should be quite competitive with other renewable energy technologies. Then I think commercialization could be possible.""Story Source:Materials provided by Cell Press. ",0.122084,0.157387,0.132371,0.185328,0.143256,0.20721,0.172633,0,0
366,Super cheap earth element to advance new battery tech to the industry,"Most of today's batteries are made up of rare lithium mined from the mountains of South America. If the world depletes this source, then battery production could stagnate.Sodium is a very cheap and earth-abundant alternative to using lithium-ion batteries that is also known to turn purple and combust if exposed to water -- even just water in the air.Worldwide efforts to make sodium-ion batteries just as functional as lithium-ion batteries have long since controlled sodium's tendency to explode, but not yet resolved how to prevent sodium-ions from ""getting lost"" during the first few times a battery charges and discharges. Now, Purdue University researchers made a sodium powder version that fixes this problem and holds a charge properly.""Adding fabricated sodium powder during electrode processing requires only slight modifications to the battery production process,"" said Vilas Pol, Purdue associate professor of chemical engineering. ""This is one potential way to progress sodium-ion battery technology to the industry.""The study was made available online in June 2018 ahead of print on August 31, 2018 in the Journal of Power Sources.This work aligns with Purdue's giant leaps celebration, acknowledging the university's global advancements made in health, space, artificial intelligence and sustainability as part of Purdue's 150th anniversary. Those are the four themes of the yearlong celebration's Ideas Festival, designed to showcase Purdue as an intellectual center solving real-world issues.Even though sodium-ion batteries would be physically heavier than lithium-ion technology, researchers have been investigating sodium-ion batteries because they could store energy for large solar and wind power facilities at lower cost.The problem is that sodium ions stick to the hard carbon end of a battery, called an anode, during the initial charging cycles and not travel over to the cathode end. The ions build up into a structure called a ""solid electrolyte interface.""""Normally the solid electrolyte interface is good because it protects carbon particles from a battery's acidic electrolyte, where electricity is conducted,"" Pol said. ""But too much of the interface consumes the sodium ions that we need for charging the battery.""Purdue researchers proposed using sodium as a powder, which provides the required amount of sodium for the solid electrolyte interface to protect carbon, but doesn't build up in a way that it consumes sodium ions.They minimized sodium's exposure to the moisture that would make it combust by making the sodium powder in a glovebox filled with the gas argon. To make the powder, they used an ultrasound -- the same tool used for monitoring the development a fetus -- to melt sodium chunks into a milky purple liquid. The liquid then cooled into a powder, and was suspended in a hexane solution to evenly disperse the powder particles.Just a few drops of the sodium suspension onto the anode or cathode electrodes during their fabrication allows a sodium-ion battery cell to charge and discharge with more stability and at higher capacity -- the minimum requirements for a functional battery.Story Source:Materials provided by Purdue University. ",0.110239,0.2805,0.16279,0.17818,0.16814,0.166977,0.188717,0,0
367,X-rays uncover a hidden property that leads to failure in a lithium-ion battery material Experiments uproot long-held assumptions and will inform future battery design,"Over the past three decades, lithium-ion batteries, rechargeable batteries that move lithium ions back and forth to charge and discharge, have enabled smaller devices that juice up faster and last longer.Now, X-ray experiments at the Department of Energy's SLAC National Accelerator Laboratory and Lawrence Berkeley National Laboratory have revealed that the pathways lithium ions take through a common battery material are more complex than previously thought. The results correct more than two decades worth of assumptions about the material and will help improve battery design, potentially leading to a new generation of lithium-ion batteries.An international team of researchers, led by William Chueh, a faculty scientist at SLAC's Stanford Institute for Materials & Energy Sciences and a Stanford materials science professor, published these findings today in Nature Materials.""Before, it was kind of like a black box,"" said Martin Bazant, a professor at the Massachusetts Institute of Technology and another leader of the study. ""You could see that the material worked pretty well and certain additives seemed to help, but you couldn't tell exactly where the lithium ions go in every step of the process. You could only try to develop a theory and work backwards from measurements. With new instruments and measurement techniques, we're starting to have a more rigorous scientific understanding of how these things actually work.""The 'popcorn effect'Anyone who has ridden in an electric bus, worked with a power tool or used a cordless vacuum has likely reaped the benefits of the battery material they studied, lithium iron phosphate. It can also be used for the start-stop feature in cars with internal combustion engines and storage for wind and solar power in electrical grids. Better understanding of this material and others like it could lead to faster-charging, longer-lasting and more durable batteries. But until recently, researchers could only guess at the mechanisms that allow it to work.When lithium-ion batteries charge and discharge, the lithium ions flow from a liquid solution into a solid reservoir. But once in the solid, the lithium can rearrange itself, sometimes causing the material to split into two distinct phases, much as oil and water separate when mixed together. This causes what Chueh refers to as a ""popcorn effect."" The ions clump together into hot spots that end up shortening the battery lifetime.In this study, researchers used two X-ray techniques to explore the inner workings of lithium-ion batteries. At SLAC's Stanford Synchrotron Radiation Lightsource (SSRL) they bounced X-rays off a sample of lithium iron phosphate to reveal its atomic and electronic structure, giving them a sense of how the lithium ions were moving about in the material. At Berkeley Lab's Advanced Light Source (ALS), they used X-ray microscopy to magnify the process, allowing them to map how the concentration of lithium changes over time.Swimming upstreamPreviously, researchers thought that lithium iron phosphate was a one-dimensional conductor, meaning lithium ions are only able to travel in one direction through the bulk of the material, like salmon swimming upstream.But while sifting through their data, the researchers noticed that lithium was moving in a completely different direction on the surface of the material than one would expect based on previous models. It was as if someone had tossed a leaf onto the surface of the stream and discovered that the water was flowing in a completely different direction than the swimming salmon.They worked with Saiful Islam, a chemistry professor at the University of Bath, UK, to develop computer models and simulations of the system. Those revealed that lithium ions moved in two additional directions on the surface of the material, making lithium iron phosphate a three-dimensional conductor.""As it turns out, these extra pathways are problematic for the material, promoting the popcorn-like behavior that leads to its failure,"" Chueh said. ""If lithium can be made to move more slowly on the surface, it will make the battery much more uniform. This is the key to developing higher performance and longer lasting batteries.""A new frontier in battery engineeringEven though lithium iron phosphate has been around for the past two decades, the ability to study it at the nanoscale and during battery operation wasn't possible until just a couple of years ago.""This explains how such a crucial property of the material has gone unnoticed for so long,"" said Yiyang Li, who led the experimental work as a graduate student and postdoctoral fellow at Stanford and SLAC. ""With new technologies, there are always new and interesting properties to be discovered about materials that make you think about them a little differently.""This work is one of the first papers to come out of a collaboration between Bazant, Chueh and several other scientists as part of a Toyota Research Institute-funded research center that utilizes theory and machine learning to design and interpret advanced experiments.These most recent findings, Bazant said, create a more complex story that theorists and engineers are going to have to consider in future work.""It further builds the argument that engineering the surfaces of lithium-ion batteries is really the new frontier,"" he said. ""We have already discovered and developed some of the best bulk materials. And we've seen that lithium-ion batteries are still progressing at a pretty remarkable pace: They keep getting better and better. This research is enabling the steady advancement of a tried technology that actually works. We're building on an important bit of knowledge that can be added to the toolkit of battery engineers as they try to develop better materials.""Spanning different scalesTo follow up on this study, the researchers will continue to combine modeling, simulation and experiments to try to understand fundamental questions about battery performance at many different length and time scales with facilities such as SLAC's Linac Coherent Light Source, or LCLS, where researchers will be able to probe single ionic hops that happen at timescales as fast as one trillionth of a second.""One of the roadblocks to developing lithium-ion battery technologies is the huge span of length and time scales involved,"" Chueh said. ""Key processes can happen in a split second or over many years. The path forward requires mapping these processes at lengths that go from meters all the way down to the motion of atoms. At SLAC, we're studying battery materials at all of these scales. Combining that with modeling and experiment is really what made this understanding possible.""In addition to SLAC, Stanford, Berkeley Lab, MIT, and the University of Bath, the collaboration includes researchers from the National Institute of Chemistry and the University of Ljubljana, both in Slovenia.LCLS, SSRL, and ALS are DOE Office of Science user facilities. The theoretical aspect of the work was supported by Toyota Research Institute, among other funding agencies.Story Source:Materials provided by DOE/SLAC National Accelerator Laboratory. ",0.0538775,0.132966,0.101042,0.149547,0.0857269,0.11121,0.133731,0,0
368,A novel approach of improving battery performance,"A recent study, affiliated UNIST has introduced a novel technology that promises to significantly boost the performance of lithium metal batteries, a promising candidate for the next generation of rechargeable batteries. The study also validates the principle of enhanced battery performance via the real-time in situ observation of charge-discharge cycling.This breakthrough has been led by Professor Hyun-Wook Lee in the School of Energy and Chemical Engineering at UNIST in collaboration with the Agency for Science, Technology and Research (A*Star) in Singapore.Lithium metal batteries are a type of rechargeable battery that has lithium as an anode. Among a number of different cathode materials, lithium metal has the lowest driving voltage and boasts about 10 times more capacity than conventional graphite anodes. Therefore, it has been gaining much attention as a potential next generation anode material for electric vehicles and large scale energy storage systems.While lithium metal anode is an ideal candidate for high energy density batteries, fully using it as an anode in commercial cells is still remain elusive. For example, lithium metal tends to grow into dendritic structures during the continuous charging/discharging processes of a battery, which may result in poor performance. This is because this dendritic structure on the lithium metal surface layer triggers internal short circuits by piercing through the battery separator.In the study, the research team suppressed dendritic growth of lithium metal anode by coating the lithium foil with a lithium silicide (LixSi) layer. Results showed excellent electrochemical performances in terms of rate capability and cycle stability.In situ optical microscopic study was also carried out to monitor the electrochemical deposition of lithium on the LixSi?modified lithium electrodes and the bare lithium electrode. Conventional lithium metal anodes tend to form dendritic structures, which are known to trigger internal short circuits by piercing through the battery separator. However, it is observed that a much more uniform lithium dissolution/deposition on the LixSi?modified lithium anode can be achieved as compared to the bare lithium electrode.""Our study provides the direct observation on the electrochemical behavior, volume expansion, as well as the lithium dendrite growth of lithium metal anodes,"" says Professor Lee. ""Applying this in real battery will also help contribute to the commercialization of lithium metal batteries.""This research has been supported by the Basic Science Research Program through the National Research Foundation of Korea funded by the Ministry of Education. Their findings have been published in Advanced Materials on July 6, 2018.Story Source:Materials provided by Ulsan National Institute of Science and Technology(UNIST). ",0.156498,0.301814,0.192008,0.214807,0.20171,0.218227,0.221214,0,0
369,New high-capacity sodium-ion could replace lithium in rechargeable batteries,"University of Birmingham scientists are paving the way to swap the lithium in lithium-ion batteries with sodium, according to research published in the Journal of the American Chemical Society.Lithium-ion batteries (LIB) are rechargeable and are widely used in laptops, mobile phones and in hybrid and fully electric vehicles. The electric vehicle is a crucial technology for fighting pollution in cities and realising an era of clean sustainable transport.However lithium is expensive and resources are unevenly distributed across the planet. Large amounts of drinking water are used in lithium extraction and extraction techniques are becoming more energy intensive as lithium demand rises -- an 'own goal' in terms of sustainability.With the ever increasing demand for electric cars, the need for reliable rechargeable batteries is rising dramatically, so there is keen interest in finding a charge carrier other than lithium that is cheap and easily accessible.Sodium is inexpensive and can be found in seawater so is virtually limitless. However, sodium is a larger ion than lithium, so it is not possible to simply ""swap"" it for lithium in current technologies. For example, unlike lithium, sodium will not fit between the carbon layers of the ubiquitous LIB anode, graphite.The scientists needed to find new materials to act as battery components for sodium-ion batteries that will compete with lithium for capacity, speed of charge, energy and power density.Running quantum mechanical models on supercomputers, Dr Andrew Morris's team from the University of Birmingham's Department of Metallurgy and Materials was able to predict what happens when sodium is inserted into phosphorus.In collaboration with Dr Lauren Marbella and Professor Clare Grey's team at the University of Cambridge, who performed the experiments which have verified the predictions, they found that the phosphorus forms helices at intermediate stages of charging.The researchers identified the final composition of the electrode, which provides a final capacity of charge carriers seven times that of graphite for the same weight. This gives us fresh insights into how to make high-capacity sodium-ion anodes.Dr Andrew Morris said: ""This is a huge win for computational materials science. We predicted how phosphorus would behave as an electrode in 2016 and were now able, with Professor Grey's team to provide insights into experiment and learn how to make our predictions better. It's amazing how powerful combined theory-experimental approaches are.""Story Source:Materials provided by University of Birmingham. ",0.106642,0.233282,0.16462,0.198063,0.159284,0.174042,0.192445,0,0
370,Cathode fabrication for oxide solid-state batteries at room temperature Thick-film cathode solidified on garnet-type oxide solid electrolyte at room temperature,"Researchers at the Toyohashi University of Technology have successfully fabricated a lithium trivanadate (LVO) cathode thick film on a garnet-type oxide solid electrolyte using the aerosol deposition method. The LVO cathode thick-film fabricated on the solid electrolyte showed a large reversible charge and discharge capacity as high as 300 mAh/g and a good cycling stability at 100 eC. This finding may contribute to the realization of highly safe and chemically stable oxide-based all-solid-state lithium batteries. The research results were reported in Materials on September 1st, 2018.Rechargeable lithium-ion batteries (LiBs) have been widely utilized globally as a power source for mobile electronic devices such as smart phones, tablets, and laptop computers because of their high-energy density and good cycling performance. Recently, the development of middle- and large-scale LiBs has been accelerated for use in automotive propulsion and stationary load-leveling for intermittent power generation from solar or wind energy. However, a larger battery size causes more serious safety issues in LIBs; one of the main reasons is the increased amount of flammable organic liquid electrolytes.All-solid-state LiBs with nonflammable inorganic Li-ion (Li+) conductors as solid electrolytes (SE) are expected to be the next generation of energy storage devices because of their high energy density, safety, and reliability. The SE materials must have not only high lithium-ion conductivity at room temperature, but also deformability and chemical stability. Oxide-based SE materials have a relatively low conductivity and poor deformability compared to sulfide-based ones; however, they have other advantages such as chemical stability and ease of handling.The garnet-type fast Li+ conducting oxide, Li7-xLa3Zr2-xTaxO12 (x = 0.4-0.5, LLZTO), is considered as a good candidate for SE because of its good ionic conducting property and high electrochemical stability. However, high-temperature sintering at 1000-1200 eC is generally needed for densification, and this temperature is too high to suppress the undesired side reaction at the interface between SE and the majority of electrode materials. Therefore, there are currently limited electrode materials that can be used for solid-state batteries with garnet-type SEs developed by the co-sintering process.Ryoji Inada and his colleagues at the Department of Electrical and Electronic Information Engineering, Toyohashi University of Technology, succeeded in fabricating a lithium trivanadate (LiV3O8, LVO) thick-film cathode on garnet-type LLZTO by using the aerosol deposition (AD) method. All-solid-state cell samples were prepared and tested using the fabricated composite.The AD method is known to be a room-temperature film-fabrication process, which uses the impact-consolidation of ceramic particles onto a substrate. By controlling the particle size and morphology, dense ceramic thick films can be fabricated on various substrates without thermal treatment. This feature is attractive in the fabrication of oxide-based solid-state batteries because various electrode active materials can be selected and formed on SE with no thermal treatment.LVO has been studied at length as a cathode material for Li-based batteries because of its large Li+ storage capacity of approximately 300 mAh/g. However, the feasibility of LVO as a cathode for solid-state batteries has not yet been investigated. The reaction of LVO initiates at the discharging (i.e., Li+ insertion) process, which differs from that of other conventional cathode materials of LiBs such as LiCoO2, LiMn2O4, and LiFePO4. Therefore, graphite anodes, which are widely used in current LiBs, are difficult to use in batteries with LVO cathodes. In solid-state batteries with garnet-type SEs, Li metal electrodes may potentially be used as anodes; thus, LVO would become an attractive candidate for high-capacity cathodes.To fabricate a dense LVO film on an LLZTO pellet, the size of the LVO particles was controlled by ball-milling. As a result, an LVO thick film with a thickness of 5-6 ?m was successfully fabricated on LLZTO at room temperature. The relative density of the LVO thick film was approximately 85%. For the electrochemical characterization of the LVO thick film as a cathode, Li metal foil was attached on the opposite end face of the LLZTO pellet as an anode to form an LVO/LLZTO/Li structured solid-state cell. The galvanostatic charge (Li+ extraction from LVO) and discharge (Li+ insertion into LVO) properties in an LVO/LLZTO/Li all-solid-state cell were measured at 50 and 100 eC.Although the polarization was considerably large at 50 eC, a reversible capacity of approximately 100 mAh/g was confirmed. With an increase in temperature to 100 eC, the polarization reduced and the capacity increased significantly to 300 mAh/g at an averaged cell voltage of approximately 2.5 V; this is a typical behavior of an LVO electrode observed in an organic liquid electrolyte. In addition, we confirm that the charge and discharge reactions in the solid-state cell are stably cycled at various current densities. This can be attributed to the strong adhesion between the LVO film fabricated via impact consolidation and the LLZTO and LVO particles in the film.These results indicate that LVO can potentially be used as a high-capacity cathode in an oxide-based solid-state battery with high safety and chemical stability, even though additional investigation is needed to enhance the performance. Researchers have carried out further studies to realize oxide-based solid-state batteries at lower operating temperatures.Story Source:Materials provided by Toyohashi University of Technology. ",0.0874693,0.195602,0.131316,0.129324,0.148281,0.138125,0.153206,0,0
371,This bright blue dye is found in fabric: Could it also power batteries? A component of wastewater in textile-making  is good at tasks associated with energy storage,"A sapphire-colored dye called methylene blue is a common ingredient in wastewater from textile mills.But University at Buffalo scientists think it may be possible to give this industrial pollutant a second life. In a new study, they show that the dye, when dissolved in water, is good at storing and releasing energy on cue.This makes the compound a promising candidate material for redox flow batteries -- large, rechargeable liquid-based batteries that could enable future wind farms and solar homes to stockpile electricity for calm or rainy days.The research appeared online on Aug. 13 in the journal ChemElectroChem.""Methylene blue is a widely used dye. It can be harmful to health, so it's not something you want to dump into the environment without treating it,"" says lead researcher Timothy Cook, PhD, assistant professor of chemistry in the UB College of Arts and Sciences. ""There's been a lot of work done on ways to sequester methylene blue out of water, but the problem with a lot of these methods is that they're expensive and generate other kinds of waste products.""""But what if instead of just cleaning the water up, we could find a new way to use it? That's what really motivated this project,"" says first author Anjula Kosswattaarachchi, a UB PhD student in chemistry.Upcycling methylene blue -- and wastewater?The study is just the first step in assessing how -- and whether -- methylene blue from industrial wastewater can be used in batteries.""For this to be practical, we would need to avoid the costly process of extracting the dye from the water,"" Cook says. ""One of the things we're interested in is whether there might be a way to literally repurpose the wastewater itself.""In textile-making, there are salts in the wastewater. Usually, to make a redox flow battery work, you have to add salt as a supporting electrolyte, so the salt in wastewater might be a built-in solution. This is all speculative right now: We don't know if it will work because we haven't tested it yet.""What Cook and Kosswattaarachchi have shown -- so far -- is that methylene blue is good at important tasks associated with energy storage. In experiments, the scientists built two simple batteries that employed the dye -- dissolved in salt water -- to capture, store and release electrons (all crucial jobs in the life of a power cell).The first battery the researchers made operated with near-perfect efficiency when it was charged and drained 50 times: Any electrical energy the scientists put in, they also got out, for the most part.Over time, however, the battery's capacity for storing energy fell as molecules of methylene blue became trapped on a membrane critical to the device's proper function.Choosing a new membrane material solved this problem in the scientists' second battery. This device maintained the near-perfect efficiency of the first model, but had no notable drop in energy storage capacity over 12 cycles of charging and discharging.The results mean that methylene blue is a viable material for liquid batteries. With this established, the team hopes to take the research one step further by obtaining real wastewater from a textile mill that uses the dye.""We'd like to evaporate the wastewater into a more concentrated solution containing the methylene blue and the salts, which can then be tested directly in a battery,"" Cook says.In Sri Lanka's textile industry, a personal connection to the studyThe project is important to Kosswattaarachchi from a personal standpoint: Before coming to UB, she worked in textiles, developing new fabric technologies for the Sri Lanka Institute of Nanotechnology (SLINTEC).Textiles are one of the country's most important economic sectors, and the industry creates many jobs. But pollution is a downside, with wastewater an environmental concern.""We believe that this work could set the stage for an alternative route for wastewater management, paving a path to a green-energy storage technology,"" Kosswattaarachchi says.Story Source:Materials provided by University at Buffalo. ",0.0769267,0.142453,0.16613,0.18539,0.115626,0.144381,0.157139,0,0
372,Portable freshwater harvester could draw up to 10 gallons per hour from the air,"For thousands of years, people in the Middle East and South America have extracted water from the air to help sustain their populations. Drawing inspiration from those examples, researchers are now developing a lightweight, battery-powered freshwater harvester that could someday take as much as 10 gallons per hour from the air, even in arid locations. They say their nanofiber-based method could help address modern water shortages due to climate change, industrial pollution, droughts and groundwater depletion.The researchers will present their results today at the 256th National Meeting & Exposition of the American Chemical Society (ACS).""I was visiting China, which has a freshwater scarcity problem. There's investment in wastewater treatment, but I thought that effort alone was inadequate,"" Shing-Chung (Josh) Wong, Ph.D., says. Instead of relying on treated wastewater, Wong thought it might be more prudent to develop a new type of water harvester that could take advantage of the abundant water particles in the atmosphere.Harvesting water from the air has a long history. Thousands of years ago, the Incas of the Andean region collected dew and channeled it into cisterns. More recently, some research groups have been developing massive mist and fog catchers in the Andean mountains and in Africa.To miniaturize water generation and improve the efficiency, Wong and his students at the University of Akron turned to electrospun polymers, a material they had already worked with for more than a decade. Electrospinning uses electrical forces to produce polymer fibers ranging from tens of nanometers up to 1 micrometer -- an ideal size to condense and squeeze water droplets out of the air. These nanoscale fiber polymers offer an incredibly high surface-area-to-volume ratio, much larger than that provided by the typical structures and membranes used in water distillers.By experimenting with different combinations of polymers that were hydrophilic -- which attracts water -- and hydrophobic -- which discharges water, the group concluded that a water harvesting system could indeed be fabricated using nanofiber technology. Wong's group determined that their polymer membrane could harvest 744 mg/cm2/h, which is 91 percent higher than similarly designed membranes without these nanofibers.Unlike existing methods, Wong's harvester could work in arid desert environments because of the membrane's high surface-area-to-volume ratio. It also would have a minimal energy requirement. ""We could confidently say that, with recent advances in lithium-ion batteries, we could eventually develop a smaller, backpack-sized device,"" he says.What's more, Wong's nanofiber design simultaneously grabs water and filters it. The electrospun fiber network can act as an anti-fouling surface, sloughing off microbes that could collect on the harvester's surface. So the water would be ""clear and free of pollutants"" and immediately drinkable once it's collected, he says.Next, Wong hopes to obtain additional funding to build a prototype of the freshwater harvester. He anticipates that, once his team is able to produce the prototype, it should be inexpensive to manufacture.Story Source:Materials provided by American Chemical Society. ",0.084216,0.129911,0.2041,0.160292,0.126811,0.13915,0.143553,0,0
373,Toward fast-charging solid-state batteries,"The low current is considered one of the biggest hurdles in the development of solid-state batteries. It is the reason why the batteries take a relatively long time to charge. It usually takes about 10 to 12 hours for a solid-state battery to fully charge. The new cell type that Jelich scientists have designed, however, takes less than an hour to recharge.""With the concepts described to date, only very small charge and discharge currents were possible due to problems at the internal solid-state interfaces. This is where our concept based on a favourable combination of materials comes into play, and we have already patented it,"" explains Dr. Hermann Tempel, group leader at the Juelich Institute for Energy and Climate Research (IEK-9).In conventional lithium-ion batteries, a liquid electrolyte is used, which usually contacts the electrodes very well. With their textured surfaces, the electrodes soak up the liquid like a sponge, creating a large contact area. In principle, two solids cannot be joined together seamlessly. The contact resistance between the electrodes and the electrolyte is correspondingly high.""In order to allow the largest possible flow of current across the layer boundaries, we used very similar materials to produce all components. The anode, cathode, and electrolyte were all made from different phosphate compounds to enable charging rates greater than 3C (at a capacity of about 50 mAh/g). This is ten times higher than the values otherwise found in the literature,"" explains Hermann Tempel.The solid electrolyte serves as a stable carrier material to which phosphate electrodes are applied on both sides using the screen printing process. The materials used are reasonably priced and relatively easy to process. Unlike conventional lithium-ion batteries, the new solid-state battery is also largely free of toxic or harmful substances.""In initial tests, the new battery cell was very stable over 500 charge and discharge cycles and retained over 84 percent of its original capacity,"" said Dr. Shicheng Yu. ""There is still room for improvement here. Theoretically, a capacity loss of less than 1 percent should even be feasible,"" said Yu, who developed and tested the battery as part of a China Scholarship Council (CSC) funding programme at the Jelich Institute for Energy and Climate Research (IEK-9).Institute director Prof. Ruediger-A. Eichel is also convinced of the advantages of the new battery concept. ""The energy density is already very high at around 120 mAh/g, even if it is still slightly below that of today's lithium-ion batteries,"" says Eichel. In addition to the development for electromobility, the spokesman for the ""battery storage"" topic in the Helmholtz Association believes solid-state batteries will also be used in other areas in future: ""Solid-state batteries are currently being developed with priority as energy storage for next-generation electric vehicles. But we also believe that solid-state batteries will prevail in other fields of application that require a long service life and safe operation, such as medical technology or integrated components in the smart home area,"" says Eichel.Story Source:Materials provided by Forschungszentrum Juelich. ",0.0720341,0.147285,0.0961309,0.121031,0.106543,0.133231,0.121514,0,0
374,A paper battery powered by bacteria,"In remote areas of the world or in regions with limited resources, everyday items like electrical outlets and batteries are luxuries. Health care workers in these areas often lack electricity to power diagnostic devices, and commercial batteries may be unavailable or too expensive. New power sources are needed that are low-cost and portable. Today, researchers report a new type of battery -- made of paper and fueled by bacteria -- that could overcome these challenges.The researchers will present their results today at the 256th National Meeting & Exposition of the American Chemical Society (ACS).""Paper has unique advantages as a material for biosensors,"" says Seokheun (Sean) Choi, Ph.D., who is presenting the work at the meeting. ""It is inexpensive, disposable, flexible and has a high surface area. However, sophisticated sensors require a power supply. Commercial batteries are too wasteful and expensive, and they can't be integrated into paper substrates. The best solution is a paper-based bio-battery.""Researchers have previously developed disposable paper-based biosensors for cheap and convenient diagnosis of diseases and health conditions, as well as for detecting contaminants in the environment. Many such devices rely on color changes to report a result, but they often aren't very sensitive. To boost sensitivity, the biosensors need a power supply. Choi wanted to develop an inexpensive paper battery powered by bacteria that could be easily incorporated into these single-use devices.So Choi and his colleagues at the State University of New York, Binghamton made a paper battery by printing thin layers of metals and other materials onto a paper surface. Then, they placed freeze-dried ""exoelectrogens"" on the paper. Exoelectrogens are a special type of bacteria that can transfer electrons outside of their cells. The electrons, which are generated when the bacteria make energy for themselves, pass through the cell membrane. They can then make contact with external electrodes and power the battery. To activate the battery, the researchers added water or saliva. Within a couple of minutes, the liquid revived the bacteria, which produced enough electrons to power a light-emitting diode and a calculator.The researchers also investigated how oxygen affects the performance of their device. Oxygen, which passes easily through paper, could soak up electrons produced by the bacteria before they reach the electrode. The team found that although oxygen slightly decreased power generation, the effect was minimal. This is because the bacterial cells were tightly attached to the paper fibers, which rapidly whisked the electrons away to the anode before oxygen could intervene.The paper battery, which can be used once and then thrown away, currently has a shelf-life of about four months. Choi is working on conditions to improve the survival and performance of the freeze-dried bacteria, enabling a longer shelf life. ""The power performance also needs to be improved by about 1,000-fold for most practical applications,"" Choi says. This could be achieved by stacking and connecting multiple paper batteries, he notes. Choi has applied for a patent for the battery and is seeking industry partners for commercialization.The researchers acknowledge support and funding from the National Science Foundation, the Office of Naval Research and the Research Foundation for the State University of New York.Story Source:Materials provided by American Chemical Society. ",0.0767275,0.135172,0.139883,0.134555,0.117444,0.142752,0.126309,0,0
375,"Quantum material is promising 'ion conductor' for research, new technologies ","Researchers have shown how to shuttle lithium ions back and forth into the crystal structure of a quantum material, representing a new avenue for research and potential applications in batteries, ""smart windows"" and brain-inspired computers containing artificial synapses.The research centers on a material called samarium nickelate, which is a quantum material, meaning its performance taps into quantum mechanical interactions. Samarium nickelate is in a class of quantum materials called strongly correlated electron systems, which have exotic electronic and magnetic properties.The researchers ""doped"" the material with lithium ions, meaning the ions were added to the material's crystal structure.The addition of lithium ions causes the crystal to expand and increases the material's conduction of the ions. The researchers also learned that the effect works with other types of ions, particularly sodium ions, pointing to potential applications in energy storage.Findings are detailed in a paper appearing this week in Proceedings of the National Academy of Sciences.""The results highlight the potential of quantum materials and emergent physics in the design of ion conductors,"" said Shriram Ramanathan, a Purdue University professor of materials engineering who is leading the research. ""There is a lot of research now going on to identify solid-state ion conductors for building batteries, for example. We showed that this general family of materials can hold these ions, so we established some general principles for the design of these sorts of solid-state ion conductors. We showed that ions like lithium and sodium can move through this solid material, and this opens up new directions for research.""Applying a voltage caused the ions to occupy spaces between atoms in the crystal lattice of the material. The effect could represent a more efficient method to store and conduct electricity. Such an effect could lead to new types of batteries and artificial synapses in ""neuromorphic,"" or brain-inspired, computers. Moreover, the ions remained in place after the current was turned off, a ""non-volatile"" behavior that might be harnessed for computer memory.Adding lithium ions to the crystal structure also changes the material's optical properties, suggesting potential applications as coatings for ""smart windows"" whose light transmission properties are altered when voltage is applied.The research paper's lead authors are Purdue materials engineering postdoctoral research associate Yifei Sun and Michele Kotiuga, a postdoctoral fellow in the Department of Physics and Astronomy at Rutgers University. The work was performed by researchers at several research institutions. A complete listing of co-authors is available in the abstract. To develop the doping process, materials engineers collaborated with Vilas Pol, a Purdue associate professor of chemical engineering and materials engineering, and Purdue graduate student Dawgen Lim.The research findings demonstrated behavior related to the ""Mott transition,"" a quantum mechanical effect describing how the addition of electrons can change the conducting behavior of a material.""As we add more electrons to the system the material becomes less and less conducting, which makes it a very interesting system to study, and this effect can only be explained through quantum mechanics,"" Ramanathan said.Kotiuga's contribution to the work was to study the electronic properties of lithium-doped samarium nickelate as well as the changes to the crystal structure after doping.""My calculations show that undoped samarium nickelate is a narrow-gapped semiconductor, meaning that even though it is not metallic, electrons can be excited into a conducting state without too much trouble,"" she said. ""As lithium is added to samarium nickelate the lithium ion will bind to an oxygen and an electron localizes on a nearby nickel-oxygen octahedron, and when an electron has localized on every nickel-oxygen octahedron the material is converted into an insulator. This is a rather counterintuitive result: the added electrons to the system make the material more insulating.""The material's crystal structure was characterized using a synchrotron-radiation light source research facility at Argonne National Laboratory.The researchers had been working on the paper for about two years and plan to further explore the material's quantum behavior and potential applications in brain-inspired computing.The research was funded or otherwise supported by several sources, including the National Science Foundation, U.S. Department of Energy, the Canadian Light Source, U.S. Army Research Office, U.S. Air Force Office of Scientific Research and the U.S. Office of Naval Research.Story Source:Materials provided by Purdue University. Original written by Emil Venere. ",0.0581527,0.150156,0.111912,0.142032,0.1173,0.110131,0.137635,0,0
376,New material cleans and splits water,"Some of the most useful and versatile materials today are the metal-organic frameworks (MOFs). MOFs are a class of materials demonstrating structural versatility, high porosity, fascinating optical and electronic properties, all of which makes them promising candidates for a variety of applications, including gas capture and separation, sensors, and photocatalysis.Because MOFs are so versatile in both their structural design and usefulness, material scientists are currently testing them in a number of chemical applications. One of these is photocatalysis, a process where a light-sensitive material is excited with light. The absorbed excess energy dislocates electrons from their atomic orbits, leaving behind ""electron holes."" The generation of such electron-hole pairs is a crucial process in any light-dependent energy process, and, in this case, it allows the MOF to affect a variety of chemical reactions.A team of scientists at EPFL Sion led by Kyriakos Stylianou at the Laboratory of Molecular Simulation, have now developed a MOF-based system that can perform not one, but two types of photocatalysis simultaneously: production of hydrogen, and cleaning pollutants out of water. The material contains the abundantly available and cheap nickel phosphide (Ni2P), and was found to carry out efficient photocatalysis under visible light, which accounts to 44% of the solar spectrum.The first type of photocatalysis, hydrogen production, involves a reaction called ""water-splitting."" Like the name suggests, the reaction divides water molecules into their constituents: hydrogen and oxygen. One of the bigger applications here is to use the hydrogen for fuel cells, which are energy-supply devices used in a variety of technologies today, including satellites and space shuttles.The second type of photocatalysis is referred to as ""organic pollutant degradation,"" which refers to processes breaking down pollutants present in water. The scientists investigated this innovative MOF-based photocatalytic system towards the degradation of the toxic dye rhodamine B, commonly used to simulate organic pollutants.The scientists performed both tests in sequence, showing that the MOF-based photocatalytic system was able to integrate the photocatalytic generation of hydrogen with the degradation of rhodamine B in a single process. This means that it is now possible to use this photocatalytic system to both clean pollutants out of water, while simultaneously producing hydrogen that can be used as a fuel.""This noble-metal free photocatalytic system brings the field of photocatalysis a step closer to practical 'solar-driven' applications and showcases the great potential of MOFs in this field,"" says Kyriakos Stylianou.Story Source:Materials provided by Ecole Polytechnique Federale de Lausanne. ",0.0679293,0.145383,0.137587,0.125157,0.114432,0.123187,0.135075,0,0
377,"Finally, a robust fuel cell that runs on methane at practical temperatures ","Fuel cells have not been particularly known for their practicality and affordability, but that may have just changed. There's a new cell that runs on cheap fuel at temperatures comparable to automobile engines and which slashes materials costs.Though the cell is in the lab, it has high potential to someday electrically power homes and perhaps cars, say the researchers at the Georgia Institute of Technology who led its development. In a new study in the journal Nature Energy the researchers detailed how they reimagined the entire fuel cell with the help of a newly invented fuel catalyst.The catalyst has dispensed with high-priced hydrogen fuel by making its own out of cheap, readily available methane. And improvements throughout the cell cooled the seething operating temperatures that are customary in methane fuel cells dramatically, a striking engineering accomplishment.Methane fuel cells usually require temperatures of 750 to 1,000 degrees Celsius to run. This new one needs only about 500, which is even a notch cooler than automobile combustion engines, which run at around 600 degrees Celsius.That lower temperature could trigger cascading cost savings in the ancillary technology needed to operate a fuel cell, potentially pushing the new cell to commercial viability. The researchers feel confident that engineers can design electric power units around this fuel cell with reasonable effort, something that has eluded previous methane fuel cells.'Sensation in our world'""Our cell could make for a straightforward, robust overall system that uses cheap stainless steel to make interconnectors,"" said Meilin Liu, who led the study and is a Regents' Professor in Georgia Tech's School of Material Science and Engineering. Interconnectors are parts that help bring together many fuel cells into a stack, or functional unit.""Above 750 degrees Celsius, no metal would withstand the temperature without oxidation, so you'd have a lot of trouble getting materials, and they would be extremely expensive and fragile, and contaminate the cell,"" Liu said.""Lowering the temperature to 500 degrees Celsius is a sensation in our world. Very few people have even tried it,"" said Ben deGlee, a graduate research assistant in Liu's lab and one of the first authors of the study. ""When you get that low, it makes the job of the engineer designing the stack and connected technologies much easier.""The new cell also eliminates the need for a major ancillary device called a steam reformer, which is normally needed to convert methane and water into hydrogen fuel.Liu, deGlee, co-first author Yu Chen, who is a postdoctoral researcher in Liu's lab, and co-first author Yu Tang of the University of Kansas published the results of their research on October 29, 2018. Their work was funded by the Office of Basic Energy Sciences and the Advanced Research Projects Agency-Energy (ARPA-E), both in the U.S. Department of Energy. It was also funded by the National Science Foundation's Division of Chemistry.'Distributed generation'The research was based on a type of fuel cell with high potential for commercial viability, the solid oxide fuel cell (SOFC). SOFCs are known for their versatility in fuels they can use.If it goes to market, though the new cell might not power automobiles for a while, it could land sooner in basements as part of a more decentralized, cleaner, cheaper electrical power grid. The fuel cell stack itself would be about the size of a shoebox, plus ancillary technology to make it run.""The hope is you could install this device like a tankless water heater. It would run off of natural gas to power your house,"" Liu said. ""That would save society and industry the enormous cost of new power plants and large electrical grid expansions.""""It would make homes and businesses more power independent,"" Liu said. ""That kind of system would be called distributed generation, and our sponsors want to develop that.""Homemade hydrogenHydrogen is the best fuel for powering fuel cells, but its cost is exorbitant. The researchers figured out how to convert methane to hydrogen in the fuel cell itself via the new catalyst, which is made with cerium, nickel and ruthenium and has the chemical formula Ce0.9Ni0.05Ru0.05O2, abbreviated CNR.When methane and water molecules come into contact with the catalyst and heat, nickel chemically cleaves the methane molecule. Ruthenium does the same with water. The resulting parts come back together as that very desirable hydrogen (H2) and carbon monoxide (CO), which the researchers surprisingly put to good use.""CO causes performance problems in most fuel cells, but here, we're using it as a fuel,"" Chen said.Making electricityH2 and CO continue on to further catalyst layers that make up the anode, the part of the fuel cell that yanks off electrons, making the carbon monoxide and hydrogen positively charged ions. The electrons travel via a wire -- creating the electricity flow -- toward the cathode.There, oxygen, which is very electron-hungry, sucks up the electrons, closing the electrical circuit and becoming O2- ions. Ionized hydrogen and oxygen meet and exit the system as water condensation; the carbon monoxide and oxygen ions meet to become pure carbon dioxide, which could be captured.For the energy produced, fuel cell technology creates far, far less carbon dioxide than combustion engines.In some fuel cells, the water in the initial reactions must be introduced from the outside. In this new fuel cell, it's replenished in the last reaction phase, which forms water that cycles back to react with the methane.Catalysts convergeThe new catalyst, CNR, manufactured by research collaborators at the University of Kansas, is the outer layer of the anode side of the cell and doubles as a protectant against decay, extending the life of the cell. CNR has strong cohort catalysts in inner layers and on the other side of the cell, the cathode.On the cathode end, oxygen's reaction and movement through the system are usually notoriously slow, but Liu's lab has recently sped it up to raise the electricity output by using what's called nanofiber cathodes, which Liu's lab developed in a prior study. (See prior study: A tailored double perovskite nanofiber catalyst enables ultrafast oxygen evolution.)""The structures of these various catalysts, as well as the nanofiber cathodes, all together allowed us to drop the operating temperature,"" Chen said.These following people coauthored the research: Bote Zhao, Lei Zhang, Seonyoung Yoo, Kai Pei, Jun Hyuk Kim and Yong Ding of Georgia Tech; Yuechang Wei and Franklin Feng Tao of the University of Kansas, and Ziyun Wang and P. Hu of The Queen's University of Belfast. The research was funded by the U.S. Department of Energy under the following agencies and programs: Advanced Research Projects Agency-Energy (ARPA-E) REBELS program (award DE-AR0000502), and SECA Core Technology Program (award DE-FE0031201), the Catalysis program of the Office of Basic Energy Sciences (grant DE- SC0014561). It was also funded by the Division of Chemistry of the National Science Foundation (award 1462121). Any results, conclusions, and opinions are those of the authors and not necessarily of the funding agencies.Story Source:Materials provided by Georgia Institute of Technology. Original written by Ben Brumfield. ",0.0561615,0.191593,0.0879378,0.125128,0.0904633,0.115251,0.12716,0,0
378,A solar cell that does double duty for renewable energy,"In the quest for abundant, renewable alternatives to fossil fuels, scientists have sought to harvest the sun's energy through ""water splitting,"" an artificial photosynthesis technique that uses sunlight to generate hydrogen fuel from water. But water-splitting devices have yet to live up to their potential because there still isn't a design for materials with the right mix of optical, electronic, and chemical properties needed for them to work efficiently.Now researchers at the U.S. Department of Energy's Lawrence Berkeley National Laboratory (Berkeley Lab) and the Joint Center for Artificial Photosynthesis (JCAP), a DOE Energy Innovation Hub, have come up with a new recipe for renewable fuels that could bypass the limitations in current materials: an artificial photosynthesis device called a ""hybrid photoelectrochemical and voltaic (HPEV) cell"" that turns sunlight and water into not just one, but two types of energy -- hydrogen fuel and electricity. The paper describing this work was published on Oct. 29 in Nature Materials.Finding a way out for electronsMost water-splitting devices are made of a stack of light-absorbing materials. Depending on its makeup, each layer absorbs different parts or ""wavelengths"" of the solar spectrum, ranging from less-energetic wavelengths of infrared light to more-energetic wavelengths of visible or ultraviolet light.When each layer absorbs light it builds an electrical voltage. These individual voltages combine into one voltage large enough to split water into oxygen and hydrogen fuel. But according to Gideon Segev, a postdoctoral researcher at JCAP in Berkeley Lab's Chemical Sciences Division and the study's lead author, the problem with this configuration is that even though silicon solar cells can generate electricity very close to their limit, their high-performance potential is compromised when they are part of a water-splitting device.The current passing through the device is limited by other materials in the stack that don't perform as well as silicon, and as a result, the system produces much less current than it could -- and the less current it generates, the less solar fuel it can produce.""It's like always running a car in first gear,"" said Segev. ""This is energy that you could harvest, but because silicon isn't acting at its maximum power point, most of the excited electrons in the silicon have nowhere to go, so they lose their energy before they are utilized to do useful work.""Getting out of first gearSo Segev and his co-authors -- Jeffrey W. Beeman, a JCAP researcher in Berkeley Lab's Chemical Sciences Division, and former Berkeley Lab and JCAP researchers Jeffery Greenblatt, who now heads the Bay Area-based technology consultancy Emerging Futures LLC, and Ian Sharp, now a professor of experimental semiconductor physics at the Technical University of Munich in Germany -- proposed a surprisingly simple solution to a complex problem.""We thought, 'What if we just let the electrons out?'"" said Segev.In water-splitting devices, the front surface is usually dedicated to solar fuels production, and the back surface serves as an electrical outlet. To work around the conventional system's limitations, they added an additional electrical contact to the silicon component's back surface, resulting in an HPEV device with two contacts in the back instead of just one. The extra back outlet would allow the current to be split into two, so that one part of the current contributes to solar fuels generation, and the rest can be extracted as electrical power.When what you see is what you getAfter running a simulation to predict whether the HPEC would function as designed, they made a prototype to test their theory. ""And to our surprise, it worked!"" Segev said. ""In science, you're never really sure if everything's going to work even if your computer simulations say they will. But that's also what makes it fun. It was great to see our experiments validate our simulations' predictions.""According to their calculations, a conventional solar hydrogen generator based on a combination of silicon and bismuth vanadate, a material that is widely studied for solar water splitting, would generate hydrogen at a solar to hydrogen efficiency of 6.8 percent. In other words, out of all of the incident solar energy striking the surface of a cell, 6.8 percent will be stored in the form of hydrogen fuel, and all the rest is lost.In contrast, the HPEV cells harvest leftover electrons that do not contribute to fuel generation. These residual electrons are instead used to generate electrical power, resulting in a dramatic increase in the overall solar energy conversion efficiency, said Segev. For example, according to the same calculations, the same 6.8 percent of the solar energy can be stored as hydrogen fuel in an HPEV cell made of bismuth vanadate and silicon, and another 13.4 percent of the solar energy can be converted to electricity. This enables a combined efficiency of 20.2 percent, three times better than conventional solar hydrogen cells.The researchers plan to continue their collaboration so they can look into using the HPEV concept for other applications such as reducing carbon dioxide emissions. ""This was truly a group effort where people with a lot of experience were able to contribute,"" added Segev. ""After a year and a half of working together on a pretty tedious process, it was great to see our experiments finally come together.""Story Source:Materials provided by DOE/Lawrence Berkeley National Laboratory. ",0.0490326,0.109072,0.081658,0.131534,0.0873532,0.104506,0.116598,0,0
379,Mysteries of polymer strands in fuel cells unravelled,"Hydrogen fuel cells offer an attractive source of continuous energy for remote applications, from spacecraft to remote weather stations. Fuel cell efficiency decreases as the Nafion membrane, used to separate the anode and cathode within a fuel cell, swells as it interacts with water.A Russian and Australian collaboration has now shown that this Nafion separator membrane partially unwinds some of its constituent fibers, which then protrude away from the surface into the bulk water phase for hundreds of microns.The research team was led by a group in Russia together with Australian professor Barry Ninham from Australian National University in Canberra, a leading specialist in colloid and interface science. Their results were published this week in The Journal of Chemical Physics, from AIP Publishing.The research team began this project to examine a proposed hypothesis that attributed a new state of water to explain swelling of the Nafion membrane. Instead, they are the first to describe the growth of polymer fibers extending from the membrane surface as it interacts with water. The number of fibers increases as a function of deuterium concentration of the water.""To increase our understanding of these membranes, we needed to describe the molecular-level interaction of deuterated water with the polymer,"" Bunkin said. ""Now that we know the structure of the 'exclusion zone', we can tailor the Nafion structure and its electrical properties by studying changes induced by ion-specific (Hofmeister) effects on its organization and function.""Nafion is the highest-performance commercially available hydrogen-oxide proton exchange membrane used to date in fuel cells. Its porous nature permits significant concentration of the electrolyte solution while separating the anode from the cathode, which allows the flow of electrons producing energy in the fuel cell.The researchers found the membrane is specifically sensitive to the deuterium content in the ambient water by unweaving the surface's structure. The polymer fibers extend from the membrane into the water. The effect is most pronounced in water with deuterium content between 100 and 1,000 parts per million.For this study, the team developed a specialized laser instrumentation (photoluminescent UV spectroscopy) to characterize the polymer fibers along the membrane-water interface. Although the individual fibers were not observed directly due to the spatial limitation of the instrumentation, the team reliably detected their outgrowth into the water.""The significance of this work may provide an entree into some very fundamental areas of biology and energy production about which we did not have a clue,"" Bunkin said.This work was supported by grants from the Russian Federation for Basic Research, the President of the Russian Federation for Young Scientists and the MEphI Academic Excellence Project.Story Source:Materials provided by American Institute of Physics. ",0.100868,0.241927,0.170263,0.157333,0.141215,0.152734,0.169829,0,0
380,"New, durable catalyst for key fuel cell reaction may prove useful in eco-friendly vehicles ","One factor holding back the widespread use of eco-friendly hydrogen fuel cells in cars, trucks and other vehicles is the cost of the platinum catalysts that make the cells work. One approach to using less precious platinum is to combine it with other cheaper metals, but those alloy catalysts tend to degrade quickly in fuel cell conditions.Now, researchers from Brown University have developed a new alloy catalyst that both reduces platinum use and holds up well in fuel cell testing. The catalyst, made from alloying platinum with cobalt in nanoparticles, was shown to beat U.S. Department of Energy (DOE) targets for the year 2020 in both reactivity and durability, according to tests described in the journal Joule.""The durability of alloy catalysts is a big issue in the field,"" said Junrui Li, a graduate student in chemistry at Brown and the study's lead author. ""It's been shown that alloys perform better than pure platinum initially, but in the conditions, inside a fuel cell the non-precious metal part of the catalyst gets oxidized and leached away very quickly.""To address this leaching problem, Li and his colleagues developed alloy nanoparticles with a specialized structure. The particles have a pure platinum outer shell surrounding a core made from alternating layers of platinum and cobalt atoms. That layered core structure is key to the catalyst's reactivity and durability, says Shouheng Sun, professor of chemistry at Brown and senior author of the research.""The layered arrangement of atoms in the core helps to smooth and tighten platinum lattice in the outer shell,"" Sun said. ""That increases the reactivity of the platinum and at the same time protects the cobalt atoms from being eaten away during a reaction. That's why these particles perform so much better than alloy particles with random arrangements of metal atoms.""The details of how the ordered structure enhances the catalyst's activity are described briefly in the Joule paper, but more specifically in a separate computer modeling paper published in the Journal of Chemical Physics. The modeling work was led by Andrew Peterson, an associate professor in Brown's School of Engineering, who was also a coauthor on the Joule paper.For the experimental work, the researchers tested the ability of the catalyst to perform the oxygen reduction reaction, which is critical to the fuel cell performance and durability. On one side of a proton exchange membrane (PEM) fuel cell, electrons stripped away from hydrogen fuel create a current that drives an electric motor. On the other side of the cell, oxygen atoms take up those electrons to complete the circuit. That's done through the oxygen reduction reaction.Initial testing showed that the catalyst performed well in the laboratory setting, outperforming a more traditional platinum alloy catalyst. The new catalyst maintained its activity after 30,000 voltage cycles, whereas the performance of the traditional catalyst dropped off significantly.But while lab tests are important for assessing the properties of a catalyst, the researchers say, they don't necessarily show how well the catalyst will perform in an actual fuel cell. The fuel cell environment is much hotter and differs in acidity compared to laboratory testing environments, which can accelerate catalyst degradation. To find out how well the catalyst would hold up in that environment, the researchers sent the catalyst to the Los Alamos National Lab for testing in an actual fuel cell.The testing showed that the catalyst beats targets set by the Department of Energy (DOE) for both initial activity and longer-term durability. DOE has challenged researchers to develop catalyst with an initial activity of 0.44 amps per milligram of platinum by 2020, and an activity of at least 0.26 amps per milligram after 30,000 voltage cycles (roughly equivalent to five years of use in a fuel cell vehicle). Testing of the new catalyst showed that it had an initial activity of 0.56 amps per milligram and an activity after 30,000 cycles of 0.45 amps.""Even after 30,000 cycles, our catalyst still exceeded the DOE target for initial activity,"" Sun said. ""That kind of performance in a real-world fuel cell environment is really promising.""The researchers have applied for a provisional patent on the catalyst, and they hope to continue to develop and refine it.Story Source:Materials provided by Brown University. ",0.0929274,0.268358,0.140562,0.175997,0.139971,0.147348,0.183993,0,0
381,New technique for turning sunshine and water into hydrogen fuel,"A research team led by DGIST Professor Jong-Sung Yu's team at the Department of Energy Science and Engineering has successfully developed a new catalyst synthesis method that can efficiently decompose water into oxygen and hydrogen using solar light. It is expected that this method will facilitate hydrogen mass production due to higher efficiency than the existing photocatalyst method.Due to the intensifying environmental problems such as air pollution and global warming caused by the increased use of fossil energy, hydrogen is recently drawing attention as an ecofriendly energy source of next generation. Accordingly, research is being conducted globally on how to produce hydrogen using solar light and photocatalyst by decomposing water. To overcome the limitations of photocatalyst that only reacts to light in ultraviolet rays, researchers have doped dual atom such as Nitrogen (N), Sulfur (S), and Phosphorus (P) on photocatalyst or synthesized new photocatalysts, developing a photocatalyst that reacts efficiently to visible light.With Professor Samuel Mao's team at UC Berkeley in the U.S., Professor Yu's research team developed a new H-doped photocatalyst by removing oxygen from the photocatalyst surface made of titanium dioxide and filling hydrogen into it through the decomposition of MgH2. Energy of long wavelength including visible light could not be used for the existing white Titanium dioxide because it has a wide band gap energy. However, the development of MgH2 reduction could overcome this through oxygen flaw induction and H-doping while enabling the use of solar light with 570nm-wavelength.MgH2 reduction can synthesize new matters by applying to Titanium oxide used in this research as well as the oxides composed of other atoms such as Zr, Zn, and Fe. This method is applicable to various other fields such as photocatalyst and secondary battery. The photocatalyst synthesized in this research has four times higher photoactivity than the existing white titanium dioxide and is not difficult to manufacture, thus being very advantageous for hydrogen mass production.Another characteristic of the photocatalyst developed by the research team is that it reduces band gap more than the existing Titanium dioxide photocatalyst used for hydrogen generation and can maintain four times higher activity with stability for over 70 days.The new method can also react to visible light unlike existing photosynthesis, overcoming the limitation of hydrogen production. With the new photocatalyst development, the efficiency and stability of hydrogen production can both dramatically improved, which will help popularize hydrogen energy in the near future.Professor Yu said ""The photocatalyst developed this time is a synthesis method with much better performance than the existing photocatalyst method used to produce hydrogen. It is a very simple method that will greatly help commercialize hydrogen energy. With a follow-up research on improving the efficiency and economic feasibility of photocatalyst, we will take the lead in creating an environment stable hydrogen energy production that can replace fossil energy.""Story Source:Materials provided by DGIST (Daegu Gyeongbuk Institute of Science and Technology). ",0.0890921,0.189363,0.155582,0.157026,0.12659,0.155977,0.167428,0,0
382,Modelling a future fuelled by sustainable energy,"University of Adelaide economists have modelled the transition from a world powered by fossil fuels to one in which sustainable sources supply all our energy needs.Dr Raul Barreto, Senior Lecturer from the University's School of Economics, has examined the short and long-term consequences of the relative productivity differential between fossil fuel and renewable energy.""The transition from fossil fuels to alternative energy is dependent on their relative productivity. Sustainable energy will increasingly replace fossil fuels as the former becomes a less productive, more expensive source and the latter becomes more productive and less expensive,"" says Dr Barreto.The research, published in the journal Economic Modelling, shows that fuel productivity determines whether renewable energy is a viable source and how economies will transition from today's world in which sustainable energy complements fossil fuels, to one solely powered by sustainable sources.""Fossil fuels have the advantage of being a relatively inexpensive and stable source of energy, but stocks are finite,"" says Dr Barreto.""Sustainable energy sources such as solar and wind power are potentially limitless, but supply is inconsistent, and they require large amounts of capital investment to make them a viable source,"" he says.Despite the world having passed 'peak oil', where abundant supply was enjoyed, it is still difficult to predict when fossil fuels will run out. Some estimates suggest that in 50 -- 100 years from now depleting reserves will become an issue.""Hypothetical scenarios predict that finite stocks of fossil fuels will be depleted and economies that are solely dependent on those sources of energy will collapse causing severe welfare problems,"" says Dr Barreto.""These predictions assume that the supply of fossil fuel cannot be influenced by productivity increases, economies of scale or substitution.""However, alternative energy substitution can alleviate the negative implications on growth and welfare of an ever-depleting fuel source on an energy dependent dynamic economy.""The further that alternative energy must improve to catch up to oil, the larger the relative decrease in consumption will be. If alternative energy remains less productive than oil, we will suffer falling growth, possibly for a prolonged period, as we are forced to switch to the less efficient alternative.""If society can improve the productivity of alternative energy in the long run to a level comparable to oil, then the future will be at least as bright as it was at the peak of the economy's oil dependence. If instead, alternative energy always remains oil's weaker cousin, then the eventual result is a world that is at best nostalgic of the heydays of cheap oil,"" says Dr Barreto.Story Source:Materials provided by University of Adelaide. ",0.120248,0.158707,0.185812,0.196166,0.134456,0.22068,0.200445,0,0
383,Artificial enzymes convert solar energy into hydrogen gas,"In a new scientific article, researchers at Uppsala University describe how, using a completely new method, they have synthesised an artificial enzyme that functions in the metabolism of living cells. These enzymes can utilize the cell's own energy, and thereby enable hydrogen gas to be produced from solar energy.Hydrogen gas has long been noted as a promising energy carrier, but its production is still dependent on fossil raw materials. Renewable hydrogen gas can be extracted from water, but as yet the systems for doing so have limitations.In the new article, published in the journal Energy and Environmental Science, an interdisciplinary European research group led by Uppsala University scientists describe how artificial enzymes convert solar energy into hydrogen gas. This entirely new method has been developed at the University in the past few years. The technique is based on photosynthetic microorganisms with genetically inserted enzymes that are combined with synthetic compounds produced in the laboratory. Synthetic biology has been combined with synthetic chemistry to design and create custom artificial enzymes inside living organisms.""We've now been able to use the method we developed to produce enzymes that use the cell's own energy to produce hydrogen gas,"" says Adam Wegelius, a PhD student at the Department of Chemistry -- engstrem Laboratory, Uppsala University.Senior Lecturer Gustav Berggren and Professor Peter Lindblad of the same department have been jointly leading the research.""Evolution has already developed and refined a tool for capturing sunlight through photosynthesis. And by introducing our artificial enzyme into photosynthetic cyanobacteria we can directly benefit from this efficient process, thus producing hydrogen gas from solar energy. We've developed a completely new method, which allows us to go beyond the solutions offered by evolution and nature, in our development of artificial enzymes"" Berggren says.Story Source:Materials provided by Uppsala University. ",0.0994964,0.210104,0.167244,0.219884,0.14458,0.160795,0.20962,0,0
384,New fuel cell concept brings biological design to better electricity generation,"Fuel cells have long been viewed as a promising power source. These devices, invented in the 1830s, generate electricity directly from chemicals, such as hydrogen and oxygen, and produce only water vapor as emissions. But most fuel cells are too expensive, inefficient, or both.In a new approach, inspired by biology and published today (Oct. 3, 2018) in the journal Joule, a University of Wisconsin-Madison team has designed a fuel cell using cheaper materials and an organic compound that shuttles electrons and protons.In a traditional fuel cell, the electrons and protons from hydrogen are transported from one electrode to another, where they combine with oxygen to produce water. This process converts chemical energy into electricity. To generate a meaningful amount of charge in a short enough amount of time, a catalyst is needed to accelerate the reactions.Right now, the best catalyst on the market is platinum -- but it comes with a high price tag. This makes fuel cells expensive and is one reason why there are only a few thousand vehicles running on hydrogen fuel currently on U.S. roads.Shannon Stahl, the UW-Madison professor of chemistry who led the study in collaboration with Thatcher Root, a professor of chemical and biological engineering, says less expensive metals can be used as catalysts in current fuel cells, but only if used in large quantities. ""The problem is, when you attach too much of a catalyst to an electrode, the material becomes less effective,"" he says, ""leading to a loss of energy efficiency.""The team's solution was to pack a lower-cost metal, cobalt, into a reactor nearby, where the larger quantity of material doesn't interfere with its performance. The team then devised a strategy to shuttle electrons and protons back and forth from this reactor to the fuel cell.The right vehicle for this transport proved to be an organic compound, called a quinone, that can carry two electrons and protons at a time. In the team's design, a quinone picks up these particles at the fuel cell electrode, transports them to the nearby reactor filled with an inexpensive cobalt catalyst, and then returns to the fuel cell to pick up more ""passengers.""Many quinones degrade into a tar-like substance after only a few round trips. Stahl's lab, however, designed an ultra-stable quinone derivative. By modifying its structure, the team drastically slowed down the deterioration of the quinone. In fact, the compounds they assembled last up to 5,000 hours -- a more than 100-fold increase in lifetime compared to previous quinone structures.""While it isn't the final solution, our concept introduces a new approach to address the problems in this field,"" says Stahl. He notes that the energy output of his new design produces about 20 percent of what is possible in hydrogen fuel cells currently on the market. On the other hand, the system is about 100 times more effective than biofuel cells that use related organic shuttles.The next step for Stahl and his team is to bump up the performance of the quinone mediators, allowing them to shuttle electrons more effectively and produce more power. This advance would allow their design to match the performance of conventional fuel cells, but with a lower price tag.""The ultimate goal for this project is to give industry carbon-free options for creating electricity,"" says Colin Anson, a postdoctoral researcher in the Stahl lab and publication co-author. ""The objective is to find out what industry needs and create a fuel cell that fills that hole.""This step in the development of a cheaper alternative could eventually be a boon for companies like Amazon and Home Depot that already use hydrogen fuel cells to drive forklifts in their warehouses.""In spite of major obstacles, the hydrogen economy seems to be growing,"" adds Stahl, ""one step at a time.""Financial support for this project was provided by the Center for Molecular Electrocatalysis, an Energy Frontier Research Center funded by the U.S. Department of Energy, Office of Science, Office of Basic Energy Sciences, and by the Wisconsin Alumni Research Foundation (WARF) through the WARF Accelerator Program.Story Source:Materials provided by University of Wisconsin-Madison. Original written by Mark E. Griffin. ",0.0612199,0.20894,0.100267,0.14706,0.100494,0.11712,0.135332,0,0
385,Artificial leaf: Highly active organic photocatalyst discovered,"Scientists from the University of Liverpool, University College London and East China University of Science and Technology have synthesized a new organic material that can convert water into hydrogen fuel using sunlight.Photocatalytic solar hydrogen production -- or water splitting -- offers an abundant clean energy source, but only if the energy in sunlight can be harvested effectively. Inorganic materials are better known as water splitting catalysts, but organic catalysts can also be built from cheap abundant elements, such as carbon, nitrogen, and sulphur.The Liverpool-led team has used a combination of experiment and computation to discover a highly active organic photocatalyst. This also revealed some basic design principles, which may guide us to even better catalysts in the future.Mr Xiaoyan Wang, the Liverpool Chemistry PhD student who led the experimental work, said: ""To achieve high hydrogen evolution rates, you need good water affinity, broad light adsorption, high surface area, and high crystallinity. By introducing all of these features in one material, we got a very active photocatalyst.""Story Source:Materials provided by University of Liverpool. ",0.109684,0.225764,0.197145,0.200651,0.166482,0.161739,0.209316,0,0
386,Eco-friendly nanoparticles for artificial photosynthesis,"Researchers at the University of Zurich have developed a nanoparticle type for novel use in artificial photosynthesis by adding zinc sulfide on the surface of indium-based quantum dots. These quantum dots produce clean hydrogen fuel from water and sunlight -- a sustainable source of energy. They introduce new eco-friendly and powerful materials to solar photocatalysis.Quantum dots are true all-rounders. These material structures, which are only a few nanometers in size, display a similar behavior to that of molecules or atoms, and their form, size and number of electrons can be modulated systematically. This means that their electrical and optical characteristics can be customized for a number of target areas, such as new display technologies, biomedical applications as well as photovoltaics and photocatalysis.Fuel production using sunlight and waterAnother current line of application-oriented research aims to generate hydrogen directly from water and solar light. Hydrogen, a clean and efficient energy source, can be converted into forms of fuel that are used widely, including methanol and gasoline. The most promising types of quantum dots previously used in energy research contain cadmium, which has been banned from many commodities due to its toxicity. The team of Greta Patzke, Professor at the Department of Chemistry of the University of Zurich, and scientists from Southwest Petroleum University in Chengdu and the Chinese Academy of Sciences have now developed a new type of nanomaterials without toxic components for photocatalysis.Indium-containing core with a thin layer of zinc sulfideThe three-nanometer particles consist of a core of indium phosphide with a very thin surrounding layer of zinc sulfide and sulfide ligands. ""Compared to the quantum dots that contain cadmium, the new composites are not only environmentally friendly, but also highly efficient when it comes to producing hydrogen from light and water,"" explains Greta Patzke. Sulfide ligands on the quantum dot surface were found to facilitate the crucial steps involved in light-driven chemical reactions, namely the efficient separation of charge carriers and their rapid transfer to the nanoparticle surface.Great potential for eco-friendly applicationsThe newly developed cadmium-free nanomaterials have the potential to serve as a more eco-friendly alternative for a variety of commercial fields. ""The water-soluble and biocompatible indium-based quantum dots can in the future also be tested in terms of biomass conversion to hydrogen. Or they could be developed into low-toxic biosensors or non-linear optical materials, for example,"" adds Greta Patzke. She will continue to focus on the development of catalysts for artificial photosynthesis within the University Research Priority Program ""LightChEC."" This interdisciplinary research program aims to develop new molecules, materials and processes for the direct storage of solar light energy in chemical bonds.Story Source:Materials provided by University of Zurich. ",0.0865933,0.189212,0.135108,0.177986,0.135214,0.149265,0.173635,0,0
387,Tracking hydrogen movement using subatomic particles Physicists detect local nuclear magnetic fields in a solid using negative muons,"A muon is an unstable subatomic particle similar to an electron but with much greater mass. The lifetime of a muon is only a couple of microseconds, but this is long compared with the lifetimes of many unstable subatomic particles. Because of their comparatively long lifetime, positive muons are often used to detect internal magnetic fields in solid materials. However, negative muons have seldom been used for this purpose because a large data set is required to obtain reliable results and experimental data collection times are normally limited. Recently, researchers developed a system that can count muon events at a much faster rate, allowing an experiment to be completed in a suitable time frame. Using this system, a Japanese collaboration has realized the long-standing goal of using negative muons to observe the local nuclear magnetic fields in a solid for the first time.As explained in their article in Physical Review Letters, the team used magnesium hydride as the solid in their experiments. Magnesium hydride has a formula of MgH2 and is a potential candidate as a hydrogen storage material. Magnesium hydride was selected for study in experiments using the negative muon beam because muons initially captured on hydrogen are transferred quickly to magnesium, which allowed the transfer process of hydrogen to be investigated.""The magnesium atoms exposed to the negative muon beam were effectively converted to sodium,"" says first author Jun Sugiyama at Toyota Central Research & Development Laboratories Inc. ""The local magnetic field of the hydrogen atoms around these converted atoms was then able to be detected, which meant that we could track hydrogen diffusion.""The experiments used a high-intensity muon beam and highly integrated positron detector system to detect the local magnetic fields in the magnesium hydride sample. The obtained spectra were consistent with the magnesium atoms having a random magnetic field, agreeing with theoretical predictions. In particular, the results agreed with estimations from dipole field calculations, indicating that the nuclear magnetic fields of hydrogens in magnesium hydride were indeed observed.""Our approach using negative muons to detect the local behavior of ions is attractive because it allows us to study the dynamics of light elements in a solid from the fixed point of the nucleus,"" says co-author Kazuhiko Ninomiya at Osaka University. ""This approach is therefore complementary to nuclear magnetic resonance spectroscopy.""Using this negative muon-based technique, it is now possible to track the movement of hydrogen in a solid, which should aid the development of hydrogen storage materials.Story Source:Materials provided by Osaka University. ",0.0964757,0.167683,0.143998,0.170561,0.126581,0.167321,0.16223,0,0
388,Microbubble scrubber destroys dangerous biofilms,"Stiff microbial films often coat medical devices, household items and infrastructure such as the inside of water supply pipes, and can lead to dangerous infections. Researchers have developed a system that harnesses the power of bubbles to propel tiny particles through the surfaces of these tough films and deliver an antiseptic deathblow to the microbes living inside.Biofilms are slimy colonies of microbes held together by internal scaffolds, clinging to anything they touch. About 80 percent of all medical infections originate from biofilms that invade the inner workings of hospital devices and implants inside patients. Eradication is difficult because traditional disinfectants and antibiotics cannot effectively penetrate a biofilm's tough surface, the researchers said.In the journal Applied Materials and Interfaces, a team led by researchers at the University of Illinois at Urbana-Champaign describes how they used diatoms -- the tiny skeletons of algae -- loaded with an oxygen-generating chemical to destroy microbes.""Most of us get those black or yellow spots in our showers at home,"" said co-author Hyunjoon Kong, a professor of chemical and biomolecular engineering and a Carle Illinois College of Medicine affiliate. ""Those spots are biofilms and most of us know it takes a lot of energy to scrub them away. Imagine trying to do this inside the confined space of the tubing of a medical device or implant. It would be very difficult.""Looking to nature and basic mechanics for a solution, the researchers developed a system that uses naturally abundant diatoms along with hydrogen peroxide and tiny oxygen-generating sheets of the compound manganese oxide.""We could have fabricated a particle using 3D printers, but luckily nature already provided us with a cheap and abundant option in diatoms,"" said co-author and postdoctoral researcher Yongbeom Seo. ""The species of diatom we selected are hollow, highly porous and rod-shaped, providing a lot of surface area for the bubbles to form and a channel for the bubbles to escape.""The chemical reaction between the hydrogen peroxide and manganese oxide nanosheets takes place within the empty space inside the diatom. The result is a flourish of microbubbles that flow through the tiny channel, propelling the rigid diatoms forward with enough force to break up the surface and internal structure of the biofilms, the researchers said.""We dope the particles with nanosheets of manganese oxide, then mix them with hydrogen peroxide and apply that to the surface of the biofilm,"" Kong said. ""Once the diatoms break through to the internal structure of the biofilm, they continue to expel bubbles and facilitate the entry of hydrogen peroxide, which is an effective disinfectant against bacteria and fungus.""The researchers believe that their success is a result of a decision to focus on the mechanical aspects of biofilm destruction, not the chemical aspects of simply killing microbes.""We have arrived at a mechanistic solution for this problem and the possibilities for this technology are endless,"" said co-author Simon Rogers, a professor of chemical and biomolecular engineering. ""We are discussing our research with clinicians who have many exciting ideas of how to use this system that we did not even think of originally, such as the removal of dental plaque.""Video of the microbubblers in action: https://www.youtube.com/watch?v=LjK4QguW0wA&feature=youtu.beStory Source:Materials provided by University of Illinois at Urbana-Champaign. ",0.0852447,0.136911,0.157648,0.182821,0.128357,0.140632,0.171082,0,0
389,A protective shield for sensitive enzymes in biofuel cells,"An international team of researchers has developed a new mechanism to protect enzymes from oxygen as biocatalysts in fuel cells. The enzymes, known as hydrogenases, are just as efficient as precious metal catalysts, but unstable when they come into contact with oxygen. They are therefore not yet suitable for technological applications. The new protective mechanism is based on oxygen-consuming enzymes that draw their energy from sugar. The researchers showed that they were able to use this protective mechanism to produce a functional biofuel cell that works with hydrogen and glucose as fuel.The team led by Dr. Adrian Ruff and Professor Wolfgang Schuhmann from the Center for Electrochemical Sciences at Ruhr-Universitet Bochum describes the results in the journal Nature Communications from 10 September 2018 together with colleagues from the Max Planck Institute for Chemical Energy Conversion in Melheim an der Ruhr and the University of Lisbon.Earlier protective mechanism reduced performanceThe team from the Bochum Center for Electrochemical Sciences had already shown in earlier studies that hydrogenases can be protected from oxygen by embedding them in a polymer. ""However, this mechanism consumed electrons, which reduced the performance of the fuel cell,"" says Adrian Ruff. ""In addition, part of the catalyst was used to protect the enzyme."" The scientists therefore looked for ways to decouple the catalytically active system from the protective mechanism.Enzymes trap oxygenWith the aid of two enzymes, they built an oxygen removal system around the current-producing electrode. First, the researchers coated the electrode with the hydrogenases, which were embedded in a polymer matrix to fix them in place. They then placed another polymer matrix on top of the hydrogenase, which completely enclosed the underlying catalyst layer. It contained two enzymes that use sugar to convert oxygen into water.Hydrogen is oxidised in the hydrogenase-containing layer at the bottom. The electrode absorbs the electrons released in the process. The top layer removes harmful oxygen.Functional fuel cell builtIn further experiments, the group combined the bioanodes described above with biocathodes, which are also based on the conversion of glucose. In this way, the team produced a functional biofuel cell. ""The cheap and abundant biomass glucose is not only the fuel for the protective system, but also drives the biocathode and thus generates a current flow in the cell,"" summarises Wolfgang Schuhmann, head of the Center for Electrochemical Sciences and member of the cluster of excellence Ruhr Explores Solvation. The cell had an open-circuit voltage of 1.15 volts -- the highest value ever achieved for a cell containing a polymer-based bioanode.Mechanism can be transferred to other enzymes""We assume that the principle behind this protective shield mechanism can be transferred to any sensitive catalyst if the appropriate enzyme is selected that can catalyse the corresponding interception reaction,"" says Wolfgang Schuhmann.Story Source:Materials provided by Ruhr-University Bochum. ",0.0980079,0.242202,0.159161,0.168632,0.143605,0.158195,0.180135,0,0
390,Detecting hydrogen using the extraordinary Hall effect in cobalt-palladium thin films,"Researchers looking to hydrogen as a next-generation clean energy source are developing hydrogen-sensing technologies capable of detecting leaks in hydrogen-powered vehicles and fueling stations before the gas turns into an explosion. The most common type of hydrogen sensors is composed of palladium-based thin films because palladium (Pd), a silvery-white metal resembling platinum, readily absorbs hydrogen gas. However, Pd also readily absorbs other gases, decreasing the overall efficiency of these sensors.Alexander Gerber's research team at Tel Aviv University recently conducted a systematic study of hydrogen detection using the Extraordinary Hall Effect (EHE) to measure the hydrogen magnetization response in cobalt-palladium (CoPd) thin films. The team reports the findings in the Journal of Applied Physics, from AIP Publishing.""We found that detection of hydrogen by EHE really works with very high sensitivity,"" said Alexander Gerber, an author on the paper. ""A goal would be to develop a compact EHE device compatible with a standard four-probe resistance measuring method to enhance gas detection through a magnetic type of sensor using the spintronics effect.""The burgeoning field of spintronics exploits an electron's spin and its resulting magnetic properties. In essence, EHE is a spin-dependent phenomenon that generates voltage proportional to magnetization across a current-carrying magnetic film.Otherwise known as the anomalous Hall effect, EHE occurs in ferromagnetic materials and can be much larger than the ordinary Hall effect. Although palladium has high hydrogen absorption capacity, it's not ferromagnetic by itself. So, the researchers added cobalt, a ferromagnetic material whose magnetic properties are affected by the hydrogen absorption in CoPd alloys to induce EHE.The researchers prepared four sets of samples with thicknesses of 7, 14, 70 and 100 nanometers with varying cobalt concentrations and tested them in an atmosphere with different levels of hydrogen up to 4 percent. They found that the thinnest films demonstrated the largest absolute response to hydrogen: The signal changes by more than 500 percent per 1 percent of hydrogen.""In practical terms, we identified the sensitive range of compositions, how the response to hydrogen depends on composition, and what the options are to operate the sensor,"" Gerber said.Gerber's research team is now in the process of recording response times and exploring the ability to release hydrogen after exposure so sensors can be reused. The researchers also plan to explore ways to improve selectivity of hydrogen and adapt their technique for selective detection of other gases.Story Source:Materials provided by American Institute of Physics. ",0.106424,0.209996,0.142798,0.16628,0.156009,0.183544,0.171125,0,0
391,"Not too wet, not too dry: Plasma-treated fuel cell gets it just right ","Fuel cells hold promise as a clean, renewable source of energy. But keeping them dry has long been a challenge, as they produce water during the process of converting hydrogen and oxygen into electricity.Now University of British Columbia researchers say they may have found a solution: pre-treating the electrode -- a key component of fuel cells -- with ionized oxygen gas, or plasma.""Fuel cells need a small amount of moisture to work, but too much water blocks the flow of gases through the cell,"" said the study's first author Benjamin Zahiri, a materials scientist. ""Plasma treatment modifies the surface of the electrode so that excess water can sink down into the bottom layers and drain away, without drying out the cell.""Researchers exposed the surface of a fuel-cell electrode, which contains porous layers of carbon, to ionized oxygen gas for one minute. The treatment deposited varying amounts of oxygen-carrying molecules, which attract water, on the surface.""The upper layers received the most molecules and the next layers received progressively fewer molecules,"" said Zahiri. ""By creating this gradually decreasing 'wettability', the water is drawn down from the upper layers through to the bottom layers, from where it exits the cell.""The structure of the carbon layer itself remains unchanged.""The study focused on proton-exchange membrane (PEM) fuel cells, a type of fuel cells commonly used for cars and trucks.""Our results show that plasma treatment is effective in facilitating water transport in a PEM fuel cell,"" said Zahiri. ""Other water transport strategies like hydrophobic or water-repelling coatings are too costly or too complex to be viable.""Plasma treatment also can be easily integrated in the manufacturing of fuel cell components, according to the researchers.""It's fast, low-cost and already widely used in manufacturing,"" said the study's senior author, Walter Merida, a mechanical engineering professor who heads the Clean Energy Research Centre at UBC. ""It generates no chemical waste, so it's environmentally friendly.""The team is currently exploring other applications of plasma treatment. According to Merida, plasma treatments can work on other porous materials used in energy storage. They can also be integrated into caustic soda-based industries, a market valued at US$80 billion.Story Source:Materials provided by University of British Columbia. ",0.083206,0.248767,0.145846,0.147974,0.135256,0.135007,0.166587,0,0
392,New technology improves hydrogen manufacturing,"Industrial hydrogen is closer to being produced more efficiently, thanks to findings outlined in a new paper published by Idaho National Laboratory researchers. In the paper, Dr. Dong Ding and his colleagues detailed advances in the production of hydrogen, which is used in oil refining, petrochemical manufacturing and as an eco-friendly fuel for transportation.The researchers demonstrated high-performance electrochemical hydrogen production at a lower temperature than had been possible before. This was due to a key advance: a ceramic steam electrode that self-assembles from a woven mat.""We invented a 3D self-assembled steam electrode which can be scalable,"" said Ding. ""The ultrahigh porosity and the 3D structure can make the mass/charge transfer much better, so the performance was better.""In a paper published by the journal Advanced Science, the researchers reported on the design, fabrication and characterization of highly efficient proton-conducting solid oxide electrolysis cells (P-SOECs) with a novel 3D self-assembled steam electrode. The cells operated below 600o C. They produced hydrogen at a high sustained rate continuously for days during testing.Hydrogen is an eco-friendly fuel in part because when it burns, the result is water. However, there are no convenient suitable natural sources for pure hydrogen. Today, hydrogen is obtained by steam reforming (or ""cracking"") hydrocarbons, such as natural gas. This process, though, requires fossil fuels and creates carbon byproducts, which makes it less suited for sustainable production.Steam electrolysis, by contrast, needs only water and electricity to split water molecules, thereby generating hydrogen and oxygen. The electricity can come from any source, including wind, solar, nuclear and other emission-free sources. Being able to do electrolysis efficiently at as low a temperature as possible minimizes the energy needed.A P-SOEC has a porous steam electrode, a hydrogen electrode and a proton-conducting electrolyte. When voltage is applied, steam travels through the porous steam electrode and turns into oxygen and hydrogen at the electrolyte boundary. Due to differing charges, the two gases separate and are collected at their respective electrodes.So, the construction of the porous steam electrode is critical, which is why the researchers used an innovative way to make it. They started with a woven textile template, put it into a precursor solution containing elements they wanted to use, and then fired it to remove the fabric and leave behind the ceramic. The result was a ceramic version of the original textile.They put the ceramic textile in the electrode and noticed that in operation, bridging occurred between strands. This should improve both mass and charge transfer and the stability of the electrode, according to Dr. Wei Wu, the primary contributor to this work.The electrode and the use of proton conduction enabled high hydrogen production below 600o C. That is cooler by hundreds of degrees than is the case with conventional high-temperature steam electrolysis methods. The lower temperature makes the hydrogen production process more durable, and also requires fewer costly, heat-resistant materials in the electrolysis cell.Although hydrogen is already used to power vehicles, for energy storage and as portable energy, this approach could offer a more efficient alternative for high-volume production.Story Source:Materials provided by DOE/Idaho National Laboratory. ",0.0846069,0.243031,0.113777,0.139987,0.12589,0.159385,0.158967,0,0
393,Catalyst advance could lead to economical fuel cells,"Researchers at Washington State University have developed a new way to make low-cost, single-atom catalysts for fuel cells -- an advance that could make important clean energy technology more economically viable.Their work is published in the Advanced Energy Materials journal.Hydrogen fuel cells are critical for the clean energy economy as they are more than two times as efficient at creating electricity than polluting combustion engines. Their only waste product is water.However, the high price of the platinum-based catalysts that are used for the chemical reaction in fuel cells significantly hinders their commercialization.Instead of the rare platinum, researchers would like to use nonprecious metals, such as iron or cobalt. But reactions with these abundantly available metals tend to stop working after a short time.""Low-cost catalysts with high activity and stability are critical for the commercialization of the fuel cells."" said Qiurong Shi, postdoctoral researcher in the School of Mechanical and Materials Engineering (MME) and a co-first author on the paper.Recently, researchers have developed single-atom catalysts that work as well in the laboratory setting as using precious metals. The researchers have been able to improve the stability and activity of the nonprecious metals by working with them at the nanoscale as single-atom catalysts.In this new work, the WSU research team, led by Yuehe Lin, an MME professor, used iron or cobalt salts and the small molecule glucosamine as precursors in a straightforward high temperature process to create the single-atom catalysts. The process can significantly lower the cost of the catalysts and could be easily scaled up for production.The iron-carbon catalysts they developed were more stable than commercial platinum catalysts. They also maintained good activity and didn't become contaminated, which is often a problem with common metals.""This process has many advantages,"" said Chengzhou Zhu, a first author on the paper who developed the high temperature process. ""It makes large-scale production feasible, and it allows us to increase the number and boost the reactivity of active sites on the catalyst.""Lin's group collaborated on the project with Scott Beckman, an MME associate professor at WSU, as well as with researchers at Advanced Photon Source at Argonne National Laboratory and Brookhaven National Laboratory for materials characterization.""The advanced materials characterization user facility at the national laboratories revealed the single-atom sites and active moieties of the catalysts, which led to the better design of the catalysts,"" said Lin.Story Source:Materials provided by Washington State University. ",0.102073,0.260155,0.144844,0.181968,0.146823,0.163753,0.187862,0,0
394,Producing hydrogen from splitting water without splitting hairs,"In our energy-hungry society, finding cheaper ways of producing and storing energy is a constant battle. Scientists have now devised a method of using copper as a catalyst in the reaction designed to split water and produce hydrogen in gaseous form. A new model explains interactions between small copper clusters used as low-cost catalysts in the production of hydrogen by breaking down water molecules.Copper nanoparticles dispersed in water or in the form of coatings have a range of promising applications, including lubrication, ink jet printing, as luminescent probes, exploiting their antimicrobial and antifungal activity, and in fuel cells. Another promising application is using copper as a catalyst to split water molecules and form molecular hydrogen in gaseous form. At the heart of the reaction, copper-water complexes are synthesised in ultra-cold helium nanodroplets as part of the hydrogen production process, according to a recent paper published in EPJ D. For its authors, Stefan Raggl, from the University of Innsbruck, Austria, and colleagues, splitting water like this is a good way of avoiding splitting hairs.Previous work showed that at the molecular level, water oxidises copper nanoparticles until their surface is saturated with molecules carrying hydrogen (called hydroxyl groups). Theoretical work further showed that a monolayer of water, once adsorbed on the copper particles, spontaneously converts to a half-monolayer of hydroxide (OH) plus half a monolayer of water while releasing hydrogen gas.In their study, Raggl and colleagues synthesised neutral copper-water complexes by successively doping helium nano-droplets -- which are kept at the ultra-cold temperature of 0.37 K in a state referred to as superfluid -- with copper atoms and water molecules. These droplets are then ionised by electrons. The authors show that the composition of the most prominent ions depends on the partial copper and water pressures in the cell where the reaction occurs. They observe ions containing several copper atoms and several dozen water molecules.The authors recognise that they failed to directly observe the predicted hydrogen formation because their instrument is not designed to detect electrically neutral entities.Story Source:Materials provided by Springer. ",0.108159,0.279499,0.178894,0.16124,0.164115,0.161684,0.189012,0,0
395,"Team shatters theoretical limit on bio-hydrogen production Method increases yield of clean, renewable energy source","In 1977, researcher Rudolf Thauer proposed a theoretical ceiling on the amount of hydrogen that bacteria could produce via fermentation, the sugar-converting process also responsible for yogurt, beer and cheese.Propelled by a genetic engineering technique that presents bacteria with a simple choice -- adapt or die -- research from the University of Nebraska-Lincoln just punched through that 40-year-old ceiling like Iron Man through papier-meche.A version of the Thermotoga maritima bacterium engineered by Raghuveer Singh, Paul Blum and their colleagues produced 46 percent more hydrogen per cell than a naturally occurring form of the same species. The team's highest reported yield -- 5.7 units of hydrogen for every unit of glucose fed to the bacterium -- easily surpassed the theoretical limit of 4 units.The feat represents a breakthrough in the global effort to scale up the sustainable production of clean-burning hydrogen for vehicles and heavy industry, Singh said. Most commercial hydrogen comes from refining non-renewable fossil fuels such as natural gas, oil and coal -- processes that generate sizable amounts of carbon dioxide.""I always had been interested in microbes and their potential to make something useful,"" said Singh, a doctoral graduate of Nebraska who conducted the research as part of his dissertation. ""The current hydrogen production technologies create a lot of environmental problems. My dream is to improve biological systems and make them more competitive with those technologies.""Slowing the Sugar RushThe T. maritima bacterium ferments sugar into simpler carbon-based molecules that fuel two processes: growing new cells and producing so-called metabolites, one of which is hydrogen. But under normal conditions, most of that carbon gets funneled into the biological machinery that cranks out new cells, leaving little left over for hydrogen production.""There's a strong coupling between hydrogen synthesis and the growth of new cells, and this coupling needs to be weakened in order to increase the yield of hydrogen,"" said Singh, now a postdoctoral researcher at the University of Florida.So the researchers decided to temporarily inactivate a gene that has no effect on cell growth but slows hydrogen production in T. maritima. When they did, a second gene -- this one involved in transporting sugar -- spontaneously mutated to prevent a lethal buildup of sugar-based metabolites. That mutation also dramatically redirected the bacterium's energy expenditure from cell growth to hydrogen production, creating a new strain that the researchers named Tma 200.After transferring the newly mutated gene into a naturally occurring version of T. maritima, the researchers found that the bacterium overproduced hydrogen just as Tma 200 did -- confirming the influence of sugar uptake on hydrogen yields.""We created the new organism using classical genetics because the necessary changes could not be predicted,"" said Blum, Charles Bessey Professor of Microbiology at Nebraska.Singh, Blum and colleague Derrick White have since worked with technology-transfer office NUtech Ventures to apply for patent protection of the genetic technique, which Singh described as a ""promising strategy"" for increasing bacterial production of any potential metabolite.""Hydrogen is just one of many possibilities,"" he said.The team recently detailed its work in the journal Applied and Environmental Microbiology. Singh and Blum authored the study with White, a recent doctoral graduate now with NuTek Food Science; Yasar Demirel, associate professor of chemical and biomolecular engineering; and researchers from North Carolina State University and the University of Connecticut.The researchers received support from the U.S. Department of Energy.Story Source:Materials provided by University of Nebraska-Lincoln. Original written by Scott Schrage. ",0.0894369,0.206576,0.145678,0.172288,0.125968,0.15154,0.183062,0,0
396,A safe and effective way to whiten teeth,"In the age of Instagram and Snapchat, everyone wants to have perfect pearly whites. To get a brighter smile, consumers can opt for over the counter teeth-whitening treatments or a trip to the dentist to have their teeth bleached professionally. But both types of treatments can harm teeth. According to an article published in ACS Biomaterials Science & Engineering, researchers have now developed a new, less destructive method.Teeth can become discolored on their outer surfaces when people consume colored foods and drinks, such as coffee, tea or red wine. As a result, many people turn to non-invasive whitening treatments that bleach the teeth. Currently, the most common bleaching agent is hydrogen peroxide, which steals electrons from the pigment molecules that cause teeth discoloration, and this process can be sped up by exposing teeth to blue light. But high concentrations of hydrogen peroxide can break down a tooth's enamel, causing sensitivity or cell death. So, Xiaolei Wang, Lan Liao and colleagues wanted to see if a different blue-light-activated compound could be a safer, but still effective, alternative.The team modified titanium dioxide nanoparticles with polydopamine (nano-TiO2@PDA) so that they could be activated with blue light. In a proof-of-concept experiment, the nano-TiO2@PDA particles were evenly coated on the surface of a tooth and irradiated with blue light. After four hours of treatment, the whitening level was similar to that obtained with hydrogen-peroxide-based agents. The group notes that no significant enamel damage was found on the surface of the tooth, and the treatment was significantly less cytotoxic than hydrogen peroxide. In addition, the nano-TiO2@PDA therapy showed antibacterial activity against certain bacteria.Story Source:Materials provided by American Chemical Society. ",0.127519,0.23204,0.202744,0.207701,0.200296,0.177731,0.215096,0,0
397,"Funded by new tax credits, US carbon-capture network could double global CO2 headed underground ","With the right public infrastructure investment, the United States could as much as double the amount of carbon dioxide emissions currently captured and stored worldwide within the next six years, according to an analysis by Princeton University researchers.The authors propose in the Proceedings of the National Academy of Sciences a pipeline network that would transfer carbon dioxide waste from ethanol refineries in the American Midwest -- where grains are fermented to produce the alcohol-based fuel -- to oilfields in West Texas. The captured carbon would then be pumped into near-depleted oil fields through a technique known as enhanced oil recovery, where the carbon dioxide helps recover residual oil while ultimately being trapped underground.The researchers found that this capture-and-storage network could prevent up to 30 million metric tons of human-made carbon dioxide from entering the atmosphere each year -- an amount equal to removing 6.5 million cars from the road. Currently, about 31 million metric tons of carbon dioxide annually are captured and stored worldwide.The authors were motivated by a tax credit passed by Congress in the 2018 Bipartisan Budget Act to encourage investment in carbon capture and storage. Their analysis showed that this large-scale capture-and-storage network would only be possible -- and profitable for the companies using it -- if the tax credits are coupled with low-interest government loans to fund the necessary pipeline infrastructure. If governments provided low-cost loans for only half of the pipelines, the resulting smaller-scale network would still sequester 19 million metric tons of carbon dioxide per year.""The new tax credits are the most significant policy incentivizing carbon capture, utilization and storage (CCUS) in the world today,"" said first author Ryan Edwards, a AAAS Congressional Science and Engineering Fellow in Washington, D.C. who earned his Ph.D. in civil and environmental engineering from Princeton in 2018. Edwards worked on the analysis as a graduate student under co-author Michael Celia, director of the Princeton Environmental Institute (PEI) and the Theodora Shelton Pitney Professor of Environmental Studies and professor of civil and environmental engineering.""This is the first time we've had a policy in front of us that lets us seriously consider deployment on a large scale,"" Edwards said. ""There's a lot of interest in CCUS at different levels, and bipartisan support. What there hasn't been is a plan for how this could happen and what it would look like.""The U.N. Intergovernmental Panel on Climate Change has identified carbon capture as a key component in reducing carbon dioxide emissions and maintaining the planet's average temperature within 2 degrees Celsius above pre-industrial levels. To do that, it is estimated that by 2050, 5,000 to 10,000 metric tons of carbon dioxide need to be captured and stored each year, Edwards and Celia report.Nonetheless, implementation of carbon storage infrastructure has been hamstrung by a lack of policy support and large upfront costs for individual projects, Celia said.""In order for CCUS to have any chance for large-scale development, the necessary infrastructure needs to be built,"" Celia said. ""This includes pipelines to transport CO2 from the locations where it is captured to the locations where it can be used. Our study provides an economically feasible roadmap to build such a pipeline, which would be a critical first step for large-scale CCUS in the United States.""Similar large-scale infrastructures such as the Interstate Highway System and electric-power grids have been built with government financing and coordination, Edwards said.""Our analysis shows that additional public support beyond the tax credits will be necessary to enable large-scale deployment in the near-term,"" Edwards said. ""That's how these things are done -- governments play a key role. An excellent recent example is the leadership on clean-energy infrastructure by the Texas state government that enabled the big wind-energy boom there.""Edwards and Celia focused on ethanol refineries because the gas they produce is more than 99 percent carbon dioxide, which makes it among the most cost-effective to capture, Edwards said. By contrast, he said, the byproduct of burning coal is only 10-15 percent carbon dioxide and the capture technology is expensive. The cost of capturing carbon dioxide from coal or gas sources ranges from $50-75 per metric ton, compared to around $20-30 per metric ton for ethanol sources.At the same time, Midwestern ethanol refineries -- which produce 43 million metric tons of carbon dioxide annually -- are located far from existing carbon dioxide pipelines and don't sit atop geological formations suitable for storing the gas, Edwards and Celia wrote.Conventional recovery from an oil well yields about 40 percent of the total oil within the rock, Edwards said. Injecting carbon dioxide into the reservoir enables recovery of more oil, typically around an additional 15 percent, he said. The carbon dioxide is separated from the oil at the surface, then the gas is returned into the ground and ultimately trapped there.The network Edwards and Celia propose could increase enhanced oil recovery in the United States -- currently limited by a lack of affordable carbon dioxide -- by 50 percent, they wrote.""This can be a win-win for climate and domestic energy security,"" Edwards said.""Global oil demand is expected to continue past 2050, so there is a need for new oil production,"" he said. ""If there has to be more oil, it's much better environmentally that we produce oil domestically from an existing field rather than drilling new wells in places such as the Arctic, or importing it from new frontiers. Being able to also sequester captured carbon makes the overall technology extremely attractive.""The proposed network would still prevent far more carbon dioxide emissions than the oil it helps extract would produce, Celia said.""The scale of the carbon problem is so large that no single technology can solve it,"" Celia said. ""Fossil fuels are likely to be a significant energy source for the foreseeable future. CCUS is the only currently available technology that can solve the carbon problem while still allowing fossil fuels to be used. All current projections of a low-carbon energy future include significant amounts of CCUS.""Story Source:Materials provided by Princeton  University. Original written by Morgan Kelly. ",0.072833,0.127904,0.162848,0.166871,0.0988114,0.140337,0.149638,0,0
398,"Blocking sunlight to cool Earth won't reduce crop damage from global warming Solar geoengineering could reduce temperatures and heat stress, but also reduces photosynthesis","Injecting particles into the atmosphere to cool the planet and counter the warming effects of climate change would do nothing to offset the crop damage from rising global temperatures, according to a new analysis by University of California, Berkeley, researchers.By analyzing the past effects of Earth-cooling volcanic eruptions, and the response of crops to changes in sunlight, the team concluded that any improvements in yield from cooler temperatures would be negated by lower productivity due to reduced sunlight. The findings have important implications for our understanding of solar geoengineering, one proposed method for helping humanity manage the impacts of global warming.""Shading the planet keeps things cooler, which helps crops grow better. But plants also need sunlight to grow, so blocking sunlight can affect growth. For agriculture, the unintended impacts of solar geoengineering are equal in magnitude to the benefits,"" said lead author Jonathan Proctor, a UC Berkeley doctoral candidate in the Department of Agricultural and Resource Economics. ""It's a bit like performing an experimental surgery; the side-effects of treatment appear to be as bad as the illness.""""Unknown unknowns make everybody nervous when it comes to global policies, as they should,"" said Solomon Hsiang, co-lead author of the study and Chancellor's Associate Professor of Public Policy at UC Berkeley. ""The problem in figuring out the consequences of solar geoengineering is that we can't do a planetary-scale experiment without actually deploying the technology. The breakthrough here was realizing that we could learn something by studying the effects of giant volcanic eruptions that geoengineering tries to copy.""Hsiang is director of UC Berkeley's Global Policy Laboratory, where Proctor is a doctoral fellow.Proctor and Hsiang will publish their findings online in the journal Nature on August 8.Some people have pointed to past episodes of global cooling caused by gases emitted during massive volcanic eruptions, such as Mt. Pinatubo in the Philippines in 1991, and argued that humans could purposely inject sulfate aerosols into the upper atmosphere to artificially cool Earth and alleviate the greenhouse warming caused by increased levels of carbon dioxide. Aerosols -- in this case, minute droplets of sulfuric acid -- reflect a small percentage of sunlight back into space, reducing the temperature a few degrees.""It's like putting an umbrella over your head when you're hot,"" Proctor said. ""If you put a global sunshade up, it would slow warming.""Pinatubo, for example, injected about 20 million tons of sulfur dioxide into the atmosphere, reducing sunlight by about 2.5 percent and lowering the average global temperature by about half a degree Celsius (nearly 1 degree Fahrenheit).The team linked maize, soy, rice and wheat production from 105 countries from 1979-2009 to global satellite observations of these aerosols to study their effect on agriculture. Pairing these results with global climate models, the team calculated that the loss of sunlight from a sulfate-based geoengineering program would cancel its intended benefits of protecting crops from damaging extreme heat.""It's similar to using one credit card to pay off another credit card: at the end of the day, you end up where you started without having solved the problem,"" Hsiang said.Some earlier studies suggested that aerosols might improve crop yields also by scattering sunlight and allowing more of the sun's energy to reach interior leaves typically shaded by upper canopy leaves. This benefit of scattering appears to be weaker than previously thought.""We are the first to use actual experimental and observational evidence to get at the total impacts that sulfate-based geoengineering might have on yields,"" Proctor said. ""Before I started the study, I thought the net impact of changes in sunlight would be positive, so I was quite surprised by the finding that scattering light decreases yields.""Despite the study's conclusions, Proctor said, ""I don't think we should necessarily write off solar geoengineering. For agriculture, it might not work that well, but there are other sectors of the economy that could potentially benefit substantially.""Proctor and Hsiang noted that their methods could be used to investigate the impact of geoengineering on other segments of the economy, human health and the functioning of natural ecosystems.They did not address other types of geoengineering, such as capture and storage of carbon dioxide, or issues surrounding geoengineering, such as its impact on Earth's protective ozone layer and who gets to set Earth's thermostat.""Society needs to be objective about geoengineering technologies and develop a clear understanding of the potential benefits, costs and risks,"" Proctor said. ""At present, uncertainty about these factors dwarfs what we understand.""The authors emphasize the need for more research into the human and ecological consequences of geoengineering, both good and bad.""The most certain way to reduce damages to crops and, in turn, people's livelihood and well-being, is reducing carbon emissions,"" Proctor said.""Perhaps what is most important is that we have respect for the potential scale, power and risks of geoengineering technologies,"" Hsiang said. ""Sunlight powers everything on the planet, so we must understand the possible outcomes if we are going to try to manage it.""Other co-authors are Jennifer Burney of UC San Diego's School of Global Policy and Strategy, Marshall Burke of Stanford University and Wolfram Schlenker of Columbia University's School of International and Public Affairs and the Earth Institute. The research was supported by a National Science Foundation Grant (CNH-L 1715557) and a National Science Foundation Graduate Research Fellowship (DGE 1752814).Story Source:Materials provided by University of California - Berkeley. Original written by Robert Sanders. ",0.042444,0.0906672,0.124656,0.129268,0.071956,0.0900965,0.123843,0,0
399,How Chile accomplished its renewable energy boom,"Chile is currently undergoing a renewable energy boom. Today, it's the second largest market for renewable energies in Latin America, and in 2016 Chile was the top-scoring renewable energy producer in the Americas and second in the world, beaten only by China. Two decades ago, when this process started, this transformation was unthinkable.An article in Policy Studies Journal helps to explain the Chilean transformation by discussing the concept of ""contingent coalitions"" -- collective actors with conflicting but partly overlapping agendas and interests that may contingently coalesce to foster those interests and/or beliefs that they share.The article's authors -- Aldo Madariaga and Mathilde Allain -- show that the contingent coalitions crafted by environmental organizations in Chile have been crucial for fostering renewable energy policy at two moments where key innovations were introduced.""The official understanding of the Chilean renewable success story highlights the role of a few government entrepreneurs, but this has hidden the crucial role played by environmental organizations and social movements in pushing this process,"" said Prof. Madariaga, of the Centro de Investigacien y Docencia Econemicas, in Mexico, and the Universidad Mayor, in Chile. ""In this article, we unveil this story and show how they managed to increase their political clout and policy influence by forming what we call contingent coalitions.""Story Source:Materials provided by Wiley. ",0.180393,0.203145,0.230798,0.257191,0.210391,0.254185,0.234367,0,0
400,Reducing US coal emissions through biomass and carbon capture would boost employment,"While the need for solutions for the impending consequences of rising global temperatures has become increasingly urgent, many people have expressed concerns about the loss of jobs as current technologies like coal-fired power plants are phased out. A new study appearing November 1 in the journal Joule has run the numbers associated with the impacts of cutting coal plant jobs while at the same time employing techniques for bioenergy coupled with carbon capture and storage (BECCS). The model indicates that the BECCS approach would not only retain 40,000 jobs currently held as part of the coal industry but would create 22,000 new jobs in the forestry and transportation sectors by the middle of this century.""In the ambitious attempt to limit global warming below 2eC, BECCS features as the dominant technology, yet it's been under considerable scrutiny for its unknown effects on the environment and society,"" says first author Piera Patrizio, a research scholar in Ecosystems Services and Management at the International Institute for Applied Systems Analysis in Laxenburg, Austria. ""Our analysis shows that acting now and investing in this emission-mitigating strategy can be beneficial for employment in the U.S. coal sector.""BECCS is a proposed technology for reducing the release of greenhouse gases into the atmosphere. It combines carbon capture and storage, in which CO2 is collected from large emission sources such as power plants and injected into underground geologic formations, with the increased use of biomass, like plant-based materials, as a source of fuel. Proponents of BECCS predict that more than 99% of carbon dioxide stored through geologic sequestration is likely to stay in place for more than 1,000 years. They say this approach is necessary because the levels of CO2 that already have been released are too high to be absorbed by conventional carbon sinks like trees and soil alone.In the new study, the researchers studied in detail the major processes and steps involved in the potential energy supply chains for the U.S. coal fleet. Specifically, they looked at the supply of sustainable forest resources for biomass and the design and cost of infrastructure for transporting and injecting CO2 into appropriate geologic sites.""We also took into account the fact that biomass must be grown and harvested in a sustainable way in order to be considered carbon-neutral and thus obtain negative emissions,"" explains co-author Sabine Fuss, of the Mercator Research Institute on Global Commons and Climate Change in Berlin. ""This aspect is sometimes neglected in studies dealing with BECCS deployment.""The investigators used a number of different models to analyze existing data, including a biophysical model called the Global Forest Model; a techno-economic model called BeWhere, which optimizes the technology development of U.S. coal, including feedstock logistics; and the Jobs and Economic Development Impact model, which they used to estimate the employment impacts of technology development.There are some limitations to this type of analysis. For one thing, it does not consider the potential substitution of coal with other low-carbon options like renewables; thus, it reflects a limited picture of the future economy. For another, the models don't include any equilibrium in the economy, but are a bottom-up supply-chain optimization for specific technologies. Depending on these issues, outcomes of employing BECCS could differ with regard to job creation. Further analysis could therefore focus on emphasizing the socioeconomic effects of substituting renewable technologies for existing fossil-based capacity or on detecting larger, macroeconomic effects of low-carbon technology deployment, the researchers say.""To increase the acceptance of mitigation actions, policymakers should embrace strategies that offer other societal benefits, such as jobs,"" concludes co-author Kasparas Spokas from Princeton University. ""The U.S. represents a very interesting case to test our approach given the current economic, political, and environmental situation. This study shows how investing in climate change mitigation could actually ease the transition for coal workers, who would otherwise be confronted with abrupt job losses due to the retirement of the coal fleet by 2050.""This research was funded by the IIASA postdoc program, the Natural Environment Research Council of the UK, the Swedish Research Council Formas, and IIASA's Tropical Futures Initiative. It was also supported by the RESTORE+ project, which is part of the International Climate Initiative (IKI), supported by the Federal Ministry for the Environment; Nature Conservation and Nuclear Safety (BMU) based on a decision adopted by the German Bundestag.Story Source:Materials provided by Cell Press. ",0.0400801,0.0819104,0.122621,0.13115,0.0618295,0.105675,0.114681,0,0
401,Coal power plant regulations neglect a crucial pollutant Researchers determine Texans would be healthier with stronger limits on sulfur dioxide,"Cleaning up or replacing coal-fired power plants that lack sulfur pollution controls could help Texans breathe cleaner, healthier air, according to researchers at Rice University.A study led by environmental engineer Daniel Cohan analyzed models that measure the effects of emissions from 13 coal plants in Texas. Along with their conclusions on the modeling systems themselves, they found residents downwind of coal plants would be far better off today had the state focused on cutting particle-forming sulfur dioxide emissions in addition to those that cause ozone.""Texas has more unscrubbed coal plants than anywhere in the country and it's causing a substantial amount of air pollution damage and impacts on our health,"" Cohan said. ""What I found eye-opening in this study is that most of the health damage is coming from particulate matter, but most of Texas' focus on air pollution has been on ozone smog. It's a real dichotomy where the standard we violate is ground-level ozone, but the biggest cause of damage is particulate matter.""The study appears in the Journal of the Air & Waste Management Association.Three of the state's coal plants have closed in 2018, but the effects of all 13 are represented in data collected from 2012 to 2017 and used by Cohan and co-authors Brian Strasert, a Rice alumnus now at GSI Environmental in Houston, and undergraduate student Su Chen Teh.The researchers wrote that as coal power gets more expensive and natural gas and renewables like wind and solar energy advance, companies seem more likely to close plants based solely on economic considerations.They also point out the state missed an opportunity to accelerate the benefits that would have come with enforcement of the Obama-era regional haze plan. That would have cut emissions of sulfur dioxide, a contributor to airborne particulate matter -- invisible particles less than 2.5 microns in diameter -- at eight of the highest-emitting plants. Instead, in 2017, the Environmental Protection Agency replaced the plan with a cap-and-trade program.""That doesn't mean the plants will get worse,"" Cohan said. ""It just means the plants that should have been forced to clean up or close down have gotten a get-out-of-jail-free card.""The researchers noted that according to EPA statistics, Texas power plants emit more than twice as much sulfur dioxide as second-ranked Missouri. The problem, Cohan said, is that even when particulate levels meet current standards, they are still health hazards, especially to those directly downwind of the plants.""We know the higher the particulate matter levels, the more people die, both on a short-term correlation -- when particulate matter levels are high one day, death rates are higher over the next few days -- and also over long-term studies,"" he said. ""When epidemiologists study people across 10 or 20 years, they find that life expectancies are better in places that have very low levels of particulate matter than places that have high levels.""Particulate matter is the deadliest of all air pollutants, and it's not just causing deaths in the way that you might think,"" Cohan said. ""It's not only by respiratory diseases, but it's also causing increases in rates of heart attacks and strokes. These particles are small enough to pass through the alveoli and enter the bloodstream. That lets them cause damage on all aspects of our bodily systems.""That was really striking to me,"" he said. ""Because Texas meets the particulate matter standard, it hasn't prioritized cleaning up sulfur anywhere near the extent that it's cleaning up nitrogen pollution. We're allowing plants to emit levels of pollution that haven't been allowed since the 1970s. They're still operating today, because the state and EPA have failed since 2009 to finalize a regional haze plan.""Cohan said the EPA continues to take comments on its plan for Texas.The paper also showed that recent, simple atmospheric models that help researchers quickly compute the health effects from pollution compare favorably with a more complicated state-of-the-art model. ""That suggests we can take easier approaches to more quickly estimate the impacts of these plants,"" Cohan said.In time, he said, coal plants seem likely to close either because air pollution standards will be enforced across the board or because renewables like wind and solar make them unprofitable.""The key message of our paper,"" Cohan said, ""is that delay has very real costs for us in Texas.""Story Source:Materials provided by Rice University. ",0.0809747,0.115959,0.173977,0.150096,0.108131,0.140001,0.151299,0,0
402,Eliminating emissions in India and China could add years to people's lives New research calculates changes in mortality and life expectancy due to power generation,"The 2.7 billion people who live in China and India -- more than a third of the world's population -- regularly breath some of the dirtiest air on the planet. Air pollution is one of the largest contributors to death in both countries, ranked 4th in China and 5th in India, and harmful emissions from coal-fire powerplants are a major contributing factor.In a recent study, researchers from Harvard University wanted to know how replacing coal-fired powerplants in China and India with clean, renewable energy could benefit human health and save lives in the future.The researchers found that eliminating harmful emissions from powerplants could save an estimated annual 15 million years of life in China and 11 million years of life in India.The research was published in the journal Environment International.Previous research has explored mortality from exposure to fine particulate matter (known as PM2.5) in India and China but few studies have quantified the impact of specific sources and regions of pollution and identified efficient mitigation strategies.Using state-of-the-art atmospheric chemistry modeling, the researchers calculated province-specific annual changes in mortality and life expectancy due to power generation. Using the province-specific approach, the researchers were able to narrow down the areas of highest priority, recommending upgrades to the existing power generating technologies in Shandong, Henan, and Sichuan provinces in China, and Uttar Pradesh state in India due to their dominant contributions to the current health risks.""This study shows how modeling advances and expanding monitoring networks are strengthening the scientific basis for setting environmental priorities to protect the health of ordinary Chinese and Indian citizens,"" said Chris Nielsen, executive director of the Harvard-China Project and a co-author of the paper. ""It also drives home just how much middle-income countries could benefit by transitioning to non-fossil electricity sources as they grow.""Story Source:Materials provided by Harvard John A. Paulson School of Engineering and Applied Sciences. ",0.0946554,0.13622,0.174143,0.175071,0.108026,0.191504,0.160771,0,0
403,Applying auto industry's fuel-efficiency standards to agriculture could net billions,"Adopting benchmarks similar to the fuel-efficiency standards used by the auto industry in the production of fertilizer could yield $5-8 billion in economic benefits for the U.S. corn sector alone, researchers have concluded in a new analysis.The work, appearing in the journal Nature Sustainability and authored by New York University's David Kanter and Princeton University's Tim Searchinger, examines the potential impact of a policy to reduce nitrogen emissions in fertilizer -- one modeled on the Corporate Average Fuel Economy (CAFE) standards that are used to increase the fuel efficiency of vehicles in the U.S.""A CAFE-style approach to reducing nitrogen pollution could provide powerful incentives for fertilizer manufacturers to learn where and how enhanced-efficiency fertilizers work best, and ultimately to develop more technically sophisticated nitrogen products tailored to specific crops, climates, and soil conditions,"" they write.Nitrogen pollution represents a significant environmental concern, scientists have concluded, and comes mainly from the inefficient use of fertilizer and manure on farms. Others have found policies to address this pollution source to be largely ineffective, largely because of the challenges in both changing farming practices and in monitoring and enforcement.""Moreover, the farm lobby is an extremely powerful political force in many countries,"" observe Kanter, a professor in NYU's Department of Environmental Studies, and Searchinger, a research scholar at Princeton's Woodrow Wilson School of Public and International Affairs. ""Consequently, new policy options for addressing this environmental issue need to be explored.""In their analysis, the researchers turned to U.S. fuel efficiency standards, which focus on the car industry instead of consumers, and evaluated whether the fertilizer industry could be a newfound regulatory focus of nitrogen policies.Specifically, they evaluated the potential impact of a requirement to steadily increase the proportion of enhanced efficiency fertilizer sold with traditional fertilizer -- with the implicit aim of incentivizing technology development in this industry. India implemented such requirements in 2015.As with cars, the price of enhanced-efficiency fertilizers (EEF) could be more costly to growers; however, they could also potentially bolster profits because lower amounts are needed to grow crops -- in the same way fuel-efficient cars require less gasoline. EEFs, already produced by several major fertilizer companies, have been shown to reduce nitrogen losses and improve yields -- yet EEFs are currently used on only about 12 percent U.S. corn cropland.In their analysis, the researchers adopted a case study -- the U.S. corn industry, which devotes approximately 33 million hectares of American cropland to corn production and has the highest nitrogen application rate of any major crop in the U.S.To estimate the impact of wider usage of EEFs, they examined, over a 10-year period and using different scenarios, how a CAFE-style standard mandating that EEFs compose higher percentages of nitrogen fertilizer sales -- e.g., 20 percent of sales by 2020 and 30 percent by 2030 -- would affect incomes from higher yields and increased fertilizer costs.Their results showed that higher efficiency standards, depending on the standard set, could produce net economic benefits of $5-$8 billion per year by 2030. These benefits include both a combination of farmer and industry profits as well as environmental and human health gains from avoided nitrogen pollution, the researchers explain.Specifically, farm profits are due to slight boosts in yield, which offset the increased cost of EEF use, while industry profits arise from increased sales of EEFs, which have a higher profit margin.The researchers add that the impact of such standards for fertilizer could be felt more immediately than that for cars -- CAFE requirements apply only to newly sold cars, which have an average fleet turnover of nearly 16 years, while fertilizer is bought annually, so improved products will have an instantaneous effect.""A state could pioneer such an approach -- possibly California, which has already adopted ambitious climate goals across all sectors,"" Kanter and Searchinger propose. ""Although the heterogeneity of agricultural, climatic, and political systems across the world requires a range of policy approaches to address the nitrogen pollution issue, industry-focused technology-forcing policies could be a promising option for reducing nitrogen losses, even as we push our planet to produce far more food.""Story Source:Materials provided by New York University. ",0.0757971,0.103964,0.161824,0.169214,0.0980553,0.16519,0.142455,0,0
404,New catalyst opens door to CO2 capture in conversion of coal to liquid fuels,"World energy consumption projections expect coal to stay one of the world's main energy sources in the coming decades, and a growing share of it will be used in CTL, the conversion of coal to liquid fuels. Researchers from the National Institute of Clean-and-Low-Carbon Energy in Beijing and Eindhoven University of Technology have developed iron-based catalysts that substantially reduce operating costs and open the door to capturing the large amounts of CO2 that are generated by CTL. Their results are published in the journal Science Advances.To understand the significance of this achievement, some knowledge of the CTL process is required. The first stage is the conversion of coal to syngas, a mixture of carbon monoxide (CO) and hydrogen (H2). Using the so-called Fischer-Tropsch process, these components are converted to liquid fuels. But before that can be done, the composition of the syngas has to be changed to make sure the right products come out in the end -- liquid fuels. So some of the CO is taken out of the syngas (rejected) by converting it to CO2, in a process called 'water-gas shift'.In this chain the researchers tackled a key problem in the Fischer-Tropsch reactor. As in most chemical processing, catalysts are required to enable the reactions. CTL catalysts are mainly iron based. Unfortunately, they convert some 30 percent of the CO to unwanted CO2, a byproduct that in this stage is hard to capture and thereby often released in large volumes, consuming a lot of energy without benefit.The Beijing and Eindhoven researchers discovered that the CO2 release is caused by the fact that the iron based catalysts are not pure, but consist of several components. They were able to produce a pure form of a specific iron carbide, called epsilon iron carbide, that has a very low CO2 selectivity. In other words, it generates almost no CO2 at all. The existence was already known but until now it had not been stable enough for the harsh Fischer-Tropsch process. The Sino-Dutch research team has now shown that this instability is caused by impurities in the catalyst. The phase-pure epsilon iron carbide they developed is, by contrast, stable and remains functional, even under typical industrial processing conditions of 23 bar and 250oC.The new catalyst eliminates nearly all CO2 generation in the Fischer-Tropsch reactor. This can reduce the energy needed and the operating costs by roughly 25 million euros per year for a typical CTL plant. The CO2 that was previously released in this stage can now be removed in the preceding water-gas shift stage. That is good news, because it is much easier to capture in this stage. The technology to make this happen is called CCUS (carbon capture, utilization and storage). It has been developed by other parties and is already being applied in several pilot plants.The conversion of coal to liquid fuels is especially relevant in coal-rich countries that have to import oil for their supply of liquid fuels, such as China and the US. ""We are aware that our new technology facilitates the use of coal-derived fossil fuels. However, it is very likely that coal-rich countries will keep on exploiting their coal reserves in the decades ahead. We want to help them do this in the most sustainable way,"" says lead researcher professor Emiel Hensen of Eindhoven University of Technology.The research results are likely to reduce the efforts to develop CTL catalysts based on cobalt. Cobalt based catalysts do not have the CO2 problem, but they are expensive and quickly becoming a scarce resource due to cobalt use in batteries, which account for half of the total cobalt consumption.Hensen expects that the newly developed catalysts will also play an import role in the future energy and basic chemicals industry. The feedstock will not be coal or gas, but waste and biomass. Syngas will continue to be the central element, as it is also the intermediate product in the conversion of these new feedstocks.Synthesis of stable and low-CO2 selective epsilon-iron carbide Fischer-Tropsch catalysts Peng Wang 1,2, Wei Chen2, Fu-Kuo Chiang1, A. Iulian Dugulan3, Yuanjun Song4, Robert Pestman2, Kui Zhang1, Jinsong Yao1, Bo Feng1, Ping Miao1, Wayne Xu1, Emiel J. M. Hensen2 1National Institute of Clean-and-Low-Carbon Energy, Beijing 2Laboratory of Inorganic Materials Chemistry, Eindhoven University of Technology 3Fundamental aspects of Materials and Energy Group, Delft University of Technology 4Beijing Key Laboratory for Magneto-Photoelectrical Composite and Interface Science, University of Science and Technology Beijing DOI: 10.1126/sciadv.aau2947Story Source:Materials provided by Eindhoven University of Technology. ",0.0647435,0.157076,0.114557,0.151197,0.0971714,0.126157,0.147074,0,0
405,Now is the time to answer questions about climate engineering disease impacts,"Radical solutions to climate change might save lives, but a commentary in the October 2018 issue of the journal Nature Climate Change calls for caution because geoengineering still lacks a ""clean bill of health.""With global fossil-fuel emissions reaching an all-time high and the United States' withdrawal from the Paris Agreement, climate experts have become increasingly interested in ""climate engineering,"" a set of ambitious and largely undeveloped technologies that could artificially counteract global warming. One proposed approach, called solar radiation management (SRM), would reduce incoming sunlight by injecting tiny aerosol particles into the stratosphere or by brightening clouds. Other approaches would directly remove carbon dioxide from the atmosphere.Even if some combination of these worked, scientists warn that the climate wouldn't be the same as it was before climate change. And those differences might make a big difference for global health, ecologists Colin Carlson and Christopher Trisos argue in the Nature Climate Change article. The article was written while both were postdoctoral fellows at the National Socio-Environmental Synthesis Center (SESYNC), a unique University of Maryland center funded by the National Science Foundation that brings together science of the natural world with science of human behavior and decision-making.So far, Carlson and Trisos say, almost nothing is known about the potential health consequences of such geoengineered ""solutions.""""We're a step before saying these technologies will probably save lives or saying they're too dangerous to use,"" says Carlson. ""Right now, what we know is climate and disease are already closely linked, and that raises basic questions about climate engineering. Now, we need answers.""Carlson gives the example of malaria, a disease mostly confined to the tropics today, but was once widespread in Europe and North America. Recently, scientists found that malaria transmits best at cooler temperatures. In some projections, SRM would disproportionately cool off the tropics -- and that might make malaria worse.""But it's all guesswork -- we can qualitatively talk through possible risks, and that's what we do here. But we can't make any judgements without solid, quantitative evidence. And no one's run those models yet. There's no data to go off.""Carlson and Trisos hope to shed some light on these issues over the next two years. They are part of an international, interdisciplinary team that has been recommended for a $50,000 grant from the DECIMALS Fund (Developing Country Impact Modelling Analysis for SRM), which was launched by the Solar Radiation Management Governance Initiative to help scientists understand how SRM could affect the ""global south"" -- a term that refers to less developed countries. Eight projects will receive DECIMALS grants that will be announced in October. The fund is administered by The World Academy of Sciences.""Links between climate change and health are often complex, so climate engineering may impact health in unexpected ways,"" says Trisos, now a research affiliate at the African Climate and Development Initiative. ""Governments have pledged to prevent 'dangerous anthropogenic interference' with the climate system, so it's critical that we can compare public health risks from climate change to those from climate engineering, in order to decide if climate engineering should even be considered.""Carlson and Trisos' DECIMALS research proposal was put together in collaboration with lead researchers Shafiul Alam and Mofizur Rahman (International Centre for Diarrhoeal Disease Research in Bangladesh) and includes epidemiologist Shweta Bansal (Georgetown University), climatologist Alan Robock (Rutgers University), and microbiologist and cholera expert Rita Colwell (University of Maryland, formerly the ninth director of the National Science Foundation).Their team is designed to produce important results on a fast deadline.""Climate scientists, ecologists and public health researchers are increasingly working together to understand what climate change means for infectious diseases,"" says Trisos. ""We're lucky to take advantage of that partnership to tackle a problem this complicated -- and this urgent.""In a perfect world, understanding the possible health impacts of climate engineering might help policymakers make the right call, if other options seem limited. But Carlson says there's another reason this work is important.""From a public health standpoint, we're not likely to be the ones making the call about climate futures. But we want to know what's coming if climate engineering does happen, and we want to be prepared, first in places like Bangladesh that might have the most to gain but also have the most to lose.""Bangladesh is the world's hot spot for cholera and has led the global research program to prevent the disease for several decades, with medical care reducing fatalities from 50 percent to less than 5 percent. Climate change will only increase the pressure that countries like Bangladesh face from infectious diseases like cholera and malaria.""Whether or not the climate engineering 'button' gets pushed, the research we do here still helps us,"" Carlson explains. ""We're building our toolbox and getting better at predicting cholera and malaria, and that should save lives, whatever climate change looks like.""Story Source:Materials provided by University of Maryland. Original written by Lisa Palmer. ",0.0722042,0.142237,0.156625,0.172647,0.104764,0.136318,0.154867,0,0
406,"Fewer biofuels, more green space: Climate action researcher calls for urgent shift ","Growing and harvesting bioenergy crops -- corn for ethanol or trees to fuel power plants, for example -- is a poor use of land, which is a precious resource in the fight against climate change, says a University of Michigan researcher.Untampered green areas like forests and grasslands naturally sequester carbon dioxide, and they are one of society's best hopes for quickly reducing the greenhouse gas in the atmosphere, says John DeCicco, research professor at the U-M Energy Institute.DeCicco and William Schlesinger, president emeritus of the Cary Institute of Ecosystem Studies have authored an opinion piece in the current edition of Proceedings of the National Academy of Sciences.The researchers call for policymakers, funding agencies, fellow academics and industry leaders to urgently shift their focus from bioenergy to what they call ""terrestrial carbon management,"" or TCM. That strategy emphasizes planting more trees and conserving more wild areas that feed on carbon dioxide.""The world needs to rethink its priorities about how to use the biosphere given the urgency of the climate problem and the risks to biodiversity,"" DeCicco said.The biosphere encompasses all life on Earth, and for climate protection, it particularly refers to trees, plants and the living carbon -- microorganisms -- in soils.""Current policies advancing bioenergy contribute to the pressure to convert natural land into harvested forest or cropland,"" DeCicco said. ""But high quality land is a limited resource. For reducing atmospheric CO2, the most efficient use of ecologically productive land is to leave it alone, or reforest it. Let it act as a natural, long-term carbon sink.""The new opinion piece expands on DeCicco's earlier findings that biofuels are not inherently carbon-neutral, as they are widely purported to be, and Schlesinger's long-time research as a leading ecologist and biogeochemist.The assumption that bioenergy simply recycles carbon -- which DeCicco and Schlesinger call a major accounting error -- is built into the lifecycle assessments used for energy policy as well as the protocols for international carbon accounting. And it has fostered major R&D investments in biofuels, which, in turn, have been assigned a key role in many climate stabilization scenarios.The core of that assumption is the idea that producing a biofuel and then burning it for energy moves a given amount of carbon from the biosphere to the atmosphere, and back again in an unending and stable cycle. That's in contrast to the current one-way flow of fossil-fuel carbon from the Earth to the atmosphere.But here's where DeCicco sees a problem: For bioenergy to be actually carbon neutral, harvesting the biomass to produce it would have to greatly speed up the net flow of carbon from the atmosphere back into vegetation. Otherwise, many decades can pass before the ""carbon debt"" of excess carbon dioxide in the air is repaid by future plant growth.""All currently commercial forms of bioenergy require land and risk carbon debts that last decades into the future. Given the urgency of the climate problem, it is puzzling why some parties find these excess near-term CO2 emissions acceptable,"" the researchers write.In 2016, DeCicco published a study finding that just 37 percent -- rather than 100 percent -- of the carbon dioxide released from burning biofuels was balanced out by increased carbon uptake in crops over the first eight years of the U.S. biofuel mandate.To reduce the concentration of carbon dioxide in the atmosphere, DeCicco and Schlesinger point out, requires increasing the rate at which trees and other plants remove it from the air. Although they don't rule out possible breakthroughs in algae or other futuristic bioenergy options, they say that for now the best biologically based carbon dioxide reduction strategy is to protect and restore carbon-rich natural ecosystems.""By avoiding deforestation and by reforesting harvested areas, up to one-third of current carbon dioxide emissions from fossil fuels could be sequestered in the biosphere,"" the researchers write. ""Terrestrial carbon management can keep carbon out of the atmosphere for many decades.""Story Source:Materials provided by University of Michigan. ",0.071381,0.144394,0.195414,0.155441,0.0900693,0.151861,0.157507,0,0
407,"Cash, carbon, crude: How to make oil fields bury emissions Path and price to coax emissions back into oil reservoirs","In February 2018, Donald Trump signed into law new tax credits that reward oil companies for capturing carbon dioxide and preventing it from entering the atmosphere -- either by burying the gas underground or by pumping it into wells to boost production. These tax credits, which have bipartisan support, are encouraging for those who believe that trapping CO2 from the fossil fuel industry -- though no substitute for deploying cleaner energy sources -- could help combat runaway climate change while society remains reliant on oil, gas and coal.Now, a Stanford University analysis published Aug. 15 in the journal Joule suggests another way the government could encourage the oil and gas industry to capture and store carbon. The article proposes a model for how relatively small government payments could pave the way for oil reservoirs to stash away more CO2 than their burned contents unleash.""If you look at air transport, shipping, heavy-duty land-based transportation, these are uses of fossil fuels that are definitely expected to grow,"" said lead author Sally Benson, a professor of energy resources engineering at the School of Earth, Energy & Environmental Sciences (Stanford Earth). ""As an insurance policy, getting everybody to contribute to solving this problem is really important, including the oil and gas industry.""Carbon in, oil outWhen injected into reservoirs, carbon dioxide can help drive oil and profits from aging wells. The technique, known as carbon dioxide-enhanced oil recovery, has been in use since the 1970s. Oil companies using it today pump in about two-and-a-half tons of carbon for every barrel of oil produced. ""When you do that, the emissions from burning the oil are almost identical to the CO2 you're putting in the reservoir,"" said Benson, who co-directs Stanford's Precourt Institute for Energy.The problem is that most of the 65 million metric tons of CO2 used in these oil recovery projects each year comes from natural reservoirs -- not refineries, power plants or other sources contributing to climate change.In their analysis, Benson and co-author John Deutch, a professor emeritus at MIT, propose a cost-effective way of encouraging oil and gas companies to double the amount of carbon injected for every barrel of oil and to draw their CO2 from human-related sources. They say a 10-fold increase in the amount drawn from these sources could shrink the nation's climate emissions by as much as 9.5 percent -- even when accounting for the additional oil extraction made possible by injecting all that carbon.All of this could be done at surprisingly low cost, the researchers claim, if companies start out trapping CO2 from relatively pure streams, like those vented from ethanol and fertilizer plants. According to Benson, experience gained on these projects could then help to drive down the cost of capturing and treating CO2 from mixed emission sources, such as cement and power plants.Lighting a fireBenson and Deutch, who is a former head of the Central Intelligence Agency and deputy defense secretary, argue that government should encourage industry to prepare for a future with an economy-wide price on carbon emissions -- in part by developing better carbon storage technology.Doubling the amount of CO2 per barrel of oil compared to the standard practice today, as Benson and Deutch envision, would almost certainly be more expensive for operators. The new analysis suggests it would add at least $22 per ton of injected carbon in a hypothetical scenario where oil costs $100 a barrel.""That's a lot of cost for an oil company,"" Benson said. ""If one company does it and they don't all do it, then your products are just more expensive than the competition. People just buy the cheapest crude oil they can.""To overcome those costs and spur development of technology for carbon-negative oil recovery, the researchers suggest the government pilot test something called a reverse Dutch auction. Owners of new oil recovery projects would submit bids to the government specifying how much money they would want as a reward for carbon injection and how much CO2 they expect the project to ultimately sequester.With this system, Benson and Deutch estimate it would cost the government about $25 per ton of carbon dioxide captured. If 30 projects earned that amount for a decade apiece, the program would trap 264 million tons of CO2, and government spending on the experiment would total $6.6 billion.For comparison, the newly expanded tax credits offer $35 per ton of CO2 captured and put to use, or $50 per ton if the carbon is simply buried. According to Benson, her proposed pilot program could cost less and provide incentives for a wider variety of companies that may not benefit from a tax credit.Story Source:Materials provided by Stanford's School of Earth, Energy & Environmental Sciences. ",0.0867487,0.135619,0.148648,0.202169,0.107576,0.166975,0.173245,0,0
408,Widely used nonprofit efficiency tool doesn't work,"A recent study from North Carolina State University finds that the tool most often used to assess the efficiency of nonprofit organizations isn't just inaccurate -- it can actually be negatively correlated with efficiency.At issue is something called the overhead ratio, which is the amount of money a nonprofit organization spends on overhead -- such as infrastructure, executive compensation and day-to-day management -- relative to overall spending. Overhead spending does not include funding spent on implementing programs, such as program staff salaries.The overhead ratio is important because it is used by many nonprofit rating sites, donors and nonprofit scholars to assess a nonprofit's efficiency at using its resources to accomplish its organizational goals.""But the overhead ratio doesn't actually measure efficiency, for two reasons,"" says Jason Coupet, an assistant professor of public administration at NC State and lead author of a paper on the study. ""First, the overhead ratio doesn't account for what organizations are actually doing with their resources. And second, the ratio doesn't account for what organizations are accomplishing with their non-overhead spending.""Some nonprofit researchers have raised concerns about the accuracy of overhead ratios as a means of assessing efficiency, but our work is the first to approach it using efficiency theory -- and we were able to demonstrate the problem using real-world data,"" Coupet says.For this study, the researchers collected data on the overhead, programmatic and fundraising expenditures of 666 Habitat for Humanity affiliates around the country, as well as how many houses they were able to offer low-income home buyers.To better understand the value of overhead ratio as a means of measuring efficiency, the researchers turned to two well-established tools for assessing efficiency: Data Envelopment Analysis (DEA) and Stochastic Frontier Analysis (SFA). The tools rely on different techniques, but have been proven repeatedly to provide valid and reliable assessments of efficiency across multiple types of organizations.The researchers used the Habitat for Humanity data to calculate the efficiency of each affiliate using DEA and SFA, as well as calculating each affiliate's overhead ratio.The researchers found that DEA and SFA efficiency scores were highly correlated. In other words, if one tool found that an affiliate was efficient, the other tool usually did too.However, the overhead ratio was negatively correlated with both tools.""In short, this demonstrates that not only is the overhead ratio bad at assessing efficiency, but also that using it to assess efficiency may actively mislead donors,"" Coupet says.""We're hoping to work with the nonprofit sector to adopt more accurate efficiency assessment tools,"" Coupet says. ""That may be tricky, since many nonprofits have organizational goals that are difficult to quantify, but we're optimistic that better tools can offer richer insights for many organizations. Ultimately, we want to help nonprofit organizations get the most out of their resources, and to identify those nonprofits that are actually operating efficiently.""If nothing else, we hope people stop using overhead ratios as proxies for efficiency.""Story Source:Materials provided by North Carolina State University. ",0.135407,0.143956,0.193031,0.310661,0.160774,0.213354,0.226849,0,0
409,Barriers and opportunities in renewable biofuels production,"Researchers at Chalmers University of Technology, Sweden, have identified two main challenges for renewable biofuel production from cheap sources. Firstly, lowering the cost of developing microbial cell factories, and secondly, establishing more efficient methods for hydrolysis of biomass to sugars for fermentation. Their study was recently published in the journal Nature Energy.The study, by Professor Jens Nielsen, Yongjin Zhou and Eduard Kerkhoven, from the Division of Systems and Synthetic Biology, evaluates the barriers that need to be overcome to make biomass-derived hydrocarbons a real alternative to fossil fuels.""Our study is of particular interest for decision makers and research funders, as it highlights recent advances and the potential in the field of biofuels. It also identifies where more research is required. This can help to prioritise what research should be funded,"" says Eduard Kerkhoven.It is technically already possible to produce biofuels from renewable resources by using microbes such as yeast and bacteria as tiny cell factories. However, in order to compete with fossil-derived fuels, the process has to become much more efficient. But improving the efficiency of the microbial cell factories is an expensive and time-consuming process, so speeding-up the cell factory development is therefore one of the main barriers.Professor Jens Nielsen and his research group are world leaders in the engineering of yeast, and in the development and application of computer models of yeast metabolism -- as well as being noted for their world-class research into human metabolism, and investigations into aging processes and diseases. Their work informs how yeast can best be engineered to manufacture new chemicals or biofuels. In their article ""Barriers and opportunities in bio-based production of hydrocarbons,"" the researchers investigate the production of various biofuels using a model of yeast metabolism.""We have calculated theoretical maximum production yields and compared this to what is currently achievable in the lab. There is still huge potential for improving the process,"" says Eduard Kerkhoven.The other main barrier is efficient conversion from biomass, such as plants and trees, to the sugars that are used by the cell factories. If this conversion were made more efficient, it would be possible to use waste material from the forest industry, or crops that are purposely grown for biofuels, to produce a fully renewable biofuel. Eduard Kerkhoven notes how important biofuels will be for the future.""In the future, whilst passenger cars will be primarily electric, biofuels are going to be critical for heavier modes of transport such as jets and trucks. The International Energy Agency projects that by 2050, 27 percent of global transport fuels will be biofuels. Meanwhile, large oil companies such as Preem and Total also predict that renewable biofuels will play an important role in the future. In their 'Sky Scenario', Shell expects that biofuels will account for 10 percent of all global end energy-use by the end of the century. That is in line with our research too,"" he concludes.Story Source:Materials provided by Chalmers University of Technology. ",0.0696603,0.127093,0.133681,0.171852,0.0986163,0.141639,0.168649,0,0
410,"Cover the U.S. In 89 percent trees, or go solar ","How many fields of switchgrass and forests of trees would be needed to offset the energy produced by burning coal? A lot, it turns out.While demand for energy isn't dropping, alarms raised by burning fossil fuels in order to get that energy are getting louder. Solutions to cancel the effects of carbon dumped into our atmosphere include carbon capture and storage or bio-sequestration. This zero-emission energy uses technical means as well as plants to take in and store carbon emissions. Another route is to use solar photovoltaics to convert sunlight directly into electricity and only sequester the carbon emissions from the production of solar cells.The Giving Tree Won't Give Enough for Carbon Neutral CoalZero-emission energy has been offered as a way to offset the carbon dioxide production while still maintaining coal's electricity generation. That's done through carbon capture and storage in saline aquifers, or by using both enhanced oil recovery and bio-sequestration through planting trees and other plants to suck up and store carbon.In a new study published in Scientific Reports, a Nature publication, Michigan Technological University researchers analyzed how much land would be required to offset greenhouse gases created by traditional coal-fired plants or coal-fired plants with carbon sequestration and then neutralizing the remaining carbon pollution with bio-sequestration. Then they compared these routes to how much bio-sequestration would be required to offset greenhouse gases produced when making solar panels.For the first time, researchers have shown that there is no comparison. It's not even close.In fact, coal-fired power plants require 13 times more land to be carbon neutral than the manufacturing of solar panels. We'd have to use a minimum of 62 percent of U.S. land covered by optimal crops or cover 89 percent of the U.S. with average forests to do it.""We know that climate change is a reality, but we don't want to live like cavemen,"" says Joshua Pearce, professor of materials science and engineering and electrical engineering at Michigan Tech. ""We need a method to make carbon neutral electricity. It just makes no sense whatsoever to use coal when you have solar available, especially with this data.""Too Big to Solve: Coal-Fired Power Plant EmissionsResearchers drew these conclusions from over 100 different data sources to compare energy, greenhouse gas emissions and land transformation needed to carbon neutralize each type of energy technology.They claim a one-gigawatt coal-fired plant would require a new forest larger than the state of Maryland to neutralize all of its carbon emissions.Researchers also found that applying the best-case bio-sequestration for all the greenhouse gases produced by coal-fired power plants, would mean using 62 percent of the nation's arable land for that process, or 89 percent of all U.S. land with average forest cover.In comparison, solar cells require 13 times less land to become carbon neutral and five times less than the best-case coal scenario.""If your goal is to make electricity without introducing any carbon into the atmosphere, you should absolutely not do a coal plant,"" Pearce says. Not only is it not realistic to capture all the carbon dioxide they release, but burning coal also puts sulfur dioxide and nitrous oxide and particulates in the air, which creates air pollution, already estimated to cause 52,000 premature deaths annually.Solar is BetterPearce says that he and his team were generous to coal-fired power plants in how they calculated the efficiency of carbon capture and storage when scaled up. They also did not consider new ways that solar farms are being used to make them even more efficient, like using higher efficiency black silicon solar cells, putting mirrors in between rows of panels so light falling between them can also be absorbed, or planting crops between rows (agrivoltaics) to achieve greater land use.Pearce says future research should focus on improving the efficiency of solar panels and solar farms, not on carbon capture of fossil fuel-powered plants in an attempt to become zero-emission energy-not when this data shows it isn't realistic in order to protect our changing climate.Story Source:Materials provided by Michigan Technological University. Original written by Jen A. Miller. ",0.0861506,0.167806,0.165542,0.159373,0.106809,0.173928,0.174597,0,0
411,"Adding power choices reduces cost and risk of carbon-free electricity A mix of fuel-saving, flexible, and highly reliable sources works best","In major legislation passed at the end of August, California committed to creating a 100 percent carbon-free electricity grid -- once again leading other nations, states, and cities in setting aggressive policies for slashing greenhouse gas emissions. Now, a study by MIT researchers provides guidelines for cost-effective and reliable ways to build such a zero-carbon electricity system.The best way to tackle emissions from electricity, the study finds, is to use the most inclusive mix of low-carbon electricity sources.Costs have declined rapidly for wind power, solar power, and energy storage batteries in recent years, leading some researchers, politicians, and advocates to suggest that these sources alone can power a carbon-free grid. But the new study finds that across a wide range of scenarios and locations, pairing these sources with steady carbon-free resources that can be counted on to meet demand in all seasons and over long periods -- such as nuclear, geothermal, bioenergy, and natural gas with carbon capture -- is a less costly and lower-risk route to a carbon-free grid.The new findings are described in a paper published today in the journal Joule, by MIT doctoral student Nestor Sepulveda, Jesse Jenkins PhD '18, Fernando de Sisternes PhD '14, and professor of nuclear science and engineering and Associate Provost Richard Lester.The need for cost effectiveness""In this paper, we're looking for robust strategies to get us to a zero-carbon electricity supply, which is the linchpin in overall efforts to mitigate climate change risk across the economy,"" Jenkins says. To achieve that, ""we need not only to get to zero emissions in the electricity sector, but we also have to do so at a low enough cost that electricity is an attractive substitute for oil, natural gas, and coal in the transportation, heat, and industrial sectors, where decarbonization is typically even more challenging than in electricity. ""Sepulveda also emphasizes the importance of cost-effective paths to carbon-free electricity, adding that in today's world, ""we have so many problems, and climate change is a very complex and important one, but not the only one. So every extra dollar we spend addressing climate change is also another dollar we can't use to tackle other pressing societal problems, such as eliminating poverty or disease."" Thus, it's important for research not only to identify technically achievable options to decarbonize electricity, but also to find ways to achieve carbon reductions at the most reasonable possible cost.To evaluate the costs of different strategies for deep decarbonization of electricity generation, the team looked at nearly 1,000 different scenarios involving different assumptions about the availability and cost of low-carbon technologies, geographical variations in the availability of renewable resources, and different policies on their use.Regarding the policies, the team compared two different approaches. The ""restrictive"" approach permitted only the use of solar and wind generation plus battery storage, augmented by measures to reduce and shift the timing of demand for electricity, as well as long-distance transmission lines to help smooth out local and regional variations. The ""inclusive"" approach used all of those technologies but also permitted the option of using continual carbon-free sources, such as nuclear power, bioenergy, and natural gas with a system for capturing and storing carbon emissions. Under every case the team studied, the broader mix of sources was found to be more affordable.The cost savings of the more inclusive approach relative to the more restricted case were substantial. Including continual, or ""firm,"" low-carbon resources in a zero-carbon resource mix lowered costs anywhere from 10 percent to as much as 62 percent, across the many scenarios analyzed. That's important to know, the authors stress, because in many cases existing and proposed regulations and economic incentives favor, or even mandate, a more restricted range of energy resources.""The results of this research challenge what has become conventional wisdom on both sides of the climate change debate,"" Lester says. ""Contrary to fears that effective climate mitigation efforts will be cripplingly expensive, our work shows that even deep decarbonization of the electric power sector is achievable at relatively modest additional cost. But contrary to beliefs that carbon-free electricity can be generated easily and cheaply with wind, solar energy, and storage batteries alone, our analysis makes clear that the societal cost of achieving deep decarbonization that way will likely be far more expensive than is necessary.""A new taxonomy for electricity sourcesIn looking at options for new power generation in different scenarios, the team found that the traditional way of describing different types of power sources in the electrical industry -- ""baseload,"" ""load following,"" and ""peaking"" resources -- is outdated and no longer useful, given the way new resources are being used.Rather, they suggest, it's more appropriate to think of power sources in three new categories: ""fuel-saving"" resources, which include solar, wind and run-of-the-river (that is, without dams) hydropower; ""fast-burst"" resources, providing rapid but short-duration responses to fluctuations in electricity demand and supply, including battery storage and technologies and pricing strategies to enhance the responsiveness of demand; and ""firm"" resources, such as nuclear, hydro with large reservoirs, biogas, and geothermal.""Because we can't know with certainty the future cost and availability of many of these resources,"" Sepulveda notes, ""the cases studied covered a wide range of possibilities, in order to make the overall conclusions of the study robust across that range of uncertainties.""Range of scenariosThe group used a range of projections, made by agencies such as the National Renewable Energy Laboratory, as to the expected costs of different power sources over the coming decades, including costs similar to today's and anticipated cost reductions as new or improved systems are developed and brought online. For each technology, the researchers chose a projected mid-range cost, along with a low-end and high-end cost estimate, and then studied many combinations of these possible future costs.Under every scenario, cases that were restricted to using fuel-saving and fast-burst technologies had a higher overall cost of electricity than cases using firm low-carbon sources as well, ""even with the most optimistic set of assumptions about future cost reductions,"" Sepulveda says.That's true, Jenkins adds, ""even when we assume, for example, that nuclear remains as expensive as it is today, and wind and solar and batteries get much cheaper.""The authors also found that across all of the wind-solar-batteries-only cases, the cost of electricity rises rapidly as systems move toward zero emissions, but when firm power sources are also available, electricity costs increase much more gradually as emissions decline to zero.""If we decide to pursue decarbonization primarily with wind, solar, and batteries,"" Jenkins says, ""we are effectively 'going all in' and betting the planet on achieving very low costs for all of these resources,"" as well as the ability to build out continental-scale high-voltage transmission lines and to induce much more flexible electricity demand.In contrast, ""an electricity system that uses firm low-carbon resources together with solar, wind, and storage can achieve zero emissions with only modest increases in cost even under pessimistic assumptions about how cheap these carbon-free resources become or our ability to unlock flexible demand or expand the grid,"" says Jenkins. This shows how the addition of firm low-carbon resources ""is an effective hedging strategy that reduces both the cost and risk"" for fully decarbonizing power systems, he says.Even though a fully carbon-free electricity supply is years away in most regions, it is important to do this analysis today, Sepulveda says, because decisions made now about power plant construction, research investments, or climate policies have impacts that can last for decades.""If we don't start now"" in developing and deploying the widest range of carbon-free alternatives, he says, ""that could substantially reduce the likelihood of getting to zero emissions.""The research received support from the MIT Energy Initiative, the Martin Family Trust, and the Chilean Navy.Story Source:Materials provided by Massachusetts Institute of Technology. ",0.0489254,0.0750372,0.0928665,0.124925,0.0620699,0.135523,0.0969983,0,0
412,Physics model acts as an 'EKG' for solar panel health,"Companies and governments have regularly invested in solar farms and lost money when weather degradation unexpectedly cut panel lifetime short.As electricity generated from solar energy increasingly matches fossil fuels in price, companies are pressured to keep panels living past their warranty and stretch the billions of dollars paid up front for their construction.Diagnosing degraded solar panels sooner through a tool functioning like an electrocardiogram would contribute to lower electric bills on clean energy as well as cut manufacturing costs.""We need to look at the heartbeat of a solar farm to understand its diseases,"" said Xingshu Sun, a recent doctoral graduate of Purdue University's School of Electrical and Computer Engineering.A solar farm's ""heartbeat"" is data on how well it generates electricity. Purdue researchers created an algorithm using the physics of panel degradation that can analyze solar farm data from anywhere, essentially as a portable EKG for solar farms.""It's the difference between daily life and the doctor's office. Previously, facilities were just checking a solar farm's heartbeat in a controlled environment, like with an EKG in a hospital lab,"" said Muhammad Ashraful Alam, Purdue's Jai N. Gupta Professor of Electrical and Computer Engineering. ""But a solar farm itself is always generating new field data for us to collect and analyze, so we need to bring the EKG to the field. This information-driven approach is transformative, because the approach would allow continuous monitoring and decision making. Ours is a first step in that direction.""Real-time diagnostics would ultimately inform better panel designs -- the cost-saving ""treatment"" that could prolong lifespan and continue to cut electrical bills.""If you look at solar modules on the market, their designs hardly differ no matter where they are in the world, just like how iPhones sold in the U.S. and China are almost identical,"" Sun said. ""But solar modules should be designed differently, since they degrade differently in different environments.""Degradation in humid environments, for example, comes in the form of corrosion, but high altitudes with no humidity cause degradation through the increased concentration of UV light. Like with human diseases, symptoms of corrosion or sun-beaten silicon tend to not show up on a solar panel until many years after the degradation started.Without knowing when degradation is happening, companies tend to compensate for different weather conditions by under- or over-designing solar panels, driving up manufacturing costs.Purdue researchers used public solar panel data provided by the National Renewable Energy Laboratory to pull together parameters of how well the panels are generating electricity, such as resistance and voltage. When fed into the algorithm, a curve generates to show the power output of a solar cell. Published findings appear in the journal Progress in Photovoltaics.The next step is improving the algorithm over time. Alam's lab has been collaborating with other Purdue research teams to develop DEEDS into a platform for preserving and sharing data, computational tools and scientific workflows from solar panel facilities as well as from a range of sources for other fields, including chemistry, nutrition science and environmental science.In the long term, the researchers hope the algorithm could show how much energy a solar farm produces in 30 years by looking at the relationship between weather forecast data and projection of electric circuit parameters. Integrating the algorithm with other physics-based models could eventually predict the lifetime of a solar farm.The algorithm is in an experimental stage, but already downloadable for other researchers to use through a National Science Foundation-funded platform called Digital Environment for Enabling Data-driven Science (DEEDS) at https://datacenterhub.org/resources/14754.Story Source:Materials provided by Purdue University. ",0.0967603,0.118006,0.161498,0.193473,0.122349,0.172888,0.156198,0,0
413,Wind turbines: How much power can they provide?,"Wind power is an important pillar in Germany's energy policy turnaround: According to the German government, the resource should cover 65 percent of German electricity needs by 2030, along with solar, hydropower and biomass. In a recent study, Dr. Christopher Jung and Dr. Dirk Schindler from the University of Freiburg show that it will be possible to cover 40 percent of the current electricity consumption with wind energy alone by the year 2030. The prerequisite is that the operators distribute the plants optimally on the German mainland. To estimate usable wind energy, the researchers have developed a new three-dimensional model. As a basis for their calculation, they used the number of new installations in 2017. If it remains constant until 2030, Germany can reach the calculated value. The team recently published its findings in the journal Energy Conversion and Management.A fundamental idea of the researchers while developing the model was to increase the efficiency with which the wind power is used. The scientists show that in particular repowering -- i.e. the replacement of old, small plants with newer, larger ones -- enables enormous increases in yields of up to several hundred percent. As a result, the cost of generating electricity, which is created when the energy is converted into electricity, can be reduced significantly to a level comparable to that of brown coal. However, in order to meet the current expansion targets of the Federal Government, a significant portion of the 30,000 wind turbines must be renewed and 6,000 additional systems must be additionally installed.Based on the researchers' model, the available wind resources can be determined for all common plant types. Also, the expansion target can be adjusted as desired. Using the model, scientists can develop and assess scenarios in which the plant density, the expansion strategy and the repowering intensity are varied. The model also allows for a balanced spatial distribution. ""In principal, we can avoid a disproportionate concentration in certain regions,"" summarizes Jung. In addition, the algorithm takes into account that the number of new installations to be installed is kept as low as possible. ""This would minimize disruptions in the landscape while taking the landscape and nature conservation into account,"" says Schindler.Story Source:Materials provided by University of Freiburg. ",0.0936692,0.0891481,0.143586,0.140918,0.100227,0.166041,0.121874,0,0
414,Changing the type of silicon etching drops solar power costs by more than 10 percent,"At the end of one of the hottest summers on record, as fights about how to power homes rage, renewable solar energy continues to present an option that does not significantly add greenhouse gases to the environment in exchange for lighting and cooling our homes. And it's just been given another edge through material science.In a new study published in Energies, researchers have found a way to reduce production costs of solar cells by more than 10 percent.""Improving cost per unit power at the cell level can have massive effects downstream,"" says Joshua Pearce, professor of material sciences and electrical engineering at Michigan Tech. Already, he says, costs of solar energy are comparable to conventional forms of electricity and is the fastest growing energy source. This 10 percent drop should push solar to the forefront even faster.Switching the Silicon Used in Solar Cells for Renewable Energy Drops CostsSilicon is the standard light-capturing material used in solar photovoltaic (PV) cells. It comes in two main forms: perfect crystals that cost more and produce higher efficiencies and multicrystalline silicon that cost less, but offers lower efficiencies. With common etching to reduce reflected light both types still lose some light, which is what gives most solar panels their blue color.Researchers already knew that nano-texturing silicon with dry etching makes black silicon (black-Si) that is more efficient at capturing light than standard etching treatments. It has no color because the dry etching process takes a normally flat silicon surface and ""etches it into a forest of nanoscale needles,"" Pearce says. ""Those needles grab the light and don't let it get away. It's like looking into the eyes of Darth Vader.""Normally such a high surface area with many surface defects would hurt electrical performance, but researchers at Aalto University found that when the silicon is also treated with an appropriate atomic layer deposition (ALD) coating, the effects of surface defects are mitigated.Typical thinking has been that the cost of black-Si cells from dry etching and ALD are too expensive for practical use, especially in an industry where, Pearce says, ""margins are extremely tight. Everyone's trying to push costs as low as possible.""However, the results of their study shocked even Pearce. While researchers did find that production of individual black-Si passive emitter rear cells (PERC) were between 15.8 and 25.1 percent more expensive than making conventional cells, they also found that the efficiency gains and the ability to go to the less-expensive multicrystalline silicon starting material far outweighed those extra costs: overall the cost per unit power dropped by 10.8 percent.The Future of Renewables and Solar Energy Production through Material ScienceBlack is not only better than blue when it comes to solar panels. The improvements could start to beat out renewables' top energy competitor in the climate change arena.""For the people that think coal technology is going to be able to compete with solar, they should know solar costs are still coming down. Most coal companies are already, or near, bankrupt now,"" Pearce says. ""There's no way coal's going to be able to compete with solar in the future.""He adds, ""This study points to where the future is going to go in PV manufacturing and what countries might want to do to give themselves a competitive advantage.""Teaming Up Across the Atlantic for Solar Energy EfficiencyPearce completed this study while on sabbatical as a Fulbright distinguished chair at Aalto University in Finland. He worked with the Hele Savin's Electron Physics Group and had access to their data on these processes. Researchers were also able to get information on manufacturing costs from companies, which is not public, but were allowed to use for this study, along with published literature on solar cells.While the spot price for solar cells may change day by day -- or even by hour -- the results still hold. ""That's 10 percent decline between cell types from whatever the number is that day,"" he says. This is because the comparisons were made on relative costs, not absolute costs. That's also why arbitrarily fluctuating tariffs were not factored into the calculations.What's Next for Solar Energy and RenewablesPearce says that while the production process can still be optimized to pull out a few more percentage points of efficiency, the next step for this study is to be used by policy makers to accelerate PV manufacturing. For a country like China, which already dominates global PV manufacturing, ""to make this relatively small change is pretty trivial."" The European Union, which currently makes a lot of the manufacturing equipment, should also ""look carefully at scaling up deep reactive ion etching and ALD tools to meet the needs of the rapidly expanding PV market."" He hopes that countries like the U.S., which used to dominate the solar field, will use this data at a policy level to leap frog international manufacturers, and invest in producing the new machines to manufacture these types of solar cells.""I don't know which technology will end up being the one to dominate the solar field,"" he said, however ""The study shows the clear economic impetus to move in the direction of dry-etched black silicon PERC that wasn't there before.""This study was conducted with Chiara Modanese, Hannu Laine, Toni Pasanen and Hele Savin from Aalto University in Finland.Story Source:Materials provided by Michigan Technological University. Original written by Jen Miller. ",0.0643875,0.114713,0.103752,0.13065,0.108449,0.125574,0.125019,0,0
415,Countries ranked by oil production emissions,"Until renewable sources of energy like wind or solar become more reliable and less expensive, people worldwide remain reliant on fossil fuels for transportation and energy. This means that if people want to reduce greenhouse gas emissions, there need to be better ways of mitigating the effects of extracting and burning oil and gas.Now, Adam Brandt, assistant professor of energy resources engineering in the School of Earth, Energy & Environmental Sciences at Stanford, and his colleagues have performed a first global analysis comparing emissions associated with oil production techniques -- a step toward developing policies that could reduce those emissions. They published their work Aug. 30 in Science.The group found that the burning of unwanted gas associated with oil production -- called flaring -- remains the most carbon-intensive part of producing oil. Brandt spoke with Stanford Report about the group's findings and strategies for reducing flaring.What is flaring and why is it especially important to track?Oil and gas are generally produced together. If there are nearby gas pipelines, then power plants, factories, businesses and homes can consume the gas. However, if you're very far offshore or can't get the gas to market, there's often no economically feasible outlet for the gas. In this case, companies want to get rid of the gas, so they often burn -- or flare -- it.Thankfully, there is some value to the gas, so there can be some savings associated with stopping flaring. I think setting the expectation that the gas will be managed properly is the role of the regulatory environment. There are some efforts underway to try to tackle this -- the World Bank has a big effort called the Global Gas Flaring Reduction Partnership, where companies have banded together to try to set flaring targets, so hopefully this will start to decline.This work represents the first study breaking down oil-industry greenhouse gas emissions at the country level. What data did you look at to do this work?This is the culmination of a larger project we've been working on for eight or so years. We used three different data sources. For some countries you can get data from governmental sources or regulatory agencies. Environmental agencies and natural resource agencies will also report information we can use. Otherwise, we go to petroleum engineering literature to get information about oil fields. Then we were able to collaborate with Aramco, an international oil company, to access a commercial data set. That allowed us to fill in gaps for a lot of smaller projects that are harder to get information on or the data gathering was just too intensive. With that, our paper covers about 98 percent of global oil supply. Necessarily, it's the first time we've been able to do this at this very resolved oil field-by-oil field level.In mapping the world's oil supply, how did you estimate emissions from flaring on a country-by-country basis?One of the challenges with flaring is that most countries don't report it. In many countries, we ended up using country-level average satellite data collected by the National Oceanic and Atmospheric Administration. Scientists there have developed ways to estimate the amount of gas flared using the brightness of the flare as seen from space. It's essentially an eye in the sky. For instance, Russia won't say how much they are flaring, but we can see it from the satellite.Where have you seen flaring regulations work?Offshore Canada has had a good success over the last 15 years. Basically, the rules there say that you're not allowed to flare above a certain amount. If flaring goes above a permitted level, Canada requires their offshore fields to shut down until they handle the gas. This can be done by reinjecting it back into the ground, converting it to liquefied natural gas or installing gas pipelines to get the gas to customers. Canadian flaring has dropped significantly, and these regulations prove that you can manage flaring and require that people do something productive with the gas or put it back underground. Really, the challenge with flaring is there needs to be a policy or a regulatory apparatus to say, ""Burning gas with no purpose isn't allowed; put it back in the ground or find something useful to do with it.""In the absence of federal action, how can we prioritize flaring reductions here in the U.S.?If you don't see action at the U.S. federal level, you can work with leadership from state agencies. A good example of this was the state of North Dakota. North Dakota contains the Bakken Formation, which is one of the main regions for producing oil from hydraulically fractured wells. Five years ago, 30 percent of the gas being produced was being flared, and essentially the state government said this is not acceptable. Thirty percent was way too high and the gas had value -- it could be sold to cities like Chicago, Calgary or Denver. The government set a target for 10 percent, with the threat of potential production restrictions if producers didn't meet the target. So what happened? Producers in the region actually met the 10 percent target ahead of time. So I think things can keep moving forward. Obviously, it'd be better if we had some sort of federal action on this, but states can do a lot.Who can drive the change needed across the globe?Globally, I think international oil companies can really take the lead. A lot of the projects with flaring are in countries where environmental issues are poorly regulated. But many of these projects are developed by the local national oil company in cooperation with international partners. It's hard to wait on developing countries without large budgets or sophisticated regulatory capacity to put flaring rules into place. Instead of waiting for that to happen, we might expect the international oil companies work to solve the problems themselves by applying best practices from places were regulations have already solved the problem. For example, companies in Nigeria have increased gas reinjection and developed liquefied natural gas projects to get the gas to markets.In the coming decades, we are going to be using a lot of oil and gas. It's inevitable. Taking best practices and applying them in places that are not as well regulated right now -- but hopefully will be -- can allow improvements in one region to benefit another region.Hopefully, we'll transition as quickly as possible to renewables, but while we use oil and gas in the meantime, let's do it responsibly.The work was funded by the Natural Sciences and Engineering Research Council of Canada, Aramco Services Co., Ford Motor Co., the Carnegie Endowment for International Peace, the Hewlett Foundation, the ClimateWorks Foundation and the Alfred P. Sloan Foundation.Story Source:Materials provided by Stanford University. Original written by Katie Brown. ",0.0567824,0.0795642,0.107875,0.13767,0.0701314,0.118488,0.113322,0,0
416,Europe may thrive on renewable energy despite unpredictable weather,"Researchers in Ireland, Switzerland, and the United Kingdom have shown how long-term weather patterns affect wind and solar renewable energy technologies across Europe. Using 30 years of meteorological data, the scientists have examined and further modelled the impact of renewable energy on the electricity sector out to the year 2030. The work suggests that despite the unpredictable nature of wind and solar energy, the European power system can comfortably generate at least 35% of its electricity using these renewables alone without major impacts on prices or system stability. The paper appears July 26 in the journal Joule.Wind and solar energy have exploded in popularity across Europe in the last decade as green alternatives to traditional carbon-based energy, quadrupling in use between 2007 and 2016. However, these technologies are not without their drawbacks -- both are susceptible to fluctuating weather patterns, raising concerns about Europe's ability to endure long spells with low winds or overcast skies. Researchers have used decades of historic weather data to model this variability in wind and solar energy and its effect on markets, but many studies only analyze data from one given year or focus solely on one country or small region.The researchers challenge both the temporal and spatial limitations of previous studies by analyzing electricity system operation across Europe -- including power transmission between countries and technical operational constraints -- using wind and solar data spanning the 30-year period from 1985 to 2014. By uncovering trends from this longstanding data trove across a vast, interconnected region, the team was able to model how Europe would fare under five different renewable energy scenarios with varying sustainability ambitions 12 years into the future. It turns out that the breadth and depth of their data pool made all the difference when it came to understanding trends in CO2 emissions, system costs, and system operation -- all of which are essential to the effective development of energy policy.""When planning future power systems with higher levels of wind and solar generation, one year of weather data analysis is not sufficient,"" says says Seen Collins, a researcher at MaREI, the Centre for Marine and Renewable Energy Ireland at the Environmental Research Institute in University College Cork. ""We find that single-year studies could yield results that deviate by as much as 9% from the long-term average at a European level and even more at a country level. When there are legally binding targets on carbon emissions and the share of renewable energy, or promises to avoid sharp price hikes, this makes all the difference.""By using multiple years to better understand the way other variables respond as wind and solar energy penetrate the market, Collins and his team found that CO2 emissions and total energy generation costs fluctuate wildly in future scenarios. These can become up to five times more uncertain as weather-dependent resources gain greater traction in the market. However, they also found that Europe could withstand this variability quite well thanks to its close integration -- their models estimate Europe could use renewables for more than two-thirds of its electricity by 2030, with more than one-third coming from wind and solar.Collins and his team believe their models and data could be used to depict a variety of possible future scenarios to help policymakers better understand the reliability and impact of renewable energy, including the impacts of a shift to 100% renewable electricity systems. By making their models and data openly available, the researchers also hope that future work will demonstrate greater awareness of these long-term weather patterns in order to accurately depict a more renewable-energy-reliant world.""For future policy developments to be robust and to capture the meteorological dependency of decarbonised energy systems, they should be based on open modeling analyses that utilise common long-term datasets,"" says Collins.Story Source:Materials provided by Cell Press. ",0.0875746,0.0941606,0.123711,0.144786,0.092856,0.168426,0.12303,0,0
417,Mapping mountaintop coal mining's yearly spread in Appalachia New mapping tool uses satellite images to track annual changes in mining's footprint,"The coal industry may have declined in the last decade because of the rise of cheap natural gas, but a coal mining method called mountaintop removal is still taking place, particularly in central Appalachia.A new web-based mapping tool shows, in more detail than ever before, the land laid bare by mountaintop coal mining in central Appalachia each year, going back more than three decades.The tool uses satellite imagery to identify and map the annual extent of mining activity across a four-state area including portions of Kentucky, Tennessee, Virginia and West Virginia.In the journal PLOS ONE, researchers estimate that between 1985 and 2015, an average of 21,000 acres was converted to bare earth and rubble in central Appalachia each year -- an area about half the size of Washington, D.C.This analysis places the total estimate since the 1970s at about 1.5 million acres. ""That is an area 18 percent larger than the state of Delaware, and only 3 percent smaller than Everglades National Park,"" said first author Andrew Pericak, who conducted the research in the lab of biology professor Emily Bernhardt at Duke University.Mountaintop mining is a form of surface coal mining in which coal companies clear the forest from a hilltop, then use explosives and heavy machinery to blast and dig through the soil and bedrock and expose the layers of coal underneath. The leftover rock and debris is pushed into adjacent valleys, burying streams under hundreds of meters of rubble called ""valley fill.""Advocates say the process allows mining companies to harvest shallow seams of coal they can't get at via traditional underground mining. But growing scientific evidence suggests it also destroys forests, fills the air with harmful dust and contaminates nearby streams.Determining the timing and extent of mining activity is crucial to assessing and mitigating these environmental and human impacts, said co-author Matthew Ross, previously at Duke and now a post-doctoral researcher at the University of North Carolina at Chapel Hill.But reliable, up-to-date estimates of mining's footprint are hard to come by. In any given year, a mining company may only be operating within a portion of the area shown on their mining permit. Previous efforts have mapped surface mining in Appalachia using satellite imagery, but these maps haven't been updated since 2006, and only give a snapshot of mining every 10 years, rather than year-to-year.The tool developed by Duke researchers, working with partners at the nonprofit organizations SkyTruth and Appalachian Voices, reveals where mountaintop mining is underway on a finer time scale, and makes it easier to keep the data current.The team used Google's Earth Engine cloud-computing platform to process U.S. government satellite images of visible and invisible light reflected from the Earth's surface, taken over 31 consecutive years in a 74-county area.Each pixel in 10,240 satellite images, going back to 1985, was analyzed by a computer algorithm that used reflectance data to determine the ""greenness"" of each small square of the image, which represents a 100-by-100-foot square of Earth.A collection of pixels that appeared largely devoid of vegetation and wasn't part of a city, road or body of water was labeled as an area where mining was likely occurring that year, with at least 83 percent accuracy in any given year.""It just took a matter of minutes to output the data set,"" Pericak said. ""It's a huge time save.""The team also combined their mine area estimates with previously published data on coal production. In the 1980s and 1990s, they found, every 100 square feet of land in the region yielded one ton of coal. By 2010, however, coal companies needed to clear and blast about 160 square feet per ton, and by 2015, it took more than 300 square feet.""It takes more land to get the same amount of coal than it had in the past,"" Pericak said.The results are consistent with what geologists since the mid-1990s have forewarned -- that as thicker, more accessible layers of coal are mined out, the ratio of waste rock and soil to coal will increase, along with operating costs, Ross said.Senior co-author Emily Bernhardt says the updated maps will help researchers come up with more accurate estimates of the costs and benefits of mountaintop mining, and how it compares with other forms of resource extraction.""Any scientist interested in studying the impacts of mountaintop mining can now see exactly where mines are in the landscape and how long those impacts have been active,"" said Bernhardt, who is using the data to understand more precisely how mountaintop mining affects water quality, how far the effects extend downstream and how long they persist.Indiana University professor Michael Hendryx, who was not involved in the study, has been using the updated maps to assess air pollution exposure and other health effects in people living near active mining sites, compared with people living farther away.""It also provides the public a better opportunity for monitoring mining operations to ensure they are adhering to the conditions in their permits,"" said co-author Christian Thomas of SkyTruth. ""Any new problems that crop up could potentially be acted on and corrected more effectively with this annual look at the region.""Thomas says SkyTruth plans to use the data to measure the effectiveness of reclamation efforts after mining has stopped. ""This is the key to helping the region recover from this legacy of mining and transition to a non-mining future,"" Thomas said.The data and computer code are free for anyone to use, and future mining activity can be quickly added to the dataset as new satellite imagery becomes available, Pericak said.Story Source:Materials provided by Duke University. ",0.0647308,0.0913911,0.156965,0.179369,0.0887616,0.130909,0.139959,0,0
418,States boost renewable energy and development when utilities adopt renewable standards,"States that require utilities to increase renewable energy see expansion of renewable energy facilities and generation -- including wind and other renewable sources, but especially solar -- according to new research from Indiana University and two other institutions.IU's Sanya Carley led a team of researchers including Nikolaos Zirogiannis, an assistant scientist at IU, and law professors Lincoln Davies of the University of Utah and David B. Spence of University of Texas at Austin. The group closely examined the history and evolution of state renewable portfolio standards and interviewed more than 40 experts about renewable portfolio standards implementation.Their findings are newly published in the peer-reviewed journal Nature Energy, in an article titled ""Empirical evaluation of the stringency and design of renewable portfolio standards.""The regulations, which require utilities to increase the percentage of energy they sell from renewable sources by a specified amount and date, have been adopted in varying forms by about 30 states. For example, New York requires 50 percent of all electricity sold in 2050 to come from solar.""As the federal government moves away from climate mitigation policy, including abandoning the Paris Agreement, the role of state-level policy tools such as RPS take on increasing importance,"" said Carley, an associate professor in the IU School of Public and Environmental Affairs.Most states have adopted such standards, except those in the Southeast and parts of the Great Plains and Interior West, where fossil fuel prices are low. Nevada and Massachusetts were the first to adopt a renewable portfolio standard in the 1990s, and Hawaii's is considered the most stringent, a pivotal measuring stick.Renewable mandates drive renewable energy development across the U.S., the researchers found. The design of the policy, however, is of fundamental importance. These are key findings:Carley said teaming up with researchers from three universities gave the project unique and unusual depth, including through its quantitative analysis and the use of structured expert interviews. The team developed a unique score to measure the stringency of renewable portfolio standard policies, then reaffirmed their findings by interviewing experts from government agencies, including public utility commissions and state energy offices, and renewable energy firms and associations.""Policymakers face tough trade-offs when designing their RPS policies, such as whether to force in-state renewable energy for local economic development purposes, or to purchase renewables from other states at a potentially lower cost,"" said Davies of the University of Utah. ""Our research also shows just how critical state energy laws are today, particularly as the Trump administration alters the national energy policy landscape. States are where the action is. They are driving the future of our electric grid.""Story Source:Materials provided by Indiana University. ",0.105913,0.130293,0.196802,0.188828,0.145722,0.185652,0.167635,0,0
419,Novel hybrid catalyst to split water discovered Catalyst uses inexpensive elements and could be scaled up for commercial use,"Researchers from the University of Houston and the California Institute of Technology have reported an inexpensive hybrid catalyst capable of splitting water to produce hydrogen, suitable for large-scale commercialization.Most systems to split water into its components -- hydrogen and oxygen -- require two catalysts, one to spur a reaction to separate the hydrogen and a second to produce oxygen. The new catalyst, made of iron and dinickel phosphides on commercially available nickel foam, performs both functions.Researchers said it has the potential to dramatically lower the amount of energy required to produce hydrogen from water while generating a high current density, a measure of hydrogen production. Lower energy requirements means the hydrogen could be produced at a lower cost.""It puts us closer to commercialization,"" said Zhifeng Ren, M.D. Anderson Chair Professor of physics at UH and lead author of a paper describing the new catalyst published Friday in Nature Communications.Hydrogen is considered a desirable source of clean energy, in the form of fuel cells to power electric motors or burned in internal combustion engines, along with a number of industrial uses. Because it can be compressed or converted to liquid, it is more easily stored than some other forms of energy, said Ren, who also is a researcher at the Texas Center for Superconductivity at UH.But finding a practical, inexpensive and environmentally friendly way to produce large amounts of hydrogen gas -- especially by splitting water into its component parts -- has been a challenge.Most hydrogen is currently produced through steam methane reforming and coal gasification; those methods raise the fuel's carbon footprint despite the fact that it burns cleanly.And while traditional catalysts can produce hydrogen from water, co-author Shuo Chen, assistant professor of physics at UH, said they generally rely on expensive platinum group elements. That raises the cost, making large-scale water splitting impractical.""In contrast, our materials are based on earth abundant elements and exhibit comparable performance with those of platinum group materials,"" she said. ""It can be potentially scaled-up at low cost, which makes it very attractive and promising for the commercialization of water splitting.""Researchers said the catalyst remained stable and effective through more than 40 hours of testing.The new catalyst, they wrote, ""proves to be an outstanding bifunctional catalyst for overall water splitting, exhibiting both extremely high OER (oxygen evolution reaction) and HER (hydrogen evolution reaction) activities in the same alkaline electrolyte. Indeed, it sets a new record in alkaline water electrolyzers (1.42 V to afford 10 mA cm-2), while at the commercially practical current density of 500 mA cm-2.""Previous catalysts have used different materials to spur a reaction to produce the hydrogen than those that are used to produce the oxygen. Ren said the interaction between the iron phosphide particles and the dinickel phosphide particles boosted both reactions. ""Somehow a joint effort of the two materials is better than any individual material,"" he said.In addition to Ren and Chen, other authors on the paper include Fang Yu, Haiqing Zhou and Jingying Sun, all with the UH Department of Physics; Fan Qin and Jiming Bao, with the Department of Electrical and Computer Engineering at UH; and Yufeng Huang and William A. Goddard III of the Materials and Process Simulation Center at the California Institute of Technology.Story Source:Materials provided by University of Houston. Original written by Jeannie Kever. ",0.0758741,0.198123,0.126936,0.138099,0.114992,0.13335,0.151824,0,0
420,Toward a smaller carbon footprint Technology that takes carbon dioxide out of the atmosphere and turn it into valuable chemicals and fuels,"Burning fossil fuels such as coal and natural gas releases carbon into the atmosphere as CO2 while the production of methanol and other valuable fuels and chemicals requires a supply of carbon. There is currently no economically or energy efficient way to collect CO2 from the atmosphere and use it to produce carbon-based chemicals, but researchers at the University of Pittsburgh Swanson School of Engineering have just taken an important step in that direction.The team worked with a class of nanomaterials called metal-organic frameworks or ""MOFs,"" which can be used to take carbon dioxide out of the atmosphere and combine it with hydrogen atoms to convert it into valuable chemicals and fuels. Karl Johnson, the William Kepler Whiteford Professor in the Swanson School's Department of Chemical and Petroleum Engineering, led the research group as principal investigator.""Our ultimate goal is to find a low-energy, low-cost MOF capable of separating carbon dioxide from a mixture of gases and prepare it to react with hydrogen,"" says Dr. Johnson. ""We found a MOF that could bend the CO2 molecules slightly, taking them to a state in which they react with hydrogen more easily.""The Johnson Research Group published their findings in the Royal Society of Chemistry (RSC) journal Catalysis Science & Technology. The journal featured their work on its cover, illustrating the process of carbon dioxide and hydrogen molecules entering the MOF and exiting as CH2O2 or formic acid -- a chemical precursor to methanol. For this process to occur, the molecules must overcome a demanding energy threshold called the hydrogenation barrier.Dr. Johnson explains, ""The hydrogenation barrier is the energy needed to add two H atoms to CO2, which transforms the molecules into formic acid. In other words, it is the energy needed to get the H atoms and the CO2 molecules together so that they can form the new compound. In our previous work we have been able to activate H2 by splitting two H atoms, but we have not been able to activate CO2 until now.""The key to reducing the hydrogenation barrier was to identify a MOF capable of pre-activating carbon dioxide. Pre-activation is basically preparing the molecules for the chemical reaction by putting it into the right geometry, the right position, or the right electronic state. The MOF they modeled in their work achieves pre-activation of CO2 by putting it into a slightly bent geometry that is able to accept the incoming hydrogen atoms with a lower barrier.Another key feature of this new MOF is that it selectively reacts with hydrogen molecules over carbon dioxide, so that the active sites are not blocked by CO2. ""We designed a MOF that has limited space around its binding sites so that there is not quite enough room to bind CO2, but there is still plenty of room to bind H2, because it is so much smaller. Our design ensures that the CO2 does not bind to the MOF but instead is free to react with the H molecules already inside the framework,"" says Dr. Johnson.Dr. Johnson believes perfecting a single material that can both capture and convert CO2 would be economically viable and would reduce the net amount of CO2 in the atmosphere. ""You could capture CO2 from flue gas at power plants or directly from the atmosphere,"" he says. ""This research narrows our search for a very rare material with the ability to turn a hypothetical technology into a real benefit to the world.""The Pitt Center for Research Computing contributed computing resources.Story Source:Materials provided by University of Pittsburgh. ",0.0796319,0.203035,0.143113,0.159061,0.117806,0.138954,0.171598,0,0
421,Efficient generation of high-density plasma enabled by high magnetic field,"An international joint research group led by Osaka University demonstrated that it was possible to efficiently heat plasma by focusing a relativistic electron beam (REB) accelerated by a high-intensity short-pulse laser with the application of a magnetic field of 600 tesla (T), about 600 times greater than the magnetic energy of a neodymium magnet (the strongest permanent magnet). Their research results were published in Nature Communications.If matter can be heated to temperatures of tens of millions of degrees using REB accelerated to nearly the speed of light by irradiating plasma with high-intensity lasers, it will become possible to ignite controlled nuclear fusion reactions.In the central ignition scheme, a prevailing scheme for inertial confinement fusion (ICF), has the problem of ignition quench, which is caused by the hot spark mixing with the surrounding cold fuel. On the other hand, in the fast ignition scheme (fast isochoric heating), a portion of low temperature fuel is heated, and then the heated region becomes the hot spark to trigger ignition before said mixing occurs. Thus, the fast ignition scheme has drawn attention as an alternative scheme.In the fast ignition scheme, first, fusion fuel is compressed to a high density using nanosecond laser beams. Next, a high-intensity picosecond laser rapidly heats the compressed fuel, making the heated region a hot spark to trigger ignition. Nuclear fusion releases a large amount of energy by burning the majority of the fuel.The REB, which is generated by a high-intensity short-pulse laser and accelerated to nearly the speed of light, travels through high-density nuclear fusion fuel plasma and deposits a portion of kinetic energy in the core, making the heated region the hot spark to trigger ignition. However, REB accelerated by high-intensity lasers has a large divergence angle (typically 100 degrees), so only a small portion of the REB collides with the core.A kilo-tesla level magnetic field is necessary to guide high-energy electrons at the speed of light, so the researchers employed magnetic fields of several hundreds of tesla. Because electrons, which are charged and have a small mass, easily move along a magnetic field line, they guided the high energy REB of 1MeV along the magnetic field lines to the core (the fusion fuel of 100 microns or less), achieving efficient heating of high-density plasma. They named the scheme ""magnetized fast isochoric heating.""In this study, laser-to-core energy coupling reached a maximum of 8 percent. The laser-to-core energy coupling, i.e., the energy deposition rate of REB, depends on the density of the plasma to be heated. In calculation based on the ignition spark formation conditions, the energy deposition rate of REB obtained in this study is several times more than that obtained by the central ignition scheme. Thus, the researchers conclude that the magnetized fast isochoric heating is very efficient and useful for the development of laser fusion energy.Senior author Shinsuke Fujioka says, ""We have made progress towards the realization of laser fusion energy in cooperation with researchers from home and abroad under the Joint Use/Research Center Project. Our research results will be applied to studies on the reproduction of the core of a star in laboratory simulation and the creation of new matter under extreme environments.""Story Source:Materials provided by Osaka University. ",0.129207,0.151883,0.144012,0.190111,0.158463,0.196727,0.20446,0,0
422,Ready-to-use recipe for turning plant waste into gasoline,"Bioscience engineers at KU Leuven, Belgium, already knew how to make gasoline in the laboratory from plant waste such as sawdust. Now the researchers have developed a roadmap, as it were, for industrial cellulose gasoline.In 2014, at KU Leuven's Centre for Surface Chemistry and Catalysis, researchers succeeded in converting sawdust into building blocks for gasoline. A chemical process made it possible to convert the cellulose -- the main component of plant fibres -- in the sawdust into hydrocarbon chains. These hydrocarbons can be used as an additive in gasoline. The resulting cellulose gasoline is a second generation biofuel, explains Professor Bert Sels. ""We start with plant waste and use a chemical process to make a product that is a perfect replica of its petrochemical counterpart. In the end product, you can only tell the difference with fossil gasoline using carbon dating.""For this type of bio-refining, the researchers built a chemical reactor in their lab, with which they can produce cellulose gasoline on a small scale. ""But the question remained how the industry can integrate this and could produce it in large quantities. Our researcher, Aron Deneyer, has now investigated this. He examined in which section of the existing petroleum refining process the cellulose is best added to the petroleum to obtain a strongly bio-sourced gasoline. In other words, we now have a ready-to-use recipe for cellulose gasoline that the industry can apply directly: without loss of quality for the gasoline and making maximum use of existing installations.""Cellulose gasoline must be seen as a transitional phase, Professor Sels emphasises. ""The cellulose is still mixed with petroleum: this gasoline will never be sourced 100 percent from renewable raw materials. Current consumption is too high to produce all gasoline from plant waste. However, our product does already offer the possibility of using greener gasoline while a large proportion of the vehicles on our roads still run on liquid fuel. In the future, we will remain dependent on liquid fuels, albeit to a lesser extent, and then they may indeed be fully bio-based. We therefore suspect that the industry will show interest in this process.""Story Source:Materials provided by KU Leuven. ",0.167152,0.199919,0.223961,0.237876,0.191428,0.226892,0.246067,0,0
423,Methane to syngas catalyst: two for the price of one,"Hokkaido University researchers have created an improved catalyst for the conversion of methane gas into syngas, a precursor for liquid fuels and fundamental chemicals.Syngas, also known as synthesis gas, is a mixture made primarily of carbon monoxide and hydrogen and is used to manufacture polymers, pharmaceuticals, and synthetic petroleum. It is made by exposing methane to water vapor at 900 eC or higher, making the process costly.The partial oxidation of methane for syngas synthesis is more economical than using steam but there have been issues with the catalysts used for this process. Noble metal catalysts, such as rhodium and platinum, are better and work at lower temperatures than base metal catalysts, such as cobalt and nickel, but they are also more expensive. The cheaper base metal catalysts require temperatures above 800 eC, exceeding the temperature range for industrial stainless-steel reactors. They are also deactivated during the reaction by re-oxidation and the accumulation of coke, a by-product of the process, making them costly in the long-term.Assistant Professor Hirokazu Kobayashi, Professor Atsushi Fukuoka, and postdoctoral fellow Yuhui Hou, working in Hokkaido University's Institute for Catalysis, succeeded in preparing a catalyst that combines the properties of both noble and base metals. Their catalyst overcomes challenges faced by previous studies in adding a small enough amount of noble metal to the base metal catalyst that it can still work at lower temperatures.In the study published in Communications Chemistry, the team successfully generated tiny particles of the base metal cobalt by dispersing them onto a mineral deposit called zeolite. They then added a minute amount of noble metal rhodium atoms onto the cobalt particles.The new, combined catalyst successfully converted 86% of methane to syngas at 650 eC while maintaining its activity for at least 50 hours. The reaction oxidizes cobalt to cobalt oxide, which is nearly inactive. But because the rhodium is contained, the noble metal generates hydrogen atoms from methane or hydrogen molecules. The hydrogen atoms spill over onto the supporting material, and the spillover hydrogen turns the cobalt oxide back into cobalt. The cobalt can then continue to act as a catalyst. The high dispersion of cobalt on zeolite also prevented the formation of coke during the reaction.Methane has drawn attention as a source of clean energy as it produces only a half amount of CO2 compared to petroleum when burned. Moreover, increased shale gas mining has made methane a more accessible resource. ""Our catalyst can efficiently convert methane to syngas at 650 eC, a much lower temperature than in conventional methods. This could lead to more efficient use of methane and contribute to the development of a low-carbon society,"" says Hirokazu Kobayashi.Story Source:Materials provided by Hokkaido University. ",0.101514,0.312091,0.161245,0.14921,0.157489,0.166218,0.17957,0,0
424,Converting carbon dioxide into methane or ethane selectively,"A research team led by Professor Su-Il In from Department of Energy Science and Engineering had succeeded in developing photo catalysts that can convert carbon dioxide into usable energy such as methane or ethane.As carbon dioxide emissions increase, the Earth's temperature rises and interest in reducing carbon dioxide, the main culprit of global warming, has been increasing. In addition, the shift to reusable fuel for existing resources due to energy depletion is also drawing attention. In order to solve trans-national environmental problems, research on photocatalysts, which are essential in converting carbon dioxide and water into hydrocarbon fuels, is gaining attention.Although many semiconductor materials with large band gaps are often used in photocatalyst studies, they are limited in absorbing solar energy in various areas. Thus, photocatalyst studies focusing on improving the photocatalyst structure and surface to increase solar energy absorption areas or utilizing two-dimensional materials with excellent electron transmission are under way.Professor In's research team developed a high-efficiency photocatalyst that can convert carbon dioxide into methane (CH4) or ethane (C2H6) by placing graphene on reduced titanium dioxide in a stable and efficient way.The photocatalyst developed by the research team can selectively convert carbon dioxide from a gas to methane or ethane. The results showed that its generation volume is 259umol/g and 77umol/g of methane and ethane respectively and its conversion rate is 5.2% and 2.7% higher than conventional reduced titanium dioxide photocatalysts. In terms of ethane generation volume, this result shows the world's highest efficiency under similar experimental conditions.In addition, the research team proved for the first time that the pore moves toward graphene due to band bending phenomena visible from titanium dioxide and graphene interfaces through the international joint research conducted with the research team led by James R. Durrant at the Department of Chemistry of Imperial College London (ICL), UK using photoelectron spectroscopy.The movement of the pore towards graphene activates reactions by causing electrons to gather on the surface of the reduced titanium dioxide and forms a large amount of radical methane (CH3) as polyelectrons engage in the reactions. The research team identified a mechanism for producing methane if this formed radical methane reacts with hydrogen ions and for producing ethane if the radical methane reacts with each other.The catalyst material developed by the research team is expected to be applied to a variety of areas such as high-value-added material production in the future and be used to solve global warming problems and energy resource depletion issues by selectively producing higher levels of hydrocarbon materials using sunlight.Professor In said, ""The reduced titanium dioxide photocatalyst with graphene that has been developed this time has the advantage of being able to selectively produce CO2 as a usable chemical element such as methane or ethane. By conducting follow-up research that increases the conversation rate so that it can be commercialized, we will contribute to the development of technology for reducing carbon dioxide and turning it into a resource.""Story Source:Materials provided by DGIST (Daegu Gyeongbuk Institute of Science and Technology). ",0.0913647,0.198205,0.171305,0.149357,0.128299,0.154854,0.172716,0,0
425,"Engineers use Tiki torches in study of soot, diesel filters ","Chemical engineers testing methods to improve efficiency of diesel engines while maintaining performance are getting help from a summer staple: Tiki torches.A team of engineers at the University of Notre Dame is using the backyard torches as part of an effort to mimic the soot oxidation process in a diesel engine -- when soot in diesel exhaust collects in the walls of a particulate filter and has to be burned off -- according to a study recently published in Catalysts.""This study is part of an effort over many years in which we have discovered and developed low-cost catalysts for soot oxidation that are based on silica glass,"" said Paul McGinn, a co-lead author of the study and professor in the Department of Chemical and Biomolecular Engineering at Notre Dame.McGinn and co-principal investigator Changsheng Su at Cummins Inc. developed a method to coat diesel particulate filters with a silica glass, which slowly releases potassium ions. The potassium acts as a catalyst, reducing temperatures required to initiate filter regeneration -- or soot oxidation -- for improved efficiency.What they needed was a simple way to simulate real-world driving conditions, including the continuous flow of soot as it passes through a diesel particulate filter.""We could do it continuously using the Tiki torch, using the Tiki soot as a surrogate for real engine soot,"" McGinn said. ""Depending on the setting, you really get a lot of soot coming off of it, which is what we want."" The team constructed a sophisticated reactor equipped with soot generator and backpressure sensors, which allows them to control conditions including oxygen rates, air-to-fuel ratios and soot production per hour.New methods of reducing soot oxidation are of particular interest to manufacturers of diesel engines. Diesel exhaust contains, among other things, soot particles and nitrogen oxides (NOx), with soot being a major contributor to global warming and a cause of breathing problems. The Environmental Protection Agency (EPA) has been working to reduce emissions from vehicles, industrial vehicles, locomotives and ships for more than a decade.For diesel engine vehicles, the EPA requires both soot and NOx be kept below certain levels but challenges remain to reducing those emissions economically without sacrificing performance. When the engine's operating conditions are adjusted to emit low levels of NOx, soot levels increase, and vice versa.A standard diesel particulate filter is a ceramic cylinder with a honeycomb-style structure and porous walls. Every other channel -- or opening -- of the filter is closed off. As exhaust enters the filter, soot collects along the interior walls as cleaned exhaust passes through.To burn off soot buildup along the filter walls, exhaust temperatures need to reach 600 degrees Celsius (1,112 F). ""When you're in an urban environment where you're stopping and starting your engine, the exhaust temperature doesn't get that hot,"" McGinn said.In some cases, fuel is used to heat up the filter and burn off the soot -- a process called active regeneration -- which delivers a hit the vehicle's fuel mileage and requires substantial noble metal (such as platinum) usage.""Everyone is looking for a low-cost way to get the temperature down,"" McGinn said. ""In our case, we've developed an inexpensive glass coating that's one to two microns thick and apply it to the diesel particulate filters. The glass delivers a potassium catalyst slowly over 150,000 miles of driving and allows for what's called passive regeneration. So when you're out on the highway at high speed, the exhaust temperature gets high enough to burn off soot buildup continuously.""With Tiki torches providing the soot buildup needed for testing, McGinn said his team will look at how to further tailor the glass composition to also reduce NOx.Story Source:Materials provided by University of Notre Dame. ",0.143579,0.202393,0.179895,0.204891,0.169866,0.195921,0.231323,0,0
426,"Using coal waste to create sustainable concrete New coal concrete reduces energy demand, greenhouse emissions","Washington State University researchers have created a sustainable alternative to traditional concrete using coal fly ash, a waste product of coal-based electricity generation.The advance tackles two major environmental problems at once by making use of coal production waste and by significantly reducing the environmental impact of concrete production.Xianming Shi, associate professor in WSU's Department of Civil and Environmental Engineering, and graduate student Gang Xu, have developed a strong, durable concrete that uses fly ash as a binder and eliminates the use of environmentally intensive cement. They report on their work in the August issue of the journal, Fuel.Reduces energy demand, greenhouse emissionsProduction of traditional concrete, which is made by combining cement with sand and gravel, contributes between five and eight percent of greenhouse gas emissions worldwide. That's because cement, the key ingredient in concrete, requires high temperatures and a tremendous amount of energy to produce.Fly ash, the material that remains after coal dust is burned, meanwhile has become a significant waste management issue in the United States. More than 50 percent of fly ash ends up in landfills, where it can easily leach into the nearby environment.While some researchers have used fly ash in concrete, they haven't been able to eliminate the intense heating methods that are traditionally needed to make a strong material.""Our production method does not require heating or the use of any cement,"" said Xu.Molecular engineeringThis work is also significant because the researchers are using nano-sized materials to engineer concrete at the molecular level.""To sustainably advance the construction industry, we need to utilize the 'bottom-up' capability of nanomaterials,"" said Shi.The team used graphene oxide, a recently discovered nanomaterial, to manipulate the reaction of fly ash with water and turn the activated fly ash into a strong cement-like material. The graphene oxide rearranges atoms and molecules in a solution of fly ash and chemical activators like sodium silicate and calcium oxide. The process creates a calcium-aluminate-silicate-hydrate molecule chain with strongly bonded atoms that form an inorganic polymer network more durable than (hydrated) cement.Aids groundwater, mitigates floodingThe team designed the fly ash concrete to be pervious, which means water can pass through it to replenish groundwater and to mitigate flooding potential.Researchers have demonstrated the strength and behavior of the material in test plots on the WSU campus under a variety of load and temperature conditions. They are still conducting infiltration tests and gathering data using sensors buried under the concrete. They eventually hope to commercialize the patented technology.""After further testing, we would like to build some structures with this concrete to serve as a proof of concept,"" said Xu.The research was funded by the U.S. Department of Transportation's University Transportation Centers and the WSU Office of Commercialization.Story Source:Materials provided by Washington State University. ",0.0895798,0.177298,0.220946,0.149288,0.120192,0.155938,0.158495,0,0
427,Newly discovered properties of ferroelectric crystal shed light on branch of materials,"In ferroelectric materials the crystal structure distorts, giving rise to a spontaneously formed polarization and electric field. Because of this unique property, ferroelectrics can be found in anything from ultrasound machines and diesel fuel injectors to computer memory. Ferroelectric materials are behind some of the most advanced technology available today. Findings that ferroelectricity can be observed in materials that exhibit other spontaneous transitions, like ferromagnetism, have given rise to a new class of these materials, known as hybrid improper ferroelectrics. The properties of this type of material, however, are still far from being fully understood. New findings published in Applied Physics Letters, from AIP Publishing, help to shine light on these materials and indicate potential for new optoelectronic and storage applications.A team of researchers from China has characterized one type of hybrid improper ferroelectric, Ca3Mn2O7. The group investigated the material's ferroelectric, magnetoelectric and optical properties. They were able to demonstrate ferroelectricity in Ca3Mn2O7 as well as coupling between its magnetism and ferroelectricity, a key property that has potential to allow for faster and more efficient bit operations in computers.""Our work solves a long-term puzzle in this field, which could push forward the frontiers and enhance the confidence to continue the research in this field,"" said Shuai Dong, an author on the paper.Like batteries, for instance, ferroelectrics have positively and negatively charged poles. A major distinguishing feature of these materials, however, is that this polarization can be reversed by using an external electric field.""This can be useful because it can be used in devices to store information as ones and zeros,"" Dong said. ""Also, the switching of polarization can generate current, which can be used in sensors.""Unlike traditional ferroelectrics, which directly derive their properties from polar distortions in the lattice of the material's crystal, hybrid improper ferroelectrics generate polarization from a combination of nonpolar distortions.When hybrid improper ferroelectrics were first theorized in 2011, two materials were proposed. In the years since, nonmagnetic Ca3Ti2O7e crystals were demonstrated experimentally, but a full characterization of its magnetic counterpart, Ca3Mn2O7, remained elusive.""Multiple transitions as well as phase separations were evidenced in Ca3Mn2O7, making it more complex than the early theoretical expectations,"" Dong said. ""This material is complex, and the leakage is serious, which prevents the direct measurement of its ferroelectricity in high temperature.""To further understand Ca3Mn2O7, Dong and his collaborators confirmed the material's ferroelectricity using pyroelectric measurements that examine its electric properties across a range of temperatures as well as measured Ca3Mn2O7's ferroelectric hysteresis loops, a method that mitigates some extrinsic leakage. Further investigation showed that Ca3Mn2O7 exhibits a weak ferromagnetism that can be modulated by an electric field.It was found that Ca3Mn2O7, a material long-rumored to have ferroelectric and magnetoelectric properties, also exhibited strong visible light absorption in a band gap well suited for photoelectric devices. This feature of Ca3Mn2O7 might pave the way for the material to be used in anything from photovoltaic cells to light sensors with the built-in electric field leading to larger photogenerated voltage than today's devices.""The most surprising thing for us was that no one noticed its prominent light absorption before,"" Dong said.In the future, Dong said he hopes to explore Ca3Mn2O7's photoelectric properties as well as investigate whether introducing iron to the crystal would enhance its magnetism.Story Source:Materials provided by American Institute of Physics. ",0.0870755,0.144252,0.152113,0.188014,0.165511,0.152695,0.167682,0,0
428,"In a warming world, could air conditioning make things worse? ","As climate change continues to push summer temperatures ever higher, the increased use of air conditioning in buildings could add to the problems of a warming world by further degrading air quality and compounding the toll of air pollution on human health, according to a new study.Writing today (July 3, 2018) in a special climate change issue of the journal Public Library of Science (PLOS) Medicine, a team of researchers from the University of Wisconsin-Madison forecasts as many as a thousand additional deaths annually in the Eastern United States alone due to elevated levels of air pollution driven by the increased use of fossil fuels to cool the buildings where humans live and work.""What we found is that air pollution will get worse,"" explains David Abel, the lead author of the new report and a UW-Madison graduate student in the Nelson Institute for Environmental Studies' Center for Sustainability and the Global Environment. ""There are consequences for adapting to future climate change.""The analysis combines projections from five different models to forecast increased summer energy use in a warmer world and how that would affect power consumption from fossil fuels, air quality and, consequently, human health just a few decades into the future.In hot summer weather, and as heat waves are projected to increase in frequency and intensity with climate change, there is no question that air conditioning does and will save lives, says Jonathan Patz, a senior author of the study and a UW-Madison professor of environmental studies and population health sciences.However, he cautions that if the increased use of air conditioning due to climate change depends on power derived from fossil fuels, there will be an air quality and human health tradeoff. ""We're trading problems,"" says Patz, an expert on climate change and human health. ""Heat waves are increasing and increasing in intensity. We will have more cooling demand requiring more electricity. But if our nation continues to rely on coal-fired power plants for some of our electricity, each time we turn on the air conditioning we'll be fouling the air, causing more sickness and even deaths.""Another senior author of the new PLOS Medicine report, air quality expert Tracey Holloway, a UW-Madison professor of environmental studies as well as atmospheric and oceanic sciences, says the study adds to our understanding of the effects of adapting to climate change by simulating the scope of fossil fuel use to cool buildings under future climate change scenarios. Buildings, she notes, are the biggest energy sinks in the United States, responsible for more than 60 percent of power demand in the Eastern United States, the geographic scope of the study. Air conditioning, she says, is a significant component of that electrical demand.""Air quality is a big issue for public health,"" she explains, noting that increases in ground-level ozone and fine particulate matter in the air -- byproducts of burning fossil fuels and known hazards to human health -- will be one result of adding to fossil-fuel power consumption.The study forecasts an additional 13,000 human deaths annually caused by higher summer levels of fine particulate matter and 3,000 caused by ozone in the Eastern U.S. by mid-century. Most of those deaths will be attributable to natural processes like atmospheric chemistry and natural emissions, which are affected by rising temperatures. However, about 1,000 of those deaths each year would occur because of increased air conditioning powered by fossil fuel. ""Climate change is here and we're going to need to adapt,"" says Abel. ""But air conditioning and the way we use energy is going to provide a feedback that will exacerbate air pollution as temperatures continue to get warmer.""The results of the new study, according to the Wisconsin team, underscore the need to change to more sustainable sources of energy such as wind and solar power, and to deploy more energy-efficient air conditioning equipment. ""The answer is clean energy,"" says Abel. ""That is something we can control that will help both climate change and future air pollution. If we change nothing, both are going to get worse.""Story Source:Materials provided by University of Wisconsin-Madison. Original written by Terry Devitt. ",0.106567,0.148567,0.172928,0.158369,0.110903,0.190715,0.167408,0,0
429,Laser technique may open door to more efficient clean fuels,"Research by the University of Liverpool could help scientists unlock the full potential of new clean energy technologies.Finding sustainable ways to replace fossil fuels is a key priority for researchers across the globe. Carbon dioxide (CO2) is a hugely abundant waste product that can be converted into energy-rich by-products, such as carbon monoxide. However, this process needs to be made far more efficient for it to work on a global, industrial scale.Electrocatalysts have shown promise as a potential way to achieve this required efficiency 'step-change' in CO2 reduction, but the mechanisms by which they operate are often unknown making it hard for researchers to design new ones in a rational manner.New research published in Nature Catalysis by researchers at the University's Department of Chemistry, in collaboration with Beijing Computational Science Research Center and STFC Rutherford Appleton Laboratory, demonstrates a laser-based spectroscopy technique that can be used to study the electrochemical reduction of CO2 in-situ and provide much-needed insights into these complex chemical pathways.The researchers used a technique called Vibrational Sum-Frequency Generation (VSFG) spectroscopy coupled with electrochemical experiments to explore the chemistry of a particular catalyst called Mn(bpy)(CO)3Br, which is one of the most promising and intensely studied CO2 reduction electrocatalysts.Using VSFG the researchers were able to observe key intermediates that are only present at an electrode surface for a very short time -- something that has not been achieved in previous experimental studies.At Liverpool, the work was carried out by the Cowan Group, a team of researchers who study and develop new catalytic systems for the sustainable production of fuels.Dr Gaia Neri, who was part of the Liverpool team, said: ""A huge challenge in studying electrocatalysts in situ is having to discriminate between the single layer of short-lived intermediate molecules at the electrode surface and the surrounding 'noise' from inactive molecules in the solution.""We've shown that VSFG makes it possible to follow the behaviour of even very short-lived species in the catalytic cycle. This is exciting as it provides researchers with new opportunities to better understand how electrocatalysts operate, which is an important next step towards commercialising the process of electrochemical CO2 conversation into clean fuel technologies.""Following on from this research, the team is now working to further improve the sensitivity of the technique and is developing a new detection system that will allow for a better signal-to-noise ratio.The study was funded by the Engineering and Physical Sciences Research Council (EPSRC).Story Source:Materials provided by University of Liverpool. ",0.0589846,0.151499,0.101281,0.156113,0.0874585,0.114388,0.142084,0,0
430,Efficient electrochemical cells for carbon dioxide conversion,"Scientists at Stanford University have developed electrochemical cells that convert carbon monoxide (CO) derived from CO2 into commercially viable compounds more effectively and efficiently than existing technologies. Their research, published October 25 in the journal Joule, provides a new strategy for capturing CO2 and converting it into chemical feedstocks.CO2 capture from emission sources is an attractive option for mitigating climate change, but it is an expensive process that harvests a product without commercial value. However, scientists can add value to captured CO2 by using electrolysis, a technique that uses an electric current to break down compounds, to convert it into more desirable products such as ethylene for polymer production or acetate as a reagent for chemical synthesis.""C2 products such as ethylene, acetate, and ethanol are inherently more valuable than C1 products such as methane because they are versatile chemical feedstocks,"" says senior author Matthew Kanan, an associate professor of chemistry at Stanford University.While converting CO2 to CO is already commercially possible, developing technology that can produce in-demand C2 chemicals from CO on an industrial scale is still a challenge. Electrolysis must convert CO into products at a high rate with a low overall energy demand in order to be viable. Previous electrochemical cells have required a large excess of CO to achieve a high electrolysis rate, which results in dilute products that must be concentrated and purified -- a process that requires more energy (at greater expense).The electrochemical cells created by Kanan and his team combat these inefficiencies with a modified design that produces a concentrated stream of ethylene gas and a sodium acetate solution 1,000 times more concentrated than product obtained with previous cells. The cell uses a gas diffusion electrode (GDE) combined with a carefully designed flow field that greatly improves the delivery of CO to the electrode surface and the removal of products. The team also eliminated the need for an electrolyte solution in the cell by interfacing the GDE directly with a membrane. As a result, both ethylene and concentrated acetate solution are produced at the electrode and swept out of the cell in a single vapor stream.""Prior to this work, the combination of a high electrolysis rate, high CO conversion, and concentrated product streams had not been achieved,"" says Kanan.The team is currently scaling up their prototype to determine whether the design needs to be modified to succeed on an industrial scale, with hopes that they can eventually combine their CO electrolysis cells with existing technologies for converting CO2 into CO. The device may also be useful for space exploration, in particular deep space missions where it is not possible to resupply from Earth. In collaboration with researchers led by John Hogan at the NASA Ames Research Center, the team is working to combine electrochemical synthesis with microbial biosynthesis to recycle the CO2 breathed out by astronauts into food and nutrients.This research was supported by NASA and the Global Climate and Energy Project.Story Source:Materials provided by Cell Press. ",0.0913866,0.234798,0.124986,0.159166,0.121629,0.148924,0.153428,0,0
431,Nanodiamonds as photocatalysts,"Climate change is in full swing and will continue unabated as long as we do not succeed in significantly reducing CO2 emissions. For this we need all the options. One idea is to return the greenhouse gas CO2 to the energy cycle: CO2 could be processed with water into methanol, a fuel that can be excellently transported and stored. However, the reaction, which is reminiscent of a partial process of photosynthesis, requires energy and catalysts. If we succeed in using this energy from sunlight and developing light-active photocatalysts that are not made of rare metals such as platinum, but of inexpensive and abundantly available materials, there would be a chance of ""green"" solar fuels being produced in a climate-neutral way.Diamond Nanomaterials need UV for activationA candidate for such photocatalysts are so-called diamond nanomaterials -- these are not precious crystalline diamonds, but tiny nanocrystals of a few thousand carbon atoms that are soluble in water and look more like black slurry, or nanostructured ""carbon foams"" with high surface area. In order for these materials to become catalytically active, however, they require UV light excitation. Only this spectral range of sunlight is rich enough in energy to transport electrons from the material into a ""free state."" Only then solvated electrons can be emitted in water and react with the dissolved 2 to form methanol.Can doping help?However, the UV component in the solar spectrum is not very high. Photocatalysts that could also use the visible spectrum of sunlight would be ideal. This is where the work of HZB-scientist Tristan Petit and his cooperation partners in DIACAT comes in: modelling the energy levels in such materials, performed by Karin Larsson in Uppsala University, shows that intermediate stages can be built into the band gap by doping with foreign atoms. Boron, a trivalent element, appears to be particularly important.Experiments at BESSY II show: yes, but...Petit and his team therefore investigated samples of polycrystalline diamonds, diamond foams and nanodiamonds. These samples had previously been synthesized in the groups of Anke Kreger in Werzburg and Christoph Nebel in Freiburg. At BESSY II, X-ray absorption spectroscopy was used to precisely measure the unoccupied energy states where electrons could possibly be excited by visible light. ""The boron atoms present near the surface of these nanodiamonds actually lead to the desired intermediate stages in the band gap,"" explains Ph.D student Sneha Choudhury, first author of the study. These intermediate stages are typically very close to the valence bands and thus do not allow the effective use of visible light. However, the measurements show that this also depends on the structure of the nanomaterials.Outlook: Morphology and doping with P or N""We can introduce and possibly control such additional steps in the diamond bandgap by specifically modifying the morphology and doping,"" says Tristan Petit. Doping with phosphorus or nitrogen could also offer new opportunities.Story Source:Materials provided by Helmholtz-Zentrum Berlin fer Materialien und Energie. ",0.0789473,0.165306,0.132071,0.157257,0.127906,0.126638,0.157876,0,0
432,Novel use of NMR sheds light on easy-to-make electropolymerized catalysts,"In the world of catalytic reactions, polymers created through electropolymerization are attracting renewed attention. A group of Chinese researchers recently provided the first detailed characterization of the electrochemical properties of polyaniline and polyaspartic acid (PASP) thin films. In AIP Advances, from AIP Publishing, the team used a wide range of tests to characterize the polymers, especially their capacity for catalyzing the oxidation of popularly used materials, hydroquinone and catechol.This new paper marks one of the first pairings of standard electrochemical tests with nuclear magnetic resonance (NMR) analysis in such an application. ""Because these materials can be easily prepared in an electric field and are cost-effective and environmentally friendly, we think they have the potential to be widely used,"" said Shuo-Hui Cao, an author on the paper.Although PASP has shown excellent electrocatalytic responses to biological molecules, newer areas of inquiry have explored the material's ability to lower the oxidational potential in oxidation-reduction reactions. Reducing the oxidation potential is key for finding further uses for two materials used extensively as raw materials and synthetic intermediates in pharmaceuticals, hydroquinone and catechol.Conductive polymers, like polyaniline, have attracted attention for their high conductivity and low cost. To better understand these materials, Cao and his colleagues tested how well PASP and polyaniline were able to oxidize hydroquinone and catechol using several standard electrochemical techniques, including attenuated total reflection Fournier transform infrared spectrophotometry, cyclic voltammetry and electrochemical impedance spectroscopy.Using proton-based NMR, they monitored the progress of each reaction by directly measuring how quickly reactants were used and products were created. Cao said that their work using NMR analysis on catechol looks to fill a gap they found in the literature.""The NMR technique allows us to find out more about their molecular structure and better compare the catalysts' characteristics quantitatively,"" Cao said.The group discovered that the polymer-modified electrodes both improved conductivity. PASP's catalytic activity of both hydroquinone and catechol was found to outpace that of polyaniline by a factor of two. Later NMR studies confirmed that electrically induced molecular transformations allowed PASP to serve as a better catalyst.The findings led the researchers to postulate that polyaspartic acid electropolymerized thin films might be more suitable for use as catalysts over polyaniline in many situations.Cao said he hopes to further develop NMR techniques that pair with electrochemical testing. So far, the group has used a type of NMR that incorporates one dimension of frequency analysis. In addition to being able to examine new material features, using two-dimensional techniques will allow the group to extend their work to more complicated molecules.Story Source:Materials provided by American Institute of Physics. ",0.0773238,0.205928,0.119553,0.150654,0.117415,0.135518,0.148478,0,0
433,"A first 'snapshot' of the complete spectrum of neutrinos emitted by the sun New results from Borexino, one of the most sensitive neutrino detectors on the planet","About 99 percent of the Sun's energy emitted as neutrinos is produced through nuclear reaction sequences initiated by proton-proton (pp) fusion in which hydrogen is converted into helium, say scientists including physicist Andrea Pocar at the University of Massachusetts Amherst. Today they report new results from Borexino, one of the most sensitive neutrino detectors on the planet, located deep beneath Italy's Apennine Mountains.""Neutrinos emitted by this chain represent a unique tool for solar and neutrino physics,"" they explain. Their new paper in Nature reports on ""the first complete study of all the components of the pp-chain performed by Borexino."" These components include not only the pp neutrinos, but others called Beryllium-7 (7Be), pep and Boron-8 (8B) neutrinos. The pp fusion reaction of two protons to produce deuteron, nuclei of deuterium, is the first step of a reaction sequence responsible for about 99 percent of the Sun's energy output, Pocar says.He adds, ""What's new today is incremental, it's not a leap, but it is the crowning of more than 10 years of data-taking with the experiment to show the full energy spectrum of the Sun at once. Our results reduce uncertainty, which is perhaps not flashy but it's type of advance that is often not recognized enough in science. The value is that measurements get more precise because with more data and thanks to the work of dedicated young physicists, we have a better understanding of the experimental apparatus.""""Borexino offers the best measurement ever made for the pp, 7Be and pep neutrinos,"" he adds. ""Other experiments measure the 8B neutrinos more precisely, but our measurement, with a lower threshold, is consistent with them.""Further, ""Once you have more precise data, you can feed it back into the model of how the Sun is behaving, then the model can be refined even more. It all leads to understanding the Sun better. Neutrinos have told us how the Sun is burning and, in turn, the Sun has provided us with a unique source to study how neutrinos behave. Borexino, scheduled to run for another two to three years, has strengthened our understanding of the Sun very profoundly.""For earlier studies of pp, 7B, pep and 8B neutrinos, the team had focused on each one separately in targeted analyses of the collected data in restricted windows of energy, ""like trying to characterize a forest by taking one picture each of many individual types of trees,"" Pocar notes. ""Multiple pictures give you an idea of a forest, but it's not the same as the photo of the entire forest.""""What we have done now is take a single photo that reflects the whole forest, the whole spectrum of all the different neutrinos in one. Instead of zooming in to look at little pieces, we see it all at once. We understand our detector so well now, we are comfortable and confident that our one shot is valid for the whole spectrum of neutrino energies.""Solar neutrinos stream out of the star at the center of our system at nearly the speed of light, as many as 420 billion hitting every square inch of Earth's surface per second. But because they only interact through the nuclear weak force, they pass through matter virtually unaffected, which makes them very difficult to detect and distinguish from trace nuclear decays of ordinary materials, Pocar says.The Borexino instrument detects neutrinos as they interact with the electrons of an ultra-pure organic liquid scintillator at the center of a large sphere surrounded by 1,000 tons of water. Its great depth and many onion-like protective layers maintain the core as the most radiation-free medium on the planet. It is the only detector on Earth capable of observing the entire spectrum of solar neutrino simultaneously, which has now been accomplished, he notes.The UMass Amherst physicist, one principal investigator on a team of more than 100 scientists, is particularly interested in now turning his focus to measure yet another type of solar neutrino known as CNO neutrinos, which he hopes will be useful in addressing an important open question in stellar physics, that is the metallicity, or metal content, of the Sun.""There are two models that predict different levels of elements heavier than helium, which for astronomers is a metal, in the Sun; a lighter metallicity and a heavier model,"" he notes. CNO neutrinos are emitted in a cyclic fusion reaction sequence different from the pp chain and subdominant in the Sun, but thought to be the main source of power for heavier stars. The CNO solar neutrino flux is greatly affected by the solar metallicity.Pocar says, ""Our data is possibly showing some slight preference for heavy metallicity, so we'll be looking into that because neutrinos from the Sun, especially CNO, can help us disentangle this.""Borexino is an international collaboration funded by National Science Foundation in the United States, the Italian National Institute for Nuclear Physics that manages the Gran Sasso labs, and funding agencies in Germany, Russia and Poland.Story Source:Materials provided by University of Massachusetts at Amherst. ",0.0670273,0.0906443,0.105733,0.12895,0.0918478,0.104366,0.120466,0,0
434,"North Korea's 2017 bomb test set off later earthquakes, new analysis finds Close look at explosion's aftereffects","Using newly refined analysis methods, scientists have discovered that a North Korean nuclear bomb test last fall set off aftershocks over a period of eight months. The shocks, which occurred on a previously unmapped nearby fault, are a window into both the physics of nuclear explosions, and how natural earthquakes can be triggered. The findings are described in two papers just published online in the journal Seismological Research Letters.The September 3, 2017 underground test was North Korea's sixth, and by far largest yet, yielding some 250 kilotons, or about 17 times the size of the bomb that destroyed Hiroshima. Many experts believe the device was a hydrogen bomb -- if true, a significant advance from cruder atomic devices the regime previously exploded. The explosion itself produced a magnitude 6.3 earthquake. This was followed 8.5 minutes later by a magnitude 4 quake, apparently created when an area above the test site on the country's Mt. Mantap collapsed into an underground cavity occupied by the bomb.The test and collapse were picked up by seismometers around the world and widely reported at the time. But later, without fanfare, seismic stations run by China, South Korea and the United States picked up 10 smaller shocks, all apparently scattered within 5 or 10 kilometers around the test site. The first two came on Sept. 23, 2017; the most recent was April 22, 2018. Scientists assumed the bomb had shaken up the earth, and it was taking a while to settle back down. ""It's not likely that there would be so many events in that small area over a small period of time,"" said the lead author of one of the studies, Won-Young Kim, a seismologist at Columbia University's Lamont-Doherty Earth Observatory. ""These are probably triggered due to the explosion.""After looking at the series of aftershock reports, Kim's group sifted more closely through the data and spotted three other aftershocks that had not previously been recognized, for a total of 13. The tremors were all modest, all between magnitude 2.1 and 3.4, and almost certainly harmless. In the past they would have been hard to pick out using far-off seismometers, he said. However, under new international cooperation agreements, he and colleagues obtained recordings from relatively nearby instruments including ones in Ussuriysk, Russia, a borehole in South Korea, and Mudanjiang, northeast China.The group then used a new analysis method developed in part by Lamont seismologist David Schaff that looks at energy waves that are much lower frequency and slower-moving seismic than those used in conventional earthquake analyses. These slow-moving waves allowed Schaff and the rest of the team to pinpoint the locations of the quakes with far greater precision than with conventional recordings. Instead of the random scatter initially seen, the quake locations lined up in a neat 700-meter-long row about 5 kilometers northwest of the blast -- indication of a hidden fracture.Seismometers have long been routinely used to verify nuclear test treaties, and scientists have become increasingly confident that they can detect even small tests and distinguish them from natural earthquakes. But the link between explosions and subsequent quakes is less studied. Seismologists documented a handful of apparent aftershocks near a Nevada test site in the 1970s, and near a Soviet test site in Kazakhstan in 1989. However, they were not able to pinpoint the locations of these quakes with the technology then available. With more instruments and the new analysis method, 'now we can see everything,"" said Paul Richards, a Lamont seismologist who coauthored the papers. ""It's a radical improvement in cataloging even tiny, tiny earthquakes. It shows not just what we can do with natural earthquakes, but that we can monitor what the North Koreans are doing. North Korea can't do anything at all now [in secret] and expect to get away with it.""Richards said the exact location of tiny quakes could also help in the so far largely fruitless quest by some seismologists to predict bigger quakes. Richards did not assert that quakes could eventually be predicted, but said, ""If you're ever going to do this, you have to understand locations, and how one earthquake affects its neighbors.""This spring, the North Koreans made a show of blowing up part of the Mt. Mantap site, though it may already have become largely unusable due to the destruction caused by previous explosions. And no nuclear tests have been detected since North Korean leader Kim Jong Un and U.S. president Donald Trump met in June to discuss ending North Korea's tests. However, despite boasts by Trump that North Korea's program has been neutralized, U.S. diplomats have noted evidence suggesting that the North continues to quietly develop its weapons.Lamont scientists have studied previous North Korean tests, including ones in 2013 and 2009; they concluded that reports of one in 2010 was a false alarm. The current studies were coauthored by Eunyoung Jo and Yonggyu Ryoo of the Korea Meteorological Administration.Story Source:Materials provided by Lamont-Doherty Earth Observatory, Columbia University. Original written by Kevin Krajick. ",0.0898199,0.0870504,0.162014,0.14533,0.0997788,0.149901,0.125786,0,0
435,Atomic movie of melting gold could help design materials for future fusion reactors,"Researchers at the Department of Energy's SLAC National Accelerator Laboratory have recorded the most detailed atomic movie of gold melting after being blasted by laser light. The insights they gained into how metals liquefy have potential to aid the development of fusion power reactors, steel processing plants, spacecraft and other applications where materials have to withstand extreme conditions for long periods of time.Nuclear fusion is the process that powers stars like the sun. Scientists want to copy this process on Earth as a relatively clean and safe way of generating virtually unlimited amounts of energy. But to build a fusion reactor, they need materials that can survive being exposed to temperatures of a few hundred millions of degrees Fahrenheit and intense radiation produced in the fusion reaction.""Our study is an important step toward better predictions of the effects extreme conditions have on reactor materials, including heavy metals such as gold,"" said SLAC postdoctoral researcher Mianzhen Mo, one of the lead authors of a study published today in Science. ""The atomic-level description of the melting process will help us make better models of the short- and long-term damage in those materials, such as crack formation and material failure.""The study used SLAC's high-speed electron camera -- an instrument for ultrafast electron diffraction (UED) -- which is capable of tracking nuclear motions with a shutter speed of about 100 millionths of a billionth of a second, or 100 femtoseconds.Melting in PocketsThe team discovered that the melting started at the surfaces of nanosized grains within the gold sample -- regions in which the gold atoms neatly line up in crystals -- and at the boundaries between them.""This behavior had been predicted in theoretical studies, but we've now actually observed it for the first time,"" said Siegfried Glenzer, head of SLAC's High Energy Density Science Division and the study's principal investigator. ""Our method allows us to examine the behavior of any material in extreme environments in atomic detail, which is key to understanding and predicting material properties and could open up new avenues for the design of future materials.""To study the melting process, the researchers focused the laser beam onto a sample of gold crystals and watched how the atomic nuclei in the crystals responded, using the UED instrument's electron beam as a probe. By stitching together snapshots of the atomic structure taken at various times after the laser hit, they created a stop-motion movie of the structural changes over time.""About 7 to 8 trillionths of a second after the laser flash, we saw the solid begin turning into a liquid,"" said SLAC postdoctoral researcher Zhijang Chen, one of the study's lead authors. ""But the solid didn't liquefy everywhere at the same time. Instead, we observed the formation of pockets of liquid surrounded by solid gold. This mix evolved over time until only liquid was left after about a billionth of a second.""Superb 'Electron Vision'To get to this level of detail, the researchers needed a special camera like SLAC's UED instrument, which is able to see the atomic makeup of materials and is fast enough to track extremely rapid motions of atomic nuclei.And because the melting process is destructive, another feature of the instrument was also absolutely crucial.""In our experiment, the sample ultimately melted and vaporized,"" said accelerator physicist Xijie Wang, head of SLAC's UED initiative. ""But even if we were able to cool it down so that it becomes a solid again, it wouldn't have the exact same starting structure. So, for every frame of the atomic movie we want to collect all the structural information in a single-shot experiment -- a single pass of the electron beam through the sample. We were able to do just that because our instrument uses a very energetic electron beam that produces a strong signal.""Video: https://www.youtube.com/watch?v=PRPpMCT9ZycStory Source:Materials provided by DOE/SLAC National Accelerator Laboratory. ",0.0613158,0.135764,0.120876,0.142493,0.105891,0.106023,0.140008,0,0
436,Nuclear physicists leap into quantum computing with first simulations of atomic nucleus,"Scientists at the Department of Energy's Oak Ridge National Laboratory are the first to successfully simulate an atomic nucleus using a quantum computer. The results, published in Physical Review Letters, demonstrate the ability of quantum systems to compute nuclear physics problems and serve as a benchmark for future calculations.Quantum computing, in which computations are carried out based on the quantum principles of matter, was proposed by American theoretical physicist Richard Feynman in the early 1980s. Unlike normal computer bits, the qubit units used by quantum computers store information in two-state systems, such as electrons or photons, that are considered to be in all possible quantum states at once (a phenomenon known as superposition).""In classical computing, you write in bits of zero and one,"" said Thomas Papenbrock, a theoretical nuclear physicist at the University of Tennessee and ORNL who co-led the project with ORNL quantum information specialist Pavel Lougovski. ""But with a qubit, you can have zero, one, and any possible combination of zero and one, so you gain a vast set of possibilities to store data.""In October 2017 the multidivisional ORNL team started developing codes to perform simulations on the IBM QX5 and the Rigetti 19Q quantum computers through DOE's Quantum Testbed Pathfinder project, an effort to verify and validate scientific applications on different quantum hardware types. Using freely available pyQuil software, a library designed for producing programs in the quantum instruction language, the researchers wrote a code that was sent first to a simulator and then to the cloud-based IBM QX5 and Rigetti 19Q systems.The team performed more than 700,000 quantum computing measurements of the energy of a deuteron, the nuclear bound state of a proton and a neutron. From these measurements, the team extracted the deuteron's binding energy -- the minimum amount of energy needed to disassemble it into these subatomic particles. The deuteron is the simplest composite atomic nucleus, making it an ideal candidate for the project.""Qubits are generic versions of quantum two-state systems. They have no properties of a neutron or a proton to start with,"" Lougovski said. ""We can map these properties to qubits and then use them to simulate specific phenomena -- in this case, binding energy.""A challenge of working with these quantum systems is that scientists must run simulations remotely and then wait for results. ORNL computer science researcher Alex McCaskey and ORNL quantum information research scientist Eugene Dumitrescu ran single measurements 8,000 times each to ensure the statistical accuracy of their results.""It's really difficult to do this over the internet,"" McCaskey said. ""This algorithm has been done primarily by the hardware vendors themselves, and they can actually touch the machine. They are turning the knobs.""The team also found that quantum devices become tricky to work with due to inherent noise on the chip, which can alter results drastically. McCaskey and Dumitrescu successfully employed strategies to mitigate high error rates, such as artificially adding more noise to the simulation to see its impact and deduce what the results would be with zero noise.""These systems are really susceptible to noise,"" said Gustav Jansen, a computational scientist in the Scientific Computing Group at the Oak Ridge Leadership Computing Facility (OLCF), a DOE Office of Science User Facility located at ORNL. ""If particles are coming in and hitting the quantum computer, it can really skew your measurements. These systems aren't perfect, but in working with them, we can gain a better understanding of the intrinsic errors.""At the completion of the project, the team's results on two and three qubits were within 2 and 3 percent, respectively, of the correct answer on a classical computer, and the quantum computation became the first of its kind in the nuclear physics community.The proof-of-principle simulation paves the way for computing much heavier nuclei with many more protons and neutrons on quantum systems in the future. Quantum computers have potential applications in cryptography, artificial intelligence, and weather forecasting because each additional qubit becomes entangled -- or tied inextricably -- to the others, exponentially increasing the number of possible outcomes for the measured state at the end. This very benefit, however, also has adverse effects on the system because errors may also scale exponentially with problem size.Papenbrock said the team's hope is that improved hardware will eventually enable scientists to solve problems that cannot be solved on traditional high-performance computing resources -- not even on the ones at the OLCF. In the future, quantum computations of complex nuclei could unravel important details about the properties of matter, the formation of heavy elements, and the origins of the universe.Story Source:Materials provided by DOE/Oak Ridge National Laboratory. ",0.0515514,0.0892032,0.0949759,0.226594,0.0938643,0.106529,0.14806,0,0
437,Innate 'fingerprint' could detect tampered steel parts Treaty compliance aided by spotting illicit artillery exchange and duplication,"Researchers using magnetic signals have found unique ""fingerprints"" on steel, which could help to verify weapons treaties and reduce the use of counterfeit bolts in the construction industry.""Magnetic signals provide a wide range of possible national security applications,"" said David Mascareeas, a research and development engineer at Los Alamos National Laboratory and lead author of the study published recently in the journal Smart Materials and Structures. ""It's a promising phenomenon that we hope to leverage to uniquely identify different pieces of artillery.""In the research, funded by the U.S Department of State, researchers applied Barkhausen noise, a magnetic phenomenon, to two types of steel -- conventional steel and an abrasive-resistant type of steel used in mining equipment. A sensor measured electromagnetic signals by repeatedly scanning the different kinds of steel over a period of time. Researchers compared the signals from those two sets of scanned images and found signatures that were instrinsic to each type of steel.The variations that occur from the production of various kinds of steel are reflected as distinct fingerprints. ""They seem to be repeatable,"" said Mascareeas.That intrinsic signature could help to discover counterfeit or low-grade steel parts in construction by looking for differences in the electromagnetic signatures. ""It could also help solve that big problem in that industry,"" said Mascareeas.Future research could involve studying other types of steel and developing a handheld sensor for treaty verification.Publication: Barkhausen noise as an intrinsic fingerprint for ferromagnetic components, in Smart Materials and Structures. Authors: David Mascareeas, Michelle Lockhart, Thomas Lienert Funding: U.S Department of State -- Key Verifications Assets Fund.About Los Alamos National Laboratory Los Alamos National Laboratory, a multidisciplinary research institution engaged in strategic science on behalf of national security, is operated by Triad, a public service oriented, national security science organization equally owned by its three founding members: Battelle Memorial Institute (Battelle), the Texas A&M University System (TAMUS), and the Regents of the University of California (UC) for the Department of Energy's National Nuclear Security Administration.Los Alamos enhances national security by ensuring the safety and reliability of the U.S. nuclear stockpile, developing technologies to reduce threats from weapons of mass destruction, and solving problems related to energy, environment, infrastructure, health, and global security concerns.Story Source:Materials provided by DOE/Los Alamos National Laboratory. ",0.0943034,0.118478,0.176041,0.217043,0.131752,0.164359,0.163017,0,0
438,Levitating particles could lift nuclear detective work Measuring recoil from nuclear decay identifies isotopes,"Laser-based 'optical tweezers' could levitate uranium and plutonium particles, thus allowing the measurement of nuclear recoil during radioactive decay. This technique, proposed by scientists at Los Alamos National Laboratory, provides a new method for conducting the radioactive particle analysis essential to nuclear forensics.""Our idea relies on trapping a particle using 'optical tweezers,' a technique which is the subject of this year's Nobel prize in Physics,"" said Alonso Castro of the Lab's Actinide Analytical Chemistry group, one of the authors of a new paper in the journal Physical Review A. ""It is only fitting that [Nobel prize winner] Arthur Ashkin's invention is still yielding novel science even after 30 years.""The team's work shows that, for micrometer and sub-micrometer particles, the kinetic energy of the emitted nuclear particle, or daughter particle, can be determined very accurately by measuring the recoil of the sample particle levitated in a laser-powered optical trap. The sample particle fully absorbs the recoil momentum of the daughter atom resulting in a well-defined oscillation in the harmonic potential of the trap.""For applications such as nuclear forensic analysis, this technique could have significant value. Thus far, we have been able to hold uranium particles in optical traps, and we find that holding them in place, with their oscillation energy 'cooled,' gives us the opportunity to see the recoil when the daughter atom's motion causes displacement of the sample,"" said Castro.""The next step would be to measure those displacements and then calculate the energy of the recoil. The impact of our approach is that it can very quickly determine the isotopic composition of the types of nanometer and micrometer-sized particles found in nuclear forensic scenarios,"" he said.Nuclear forensics is the examination of nuclear and other radioactive materials to determine the origin and history of the materials in the context of law enforcement investigations or the assessment of nuclear security vulnerabilities. Los Alamos conducts a wide range of nuclear forensic science in support of the Laboratory's national security mission.Story Source:Materials provided by DOE/Los Alamos National Laboratory. ",0.114643,0.171521,0.183317,0.207578,0.157686,0.175901,0.213808,0,0
439,"A faster, cheaper path to fusion energy New, powerful magnets key to building the world's first energy-producing fusion experiment","Scientists are working to dramatically speed up the development of fusion energy in an effort to deliver power to the electric grid soon enough to help mitigate impacts of climate change. The arrival of a breakthrough technology -- high-temperature superconductors, which can be used to build magnets that produce stronger magnetic fields than previously possible -- could help them achieve this goal. Researchers plan to use this technology to build magnets at the scale required for fusion, followed by construction of what would be the world's first fusion experiment to yield a net energy gain.The effort is a collaboration between Massachusetts Institute of Technology's Plasma Science & Fusion Center and Commonwealth Fusion Systems, and they will present their work at the American Physical Society Division of Plasma Physics meeting in Portland, Ore.Fusion power is generated when nuclei of small atoms combine into larger ones in a process that releases enormous amounts of energy. These nuclei, typically heavier cousins of hydrogen called deuterium and tritium, are positively charged and so feel strong repulsion that can only be overcome at temperatures of hundreds of millions of degrees. While these temperatures, and thus fusion reactions, can be produced in modern fusion experiments, the conditions required for a net energy gain have not yet been achieved.One potential solution to this could be increasing the strength of the magnets. Magnetic fields in fusion devices serve to keep these hot ionized gases, called plasmas, isolated and insulated from ordinary matter. The quality of this insulation gets more effective as the field gets stronger, meaning that one needs less space to keep the plasma hot. Doubling the magnetic field in a fusion device allows one to reduce its volume -- a good indicator of how much the device costs -- by a factor of eight, while achieving the same performance. Thus, stronger magnetic fields make fusion smaller, faster and cheaper.A breakthrough in superconductor technology could allow fusion power plants to come to fruition. Superconductors are materials that allow currents to pass through them without losing energy, but to do so they must be very cold. New superconducting compounds, however, can operate at much higher temperatures than conventional superconductors. Critical for fusion, these superconductors function even when placed in very strong magnetic fields.While originally in a form not useful for building magnets, researchers have now found ways to manufacture high-temperature superconductors in the form of ""tapes"" or ""ribbons"" that make magnets with unprecedented performance. The design of these magnets is not suited for fusion machines because they are much too small. Before the new fusion device, called SPARC, can be built, the new superconductors must be incorporated into the kind of large, strong magnets needed for fusion.Once the magnet development is successful, the next step will be to construct and operate the SPARC fusion experiment. SPARC will be a tokamak fusion device, a type of magnetic confinement configuration similar to many machines already in operation.As an accomplishment analogous to the Wright brothers' first flight at Kitty Hawk, demonstrating a net energy gain, the aim of fusion research for more than 60 years, could be enough to put fusion firmly into national energy plans and launch commercial development. The goal is to have SPARC operational by 2025.Story Source:Materials provided by American Physical Society. ",0.138071,0.15575,0.173447,0.21724,0.162725,0.214785,0.211271,0,0
440,New quantum criticality discovered in superconductivity,"Using solid state nuclear magnetic resonance (ssNMR) techniques, scientists at the U.S. Department of Energy's Ames Laboratory discovered a new quantum criticality in a superconducting material, leading to a greater understanding of the link between magnetism and unconventional superconductivity.Most iron-arsenide superconductors display both magnetic and structural (or nematic) transitions, making it difficult to understand the role they play in superconducting states. But a compound of calcium, potassium, iron, and arsenic, and doped with small amounts of nickel, CaK(Fe1-xNix)4As4, first made at Ames Laboratory, has been discovered to exhibit a new magnetic state called a hedgehog spin-vortex crystal antiferromagnetic state without nematic transitions.""Spin or nematic fluctuations can be considered to play an important role for unconventional superconductivity,"" said Yuji Furukawa, a senior scientist at Ames Laboratory and a professor of Physics and Astronomy at Iowa State University. ""With this particular material, we were able to examine only the magnetic fluctuations, and NMR is one of the most sensitive techniques for examining them."" He continued, ""using 75As NMR, we discovered that CaKFe4As4 is located at a hedgehog spin-vortex crystal antiferromagnetic quantum critical point which is avoided due to superconductivity. The discovery of the magnetic quantum criticality without nematicity in CaK(Fe1?xNix)4As4 suggests that the spin fluctuations are the primary driver of superconductivity.""Furukawa's discovery was a collaboration between Ames Laboratory's world-leading SSNMR team and the lab's condensed matter physicists, including Paul Canfield, a senior scientist at Ames Laboratory and a Distinguished Professor and the Robert Allen Wright Professor of Physics and Astronomy at Iowa State University.""This is a new type of magnetic order,"" said Canfield. ""You have this interesting interaction between superconductivity and magnetism from high temperatures in the normal state. This gives us some sense that this high temperature superconductivity may be coming from this near quantum critical antiferromagnetic transition.""Story Source:Materials provided by DOE/Ames Laboratory. ",0.178384,0.199973,0.180472,0.203396,0.209546,0.243761,0.197447,0,0
441,Ultrafast optical fiber-based electron gun to reveal atomic motions,"One of the most enduring ""Holy Grail"" experiments in science has been attempts to directly observe atomic motions during structural changes. This prospect underpins the entire field of chemistry because a chemical process occurs during a transition state -- the point of no return separating the reactant configuration from the product configuration.What does that transition state look like and, given the enormous number of different possible nuclear configurations, how does a system even find a way to make it happen?Now in the journal Applied Physics Letters, from AIP Publishing, researchers at the Max Planck Institute for the Structure and Dynamics of Matter are reporting ""ultrabright"" electron sources with sufficient brightness to literally light up atomic motions in real time -- at a time scale of 100 femtoseconds, making these sources particularly relevant to chemistry because atomic motions occur in that window of time.After seeing the first atomic movies of phase transitions in bulk thin films using high-energy (100 kilovolt) electron bunches, the researchers wondered if they could achieve atomic resolution of surface reactions -- occurring within the first few monolayers of materials -- to gain a better understanding of surface catalysis.So they devised a low-energy (1-2 kilovolt) time-resolved electron diffraction concept of using fiber optics for miniaturization and the ability to stretch the electron pulse, then apply streak camera technology to potentially obtain subpicosecond temporal resolution -- a difficult feat within the low-electron energy regime.""The first atomic movies use a stroboscopic approach akin to an old 8-millimeter camera, frame by frame, in which a laser excitation pulse triggers the structure, then an electron pulse is used to light up the atomic positions,"" said co-author Dwayne Miller. ""We believed that a streak camera could get a whole movie in one shot within the window defined by the deliberately stretched electron pulse. It solves the problem of low electron numbers and greatly improves image quality.""Of the myriad possible nuclear configurations, the group discovered that the system collapses to just a few key modes that direct chemistry and that a reduction in dimensionality that occurs in the transition state or barrier-crossing region can be inferred. ""We see it directly with the first atomic movies of ring closing, electron transfer and bond breaking,"" said Miller.Story Source:Materials provided by American Institute of Physics. ",0.0957994,0.155498,0.122558,0.18047,0.140664,0.164588,0.164436,0,0
442,A new lead on a 50-year-old radiation damage mystery,"For half a century, researchers have seen loops of displaced atoms appearing inside nuclear reactor steel after exposure to radiation, but no one could work out how.Now, a simulation done by researchers at the University of Michigan, Hunan University (China) and Rensselaer Polytechnic Institute has shown that a shockwave produces these loops in iron. The result could help engineers design better radiation-resistant steel for reactors -- or stronger steel in general.Iron and steel, like most metals, organize themselves in a crystal lattice -- an arrangement of atoms based on a repeated pattern. In this case, it's a cube with an atom at each corner and one in the center. Radiation and other stresses can create a variety of defects.In ""loop"" defects, the out-of-place atoms form rough rings. Some loops can travel through the lattice, and their mobility means that they don't get in the way of the steel bending. But the defect in question (known as a <100> interstitial dislocation loop) tends to stay put. Placed in an uncontrolled way, these stationary loops cause brittleness, but if they were placed deliberately, they could strengthen steel by improving its stiffness.""Now that we know the mechanism, we can reduce radiation damage by limiting the energy of the particles that materials are exposed to,"" said Qing Peng, a research fellow in the lab of Fei Gao, a professor of nuclear engineering and radiological sciences.""We can also use it to engineer the defect inside materials. Depending on the energy, you can generate different types of dislocations to tune the material's properties.""Five earlier explanations are in the running to account for the mysterious loops, but none is particularly satisfying because they all require special conditions and relatively long times to create the loops.Since the defects appear too quickly to measure, researchers expected that they would be able to simulate the mechanism on a computer. But that didn't happen either. They supposed that it took too long to map out their real time trajectories -- there just wasn't enough power to simulate all those atoms in a reasonable time.That last observation turned out to be partially true: there were too many atoms to model. But the process itself was short; the trouble was making the volume of iron big enough to get the reaction.""If the simulation is too small, a high-energy particle just passes through. No reaction,"" Peng said.Gao's team created a computer model of a box of 200 million iron atoms, arranged in the typical lattice, and slammed a high energy particle into it. What they saw was a powerful shockwave ripping through the lattice, branching out in different directions.Millions of iron atoms were displaced from their spots, and millions of them fell back into the lattice as the wave dissipated. Left behind were hundreds of ""point"" defects in which single atoms were out of place -- and a handful of loops. Many of these were loops that can travel, which aren't a major cause of brittleness, but often one or two were the stationary type.It turned out that the loops were created in the initial shockwave, a process that takes just 13 trillionths of a second or so. This explanation was floated as early as 40 years ago, but it was used to explain defects that appeared in lines rather than closed loops.Now that the mechanism is known, similar computer modeling may be used to recommend operating conditions for steel alloys in environments with radiation. Less energetic particles won't create shockwaves strong enough to produce this defect.Or, defects like this can be deliberately placed in steel to enhance its rigidity. These stationary loops of atoms, jammed in between other atoms in the crystal, make it harder for steel to bend.Story Source:Materials provided by University of Michigan. ",0.0862375,0.124246,0.14624,0.166603,0.123224,0.154218,0.153252,0,0
443,Identifying an initial growth process of calcium phosphate,"Scientists from various fields succeeded in discovering a new process for early growth of substances that contradicts existing theories of material growth by conducting a fusion study.A joint research team comprising DGIST's research team led by Professor DaeWon Moon from the Department of New Biology and Seoul National University's research team led by Professor Ki Tae Nam from the Department of Material Engineering, and KAIST's research team led by Professor Yong-Hyun Kim, has succeeded in identifying the early stages of material growth through a new analysis method that differs from previous theories of critical nuclear growth.Classical nucleation theory (CNT), which is widely known in the scientific community, particularly in regard to material growth, is recognized throughout the scientific community as it has been published in various textbooks on materials, chemistry, and physics. This theory assumes that critical nuclei are created early in the material growth, that bulk materials are formed later around the critical nuclei, and that the material grows.The researchers analyzed nanoparticles of calcium phosphate, a key component in bones, through 'Time-of-Flight Medium Energy Ion Scattering (TOF-MEIS)' spectroscopy. In addition, Professor Yong-Hyun Kim's research team conducted a study on the early growth of implementing a related theoretical calculation. As a result, it was found that in the early stages of nanomaterial growth, nanoparticles, rather than critical nuclei, continue to grow and transform into materials such as bulk materials.Although the scientific community has recently published a study that differs from the existing theory on the early growth of materials, there have been controversies over the failure to provide direct research results. However, Professor Moon's team presented solid evidence to reverse the existing theory by accurately measuring the size and composition of nanoparticles.The 'Time-of-Flight Medium Energy Ion Scattering (TOF-MEIS)' spectroscopy that the research team has developed for the first time in the world played a major role in measuring the early growth of materials. This nanometric technology can measure the process of material growth with an ionizing current that is 10,000 times lower than the MEIS (Medium Energy Ion Scatter). As such, it was able to quantitatively measure the average size and specific structure of nanoparticles in this study without any specific damage to the ions.Professor Moon said, ""This research was based on a long-term project that took seven years from planning experiments to measuring, writing papers and publishing. Based on the combined research of new nanotechnology, material science and theoretical chemistry, it has produced detailed theoretical calculations and research results that upset existing traditional theories."" He then added, ""In the future, it is expected to be used in research to control the growth and characteristics of nanoparticles.""Story Source:Materials provided by DGIST (Daegu Gyeongbuk Institute of Science and Technology). ",0.0783591,0.151416,0.165657,0.191421,0.116618,0.13453,0.176313,0,0
444,The cosmological lithium problem One of the unknowns in the current description of the Big Bang,"The international collaboration n_TOF, in which a group of University of Seville researchers participated, has made use of the unique capacities of three of the world's nuclear facilities, such as PSI (Paul Scherrer Institute, Suiza), ISOLDE (ISotope On-Line DEvice, CERN) and n_TOF (neutron-Time Of Flight, CERN), to carry out a new experiment aimed at exploring an explanation of the Cosmological Lithium Problem based on a neutron channel.This problem is one of the still unresolved questions of the current standard description of the Big Bang. The new experimental results, their theoretical interpretations and the implications of these have been published in the scientific review Physical Review Letters.Different nuclear reactions responsible for the creation and destruction of atomic nucleuses in the nucleosynthesis during the Big Bang are crucial in the determining the primordial abundance of lithium, the third (and last) chemical element formed during the very early phase of the creation of the Universe.The standard models of the Big Bang that are currently used predict an abundance of Li-7, the main lithium isotope, which is three or four times more than that determined via astronomical observations. Recently, at the n_TOF facility at CERN, the possibility has been investigated of a neutron channel which might be able to increase the destruction rate of the isotope Be-7, the precursor of Li-7, and, therefore, make the calculated and observed cosmological abundance of lithium compatible.""Potentially, a neutron reaction channel might be able to resolve the Cosmological Lithium Problem, which is one of the still unresolved aspects of the current standard description of the Big Bang,"" indicates the University of Seville professor Jose Manuel Quesada.At the SINQ facility at PSI (Villigen Switzerland) the ""uncut"" material, destined to be used in the new experiment, was separated. The material was then sent to the ISOLDE radioactive beam facility at CERN to produce a pure target with less than 0.1 milligrams of Be-7, which was then sent to the n_TOF facility to be included in neutron measurements.This has been the first time that the two CERN facilities dedicated to nuclear physics experiments have carried out an experiment together, using the ISOLDE radioactive ion beam to produce the target necessary for an experiment at n_TOF using the neutron time-of-flight technique.In a previous experiment at n_TOF, the effective section of the 7Be(n,a)4He reaction was measured in a wide range of energies, which allowed for the imposition of strict restrictions on one of the destruction mechanisms of isotope Be-7 during the Big Bang. In this experiment, however, the reaction 7Be(n,p)7Li was measured, extending previously acquired data to a greater range of energies and, therefore, allowing for the updating of the reaction rate used in the calculations in the standard model of the Big Bang.""Although the new data obtained from the experiments at n_TOF allow for the establishment of a much firmer basis for BBN calculations, the conclusion of this project is that neutron channels are not enough to resolve the Cosmological Lithium Problem. The scientific community has a challenge that will require additional efforts to resolve, and this will involve the fields of nuclear astrophysics, astronomic observations, non-standard cosmology and even new physics beyond the Standard Model of particle physics,"" the researchers assure.Story Source:Materials provided by University of Seville. ",0.117095,0.170458,0.163149,0.190831,0.144361,0.183181,0.184312,0,0
445,Neutrons produce first direct 3-D maps of water during cell membrane fusion Research could provide new insights into diseases in which normal cell fusion is disrupted,"New 3D maps of water distribution during cellular membrane fusion are accelerating scientific understanding of cell development, which could lead to new treatments for diseases associated with cell fusion. Using neutron diffraction at the Department of Energy's Oak Ridge National Laboratory, researchers have made the first direct observations of water in lipid bilayers used to model cell membrane fusion.The research, published in Journal of Physical Chemistry Letters, could provide new insights into diseases in which normal cell fusion is disrupted, such as Albers-Schenberg disease (osteopetrosis), help facilitate the development of fusion-based cell therapies for degenerative diseases, and lead to treatments that prevent cell-to-cell fusion between cancer cells and non-cancer cells.When two cells combine during fertilization, or a membrane-bound vesicle fuses during viral entry, neuron signaling, placental development and many other physiological functions, the semi-permeable membrane bilayers between the fusing partners must be merged to exchange their internal contents. As the two membranes approach each other, hydration forces increase exponentially, which requires a significant amount of energy for the membranes to overcome. Mapping the distribution of water molecules is key to understanding the fusion process.Researchers used the small-angle neutron scattering (EQ-SANS) instrument at ORNL's Spallation Neutron Source and the biological small-angle neutron scattering (Bio-SANS) instrument at the High Flux Isotope Reactor, both of which can probe structures as small as a few nanometers in size.""We used neutrons to probe our samples, because water typically can't be seen by x-rays, and because other imaging techniques can't accurately capture the extremely rapid and dynamic process of cellular fusion,"" said Durgesh K. Rai, co-author and now a post-doctoral associate at the Cornell High Energy Synchrotron Source at Cornell University. ""Additionally, the cold, lower-energy neutrons at EQ-SANS and Bio-SANS won't cause radiation damage or introduce radicals that can interfere with lipid chemistry, as x-rays can do.""The researchers' water density map indicates the water dissociates from the lipid surfaces in the initial lamellar, or layered, phase. In the intermediate fusion phase, known as hemifusion, the water is significantly reduced and squeezed into pockets around a stalk -- a highly curved lipid ""bridge"" connecting two membranes before fusion fully occurs.""For the neutron scattering experiments, we replaced some of the water's hydrogen atoms with deuterium atoms, which helped the neutrons observe the water molecules during membrane fusion,"" said Shuo Qian, the study's corresponding author and a neutron scattering scientist at ORNL. ""The information we obtained could help in future studies of membrane-acting drugs, membrane-associated proteins, and peptides in a membrane complex.""The research was supported by the DOE Office of Science and ORNL Laboratory Directed Research and Development program.Story Source:Materials provided by DOE/Oak Ridge National Laboratory. ",0.110993,0.2522,0.172084,0.204236,0.167943,0.152238,0.223235,0,0
446,"Nuclear pasta, the hardest known substance in the universe ","A team of scientists has calculated the strength of the material deep inside the crust of neutron stars and found it to be the strongest known material in the universe.Matthew Caplan, a postdoctoral research fellow at McGill University, and his colleagues from Indiana University and the California Institute of Technology, successfully ran the largest computer simulations ever conducted of neutron star crusts, becoming the first to describe how these break.""The strength of the neutron star crust, especially the bottom of the crust, is relevant to a large number of astrophysics problems, but isn't well understood,"" says Caplan.Neutron stars are born after supernovas, an implosion that compresses an object the size of the sun to about the size of Montreal, making them ""a hundred trillion times denser than anything on earth."" Their immense gravity makes their outer layers freeze solid, making them similar to earth with a thin crust enveloping a liquid core.This high density causes the material that makes up a neutron star, known as nuclear pasta, to have a unique structure. Below the crust, competing forces between the protons and neutrons cause them to assemble into shapes such as long cylinders or flat planes, which are known in the literature as 'lasagna' and 'spaghetti,' hence the name 'nuclear pasta.' Together, the enormous densities and strange shapes make nuclear pasta incredibly stiff.Thanks to their computer simulations, which required 2 million hours worth of processor time or the equivalent of 250 years on a laptop with a single good GPU, Caplan and his colleagues were able to stretch and deform the material deep in the crust of neutron stars.""Our results are valuable for astronomers who study neutron stars. Their outer layer is the part we actually observe, so we need to understand that in order to interpret astronomical observations of these stars,"" Caplan adds.The findings, accepted for publication in Physical Review Letters, could help astrophysicists better understand gravitational waves like those detected last year when two neutron stars collided. Their new results even suggest that lone neutron stars might generate small gravitational waves.""A lot of interesting physics is going on here under extreme conditions and so understanding the physical properties of a neutron star is a way for scientists to test their theories and models, Caplan adds. With this result, many problems need to be revisited. How large a mountain can you build on a neutron star before the crust breaks and it collapses? What will it look like? And most importantly, how can astronomers observe it?""Story Source:Materials provided by McGill University. ",0.120468,0.176864,0.188953,0.221875,0.165916,0.16781,0.223408,0,0
447,New world record magnetic field,"A group of scientists at the University of Tokyo has recorded the largest magnetic field ever generated indoors -- a whopping 1,200 tesla, as measured in the standard units of magnetic field strength.By comparison, this is a field strength about 400 times higher than those generated by the huge, powerful magnets used in modern hospital MRI machines, and it is about 50 million times stronger than Earth's own magnetic field.Stronger magnetic fields have previously been achieved in outdoor experiments using chemical explosives, but this is a world record for magnetic fields generated indoors in a controlled manner. That greater control means the discovery could open new frontiers in solid-state physics, perhaps allowing scientists to reach what is known as the ""quantum limit,"" a condition where all the electrons in a material are confined to the lowest ground state, where exotic quantum phenomena may appear.The high magnetic field also has implications for nuclear fusion reactors, a tantalizing if unrealized potential future source of abundant clean energy. To reach the quantum limit or sustain nuclear fusion, scientists believe magnetic field strengths of 1,000 tesla or more may be needed.The experiments that set the new world record are described in an article appearing this week in the journal Review of Scientific Instruments, from AIP Publishing.The work opens up a new scientific horizon, said Daisuke Nakamura, first author on the paper, and ""has pushed the envelope for ultrahigh magnetic fields.""Story Source:Materials provided by American Institute of Physics. ",0.126152,0.154785,0.151732,0.179784,0.140005,0.222601,0.168017,0,0
448,Separating the sound from the noise in hot plasma fusion,"In the search for abundant clean energy, scientists around the globe look to fusion power, where isotopes of hydrogen combine to form a larger particle, helium, and release large amounts of energy in the process. For fusion power plants to be effective, however, scientists must find a way to trigger the low-to-high confinement transition, or ""L-H transition"" for short. After a L-H transition, the plasma temperature and density increase, producing more power.Scientists observe the L-H transition is always associated with zonal flows of plasma. Theoretically, zonal flows in a plasma consist of both a stationary flow with a near-zero frequency and one that oscillates at a higher frequency called the geodesic acoustic mode (GAM), which is a global sound wave of the plasma. For the first time, researchers at Hefei University of Technology have detected GAM at two different points simultaneously within the reactor. This new experimental setup will be a useful diagnostic tool for investigating the physics of zonal flows, and their role in the L-H transition. The researchers report these findings in a new paper published in Physics of Plasmas, from AIP Publishing.Zonal flows occur anywhere there is turbulence, such as inside a fusion device or in a planet's atmosphere. ""The most famous zonal flows in nature may be the well-known Jovian belts and zones, which make Jupiter look like a colorful, multilayered cake,"" said Ahdi Liu, an author on the paper. In fusion plasmas, zonal flows are crucial for regulating turbulence and particle transport within the reactor. ""With the gradual improvement of diagnostic technology, zonal flows in fusion plasma has become a research hot spot in the past two decades,"" Liu said.In these experiments, researchers used the Experimental Advanced Superconducting Tokamak (EAST), a magnetic fusion energy reactor in Hefei, China. They installed two Doppler reflectometers on different sides of EAST, which can detect fluctuations in turbulence and plasma density with high precision. The detected GAM had a pitch of F, five octaves above middle C.Previously, researchers at ASDEX-U, the fusion research device at the Max Plank Institute of Plasma Physics, used a similar system to detect GAM, but they measured the plasma at a single location, which makes the setup prone to interference. ""This disadvantage is the main motivation for using two sets of Doppler reflectometers,"" Liu said. ""We could 'purify' the GAM information by comparing the two location's measurements.""The measurements taken at the two points did not entirely agree, showing that each reflectometer also picked up information from nonzonal flows. ""It is completely necessary to extract accurate zonal flows information from multipoint measurement,"" Liu said. Using both measurements, they could clearly show that GAM interacted with the ambient turbulence. Going forward, the researchers will further investigate the role of zonal flows in turbulence and turbulent transport within EAST.Story Source:Materials provided by American Institute of Physics. ",0.134971,0.132785,0.164696,0.177986,0.146828,0.175692,0.170711,0,0
449,Decoupling stress and corrosion to predict metal failure New alloys needed to prevent stress corrosion cracking,"An Arizona State University research team has released new insights about intergranular stress-corrosion cracking (SCC), an environmental cause of premature failure in engineered structures, including bridges, aircraft and nuclear power generating plants.The research, Decoupling the role of stress and corrosion in the intergranular cracking of noble alloys, released today in Nature Materials, addresses the assumption that intergranular SCC is the result of the simultaneous presence of a tensile stress and corrosion, and demonstrates that the roles of stress and corrosion can be decoupled, or can act independently.""The finding is the culmination of about 30 years' work on this kind of stress corrosion problem,"" said lead researcher Karl Sieradzki, a professor of materials science and engineering at ASU. ""We now have a view into how new alloys can be designed to avoid this form of stress corrosion-induced failure.""When metals are exposed to water containing salts, the strength of the metal can be severely compromised and lead to unexpected failure. An example of a SCC failure is the 2003 Kinder Morgan gasoline pipeline in Tucson, AZ.The conventional paradigm for understanding SCC conditions is the simultaneous presence of a sufficient level of tensile stress, a corrosive environment and a susceptible material.The research challenges this viewpoint and illustrates that the simultaneous presence of stress and a corrosive environment is not a requirement for SCC, and that it can occur if the corrosion happens first and the material is subsequently subjected to stress.In addition to Sieradzki, the paper's authors include Nilesh Badwe, Xiying Chen, Erin Karasz, and Ariana Tse from ASU and Daniel Schreiber, Matthew Olszta, Nicole Overman and Stephen Bruemmer from Pacific Northwest National Laboratory. The research was supported by the U.S. Department of Energy.The team examined the behavior of a laboratory model silver-gold alloy, which mimics the corrosion behavior of important engineering alloys, such as stainless steels and nickel-base alloys used in nuclear power plants.Corrosion in these engineering alloys, as in the model silver-gold alloy, results in the formation of nanometer-sized holes within the corroded layer. According to Sieradzki, the key parameter determining the occurrence of rapid SCC is the adhesion between the corroded layer and the un-corroded alloy. Using the atomic scale techniques of high-resolution electron microscopy and atom probe tomography, together with statistical characterizations, the team determined that the apparent requirement for the simultaneous presence of stress and corrosion exists because of time-dependent morphology changes that affect adhesion.As long as adequate adhesion between the layers is maintained, a crack that starts with the corroded layer may penetrate into the uncorroded alloy. This means that there can be a significant mechanical component to stress corrosion cracking that cannot be identified by any measurement of corrosion. The result is that a corrosion measurement can underestimate the rate of SCC by multiplicative factors of 10 or more.""In nuclear plants, SCC maintenance and plant shut downs are based on previous experience with similarly designed reactors,"" Sieradzki explained. ""While we are not building new nuclear plants in the U.S., these findings should trigger the search for new, corrosion resistant alloys that can be used for replacement parts in existing plants and in other important structural applications.""Story Source:Materials provided by Arizona State University. ",0.0818636,0.143894,0.235916,0.173862,0.12138,0.153574,0.186327,0,0
450,Toward fusion power: Optimal magnetic fields for suppressing instabilities in tokamaks,"Fusion, the power that drives the sun and stars, produces massive amounts of energy. Scientists here on Earth seek to replicate this process, which merges light elements in the form of hot, charged plasma composed of free electrons and atomic nuclei, to create a virtually inexhaustible supply of power to generate electricity in what may be called a ""star in a jar.""A long-time puzzle in the effort to capture the power of fusion on Earth is how to lessen or eliminate a common instability that occurs in the plasma called edge localized modes (ELMs). Just as the sun releases enormous bursts of energy in the form of solar flares, so flare-like bursts of ELMs can slam into the walls of doughnut-shaped tokamaks that house fusion reactions, potentially damaging the walls of the reactor.Ripples control new burstsTo control these bursts, scientists disturb the plasma with small magnetic ripples called resonant magnetic perturbations (RMPs) that distort the smooth, doughnut shape of the plasma -- releasing excess pressure that lessens or prevents ELMs from occurring. The hard part is producing just the right amount of this 3D distortion to eliminate the ELMs without triggering other instabilities and releasing too much energy that, in the worst case, can lead to a major disruption that terminates the plasma.Making the task exceptionally difficult is the fact that a virtually limitless number of magnetic distortions can be applied to the plasma, causing finding precisely the right kind of distortion to be an extraordinary challenge. But no longer.Physicist Jong-Kyu Park of the U.S. Department of Energy's (DOE) Princeton Plasma Physics Laboratory (PPPL), working with a team of collaborators from the United States and the National Fusion Research Institute (NFRI) in Korea, have successfully predicted the entire set of beneficial 3D distortions for controlling ELMs without creating more problems. Researchers validated these predictions on the Korean Superconducting Tokamak Advanced Research (KSTAR) facility, one of the world's most advanced superconducting tokamaks, located in Daejeon, South Korea.KSTAR ideal for testsKSTAR was ideal for testing the predictions because of its advanced magnet controls for generating precise distortions in the near-perfect, doughnut-shaped symmetry of the plasma. Identifying the most beneficial distortions, which amount to less than one percent of all the possible distortions that could be produced inside KSTAR, would have been virtually impossible without the predictive model developed by the research team.The result was a precedent-setting achievement. ""We show for the first time the full 3D field operating window in a tokamak to suppress ELMs without stirring up core instabilities or excessively degrading confinement,"" said Park, whose paper -- written with 14 coauthors from the United States and South Korea -- is published in Nature Physics. ""For a long time we thought it would be too computationally difficult to identify all beneficial symmetry-breaking fields, but our work now demonstrates a simple procedure to identify the set of all such configurations.""Researchers reduced the complexity of the calculations when they realized that the number of ways the plasma can distort is actually far fewer than the range of possible 3D fields that can be applied to the plasma. By working backwards, from distortions to 3D fields, the authors calculated the most effective fields for eliminating ELMs. The KSTAR experiments confirmed the predictions with remarkable accuracy.Findings provide new confidenceThe findings on KSTAR provide new confidence in the ability to predict optimal 3D fields for ITER, the international tokamak under construction in France, which plans to employ special magnets to produce 3D distortions to control ELMs. Such control will be vital for ITER, whose goal is to produce 10 times more energy than it will take to heat the plasma. Said authors of the paper, ""the method and principle adopted in this study can substantially improve the efficiency and fidelity of the complicated 3D optimizing process in tokamaks.""Story Source:Materials provided by DOE/Princeton Plasma Physics Laboratory. ",0.09773,0.108345,0.119352,0.159111,0.114004,0.15979,0.147438,0,0
451,Experts voice safety concerns about new pebble-bed nuclear reactors,"Researchers advise caution as a commercial-scale nuclear reactor known as HTR-PM prepares to become operational in China. The reactor is a pebble-bed, high-temperature gas-cooled reactor (HTGR), a new design that is ostensibly safer but that researchers in the U.S. and Germany warn does not eliminate the possibility of a serious accident. Their commentary, publishing August 23 in the journal Joule, recommends continued research, additional safety measures, and an extended startup phase that would allow for better monitoring.""There is no reason for any kind of panic, but nuclear technology has risk in any case,"" says first author Rainer Moormann, a nuclear safety researcher based in Germany. ""A realistic understanding of those risks is essential, especially for operators, and so we urge caution and a spirit of scientific inquiry in the operation of HTR-PM.""In addition to generating electrical power more efficiently, pebble-bed HTGRs such as HTR-PM avoid some of the safety challenges that earlier reactor designs faced. They use graphite- and ceramic-coated grains of uranium fuel that can withstand the core's very high temperatures and passive cooling systems, which together should eliminate the possibility of a core meltdown. ""Pebble-bed reactors have been described by their supporters as 'free from catastrophes' and 'walk away safe,'"" he says.What this means in practice, however, is that the soon-to-be-operational HTR-PM has been built without the safeguards that nuclear reactors in operation today are usually equipped with: it does not have a high-pressure, leak-tight containment structure to serve as a backup in case of an accidental release of radioactive material. It also does not have a redundant active cooling system.""No reactor is immune to accidents. The absence of core meltdown accidents does not mean that a dangerous event is not possible,"" Moormann says. He and his coauthors, Scott Kemp and Ju Li of the Massachusetts Institute of Technology, argue that with new technology, there is always a higher chance of user error. And prototype HTGRs have surprised their operators in the past by forming localized hot spots in the core and unexpectedly high levels of radioactive dust. The pebble-bed design also produces a larger volume of radioactive waste, which is challenging to store or treat.Moormann acknowledges the potential of HTGRs and supports further research into them. ""HTGR designs with what's known as a prismatic core seem to be less problematic than the pebble-bed one, so development work should concentrate on that,"" he says.But to reduce risk, he and his colleagues advocate for several precautionary steps, including rigorous, continuous monitoring, the installation of containment and cooling systems, and an extended startup phase to allow the reactor to be observed and monitored as it comes to temperature. They also recommend investigating more secure long-term storage options for the fuel waste, which currently will be stored in aboveground canisters potentially vulnerable to environmental stresses and terrorism.""There was already some controversy about pebble-bed HTGRs, but my impression was that many problems of them were not sufficiently published and thus not known to some of my colleagues,"" says Moormann. ""I hope that the pros and cons will be broadly discussed.""Story Source:Materials provided by Cell Press. ",0.0874563,0.13896,0.131356,0.185935,0.109172,0.173518,0.164481,0,0
452,The physics of extracting gas from shale formations,"Scientists have distilled the current state of knowledge regarding the multi-scale flow processes occurring during shale gas extraction. This know-how is deemed essential for improving gas recovery and lowering production costs.Extracting gas from new sources is vital in order to supplement dwindling conventional supplies. Shale reservoirs host gas trapped in the pores of mudstone, which consists of a mixture of silt mineral particles ranging from 4 to 60 microns in size, and clay elements smaller than 4 microns. Surprisingly, the oil and gas industry still lacks a firm understanding of how the pore space and geological factors affect gas storage and its ability to flow in the shale. In a study published in EPJ E, Natalia Kovalchuk and Constantinos Hadjistassou from the University of Nicosia, Cyprus, review the current state of knowledge regarding flow processes occurring at scales ranging from the nano- to the microscopic during shale gas extraction. This knowledge can help to improve gas recovery and lower shale gas production costs.Extracting gas from shale has become a popular method in North America and has attracted growing interest in South America and Asia, despite some public opposition. Unlike conventional reservoirs, the pore structures of shale gas reservoirs range from the nanometric to microscopic scale; most natural gas reservoirs display microscopic or larger scale pores.In this paper, the authors outline the latest insights into how the pore distribution and geometry of the shale matrix affect the mechanics of the gas transport process during extraction. In turn, they present a model based on a microscopic image obtained via scanning electron microscopy to determine how gas pressure and gas speed vary throughout the shale. The model is in agreement with experimental evidence.The authors reveal that the orientation, density and magnitude of rock bottlenecks can affect the volume and flow in gas production, due to their impact on the distribution of pressure throughout the reservoir. The findings of their numerical simulation match available theoretical evidence.Story Source:Materials provided by Springer. ",0.140296,0.211239,0.212668,0.201715,0.160387,0.208429,0.208196,0,0
453,Monitoring air pollution after Hurricane Maria,"When Hurricane Maria struck Puerto Rico on September 20, 2017, the storm devastated the island's electrical grid, leaving many people without power for months. This lack of electricity, as well as other storm-related damage, prevented air-quality monitoring in many areas. Now researchers have shown that low-cost sensors that run on solar energy can be used to monitor air pollution after a disaster. They report their results in ACS Earth and Space Chemistry.Three months after Hurricane Maria, half of Puerto Rico still lacked electricity, while the other half experienced frequent power outages. As a result, backup generators that ran on gasoline or diesel were widely used, potentially increasing air pollution. Yet in many areas, official air-quality data were unavailable because of storm damage. R. Subramanian and colleagues wondered if the low-cost, solar-powered sensors that they had previously developed to monitor air quality -- called Real-time Affordable Multi-Pollutant (RAMP) monitors -- could help fill this gap.The team deployed four RAMPs at different locations in the San Juan Metro Area of Puerto Rico in November 2017. Over a month-long period, the devices measured various pollutants, such as carbon monoxide, nitrogen dioxide, sulfur dioxide, nitric oxide and particulate matter. The researchers found that the concentrations of sulfur dioxide and carbon monoxide peaked between 4 a.m. and 8 a.m. local time each day, possibly due to nighttime atmospheric conditions that trapped pollutants. The levels of sulfur dioxide, carbon monoxide and black carbon were closely correlated, suggesting that the pollutants arose from the same combustion source (likely gas- and diesel-powered generators). The RAMP data also indicated that sulfur dioxide levels exceeded the U.S. Environmental Protection Agency's air quality standards on almost 80 percent of the days during the monitoring period. The researchers say that areas prone to natural disasters and other emergencies should include a set of calibrated, low-cost air-quality monitors in their preparedness plans.The authors acknowledge funding from the U.S. Environmental Protection Agency.Story Source:Materials provided by American Chemical Society. ",0.102393,0.137541,0.200335,0.171376,0.128131,0.175607,0.1735,0,0
454,"Deep sea chemical dispersants ineffective in Deepwater Horizon oil spill, study finds Research suggests dispersants suppress oil's natural ability to biodegrade","A new study of the Deepwater Horizon response showed that massive quantities of chemically engineered dispersants injected at the wellhead -- roughly 1,500 meters (4,921 feet) beneath the surface -- were unrelated to the formation of the massive deepwater oil plume.A University of Miami (UM) Rosenstiel School of Marine and Atmospheric Science-led research team analyzed polycyclic aromatic hydrocarbons (PAHs), the most toxic components of petroleum, based on the BP Gulf Science Data's extensive water chemistry samples taken within a 10-kilometer (6-mile) radius of the blowout site. The results of this analysis demonstrated that substantial amounts of oil continued to surface near the response site, despite 3,000 tons of subsea dispersants injection (or SSDI) -- a new spill response strategy meant to curb the spread of oil and facilitate its degradation.Dispersants application to manage surface oil spills has been shown to break the oil into small, easily dissolved droplets. However, the Deepwater Horizon was very different in that the oil entered the system at depth. The turbulent energy and pressure at such immense depths not only contributed to the rapid expansion of the spill, but these natural forces helped disperse oil in micro-droplets and render the dispersant ineffective and unnecessary.""The results of this study are critically important,"" said the study's coauthor Samantha Joye from the University of Georgia. ""This work shows clearly that the eruptive nature of the Macondo discharge was more than sufficient to generate the deepwater oil plume. Further, application of dispersant did not increase the amount of oil in the aqueous phase or change the distribution of oil over depth. These findings should change the way we think about spill response and calls for a reconsideration and reprioritization of response measures.""The team's research, led by Claire Paris, professor of ocean sciences at UM, founded on an unprecedented volume of data publicly available through the Gulf of Mexico Research Initiative Information and Data Cooperative (GRIIDC), demonstrated that the formation of the massive deepwater oil plumes was unrelated to the new response. They further show, in agreement with previous studies, that plumes of oil persisted in the Gulf months after the spewing wellhead was capped 87 days later. The powerful chemical dispersant, called Corexit, may have added to the ecological damage by suppressing the growth of natural oil-degrading bacteria and by increasing the toxicity of the oil itself.""Our earlier work using computer modeling and high-pressure experimental approaches suggested that pumping chemical dispersants at the spewing wellhead may have had little effect on the amount of oil that ultimately surfaced. But empirical evidence was lacking until the release of the BP Gulf Science Data. When completely different approaches converge to the same conclusion, it is time to listen,"" said Paris. ""There is no real trade-off because there is no upside in using ineffective measures that can worsen environmental disasters.""As the oil industry drills in deeper and deeper water, it must find alternative strategies to manage blowouts, says the study's authors. The ""capping stack"" method in which BP used to stop the wellhead outflow may be a more effective first response strategy. Bio-surfactants, which are less toxic and more efficacious to biodegeneration, may offer a viable alternative for oil spills in shallow waters, according to the researchers.As part of the massive response and damage assessment effort, a robust data collection and management strategy was employed, including the BP Gulf Science Data water chemistry data used in this new study and compiled under the Gulf of Mexico Research Initiative.""This type of data management is a strategic asset enhancing both science and management as it allows scientists to use data-driven approaches and test important hypotheses for better understanding and managing future oil spills,"" said Igal Berenshtein, a coauthor of the study and postdoctoral researcher at the UM Rosenstiel School.On April 20, 2010, the Deepwater Horizon oil rig exploded, releasing 210 million gallons of crude oil into the Gulf of Mexico for a total of 87 days, making it the largest oil spill in U.S. history.The study, titled ""BP Gulf Science Data Reveals Ineffectual Sub-Sea Dispersant Injection for the Macondo Blowout,"" was published on Oct. 30 in the journal Frontiers in Marine Science's Marine Pollution section. The study's authors include: Claire B. Paris, Igal Berenshtein, Marcia L. Trillo, Robin Faillettaz, and Maria J. Olascoaga of the UM Rosenstiel School of Marine and Atmosheric Science; Zachary M. Aman of the University of Western Australia's School of Mechanical and Chemical Engineering; Michael Schleter of Hamburg University of Technology, Institute of Multiphase Flows; and Samantha B. Joye of the University of Georgia's Department of Marine Sciences.The study was supported by a grant from the Gulf of Mexico Research Initiative (GOMRI) Center for Integrated Modeling and Analysis of Gulf Ecosystems (C-IMAGE) II and the Ecosystem Impacts of Oil and Gas Input to the Gulf (ECOGIG). Data are publicly available through the Gulf of Mexico Research Initiative Information and Data Cooperative (GRIIDC) at https://data.gulfresearchinitiative.org.Story Source:Materials provided by University of Miami Rosenstiel School of Marine & Atmospheric Science. Original written by Diana Udel. ",0.0873388,0.136138,0.200097,0.184723,0.11525,0.152431,0.174963,0,0
455,Persistent gap in natural gas methane emissions measurements explained,"A new study offers answers to questions that have puzzled policymakers, researchers and regulatory agencies through decades of inquiry and evolving science: How much total methane, a greenhouse gas, is being emitted from natural gas operations across the U.S.? And why have different estimation methods, applied in various U.S. oil and gas basins, seemed to disagree?The Colorado State University-led study, published Oct. 29 in Proceedings of the National Academy of Sciences, resulted from a large, multi-institutional field campaign called the Basin Methane Reconciliation Study. The researchers found that episodic releases of methane that occur mostly during daytime-only maintenance operations, at a few facilities on any given day, may explain why total emissions accountings have not agreed in past analyses.With invaluable assistance from industry partners, the researchers have significantly advanced basin-level emission quantification methods and shed new light on important emissions processes.""Our study is the first of its kind, in its scope and approach,"" said Dan Zimmerle, senior author of the PNAS study, and a senior research associate at the CSU Energy Institute. ""It utilized concurrent ground and aircraft measurements and on-site operations data, and as a result reduces uncertainties of previous studies.""The Basin Methane Reconciliation Study included scientists from CSU, Colorado School of Mines, University of Colorado Boulder, the National Oceanic and Atmospheric Administration, and the National Renewable Energy Laboratory. Other scientific partners were University of Wyoming, Aerodyne, AECOM, Scientific Aviation and GHD. The field campaign took place in 2015 in the Fayetteville shale gas play of Arkansas' Arkoma Basin.Coordinated effortThe campaign involved more than 60 researchers making coordinated facility- and device-level measurements of key natural gas emissions sources. The campaign also included a series of aircraft flyovers to collect measurements during the same period when researchers were taking measurements on the ground. The flights took place when meteorological conditions allowed accurate regional emissions estimates.The research team set out to investigate the persistent gap between two widely used methods of estimating methane emissions from natural gas operations. ""Bottom-up"" estimates, such as those used in the EPA Inventory of U.S. Greenhouse Gas Emissions and Sinks, are developed by measuring emissions from a representative sample of devices, scaled up by the number of devices or emission events. In contrast, ""top-down"" measurements can be performed at a regional scale, such as flying an aircraft upwind and downwind of a study area to derive total emissions from methane entering or leaving a basin.In the past, most aircraft-based, basin-scale emissions estimates have been statistically higher than estimates based on bottom-up accounting.""The key to our efforts was having everyone out in the same field, at the same time,"" said Gabrielle Petron, a research scientist from CIRES at CU Boulder and NOAA who was the principal investigator for the top-down measurement team. ""By comparing the merits and pitfalls of multiple methods of measurement, we were able to paint a much more comprehensive, and we believe accurate, picture of the methane emissions landscape for natural gas infrastructure.""Lead-up papers evaluate multiple methodsThe PNAS paper utilized a comprehensive set of results published in lead-up papers evaluating emissions from natural gas facilities, including well pads, gathering stations, gathering pipelines, and transmission and distribution sectors. The teams made simultaneous measurements using multiple methods at well pads and compressor stations -- the largest emissions sources identified in the basin -- which highlighted the strengths and weaknesses of various on-site and downwind methods. Natural gas distribution systems were only a small fraction of total natural gas emissions. The team also estimated methane emissions from biogenic sources, such as agriculture and landfills.The entirety of those research efforts culminated in the CSU-led PNAS paper that synthesized all the data taken by the research teams. This capstone paper compared a bottom-up estimate that accounted for the location and timing of emissions, with a top-down estimate developed via aircraft measurement -- an analysis that had never been done before. One of the study's key insights was the importance of understanding the timing of measurements and daytime maintenance activities, which likely explains the persistent gap between previous top-down and bottom-up estimates.Short periods of high emissionsRoutine maintenance activities occur in the daytime and can cause short periods of high emissions in the middle of the day. One of these activities is called ""manual liquids unloading,"" which removes liquids buildup at a natural gas well in order to restore gas production flows. The unloading process can temporarily divert the flow of natural gas from the well to an atmospheric vent.These activities typically happen during the day, around the same time a research aircraft would be conducting measurements. This diurnal variability of methane emissions, the researchers concluded, may help explain why estimates from aircraft measurements have previously been higher than estimates based on annual bottom-up inventories.In this study, for the first time, the researchers showed that the east-to-west variation of methane emissions across the basin, derived from aircraft measurements, were reproduced by the high-resolution, bottom-up emissions model developed by the CSU team.Industry partnerships and public/private fundingWorking with industry technical experts gave the study team invaluable full site access for bottom-up measurements and hourly and spatially resolved operational data. The data improved the understanding of the magnitude and timing of emissions, particularly for episodic events like maintenance operations.""What we have found is that we have two good methods of measurements. If you want to compare them, you have to account for timing and location of emissions,"" said PNAS lead author Tim Vaughn, a CSU research scientist.Applicability of these results to other production basins is not known, the researchers say, without understanding the timing of emissions in those other basins.Story Source:Materials provided by Colorado State University. Original written by Anne Manning. ",0.0784486,0.114325,0.172657,0.164168,0.0925852,0.154635,0.140312,0,0
456,Novel material could make plastic manufacturing more energy-efficient,"An innovative filtering material may soon reduce the environmental cost of manufacturing plastic. Created by a team including scientists at the National Institute of Standards and Technology (NIST), the advance can extract the key ingredient in the most common form of plastic from a mixture of other chemicals -- while consuming far less energy than usual.The material is a metal-organic framework (MOF), a class of substances that have repeatedly demonstrated a talent for separating individual hydrocarbons from the soup of organic molecules produced by oil refining processes. MOFs hold immense value for the plastic and petroleum industries because of this capability, which could allow manufacturers to perform these separations far more cheaply than standard oil-refinement techniques.This promise has made MOFs the subject of intense study at NIST and elsewhere, leading to MOFs that can separate different octanes of gasoline and speed up complex chemical reactions. One major goal has proved elusive, though: an industrially preferred method for wringing out ethylene -- the molecule needed to create polyethylene, the plastic used to make shopping bags and other everyday containers.However, in today's issue of the journal Science, the research team reveals that a modification to a well-studied MOF enables it to separate purified ethylene out of a mixture with ethane. The team's creation -- built at The University of Texas at San Antonio (UTSA) and China's Taiyuan University of Technology and studied at the NIST Center for Neutron Research (NCNR) -- represents a major step forward for the field.Making plastic takes lots of energy. Polyethylene, the most common type of plastic, is built from ethylene, one of the many hydrocarbon molecules found in crude oil refining. The ethylene must be highly purified for the manufacturing process to work, but the current industrial technology for separating ethylene from all the other hydrocarbons is a chilly but high-energy process that cools down the crude to more than 100 degrees below zero Celsius.Ethylene and ethane constitute the bulk of the hydrocarbons in the mixture, and separating these two is by far the most energy-intensive step. Finding an alternative method of separation would reduce the energy needed to make the 170 million tons of ethylene manufactured worldwide each year.Scientists have been searching for such an alternative method for years, and MOFs appear promising. On a microscopic level, they look a bit like a half-built skyscraper of girders and no walls. The girders have surfaces that certain hydrocarbon molecules will stick to firmly, so pouring a mixture of two hydrocarbons through the right MOF can pull one kind of molecule out of the mix, letting the other hydrocarbon emerge in pure form.The trick is to create a MOF that allows the ethylene to pass through. For the plastics industry, this has been the sticking point.""It's very difficult to do,"" said Wei Zhou, a scientist at the NCNR. ""Most MOFs that have been studied grab onto ethylene rather than ethane. A few of them have even demonstrated excellent separationperformance, by selectively adsorbing the ethylene. But from an industrial perspective you would prefer to do the opposite if feasible. You want to adsorb the ethane byproduct and let the ethylene pass through.""The research team spent years trying to crack the problem. In 2012, another research team that worked at the NCNR found that a particular framework called MOF-74 was good for separating a variety of hydrocarbons, including ethylene. It seemed like a good starting point, and the team members scoured the scientific literature for additional inspiration. An idea taken from biochemistry finally sent them in the right direction.""A huge topic in chemistry is finding ways to break the strong bond that forms between carbon and hydrogen,"" said UTSA professor Banglin Chen, who led the team. ""Doing that allows you to create a lot of valuable new materials. We found previous research that showed that compounds containing iron peroxide could break that bond.""The team reasoned that to break the bond in a hydrocarbon molecule, the compound would have to attract the molecule in the first place. When they modified MOF-74's walls to contain a structure similar to the compound, it turned out the molecule it attracted from their mixture was ethane.The team brought the MOF to the NCNR to explore its atomic structure. Using a technique called neutron diffraction, they determined what part of the MOF's surface attracts ethane -- a key piece of information for explaining why their innovation succeeded where other efforts have fallen short.""Without the fundamental understanding of the mechanism, no one would believe our results,"" Chen said. ""We also think that we can try to add other small groups to the surface, maybe do other things. It's a whole new research direction and we're very excited.""While Zhou said the team's modified MOF does work efficiently, it may require some additional development to see action at a refinery.""We proved this route is promising,"" Zhou said, ""but we're not claiming our materials perform so well they can't be improved. Our future goal is to dramatically increase their selectivity. It's worth pursuing further.""Story Source:Materials provided by National Institute of Standards and Technology (NIST). ",0.0414701,0.110194,0.0903724,0.130302,0.0723399,0.0798211,0.117863,0,0
457,Wood sponge soaks up oil from water,"Oil spills and industrial discharge can contaminate water with greasy substances. Although it's true that oil and water don't mix, separating and recovering each component can still be challenging. Now, researchers have created sponges made from wood that selectively absorb oil, and then can be squeezed out and used again. They report their results in ACS Nano.Over the years, scientists have developed numerous techniques to clean up oily water, from gravity separation to burning to bioremediation. But many of these methods suffer from limitations, such as low efficiency, secondary pollution and high cost. More recently, researchers have explored 3D porous materials, such as aerogels or sponges, based on various building blocks including synthetic polymers, silica or cellulose nanofibers. However, these are often difficult to fabricate, lack mechanical robustness or are made from nonrenewable, nondegradable materials. Xiaoqing Wang and colleagues wanted to develop a sponge made from wood -- a renewable resource -- that would absorb oil and tolerate repeated squeezing without structural failure.The team made the wood sponge by treating natural balsa wood with chemicals that removed lignin and hemicellulose, leaving behind a cellulose skeleton. They then modified this highly porous structure with a hydrophobic coating that attracted oil, but not water. When placed in a mixture of water and silicone oil, the wood sponge removed all of the red-dyed oil, leaving clean water behind. Depending on the oil tested, the sponge absorbed 16 to 41 times its own weight, which is comparable to or better than many other reported absorbents. In addition, the sponge could endure at least 10 cycles of absorption and squeezing. The researchers incorporated the wood sponge into an oil-collecting device in the lab that continuously separated oils from the water surface.Video: https://www.youtube.com/watch?v=3JqiSrMflN4&feature=youtu.beStory Source:Materials provided by American Chemical Society. ",0.103494,0.188046,0.236731,0.199609,0.159919,0.156612,0.20921,0,0
458,Fracking wastewater accumulation found in freshwater mussels' shells,"Elevated concentrations of strontium, an element associated with oil and gas wastewaters, have accumulated in the shells of freshwater mussels downstream from fracking wastewater disposal sites, according to researchers from Penn State and Union College.""Freshwater mussels filter water and when they grow a hard shell, the shell material records some of the water quality with time,"" said Nathaniel Warner, assistant professor of environmental engineering at Penn State. ""Like tree rings, you can count back the seasons and the years in their shell and get a good idea of the quality and chemical composition of the water during specific periods of time.""In 2011, it was discovered that despite treatment, water and sediment downstream from fracking wastewater disposal sites still contained fracking chemicals and had become radioactive. In turn, drinking water was contaminated and aquatic life, such as the freshwater mussel, was dying. In response, Pennsylvania requested that wastewater treatment plants not treat and release water from unconventional oil and gas drilling, such as the Marcellus shale. As a result, the industry turned to recycling most of its wastewater. However, researchers are still uncovering the long-lasting effects, especially during the three-year boom between 2008 and 2011, when more than 2.9 billion liters of wastewater were released into Pennsylvania's waterways.""Freshwater pollution is a major concern for both ecological and human health,"" said David Gillikin, professor of geology at Union College and co-author on the study. ""Developing ways to retroactively document this pollution is important to shed light on what's happening in our streams.""The researchers began by collecting freshwater mussels from the Alleghany River, both 100 meters (328 feet) upstream and 1 to 2 kilometers (0.6 to 1.2 miles) downstream of a National Pollutant Discharge Elimination System-permitted wastewater disposal facility in Warren, Pennsylvania, as well as mussels from two other rivers -- the Juniata and Delaware -- that had no reported history of oil and gas discharge.Once at the lab, they dissected the shell and then drilled and collected the powder from the shell layer by layer to look for isotopes of two elements: oxygen, used to determine the year and season, and strontium, both of which carry a distinctive signature of the rock formation where they were produced. The results were recently published in Environmental Science & Technology.What the team found was significantly elevated concentrations of strontium in the shells of the freshwater mussels collected downstream of the facility, whereas the shells collected upstream and from the Juniata and Delaware Rivers showed little variability and no trends over time.Surprisingly, the amount of strontium found in the layers of shell created after 2011 did not show an immediate reduction in contaminants. Instead, the change appeared more gradually. This suggests that the sediment where freshwater mussels live may still contain higher concentrations of heavy metals and other chemicals used in unconventional drilling. ""We know that Marcellus development has impacted sediments downstream for tens of kilometers,"" said Warner. ""And it appears it still could be impacted for a long period of time. The short timeframe that we permitted the discharge of these wastes might leave a long legacy.""According to the U.S. Department of Energy, up to 95 percent of new wells drilled today are hydraulically fractured, accounting for two-thirds of total U.S. marketed natural gas production and about half of U.S. crude oil production.""The wells are getting bigger, and they're using more water, and they're producing more wastewater, and that water has got to go somewhere,"" said Warner. ""Making the proper choices about how to manage that water is going to be pretty vital.""Warner added that there is not much difference between conventional and unconventional wastewater from a pollution standpoint. He said high levels of strontium, sodium, chloride and other contaminants are still present with conventional oil and gas development.Now that the researchers know that freshwater mussels can be used as chemical recorders of fracking pollutants in waterways, they would like to look at the soft tissue of the freshwater mussels, since muskrats and fish feed off them. They also hope to expand their research to include other specific pollutants that likely bioaccumulated in areas of surface water disposal.""We want to see what metals the mussel incorporates predictably and which ones it doesn't,"" said Thomas Geeza, a doctoral student in environmental engineering at Penn State and co-author on the study. ""We're trying to develop this as a tool that can be used in other waterways to answer other questions.""The mussels could also be used to investigate possible seepages occurring at facilities.""We tested if fracking fluid discharge from a wastewater plant was recorded in shells, but one could imagine also using this technique to investigate leaks from holding ponds or accidental discharge into streams nearby fracking operations,"" said Gillikin.Story Source:Materials provided by Penn State. ",0.0765952,0.134699,0.211925,0.157773,0.11113,0.146061,0.160489,0,0
459,Renewable energy is common ground for Democrats and Republicans,"As the battle lines are drawn for next month's hotly contested midterm elections, some Americans may be comforted to know there is at least one area of common ground for Democrats and Republicans.Regardless of political standing, age or gender, U.S. voters are in favor of renewable energy, according to research by Christine Horne, professor of sociology at Washington State University.Horne and Emily Kennedy, a former WSU sociology professor now at the University of British Columbia, are the authors of a new study in the journal Environmental Politics that shows while conservatives and liberals tend to disagree on many environmental issues, they both view the development of solar power and other forms of renewable energy as financially savvy and a step towards self-sufficiency.The research identifies an area where policymakers on both sides of the aisle could work together. It also could have important implications for utility companies and other businesses involved in the manufacture and sale of renewable energy technologies.""I think anyone who is paying attention to our current political climate might be interested to see there is an area of common ground,"" Horne said. ""Marketing renewable energy as a way to be more self-sufficient is a message that would appeal to both liberals and conservatives.""Bipartisan support for renewablesSupport for renewable energy has been growing steadily across the U.S. in recent years. A 2016 study by the Pew Research Center found 83 percent of conservative Republicans and 97 percent of liberal Democrats favor solar farms. Conservative states are also as likely to support renewable energy and energy efficiency policies as liberal states, according to a 2016 study led by researchers at Vanderbilt University.While support for renewable energy among Democrats is largely thought to stem from environmental concerns, the reasoning behind Republicans' support is less well understood.To better understand support for renewable energy among both conservatives and liberals, Horne and Kennedy conducted in-person interviews at the homes of 64 registered Democrats and Republicans across Washington state.The researchers asked study participants about their views on people who had solar panels or who engaged in other pro-environmental behaviors and their own interest in installing solar panels.The team also conducted a larger, nationally representative online survey that asked participants about their views of a family in their neighborhood that recently installed solar panels on their home.Funding for the research was provided by the National Science Foundation and a WSU New Faculty Seed Grant.Financially wise use of resourcesRather than seeing renewable energy as tightly bound to environmentalism, the study's conservative respondents were more likely to view it as financially wise and a good use of resources, Horne's team found.For instance, Ted, a conservative interviewed in the study, said he thinks very highly of people who have solar panels on their home and assumes they are smart, frugal and self-sufficient. When asked if he thinks these people may be concerned about the environment, Ted responded, ""I think if people thought to put solar panels on their roof, they would not think that was helping the environment at all. They would think that was helping them financially, because they're not paying a power bill.""Democrats interviewed in the study also viewed renewable energy as financially smart and contributing to self-sufficiency.However, unlike their Republican counterparts, the Democrats strongly linked renewable energy with environmental protection, reducing carbon emissions and helping other people.When asked if she feels responsible for reducing her personal environmental impact, Caitlyn, a Democrat, responded:""Because I enjoy being out in nature, I think everyone else should be able to enjoy it. I think our kids, and our kids' kids and everyone should have the same benefits...that we are afforded right now. I think that they should all be afforded that as well, if not even more than we have right now.""ImplicationsThe research helps explain how politically polarized attitudes about the environment can exist alongside bipartisan support for renewable energy. It also suggests that efforts to encourage Republicans to invest in solar may be more effective if they emphasize self-sufficiency.Story Source:Materials provided by Washington State University. ",0.0952181,0.122756,0.184759,0.214531,0.132102,0.178786,0.167002,0,0
460,Intense microwave pulse ionizes its own channel through plasma,"Breakthrough new research shows that ionization-induced self-channeling of a microwave beam can be achieved at a significantly lower power of the microwave beam and gas pressure for radially nonuniform plasma with minimal on-axis density than in the case of plasma formed as the result of gas ionization.In the journal Physics of Plasmas, from AIP Publishing, Israel Institute of Technology researchers report observing this effect for the first time and studying it in detail in a plasma preliminarily formed by a radiofrequency discharge, in a low-pressure gas (<150 Pa). They were able to do this by using analytical modeling and numerical particle-in-cell simulations, and their work centers on the concept of the nonlinear effect on plasma and magnetic wave interaction.""Ionization-induced plasma self-channeling is the foundation for microwave plasma wakefield research,"" said lead author Yang Cao. ""A plasma wakefield is a wave generated by particles traveling through a plasma. And a microwave plasma wakefield experiment could give us information about laser wakefield research that's extremely difficult to obtain due to the short time (femtosecond) and geometry scale involved.""This work is significant because microwaves will always diverge, unlike lasers that can be trapped within optical fibers. ""In this [way], a self-induced 'microwave fiber' is created that may help the microwave propagate a much longer distance,"" Cao said.In the future, the microwave ionization-induced self-channeling effect could be used for further exploring the microwave plasma wakefield or, since it's a form of directed energy, it may also find military applications as a directed-energy weapon.Story Source:Materials provided by American Institute of Physics. ",0.189977,0.195004,0.199339,0.211682,0.220936,0.228468,0.20996,0,0
461,Physicists train robotic gliders to soar like birds The novel study applies reinforcement learning to set a course toward artificial intelligence,"The words ""fly like an eagle"" are famously part of a song, but they may also be words that make some scientists scratch their heads. Especially when it comes to soaring birds like eagles, falcons and hawks, who seem to ascend to great heights over hills, canyons and mountain tops with ease. Scientists realize that upward currents of warm air assist the birds in their flight, but they don't know how the birds find and navigate these thermal plumes.To figure it out, researchers from the University of California San Diego used reinforcement learning to train gliders to autonomously navigate atmospheric thermals, soaring to heights of 700 meters--nearly 2,300 feet. The novel research results, published in the Sept. 19 issue of Nature, highlight the role of vertical wind accelerations and roll-wise torques as viable biological cues for soaring birds. The findings also provide a navigational strategy that directly applies to the development of autonomous soaring vehicles, or unmanned aerial vehicles (UAVs).""This paper is an important step toward artificial intelligence--how to autonomously soar in constantly shifting thermals like a bird. I was surprised that relatively little learning was needed to achieve expert performance,"" said Terry Sejnowski, a member of the research team from the Salk Institute for Biological Studies and UC San Diego's Division of Biological Sciences.Reinforcement learning is an area of machine learning, inspired by behavioral psychology, whereby an agent learns how to behave in an environment based on performed actions and the results. According to UC San Diego Department of Physics Professor Massimo Vergassola and PhD candidate Gautam Reddy, it offers an appropriate framework to identify an effective navigational strategy as a sequence of decisions taken in response to environmental cues.""We establish the validity of our learned flight policy through field experiments, numerical simulations and estimates of the noise in measurements that is unavoidably present due to atmospheric turbulence,"" explained Vergassola. ""This is a novel instance of learning a navigational task in the field, where learning is severely challenged by a multitude of physical effects and the unpredictability of the natural environment.""In the study, conducted collaboratively with the UC San Diego Division of Biological Sciences, the Salk Institute and the Abdus Salam International Center for Theoretical Physics in Trieste, Italy, the team equipped two-meter wingspan gliders with a flight controller. The device enabled on-board implementation of autonomous flight policies via precise control over bank angle and pitch. A navigational strategy was determined solely from the gliders' pooled experiences collected over several days in the field using exploratory behavioral strategies. The strategies relied on new on-board methods, developed in the course of the research, to accurately estimate the gliders' local vertical wind accelerations and the roll-wise torques, which served as navigational cues.The scientists' methodology involved estimating the vertical wind acceleration, the vertical wind velocity gradients across the gliders' wings, designing the learning module, learning the thermalling strategy in the field, testing the performance of the learned policy in the field, testing the performance for different wingspans in simulations and estimating the noise in gradient sensing due to atmospheric turbulence.""Our results highlight the role of vertical wind accelerations and roll-wise torques as viable biological mechanosensory cues for soaring birds, and provide a navigational strategy that is directly applicable to the development of autonomous soaring vehicles,"" said Vergassola.eStory Source:Materials provided by University of California - San Diego. ",0.136851,0.103921,0.17885,0.215417,0.123422,0.170464,0.175553,0,0
462,Windier wind farms Mechanical engineer develops ways to improve windfarm productivity,"You've probably seen them, perhaps on long roadtrips: wind turbines with enormous, hypnotic rolling blades, harnessing the clean power of wind for conversion into electric energy. What you may not know is that for the explosion in the number of wind turbines in use as we embrace cleaner sources of energy, these wind farms are quite possibly not as productive as they could be.""We've been designing turbines for use by themselves, but we almost never use them by themselves anymore,"" said UC Santa Barbara mechanical engineering professor Paolo Luzzatto-Fegiz, whose specialty lies in fluid mechanics. Historically, he said, wind turbines were used individually or in small groups, but as the world moves toward greener energy technologies, they are now found in groups of hundreds or thousands.The problem with these large installations is that each machine, which has been designed to extract as much energy as possible from oncoming wind, may not ""play well"" with the others, Luzzatto-Fegiz explained. Depending on how the turbines are situated relative to each other and to the prevailing wind, those not directly in the path of the wind could be left to extract energy from significantly depleted airflow.""These turbines are now very good at extracting power from wind, but they also form these very big wind shadows,"" said Luzzatto-Fegiz, who is the lead author of ""Entrainment models for fully-developed wind farms: Effects of atmospheric stability and an ideal limit for wind farm performance,"" published in the American Physical Society journal Physical Review Fluids. Similar to how structures can attenuate the flow of light from one side to another, wind power also is lessened as it flows from the front of the turbine to its rear. The result is that not all turbines in a wind farm are living up to their potential.""So, you can see that it's not a matter of packing more turbines on your piece of land, because at some point you hit these diminishing returns,"" he said. ""There's a point where if you keep adding turbines the amount of power you get becomes less.""However, according to Luzzatto-Fegiz and co-author Colm-cille P. Caulfield, a professor at the University of Cambridge in the U.K., there are ways to get around this issue of diminishing wind returns. And, they said, these enhancements could result in orders-of-magnitude improvements in the energy production of wind farms.The main goal, according to the researchers, is to give all turbines access to high-velocity airflow, from which they can extract a greater amount of energy. Since the wind above the farm is much faster than between the turbines, mixing the airflow in the wake of the turbines with the air above could be the key to getting more bang for your wind turbine buck.""If you could somehow invent a gadget that for each of these turbines causes these wakes to mix very quickly, you can potentially have these huge improvements,"" Luzzatto-Fegiz said.Yet another potential solution is a relatively new version of the wind turbine, in which the blades rotate on a vertical axis -- like eggbeater blades -- as opposed to the traditional horizontal axis.""These don't perform as well ordinarily by themselves, but it's significant that they essentially can cause much stronger mixing in their wakes,"" he said, ""and people have shown that if you put them in an arrangement where they spin in opposite directions to each other they can cause very nice mixing.""The models developed by the researchers could lead to better-performing wind farms, which in some cases may not require as many turbines as previously thought, thereby reducing potential costs. The models also could result in custom solutions that involve the farm sites' specific terrain, and local weather patterns.""We're really excited that we can model all that very accurately,"" said Luzzatto-Fegiz.Story Source:Materials provided by University of California - Santa Barbara. Original written by Sonia Fernandez. ",0.143943,0.111508,0.152546,0.17136,0.120089,0.229162,0.156024,0,0
463,"Large wind and solar farms in the Sahara would increase heat, rain, vegetation ","Wind and solar farms are known to have local effects on heat, humidity and other factors that may be beneficial -- or detrimental -- to the regions in which they are situated. A new climate-modeling study finds that a massive wind and solar installation in the Sahara Desert and neighboring Sahel would increase local temperature, precipitation and vegetation. Overall, the researchers report, the effects would likely benefit the region.The study, reported in the journal Science, is among the first to model the climate effects of wind and solar installations while taking into account how vegetation responds to changes in heat and precipitation, said lead author Yan Li, a postdoctoral researcher in natural resources and environmental sciences at the University of Illinois.""Previous modeling studies have shown that large-scale wind and solar farms can produce significant climate change at continental scales,"" Li said. ""But the lack of vegetation feedbacks could make the modeled climate impacts very different from their actual behavior.The new study, co-led with Eugenia Kalnay and Safa Motesharrei at the University of Maryland, focused on the Sahara for several reasons, Li said.""We chose it because it is the largest desert in the world; it is sparsely inhabited; it is highly sensitive to land changes; and it is in Africa and close to Europe and the Middle East, all of which have large and growing energy demands,"" he said.The wind and solar farms simulated in the study would cover more than 9 million square kilometers and generate, on average, about 3 terawatts and 79 terawatts of electrical power, respectively.""In 2017, the global energy demand was only 18 terawatts, so this is obviously much more energy than is currently needed worldwide,"" Li said.The model revealed that wind farms caused regional warming of near-surface air temperature, with greater changes in minimum temperatures than maximum temperatures.""The greater nighttime warming takes place because wind turbines can enhance the vertical mixing and bring down warmer air from above,"" the authors wrote. Precipitation also increased as much as 0.25 millimeters per day on average in regions with wind farm installations.""This was a doubling of precipitation over that seen in the control experiments,"" Li said. In the Sahel, average rainfall increased 1.12 millimeters per day where wind farms were present.""This increase in precipitation, in turn, leads to an increase in vegetation cover, creating a positive feedback loop,"" Li said.Solar farms had a similar positive effect on temperature and precipitation, the team found. Unlike the wind farms, the solar arrays had very little effect on wind speed.""We found that the large-scale installation of solar and wind farms can bring more rainfall and promote vegetation growth in these regions,"" Kalnay said. ""The rainfall increase is a consequence of complex land-atmosphere interactions that occur because solar panels and wind turbines create rougher and darker land surfaces.""The increase in rainfall and vegetation, combined with clean electricity as a result of solar and wind energy, could help agriculture, economic development and social well-being in the Sahara, Sahel, Middle East and other nearby regions,"" Motesharrei said.Story Source:Materials provided by University of Illinois at Urbana-Champaign. ",0.133314,0.148496,0.214054,0.151686,0.138173,0.195423,0.154874,0,0
464,3D printed impeller allows unmanned aircraft to operate for thousands of hours,"Southwest Research Institute engineers are developing a cooled, radial gas turbine for a small generator that provides thousands of hours of electricity to an unmanned aerial vehicle (UAV), a significant improvement to current UAV turbines that only operate a few hundred hours before wearing out.Turbines are rotary mechanical devices that, when combined with a generator, produce electrical power.""This turbine is part of a generator that's similar to what the average person might use to generate electricity in their home when the power goes out,"" said David Ransom of SwRI's Mechanical Engineering Division. ""The version we're creating is more compact and efficient, tailored to the needs of a small, unmanned aircraft.""The problem with current small turbine models is that during the generator's combustion process, the turbine is constantly bathed in high temperature gas that ultimately damages or destroys it.""The hotter the turbine gets, the better its performance,"" Ransom said. ""But these smaller turbines can't survive the temperature, so we've designed one that has tiny airflow passages that cool the turbine without sacrificing the power of its performance. Normally with small turbines you have to make a choice between performance or reliability, but we're making it possible to have both.""SwRI has worked with internal passages of high temperature turbines on large version used in power plants and passenger airplanes. To create the small, intricate design with internal air passages, engineers are using a new selective laser melting (SLM) machine, which builds metal parts layer by layer. The new SLM machine, which arrived at SwRI in December 2017, sets itself apart from other 3D printers in that it's built to craft layered and highly detailed metal parts rather than plastic ones.To leverage the capability of the new SLM machine, the SwRI Advisory Committee for Research launched the Metals Additive Kickoff Emphasizing Research Synergies (MAKERS) program, an internal research and development effort. MAKERS is designed to encourage collaboration between SwRI researchers on projects utilizing the revolutionary new technology. The new turbine is one of the first products to result from the MAKERS program. MAKERS and other internal research programs invest in innovative technologies that ultimately benefit client-sponsored programs.""Generators that provide power to us and to big aircraft already have cooled turbines, whereas a generator of this size for a small craft does not,"" Ransom said. ""It's an exciting engineering challenge, and having the ability to print parts with the SLM machine is a real advantage.""Story Source:Materials provided by Southwest Research Institute. ",0.115365,0.105341,0.124069,0.203141,0.124147,0.209808,0.167767,0,0
465,"A new way to remove ice buildup without power or chemicals Passive solar-powered system could prevent freezing on airplanes, wind turbines, powerlines, and other surfaces","From airplane wings to overhead powerlines to the giant blades of wind turbines, a buildup of ice can cause problems ranging from impaired performance all the way to catastrophic failure. But preventing that buildup usually requires energy-intensive heating systems or chemical sprays that are environmentally harmful. Now, MIT researchers have developed a completely passive, solar-powered way of combating ice buildup.The system is remarkably simple, based on a three-layered material that can be applied or even sprayed onto the surfaces to be treated. It collects solar radiation, converts it to heat, and spreads that heat around so that the melting is not just confined to the areas exposed directly to the sunlight. And, once applied, it requires no further action or power source. It can even do its de-icing work at night, using artificial lighting.The new system is described today in the journal Science Advances, in a paper by MIT associate professor of mechanical engineering Kripa Varanasi and postdocs Susmita Dash and Jolet de Ruiter.""Icing is a major problem for aircraft, for wind turbines, powerlines, offshore oil platforms, and many other places,"" Varanasi says. ""The conventional ways of getting around it are de-icing sprays or by heating, but those have issues.""Inspired by the sunThe usual de-icing sprays for aircraft and other applications use ethylene glycol, a chemical that is environmentally unfriendly. Airlines don't like to use active heating, both for cost and safety reasons. Varanasi and other researchers have investigated the use of superhydrophobic surfaces to prevent icing passively, but those coatings can be impaired by frost formation, which tends to fill the microscopic textures that give the surface its ice-shedding properties.As an alternate line of inquiry, Varanasi and his team considered the energy given off by the sun. They wanted to see, he says, whether ""there is a way to capture that heat and use it in a passive approach."" They found that there was.It's not necessary to produce enough heat to melt the bulk of the ice that forms, the team found. All that's needed is for the boundary layer, right where the ice meets the surface, to melt enough to create a thin layer of water, which will make the surface slippery enough so any ice will just slide right off. This is what the team has achieved with the three-layered material they've developed.Layer by layerThe top layer is an absorber, which traps incoming sunlight and converts it to heat. The material the team used is highly efficient, absorbing 95 percent of the incident sunlight, and losing only 3 percent to re-radiation, Varanasi saysIn principle, that layer could in itself help to prevent frost formation, but with two limitations: It would only work in the areas directly in sunlight, and much of the heat would be lost back into the substrate material -- the airplane wing or powerline, for example -- and would not help with the de-icing.So, to compensate for the localization, the team added a spreader layer -- a very thin layer of aluminum, just 400 micrometers thick, which is heated by the absorber layer above it and very efficiently spreads that heat out laterally to cover the entire surface. The material was selected to have ""thermal response that is fast enough so that the heating takes place faster than the freezing,"" Varanasi says.Finally, the bottom layer is simply foam insulation, to keep any of that heat from being wasted downward and keep it where it's needed, at the surface.""In addition to passive de-icing, the photothermal trap stays at an elevated temperature, thus preventing ice build-up altogether,"" Dash says.The three layers, all made of inexpensive commercially available material, are then bonded together, and can be bonded to the surface that needs to be protected. For some applications, the materials could instead be sprayed onto a surface, one layer at a time, the researchers say.The team carried out extensive tests, including real-world outdoor testing of the materials and detailed laboratory measurements, to prove the effectiveness of the system.The system could even find wider commercial uses, such as panels to prevent icing on roofs of homes, schools, and other buildings, Varanasi says. The team is planning to continue work on the system, testing it for longevity and for optimal methods of application. But the basic system could essentially be applied almost immediately for some uses, especially stationary applications, he says.Story Source:Materials provided by Massachusetts Institute of Technology. ",0.0800342,0.102737,0.132172,0.13846,0.100636,0.128104,0.1353,0,0
466,The science behind blowing bubbles New findings point to new ways to produce a range of consumer products,"What exactly happens when you blow on a soap film to make a bubble? Behind this simple question about a favorite childhood activity is some real science, researchers at New York University have found.In a series of experiments replicating bubble blowing, NYU's Applied Math Lab has discovered two ways in which bubbles can be made: one, by pushing with a steady but strong wind on a soap film through a circular wand, which causes it to grow into a bubble, and two, by pushing with a gentle wind on an already-inflated film in order to drive its further growth.""This second method might explain how we often blow bubbles as kids: a quick puff bends the film outward and thereafter the film keeps growing even as the flow of air slows,"" says Leif Ristroph, an assistant professor at NYU's Courant Institute of Mathematical Sciences who led the study.The first method is more intuitive, but less common.""This is used by the bubble blowers we see in parks in the summertime,"" explains Ristroph. ""They simply walk, sufficiently fast, it seems, with a soapy loop of rope, which provides the relative wind needed to stretch out the film.""The results, reported in the journal Physical Review Letters, point to potential applications in consumer products that contain bubbles or droplets, such as sprays, foams, and emulsions, which are combinations of unmixable liquids.The paper's other researchers were Likhit Ganedi, an NYU undergraduate at the time of the work and now a graduate student at Carnegie Mellon University, Anand Oza, a postdoctoral fellow at the time of the research and now a professor at the New Jersey Institute of Technology, and Michael Shelley, a professor at the Courant Institute.As a physics problem, blowing bubbles is a question of how a liquid film -- typically soapy water -- interacts with an imposed flow of an external fluid, which is air in the case of bubble blowing. This dynamic is crucial in understanding how to enhance industrial production of many chemical products.To break down the science that explains this process -- i.e., what events precede the formation of bubbles -- the researchers created an experiment, replicating the blowing of bubbles, using oil films suspended in flowing water and pushed through a wire loop wand.""Working with water instead of air has many advantages in terms of controlling, measuring, and seeing flows,"" Ristroph explains. ""This is the trick that made these experiments possible.""Their experimental observations, combined with predictions drawn from mathematical models, allowed the researchers to understand the forces that produced the resulting film shapes.Their findings give a precise recipe or set of instructions for how to blow bubbles -- and with it, related production processes.""We can now say exactly what wind speed is needed to push out the film and cause it to form a bubble, and how this speed depends on parameters like the size of the wand,"" explains Ristroph.The research was supported, in part, by a grant from the National Science Foundation (CBET-1805506).Story Source:Materials provided by New York University. ",0.108487,0.130021,0.143306,0.180612,0.12207,0.165162,0.178504,0,0
467,Magnetic pumping pushes plasma particles to high energies A new model may explain a long-standing mystery of what makes the solar wind so hot,"As you walk away from a campfire on a cool autumn night, you quickly feel colder. The same thing happens in outer space. As it spins, the sun continuously flings hot material into space, out to the furthest reaches of our solar system. This material, called the solar wind, is very hot close to the sun, and we expect it to cool quickly as it streams away. Satellite observations, however, show this is not the case -- the solar wind cools as it streams out, but stays hotter than expected. There must be some additional way the solar wind heats up as it travels from the sun to Earth.The solar wind is not like a calm summer breeze. Instead, it is a roiling, chaotic mess of turbulence and waves. There is a lot of energy stored in this turbulence, so scientists have long thought that it heats the solar wind. There is, however, a big issue -- the heating expected from turbulence is not the heating observed.Scientists at the University of Wisconsin -- Madison have a new idea about what heats the solar wind, a theory called magnetic pumping. ""If we imagine a toy boat on a lake, waves move the toy boat up and down. However, if a rubber duck comes by and hits the toy boat it can get out of sync with the waves. Instead of moving along with the waves the toy boat is pushed by the waves, making it move faster. Magnetic pumping works the same way -- waves push the particles in the solar wind,"" said Emily Lichko, a graduate student who will be presenting her work at the American Physical Society's Division of Plasma Physics meeting in Portland, Ore.A special feature of the idea is that all the particles in the solar wind should be affected by magnetic pumping, including the most energetic. Heating due to turbulence has an upper limit, but the new idea allows for heating of even extremely fast particles.Where the solar wind hits Earth's magnetic field is a perfect place to look for magnetic pumping in nature. Satellites from NASA's Magnetospheric Multiscale (MMS) mission can measure the velocities of particles in incredible, unprecedented detail. The data shows evidence of magnetic pumping.This research, funded by NASA, the National Science Foundation, and the Department of Defense, is important because if energetic particles reach the space near Earth, they can damage satellites, harm astronauts, and even interrupt military communication. Understanding how these particles are energized, and what happens to them as they travel from the sun to Earth, will someday help scientists develop methods to better protect us from the effects of these particles. Additionally, it is possible that magnetic pumping could also be happening beyond the solar wind in places like the sun's atmosphere, the interstellar medium, or supernova explosions. This research has the potential to shed light not just on the solar wind, but on how particles throughout the universe are heated.Story Source:Materials provided by American Physical Society. ",0.181773,0.134448,0.160789,0.169682,0.138075,0.233849,0.173515,0,0
468,Different strategy of cycling teams in escape attempts A cyclist in a peloton experiences 5 to 10 times less air resistance than previously assumed,"In the middle of a peloton, racing cyclists experience only five to ten percent of the air resistance they face when cycling alone. A new study, published in the Journal of Wind Engineering & Industrial Aerodynamics, based on wind tunnel research on a peloton of 121 cyclists may explain why so few 'breakaways' in professional cycling races, like this year's Tour de France, are successful.""It turns out that current calculation models used by some race teams to determine the best time to escape are based on the wrong assumptions,"" explains lead author Professor Bert Blocken at the Eindhoven University of Technology & KU Leuven. ""Perhaps these new results will lead to more successful escapes and partly explain why so few escapes succeed, and why the peloton often hauls in the riders that do escape,"" Dr. Blocken added.It is well known that in the middle of a cycling peloton you ride 'out of the wind' and therefore experience less air resistance. How much less has never been thoroughly investigated. From previous research with smaller groups of riders, estimates have been made of 50 to 70 percent of the air resistance is experienced when compared to individual riders. Professional cyclists however, suggest that in a peloton you 'sometimes hardly have to pedal', which assumes that air resistance must be much lower.The research team, led by Dr. Blocken systematically chart, for the first time, the air resistance for each rider in a cycling peloton of 121 riders. The results showed that in the middle and at the back of the peloton the drag is about five to seven percent of what is experienced by a single rider. ""Put it another way: it is as if a rider is cycling at 12 to 15 km/h in a peloton that is speeding along at 54 km/h,"" Dr. Blocken added. ""That's why it feels like riders expend so little energy at the back."" Riders can now use these data to see what the best place is in a peloton.But Dr. Blocken warned that ""this does not mean a recreational cyclist can cycle along with the professional peloton. This might be possible for a short distance with a straight road on flat terrain, but as soon as bends are taken, the accordion effect sets in, and the peloton stretches out, causing the resistance to become much higher.""The study is a combination of computer simulations and wind tunnel measurements (including a complete peloton of 121 models), which independently gave the same results. Researchers examined two pelotons of 121 riders, where the distance between the rows differed slightly. Computer simulations amounted to 3 billion cells -- a world record for a sports application -- and required CRAY's American supercomputers and tens of thousands of software licenses from the company ANSYS. Simulations had to run for 54 hours for the calculation of a peloton, with a total of 49 terabytes of working memory.""At the very back, the air resistance is very low, but there is less opportunity to react to attacks and chances increase greatly for getting caught in a crash,"" says Dr. Blocken. ""So for classification riders or sprinters, the best position is in row six, seven or eight: there you are sufficiently shielded by other riders and you're near enough to the front.""The study also considered a peloton with a regular arrangement of riders, in order to allow clear conclusions to be drawn. The cyclists were static, not pedaling and not moving from one position to another in the peloton. There was no strong head-, tail- or side-wind. Only aerodynamic resistance was considered. The effects of these limitations will be investigated in follow-up studies by the researchers.Story Source:Materials provided by Elsevier. ",0.103936,0.0797425,0.133389,0.143473,0.0949923,0.15406,0.120474,0,0
469,Oil rigs may end their days as valuable artificial reefs,"A large group of international researchers have just published a scientific article in which they encourage environmental authorities across the globe to rethink the idea of removing oil rigs, wind turbines and other installations in the sea when they are worn out.A submerged camera at an old worn out oil rig shows an extensive life of flatfish, cod and bottom fauna in all its forms. A life usually not see in these parts of the North Sea, where the oil rig awaits decommissioning after 25 years' of loyal service in pumping oil and gas from the ground.""We also see many more porpoises around oil rigs than in the surrounding sea,"" says senior researcher Jonas Teilmann from Aarhus University, who has been involved in the studies that have just been published in the international journal Frontiers in Ecology and the Environment.""It's easy to understand why the porpoises enjoy the area. One can't throw a fish hook without catching one of the many cod around the legs of the oil rig,"" says Jonas Teilmann.Artificial reefs form oases in the seaAn oil rig or other artificial installations are typically present for 20-30 years in the sea. Through this period, the tubes, bars, concrete bricks and much more turn into beneficial substrate for adhering plants and animals. And this rich environment attracts fish and mammals.Internationally, it has been decided that that all artificial installations in the sea must be removed when they are no longer in use. But now almost 30 international researchers say that this decision perhaps should be reconsidered.""In, for example, the North Sea, an old oil rig will have the same function as a natural stone reef,"" Jonas Teilmann explains.And stone reefs are in short supply as stones have been removed and used for, among other things, pier construction or been destroyed and spread due to use of heavy trawls.""We have observed a significantly increased biodiversity around the old facilities and encourage the authorities to consider, in each individual case, whether an exemption from the demand for removal can be granted. When making the assessment, the environmental conditions must, of course, be of sufficient quality,"" says Jonas Teilmann.Avoid trawlingAround the world, there are more than 7,500 oil and gas platforms and between 10,000 and 20,000 wind turbines that need to be removed at some point. It is estimated that it will cost up to EUR 100 billion to remove these installations.But perhaps the money can be saved and conditions for marine life improved instead.""By leaving the rig in place, we may ensure greater biodiversity in the sea. The physical structures also ensure that the areas will not be trawled. The heavy trawls turn the seabed into a uniform desert with poor biodiversity,"" says Jonas Teilmann.The research group behind the article therefore urges politicians and officials throughout the world to introduce a temporary halt on the mandatory decommissioning of offshore infrastructure. According to the researchers, the competent authorities should instead make an environmental assessment of which structures to leave and conduct follow-up investigations of the effect of the new reefs.Story Source:Materials provided by Aarhus University. ",0.098889,0.138514,0.235558,0.169904,0.133765,0.168909,0.164108,0,0
470,Evaluation method for the impact of wind power fluctuation on power system quality Theory for stable power supply with widely introduced renewable energies,"Abrupt changes of wind power generation output are a source of severe damage to power systems. Researchers at Kyoto University developed a stochastic modeling method that enables to evaluate the impact of such phenomena. The feature of the method lies in its significant computational effectiveness in comparison to standard Monte Carlo simulation, and its applicability to analysis and synthesis of various systems subject to extreme outliers.Introduction of wind power generation into the electric power system is proceeding actively, mainly in the United States and Europe, and is expected to continue in Japan. However, upon the implementation, it is crucial to deal with prediction uncertainty of output fluctuation. The fluctuation of wind power generation is usually small, but it becomes extremely large due to the occurrence of gusts and turbulence at a non-negligible frequency. Such extreme outliers have been regarded as a source of severe damage to power systems.To cope with such a fluctuation of wind power generation, the goal setting such as ""absolutely keep the frequency fluctuation within 0.2 Hz"" would be unattainable or would result in an overly conservative design. Therefore, the probabilistic goal setting such as ""keep the frequency fluctuation within 0.2 Hz with 99.7% or more"" is indispensable.Probabilistic uncertainty is evaluated statistically, commonly by assuming that it obeys normal distribution for its mathematical processability. The output outliers in wind power generation are, however, more frequent than represented by normal distribution. Even if a complicated simulator can be constructed without assuming normal distribution, it is not realistic to investigate the statistical property by Monte Carlo simulation. This is because the required number of samples explodes before sufficiently many extreme outliers occur.An evaluation method was developed for the impact of wind power fluctuation on power system quality. The method first builds probabilistic models assuming the stable distribution (an extension of the normal distribution) on the uncertainty. Then, instead of using the model as a simulator to generate data samples, we compute the statistical properties directly from parameters in the model. The important feature is 1. the influence of extreme outliers can be properly considered, 2. model can be determined easily from actual data, and 3. computation cost is very low. The method was proved to be valid through its application to frequency deviation estimation based on actual power system data.This newly proposed probabilistic evaluation method enables us to quantitatively evaluate the power system risk caused by the occurrence of extremally abrupt changes of wind power generation. Countermeasures based on the evaluation would contribute to improvement of the reliability and economic efficiency of the electric power system. It should be also noted that the proposed method is applicable to analysis and synthesis of various systems which have extreme outliers.Story Source:Materials provided by Japan Science and Technology Agency. ",0.132039,0.096353,0.146397,0.196133,0.124968,0.198962,0.146322,0,0
471,Does living near wind turbines negatively impact human health?,"Wind turbines are a source of clean renewable energy, but some people who live nearby describe the shadow flicker, the audible sounds and the subaudible sound pressure levels as ""annoying."" They claim this nuisance negatively impacts their quality of life.A team of researchers from the University of Toronto and Ramboll, an engineering company funding the work, set out to investigate how residential distance from the wind turbines -- within a range of 600 meters (1,968.5 feet) to 10 kilometers (6.2 miles) -- affects people's health.They reanalyzed data collected for the ""Community Noise and Health Study"" from May to September 2013 by Statistics Canada, the national statistical office. The team reports their new analysis in the Journal of the Acoustical Society of America.""The Community Noise and Health Study generated data useful for studying the relationship between wind turbine exposures and human health -- including annoyance and sleep disturbances,"" said Rebecca Barry, an author on the paper. ""Their original results examined modeled wind turbine noise based on a variety of factors -- source sound power, distance, topography and meteorology, among others.""The team's new assessment confirmed Statistics Canada's initial findings. ""Respondents who live in areas with higher levels of modeled sound values (40 to 46 decibels) reported more annoyance than respondents in areas with lower levels of modeled sound values (<25 dB),"" Barry said. Unsurprisingly, the survey's respondents who live closer to the turbines ""were more likely to report being annoyed than respondents who live further away.""The earlier Statistics Canada study found no direct link between residents' distance from wind turbines and sleep disturbances (as measured by sleep assessments and the Pittsburgh Sleep Quality Index), blood pressure, or stress (either self-reported or measured via hair cortisol). However, the more recent study showed that survey respondents closer to wind turbines reported lower ratings for their environmental quality of life. Barry and her co-authors note that their cross-sectional study cannot distinguish whether these respondents were dissatisfied before the wind turbines were installed.""Wind turbines might have been placed in locations where residents were already concerned about their environmental quality of life,"" said Sandra Sulsky, a researcher from Ramboll. ""Also, as is the case with all surveys, the respondents who chose to participate may have viewpoints or experiences that differ from those who chose not to participate. Survey respondents may have participated precisely to express their dissatisfaction, while those who did not participate might not have concerns about the turbines.""The team's more recent study didn't explicitly find evidence that exposure to wind turbines actually impacts human health, but in the future, ""measuring the population's perceptions and concerns before and after turbine installation may help to clarify what effects -- if any -- exposure to wind turbines may have on quality of life,"" Sulsky said.Story Source:Materials provided by American Institute of Physics. ",0.12297,0.118989,0.197634,0.180348,0.143121,0.190575,0.153225,0,0
472,Construction delays make new nuclear power plants costlier than ever,"The cost of building new nuclear power plants is nearly 20 per cent higher than expected due to delays, a new analysis has found.A new analysis of the history of nuclear power plant projects shows since 2010 delays have contributed 18 per cent the costs.These delays -- which can run into years or even decades -- increase the cost compared with older projects and are often overlooked when new projects are planned. The authors say that these extra costs need to be properly assessed when considering new nuclear projects.They say nuclear projects are more like 'mega-projects', such as large dams, which require more rigorous financial assessments due to their high uncertainty and risk.In the study, published today in the journal Energy Policy, the authors also suggest that because these delay costs make nuclear projects high risk, decision makers might instead focus on more low-risk low-carbon technologies such as wind or solar power.When assessing the cost of new nuclear projects, decision makers often use 'overnight construction costs', which assume the project is built on time, usually within five years. However, the 'lead-time' -- the time between initiation of the project and completion -- can cause significant extra costs.The research team, from Imperial College London, the Universidade Federal do Rio de Janeiro and the University of Minho, looked at total costs of nuclear projects between 1955-2016, including delay costs.Usually, as technologies mature and experience is gained in construction, costs come down. However, the team found that for nuclear, there has been a blip in the learning curve, with costs currently increasing, especially for projects since 2010.Lead author Dr Joana Portugal Pereira, from the Centre for Environmental Policy at Imperial, said: ""Nuclear projects are actually becoming more complex to carry out, inducing delays and higher costs. Safety and regulatory considerations play heavily into this, particularly in the wake of the 2011 Fukushima Dai-ichi nuclear accident in Japan.""The analysis is one of the first to assess full financial costs of building nuclear projects throughout time, and not just the 'overnight' costs. It also looked at projects around the world, including newer nuclear builders like China, India, and the UAE, rather than just the traditional builders in Europe, the USA and Japan.They say that while nuclear projects can help bridge the gap between fossil fuels and renewable energy, they could hinder progress if projects stall.Dr Portugal Pereira said: ""If we want to decarbonise our energy system, nuclear may not be the best choice for a primary strategy. Nuclear power is better late than never, but to really address climate change, it would be best if they were not late at all, as technologies like wind and solar rarely are.""Story Source:Materials provided by Imperial College London. ",0.123093,0.128071,0.180657,0.206826,0.128811,0.221405,0.175282,0,0
473,Can we get 100 percent of our energy from renewable sources? New article gathers the evidence to address the sceptics,"Is there enough space for all the wind turbines and solar panels to provide all our energy needs? What happens when the sun doesn't shine and the wind doesn't blow? Won't renewables destabilise the grid and cause blackouts?In a review paper last year in the high-ranking journal Renewable and Sustainable Energy Reviews, Master of Science Benjamin Heard and colleagues presented their case against 100% renewable electricity systems. They doubted the feasibility of many of the recent scenarios for high shares of renewable energy, questioning everything from whether renewables-based systems can survive extreme weather events with low sun and low wind, to the ability to keep the grid stable with so much variable generation.Now scientists have hit back with their response to the points raised by Heard and colleagues. The researchers from the Karlsruhe Institute of Technology, the South African Council for Scientific and Industrial Research, Lappeenranta University of Technology, Delft University of Technology and Aalborg University have analysed hundreds of studies from across the scientific literature to answer each of the apparent issues. They demonstrate that there are no roadblocks on the way to a 100% renewable future.""While several of the issues raised by the Heard paper are important, you have to realise that there are technical solutions to all the points they raised, using today's technology,"" says the lead author of the response, Dr. Tom Brown of the Karlsruhe Institute of Technology.""Furthermore, these solutions are absolutely affordable, especially given the sinking costs of wind and solar power,"" says Professor Christian Breyer of Lappeenranta University of Technology, who co-authored the response.Brown cites the worst-case solution of hydrogen or synthetic gas produced with renewable electricity for times when imports, hydroelectricity, batteries, and other storage fail to bridge the gap during low wind and solar periods during the winter. For maintaining stability there is a series of technical solutions, from rotating grid stabilisers to newer electronics-based solutions. The scientists have collected examples of best practice by grid operators from across the world, from Denmark to Tasmania.Furthermore, these solutions are absolutely affordable, especially given the sinking costs of wind and solar power.The response by the scientists has now appeared in the same journal as the original article by Heard and colleagues.""There are some persistent myths that 100% renewable systems are not possible,"" says Professor Brian Vad Mathiesen of Aalborg University, who is a co-author of the response.""Our contribution deals with these myths one-by-one, using all the latest research. Now let's get back to the business of modelling low-cost scenarios to eliminate fossil fuels from our energy system, so we can tackle the climate and health challenges they pose.""Story Source:Materials provided by Lappeenranta University of Technology. ",0.110409,0.122235,0.137468,0.174348,0.114675,0.21493,0.151658,0,0
474,European wind energy generation potential in a 1.5 degree C warmer world,"The UK and large parts of northern Europe could become windier if global temperatures reach 1.5 degrees C above pre-industrial levels, according to a new study.This has implications for wind energy generation among other things. The results suggest that wind could be a more important source of energy generation than previously thought, with stronger winds across the UK. The research team concludes there could be a 10% increase in UK onshore wind energy generation, which would be sufficient to power the equivalent of an extra 700,000 homes every year based on current installed capacity. The results are relevant for decisions about future investment in onshore wind farms.To evaluate potential changes in European wind power generation in a 1.5 degrees C warmer world, researchers from British Antarctic Survey, the University of Oxford and the University of Bristol combined data from 282 onshore wind turbines collected over 11 years with climate model data from the HAPPI project.This study did not consider offshore wind energy generation potential.Across northern Europe, the results suggest that large areas of Germany, Poland and Lithuania could become more viable for wind power in future. But the biggest increases in wind could be seen in the UK -- along with marked seasonal shifts in wind.Lead author, climate modeller Dr Scott Hosking at British Antarctic Survey, says:""In future, nine months of the year could see UK wind turbines generating electricity at levels currently only seen in winter. Future summers could see the largest increase in wind generation. Therefore, wind could provide a greater proportion of the UK's energy mix than has been previously assumed.""Wind power is central to a low carbon economy. In Europe, wind energy currently accounts for 18% of total generating capacity and the European Commission's 2030 energy strategy set a renewables target of at least 27%.However, wind is also a highly variable energy source. While weather forecasts can help even out short-term differences in supply and demand, governments and industry need more information on longer-term changes in wind.Signatories of the Paris Agreement have committed to keep global average temperatures to well below 2 degrees C increase from pre-industrial times, with an ambition to limit the increase to 1.5 degrees C.Story Source:Materials provided by British Antarctic Survey. ",0.14622,0.130245,0.174891,0.161415,0.127574,0.223547,0.150153,0,0
475,Impacts of windfarm construction on harbor porpoises New model predicting the impacts of anthropogenic disturbances on marine populations,"Scientists from Germany, Denmark and the UK have built a model tool to predict what happens to marine animals when exposed to noise from the construction and operation of windfarms at sea. Using the North Sea harbor porpoise (Phocoena phocoena) population as a case study, they demonstrate how the model can be used to evaluate the impact of offshore wind farm construction noise.This type of noise is increasingly prevalent due to the high demand for green energy, and currently there are >900 offshore wind farms at various stages of development in Europe alone. Porpoises are strictly protected in European waters, so assessing the impacts of construction noise is critical for regulators. We demonstrate how the framework can be used for spatial planning to partly mitigate population impacts of disturbances.The model -- named DEPONS -- builds directly on how disturbances influence animal movements, foraging and energetics, and is therefore applicable to a wide range of species. To demonstrate the model, the impact of wind farm construction noise on the North Sea harbor porpoise population was assesed. The scientists monitored the population density during construction of Gemini, a Dutch offshore wind farm, by recording the echolocation sounds that porpoises use for navigating. Afterwards a virtual Gemini landscape where wind turbines were built in the same order, and generating the same amount of noise, as in the wind farm where porpoises had been monitored. This landscape was used for running scenarios in the model.Story Source:Materials provided by Aarhus University. ",0.186063,0.167512,0.296431,0.216957,0.189437,0.244106,0.207185,0,0
476,"Natural gas prices, not 'war on coal,' were key to coal power decline ","New research from North Carolina State University and the University of Colorado Boulder finds that steep declines in the use of coal for power generation over the past decade were caused largely by less expensive natural gas and the availability of wind energy -- not by environmental regulations.""From 2008 to 2013, coal dropped from about 50 percent of U.S. power generation to around 30 percent,"" says Harrison Fell, an associate professor of resource economics at NC State and co-lead author of a paper on the work.""Coal boosters blamed stiffer regulations, calling it a 'war on coal.' But that same time period saw a steep drop in the cost of natural gas and an increase in wind generation. We wanted to know how big a role each of these factors played in driving down the demand for coal.""To answer this question, the researchers looked at how much of their daily power generation capacity coal plants were using in four power transmission regions from the beginning of 2008 to the end of 2013. The four power regions -- the Electric Reliability Corporation of Texas (ERCOT), the Southwest Power Pool, the Midcontinent Independent System Operator and PJM Interconnection (PJM) -- spread across more than 20 states.Consistent with national declines, coal plants in all four regions used much less of their power capacity in 2013 than in 2008. Similarly, the price of natural gas dropped in all four regions, and the amount of available wind power increased.The researchers then created a model that accounted for an array of variables, such as daily power demand, and ran it to see how power use would have changed in 2008 if gas had been available at 2013 prices and wind power had been available at 2013 levels.""This work uses the observed data -- capacity factors, fuel prices, power demand and so on -- to make predictions about how capacity factors are affected by different variables,"" Fell says. ""In short, we can get a good idea of what influences the extent to which we use coal power generation.""And what the researchers found was that, if in 2008 natural gas had been available at 2013 prices and wind power had been available at 2013 levels, coal power use would have dropped significantly compared to what was observed in 2008. In fact, these predicted ""counterfactual"" 2008 capacity usages were similar to observed 2013 capacity usages. This suggests the so-called ""war on coal"" regulations were not the driver of the coal generation decline over this period.""If the 'war on coal' was what drove down power generation, our econometric models would not have predicted a drop in coal use caused by changes in gas and wind,"" Fell says. ""But they did. It looks like the changes in coal power production were actually driven largely by capitalism.""Story Source:Materials provided by North Carolina State University. ",0.129145,0.162201,0.186191,0.201551,0.146554,0.245333,0.182626,0,0
477,Environmental impact of electric vehicles in China? It depends on how they are charged Research offers a strategy for reducing CO2 emissions and improving air quality with electric vehicles in Beijing,"Electric vehicles play a key role in China's plan to improve air quality and reduce CO2 emissions but, with the majority of China's electricity still coming from coal-fired power plants, many question just how effective this strategy will be.Now, researchers from Harvard University and Tsinghua University in Beijing found that private electric vehicles in China can have a positive effect on CO2 reduction if owners can be incentivized to slowly charge vehicles during off-peak hours, allowing for more effective use of wind-generated power. Quickly charging vehicles in the higher energy ""fast mode,"" on the other hand, can be counterproductive.Meanwhile, buses and taxis In China make a significant contribution to NOX emissions, a major precursor for air pollution. The team determined that electrifying the bus and taxi fleet offers the most effective option for improving air quality.The research is published in Nature Energy.""This research offers a more nuanced strategy for reducing CO2 emissions and improving air quality in China,"" said Michael B. McElroy, the Gilbert Butler Professor of Environmental Studies at the Harvard John A. Paulson School of Engineering and Applied Sciences (SEAS) and the Department of Earth and Planetary Sciences and co-author of the research.""It is critically important that electric vehicle charging is managed properly to maximize the benefits of renewables,"" said Xinyu Chen, Research Associate in Environmental Science and Engineering at SEAS and co-author of the study.The researchers used real-time power demand data and driving patterns for Beijing and its suburbs to develop a comprehensive model of the energy system. They found that how electric vehicles are charged -- whether in the low-energy slow mode or high-energy fast mode -- plays a significant role in the integration of wind energy. Typically, vehicle charging in the 30-minutes or less fast mode occurs during hours of peak power demand. As a result, peak power demand is increased, triggering additional coal generators to come online. With these generators operational at night, the opportunity to take advantage of available wind power is reduced.""If people were incentivized to wait until evening and charge their vehicles in the slow-charge mode, which takes hours, the power load could take advantage of wind energy available during off-peak hours,"" said Chris P. Nielsen, Executive Director of the Harvard China Project and co-author of the study.In terms of lowering air pollution, the researchers found that the gas- or diesel-fueled fleet of public vehicles -- about 30,000 buses and 66,000 taxis -- is responsible for nearly 20 percent of total NOX emissions, equivalent to the contribution from 8.2 million private vehicles. Electrifying the fleet of buses and taxis in Beijing would significantly reduce total NOX emissions and increase air quality.""Electrifying the public fleet and introducing incentives to charge personal electric vehicles at off-peak times would be the most effective strategy to reduce NOX and CO2 emissions in Beijing,"" said McElroy. ""This strategy could also be applied to cities across the world that have a significant source of electricity from coal.""Story Source:Materials provided by Harvard John A. Paulson School of Engineering and Applied Sciences. ",0.115598,0.129138,0.125464,0.156281,0.107855,0.206017,0.143704,0,0
478,New control strategy helps reap maximum power from wind farms Researchers use supercomputers at the Texas Advanced Computing Center to improve wind energy production,"Every two and a half hours, a new wind turbine rises in the U.S. In 2016, wind provided 5.6 percent of all electricity produced, more than double the amount generated by wind in 2010, but still a far cry from its potential.A team of researchers from The University of Texas at Dallas (UT Dallas) has developed a new way to extract more power from the wind. This approach has the potential to increase wind power generation significantly with a consequent increase in revenue. Numerical simulations performed at the Texas Advanced Computing Center (TACC) indicate potential increases of up to six to seven percent.According to the researchers, a one percent improvement applied to all wind farms in the nation would generate the equivalent of $100 million in value. This new method, therefore, has the potential to generate $600 million in added wind power nationwide.The team reported their findings in Wind Energy in December 2017 and Renewable Energy in December 2017.In the branch of physics known as fluid dynamics, a common way to model turbulence is through large eddy simulations. Several years ago, Stefano Leonardi and his research team created models that can integrate physical behavior across a wide range of length scales -- from turbine rotors 100 meters long, to centimeters-thick tips of a blades -- and predict wind power with accuracy using supercomputers.""We developed a code to mimic wind turbines, taking into account the interference between the wake of the tower and the nacelle [the cover that houses all of the generating components in a wind turbine] with the wake of the turbine rotor,"" said Leonardi, associate professor of mechanical engineering and an author on the Wind Energy paper, which was selected for the cover.Beyond the range of length scales, modeling the variability of wind for a given region at a specific time is another challenge. To address this, the team integrated their code with the Weather Research and Forecasting Model (WRF), a leading weather prediction model developed at the National Center for Atmospheric Research.""We can get the wind field from the North American Mesoscale Model on a coarse grid, use it as an input for five nested domains with progressively higher resolution and reproduce with high fidelity the power generation of a real wind farm,"" Leonardi said.The growing power of computers allows Leonardi and his team to accurately model the wind field on a wind farm and the power production of each single turbine. Testing their model's results against data from a wind farm in North Texas, they saw a 90 percent agreement between their predictions and the turbine's efficiency. They will present their results at Torque 2018, a major wind energy research conference.TAKING THE TURBULENCE OUT OF THE OPTIMIZATION CONTROL ALGORITHMWind doesn't simply flow smoothly in one direction. It contains turbulence and wakes which are magnified when turbines are grouped together as they are on a wind farm.Wake interactions lead to losses of up to 20 percent of annual production, according to the U.S. Department of Energy. Understanding how turbulence impacts energy generation is important to adjust the behavior of the turbines in real-time to reap maximum power.Using their modeling capabilities, they tested control algorithms that are used to manage the operation of dynamic systems at wind farms. This included the control algorithms known as extremum seeking control, a model-free way of getting the best performance out of dynamic systems when only limited knowledge of the system is known.""Many thought it would not be possible to use this approach because of turbulence and the fact that it provides a situation where turbines are changing all the time,"" Leonardi said. ""But we did a huge number of simulations to find out a way to filter turbulence out of the control scheme. This was the major challenge.""With extremum seeking control, the system increases and reduces the rotational speed of a spinning turbine blade, all the while measuring the power, and calculating the gradient. This is repeated until the controller finds the optimal operating speed.""The important thing is that the control algorithm does not rely on a physics-based model,"" Leonardi said. ""There are many uncertainties in a real wind farm, so you cannot model everything. The extremum seeking control can find the optimum no matter if there is erosion or icing on the blades. It's very robust and works despite uncertainties in the system.""SIMULATING THE WINDTo test their new approach, the team ran virtual wind experiments using supercomputers at the TACC, including Stampede2 and Lonestar5 -- two of the most powerful in the world. They were able to use these systems through the University of Texas Research Cyberinfrastructure (UTRC) initiative, which, since 2007, has provided researchers at any of the University of Texas System's 14 institutions access to TACC's resources, expertise and training.Access to powerful supercomputers is important because wind turbines are expensive to build and operate and few wind research facilities are available to researchers.""The benefits of using high performance computing to create a virtual platform for doing analyses of proposed solutions for wind energy are enormous,"" said Mario Rotea, professor of mechanical engineering at UT Dallas, and site director of the National Science Foundation-supported Wind-Energy Science, Technology and Research (WindSTAR) Industry-University Cooperative Research Center (IUCRC). ""The more we can do with computers, the less we have to do with testing, which is a big part of the costs. This benefits the nation by lowering the cost of energy.""While the application of extremum seeking control to wind farms is yet to be field tested, the UT Dallas team already applied the method to a single turbine at the National Renewable Energy Laboratory (NREL).""The NREL test gave us experimental data supporting the value of extremum seeking control for wind power maximization,"" said Rotea. ""The experimental results show that extremum seeking control increases the power capture by 8-12% relative to a baseline controller.""Given the encouraging experimental and computational results, the UT Dallas team is planning an experimental campaign involving a cluster of turbines in a wind farm.COLLABORATIONS AND NEXT STEPSThe development of the fluid dynamics model for wind turbines was part of an international collaboration between four U.S. institutions (Johns Hopkins University, UT Dallas, Texas Tech and Smith College) and three European institutions (Technical University of Denmark, ecole polytechnique federale de Lausanne and Katholieke Universiteit Leuven) funded by the National Science Foundation.Through the WindSTAR center, they collaborate with nine leading wind energy companies and equipment manufacturers. These companies are interested in adopting or commercializing the work.""The members of our center do not have access to a lot of horsepower in terms of HPC [high-performance computing],"" said Rotea. ""The computers at TACC are an asset for us and give us a competitive advantage over other groups. In terms of solving actual problems, we create control systems that they may incorporate, or they may use HPC to develop new tools for forecasting wind resources or determine if there are turbines that are not performing.""In addition to developing the new turbulence algorithms and control strategies, members of the WindSTAR team have introduced methods to predict accurate results on less-powerful computers (work that appeared in the March 2018 issue of Wind Energy) and to determine how closely to place turbines to maximize profits, depending on the cost of land (presented at the 2018 Wind Symposium).The long-term effects of the work go beyond the theoretical.""The research allows us to optimize wind energy power production and increase the penetration of renewable energy in the grid,"" Leonardi said. ""There will be more power generated by the same machines because we understand more about the flow physics in a wind farm, and for the same land use and deployment, we can get more energy.""Story Source:Materials provided by University of Texas at Austin, Texas Advanced Computing Center. ",0.103373,0.0706287,0.105106,0.154946,0.0843328,0.164605,0.111295,0,0
479,Silent marine robots record sounds underwater,"Silent marine robots that record sounds underwater are allowing researchers at the University of East Anglia (UEA) to listen to the oceans as never before.The robots are about the same size as a small human diver, but can reach depths of 1000 metres and travel the ocean for months, covering thousands of kilometres. They communicate by satellite with their pilot to build an underwater soundscape of the world's oceans.Pierre Cauchy, a PhD researcher from UEA's School of Environmental Sciences, has been using one of these autonomous submarines for five years, recording underwater noises in the Mediterranean Sea and the North Atlantic and Southern oceans.The recordings can be used to measure sea-surface wind speed and monitor storms as well as eavesdropping on marine life.Mr Cauchy will present his research at the General Assembly of the European Geosciences Union in Vienna. He will show how the robot -- called a Seaglider -- can measure the wind speed, listen in to the sounds made by fishes and whales, and pick up human activities, such as marine traffic and seismic surveys.By recording sounds in remote locations where there are no permanent weather stations, the robots provide valuable information on wind or storm patterns, which can help to finetune climate models.Mr Cauchy said: ""As an acoustician, it is fascinating to listen in to underwater life such as long-finned pilot whales in the North Atlantic, but also to hear the echoes of what is happening in the skies above.""While pilot whales make whistles, buzzes and clicks, pods of hunting dolphins create high- pitched echolocation clicks and larger species such as sperm whales make louder, slower clicks.High winds raise the background noise level, seismic surveys' intense pulses are unique and easily identifiable, and marine vessels are clearly identified by low-frequency rumbles.The Seaglider weighs just over 50kg and is 1.5 metres tall. It is remotely controlled by a pilot and is silent, so records only sound from the ocean without adding its own tones.Mr Cauchy said: ""Now that they have been shown to be useful for modelling climate, monitoring storms or protecting marine life, I hope that other researchers will integrate the silent robot divers into their work and their use will broaden.""Story Source:Materials provided by University of East Anglia. ",0.175028,0.15434,0.214573,0.185087,0.162333,0.209213,0.187241,0,0
480,Banking on sunshine: World added far more solar than fossil fuel power generating capacity in 2017. China leads with more than half of world's new solar capacity.  Global solar investment jumps 18 percent to $160.8 billion.," Cumulative renewable energy investment since 2004: $2.9 trillion. Solar energy dominated global investment in new power generation like never before in 2017.The world installed a record 98 gigawatts of new solar capacity, far more than the net additions of any other technology -- renewable, fossil fuel or nuclear.Solar power also attracted far more investment, at $160.8 billion, up 18 per cent, than any other technology. It made up 57 per cent of last year's total for all renewables (excluding large hydro) of $279.8 billion, and it towered above new investment in coal and gas generation capacity, estimated at $103 billion.A driving power behind last year's surge in solar was China, where an unprecedented boom saw some 53 gigawatts added -- more than half the global total -- and $86.5 billion invested, up 58 per cent.The Global Trends in Renewable Energy Investment 2018 report, released today by UN Environment, Frankfurt School -- UNEP Collaborating Centre, and Bloomberg New Energy Finance, finds that falling costs for solar electricity, and to some extent wind power, is continuing to drive deployment. Last year was the eighth in a row in which global investment in renewables exceeded $200 billion -- and since 2004, the world has invested $2.9 trillion in these green energy sources.""The extraordinary surge in solar investment shows how the global energy map is changing and, more importantly, what the economic benefits are of such a shift,"" said UN Environment head Erik Solheim. ""Investments in renewables bring more people into the economy, they deliver more jobs, better quality jobs and better paid jobs. Clean energy also means less pollution, which means healthier, happier development.""Overall, China was by far the world's largest investing country in renewables, at a record $126.6 billion, up 31 per cent on 2016.There were also sharp increases in investment in Australia (up 147 per cent to $8.5 billion), Mexico (up 810 per cent to $6 billion), and in Sweden (up 127 per cent to $3.7 billion).A record 157 gigawatts of renewable power were commissioned last year, up from 143 gigawatts in 2016 and far out-stripping the net 70 gigawatts of fossil-fuel generating capacity added (after adjusting for the closure of some existing plants) over the same period.""The world added more solar capacity than coal, gas, and nuclear plants combined,"" said Nils Stieglitz, President of Frankfurt School of Finance & Management. ""This shows where we are heading, although the fact that renewables altogether are still far from providing the majority of electricity means that we still have a long way to go.""Some big markets, however, saw declines in investment in renewables. In the United States, investment dropped 6 per cent, coming in at $40.5 billion. In Europe there was a fall of 36 per cent, to $40.9 billion, with big drops in the United Kingdom (down 65 per cent to $7.6 billion) and Germany (down 35 per cent to $10.4 billion). Investment in Japan slipped 28 per cent to $13.4 billion.Angus McCrone, Chief Editor of Bloomberg New Energy Finance and lead author of the report, said: ""In countries that saw lower investment, it generally reflected a mixture of changes in policy support, the timing of large project financings, such as in offshore wind, and lower capital costs per megawatt.""Global investments in renewable energy of $2.7 trillion from 2007 to 2017 (11 years inclusive) have increased the proportion of world electricity generated by wind, solar, biomass and waste-to-energy, geothermal, marine and small hydro from 5.2 per cent to 12.1 per cent.The current level of electricity generated by renewables corresponds to about 1.8 gigatonnes of carbon dioxide emissions avoided -- roughly equivalent to those produced by the entire U.S. transport system.The Global Trends in Renewable Energy Investment 2018 report was funded by the German Federal Ministry for the Environment, Nature Conservation and Nuclear SafetyTo download the report: https://drive.google.com/file/d/1SmhaI-WAcmEMqR8R9oL5Fxn0cZ0kfY8Z/viewAll renewable energy investment totals exclude large hydro, which falls outside the scope of the report.Story Source:Materials provided by UN Environment. ",0.13799,0.165574,0.175193,0.206165,0.159213,0.245916,0.195923,0,0
481,New records in perovskite-silicon tandem solar cells through improved light management,"Tandem solar cells made of silicon and metal halide perovskite compounds can convert a particularly large portion of the solar spectrum into electrical energy. However, part of the light is reflected and is thus lost for purposes of energy conversion. Using nanostructures, the reflection can be reduced significantly ensuring that the solar cell captures more light. For example, pyramid-shaped microfeatures can be etched into silicon. However, these features cause microscopic roughness in the silicon surface, making it no longer suitable as a substrate for deposition of extremely thin perovskite layers. This is because perovskites are normally deposited to a polished wafer using solution processing to form an extremely thin film, much thinner than the pyramidal features. A rough-etched silicon surface layer therefore prevents formation of a uniform conformal layer.Efficiency improved from 23,4 % to 25.5 %A team headed by HZB physicist Steve Albrecht has investigated an alternative approach of light management with textures in tandem solar cells. The team fabricated an efficient perovskite/silicon tandem device whose silicon layer was etched on the back-side. The perovskite layer could be applied by spincoating onto the smooth front-side of the silicon. The team afterwards applied a polymer light management (LM) foil to the front-side of the device. This enabled processing of a high-quality perovskite film on a flat surface, while still benefiting from the front-side texture. ""In this way, we succeeded in considerably improving the efficiency of a monolithic perovskite-silicon heterojunction tandem cell from 23.4 % to 25.5 %,"" says Marko Joet, first author of the study and postdoctoral fellow in Albrecht's team.Numerical model shows possibility for up to 32.5 %In addition, Joet and colleagues have developed a sophisticated numerical model for complex 3D features and their interaction with light. This enabled the team to calculate how different device designs with textures at various interfaces affect efficiency. ""Based on these complex simulations and empirical data, we believe that an efficiency of 32.5 % can realistically be achieved -- if we succeed to incorporate high quality perovskites with a band gap of 1.66 eV,"" says Joet.Suitable for building integrated PVAnd team leader Steve Albrecht adds: ""Based on real weather data, we were able to calculate the energy yield over the course of a year -- for the different cell designs and for three different locations."" In addition, the simulations show that the LM foil on the front-side of the solar cell device is particularly advantageous under diffuse light irradiation, i.e. not only under perpendicularly incident light. Tandem solar cells with the new LM foil could therefore also be suitable for incorporation in building-integrated photovoltaics (BIPV), opening up huge new areas for energy generation from large sky scraper facades.Story Source:Materials provided by Helmholtz-Zentrum Berlin fer Materialien und Energie. Note: Content may be edited for style and length.Journal Reference:Cite This Page:Get the latest science news with ScienceDaily's free email newsletters, updated daily and weekly. Or view hourly updated newsfeeds in your RSS reader:Keep up to date with the latest news from ScienceDaily via social networks:Tell us what you think of ScienceDaily -- we welcome both positive and negative comments. Have any problems using the site? Questions?",0.106804,0.138331,0.1479,0.169457,0.185461,0.132744,0.146956,0,0
482,"Ultra-thin transparent silver films for solar cells New silver films may boost the efficiency of solar cells and light-emitting diodes. However, they have been difficult to fabricate.","A new fabrication process for transparent ultra-thin silver films has been developed by researchers at Ruhr-Universitet Bochum and the University of Wuppertal. The material may help build highly efficient solar cells and light-emitting diodes. However, traditional chemical methods have not been able to produce ultra-thin and pure silver films. A team headed by Professor Anjana Devi and Nils Boysen from the Bochum-based research group Inorganic Materials Chemistry, in collaboration with the group of Professor Thomas Riedl from the Chair of Electronic Devices in Wuppertal, published an article on a new synthesis method in the journal Angewandte Chemie. The article was published online on 27 September 2018.New precursor chemistry""Precursors for the fabrication of ultra-thin silver films are highly sensitive to air and light,"" explains Nils Boysen. The silver precursors can be stabilised with fluorine, phosphorus or oxygen. ""However, these elements contaminate the thin films as well as the equipment used for the production,"" continues the researcher. In the course of his Master thesis, Boysen and his colleagues developed an alternative solution to tackle the problems associated with common silver precursors.The researchers created a chemical silver precursor, where the silver is surrounded by an amide and a carbene, which is even stable without elements like fluorine, phosphorus or oxygen. They demonstrated that a silver thin film can be applied to an electrode with the new precursor by atomic layer deposition. In the process, the gaseous precursor is transported to the electrode and a silver film is deposited there as a layer with a thickness of merely a few atoms. Because it is so thin, the silver film is transparent.""As the process can be operated under atmospheric pressure and at low temperatures, the conditions for industrial production are quite favourable,"" says Anjana Devi.A chance for highly efficient solar cells and lightsFollowing a series of tests, the researchers showed that the thin silver films manufactured using this method are pure and electrically conductive. ""As far as process technology is concerned, the successful synthesis of the new precursor paves the way for the development of ultra-thin silver films,"" concludes Thomas Riedl. ""It constitutes a first step towards the production of novel electrodes for highly efficient solar cells and lights.""""The collaboration between the chemists from Bochum and the engineers from Wuppertal was the key to success,"" stresses Anjana Devi.Story Source:Materials provided by Ruhr-University Bochum. ",0.129618,0.261624,0.169724,0.206806,0.189768,0.179048,0.208449,0,0
483,Solar power: largest study to date discovers 25 percent power loss across UK,"Researchers at the University of Huddersfield have undertaken the largest study to date into the effectiveness of solar panels across the UK and discovered that parts of the country are suffering an overall power loss of up to 25% because of the issue of regional 'hot spots'. Hot spots were also found to be more prevalent in the North of England than in the south.Dr Mahmoud Dhimish, a lecturer in Electronics and Control Engineering and co-director of the Photovoltaics Laboratory at the University, analysed 2,580 polycrystalline silicon photovoltaic (PV) panels distributed across the UK. The UK has been fossil-free for two years and demand is constantly increasing for renewable energy.After quantifying the data, Dr Dhimish discovered that the panels found to have hot spots generated a power output notably less than those that didn't. He also discovered that location was a primary contributor in the distribution of hot spots.Photovoltaics hot spots are areas of elevated temperature which can affect only part of the solar panel. They are a result of a localised decrease in efficiency and the main cause of accelerated PV ageing, often causing permanent damage to the solar panel's lifetime performance.According to Dr Dhimish, this is the first time an investigation into how hot spots impact the performance of PV panels has been conducted from such a large scale dataset and says the project uncovered results which demonstrate the preferred location of UK hot spots.""This research showed the unprecedented density of hot spots in the North of England,"" said Dr Dhimish. ""Over 90% of the hot spots are located in the north and most of these are inland, with considerably less seen on the coast.""Solar panel performance disparityDr Dhimish says this confirms results from previous research, which concludes when affected cells are partially shaded, it increases the likelihood of hot spots.But he said what was surprising, was the lesser amount seen around coastal regions, which leads him to believe cooler winds coming in from the ocean are keeping the overall temperature of the PV panels down and are preventing hot spots occurring.In order to increase the reliability and durability of future residential photovoltaics installations, Dr Dhimish has made three recommendations.""PV panels, or defective bypass diodes, affected by multiple hot spotted PV Cells have to be replaced since they significantly reduce the reliability and yield energy of the PV installation,"" he said.""Secondly, it is recommended to install the solar panels in coastal locations because they are less likely to be affected by the hot spotting phenomenon and finally, the solar energy industry must start investigating the impact of PV hot spotting on the accuracy of existing maximum power point tracking (MPPT) units available in the market.""Story Source:Materials provided by University of Huddersfield. ",0.128928,0.158625,0.194018,0.178572,0.15937,0.202366,0.186653,0,0
484,Highly efficient wet-processed solar cells with molecules in the same orientation,"Solar cells are a cost-effective, alternate source of energy. A subtype of these, organic solar cells make use of organic polymers inside the cell. Using these polymers makes the cells light-weight and increases their flexibility. Organic solar cells are produced by two different chemical methods: dry processing and wet processing, with the latter being a faster method. There are several parameters used to assess the efficiency of solar cells with absorption of light and transportation of charge being widely used.A prevailing problem with the structure of organic cells is that molecules in the active organic layer responsible for light absorption and charge transport tend to face both towards the edges of cells, as well as towards the light absorbing substrate. Maximizing the number of molecules facing the substrate, however, is the key to maximising absorption and conductivity of the cell. Scientists have modified the dry processing method to achieve such an orientation, but it has not been possible with the wet method. The research team led by Tetsuya Taima at Kanazawa University, is the first to successfully do so.The premise of their method is the introduction of a copper iodide (CuI) layer between the active molecules and the substrate. In their study, the researchers used a film of active molecules called DRCN5T and coated them onto either CuI/PEDOT: PSS (30 nm)/indium tin oxide (ITO) mixed substrates, or substrates without the CuI layer. The ratio of substrate facing to edge facing DRCN5T molecules was then compared between both. Subsequent high-resolution imaging revealed that the CuI containing cells had active molecules with a ten times higher substrate facing orientation, along with enhanced light absorption. The researchers attributed this altered orientation of the molecules to strong chemical interactions between the DRCN5T and CuI atoms. To further confirm this, DRCN5T molecules with bulky side chains that do not interact with CuI were used, and a higher substrate facing ratio was not seen.This is the first study that effectively demonstrates a method of producing such efficient organic solar cells using the wet processing method. Besides saving time, the wet method also results in larger film areas. ""This technique is expected to greatly contribute to the development of organic thin film solar cells fabricated by wet processing in the future,"" conclude the authors. Their approach paves the way for producing high-performance solar cells faster.Story Source:Materials provided by Kanazawa University. ",0.111764,0.202302,0.178345,0.162702,0.182099,0.147644,0.165483,0,0
485,Efficiently turning light into electricity New research has now shown that certain perovskites in fact do have this desirable property,"Perovskites form a group of crystals that have many promising properties for applications in nano-technology. However, one useful property that so far was unobserved in perovskites is so-called carrier multiplication -- an effect that makes materials much more efficient in converting light into electricity. New research performed in collaboration between the University of Amsterdam (UA) and Osaka University (OU) and led by prof. Tom Gregorkiewicz (UA, OU) and prof. Yasufumi Fujiwara (OU), has now shown that certain perovskites in fact do have this desirable property.Crystals are configurations of atoms, molecules or ions, that are ordered in a structure that repeats itself in all directions. We have all encountered some crystals in everyday life: ordinary salt, diamond and even snowflakes are examples. What is perhaps less well-known is that certain crystals show very interesting properties when their size is not that of our everyday life but that of nanometers -- a few billionths of a meter. There, we enter the world of nanocrystals, structures that have shown to be extremely useful in constructing technological applications at tiny scales.Perovskites -- named after 19th century Russian mineralogist Lev Perovski -- form a particular family of nanocrystals that all share the same crystal structure. At the nanoscale, these perovskites have many desirable electronic properties, making them useful for constructing for example LEDs, TV-screens, solar cells and lasers. For this reason, in the past years perovskite nanocrystals have been studied extensively by physicists.Carrier multiplicationA property which so-far had not been shown to exist in perovskites is carrier multiplication. When nanocrystals -- in solar cells, for example -- convert the energy of light into electricity, this is usually done one particle at a time: a single infalling photon results in a single excited electron (and the corresponding ""hole"" where the electron used to be) that can carry an electrical current. However, in certain materials, if the infalling light is energetic enough, further electron-hole pairs can be excited as a result; it is this process that is known as carrier multiplication.When carrier multiplication occurs, the conversion from light into electricity can become much more efficient. For example, in ordinary solar cells there is a theoretical limit (the so-called Shockley-Queisser limit) on the amount of energy that can be converted in this way: at most a little of 30% of the solar power gets turned into electrical power. In materials that display the carrier multiplication effect, however, an efficiency of up to 44% has already been obtained.PhDThis makes it very interesting to search for the carrier multiplication effect in perovskites as well, and that is precisely what dr. Chris de Weerd and dr. Leyre Gomez from the Optoelectronic Materials group led by prof. Tom Gregorkiewicz, in collaboration with the group of prof. Yasufumi Fujiwara, and with support of their colleagues from the National AIST Institute in Tsukuba and Technical University Delft have now done. Using spectroscopy methods -- studying the frequencies of the radiation that comes from a material after very briefly illuminating it with a flash of light -- the researchers showed that a perovskite nanocrystals made out of cesium, lead and iodine, do indeed display carrier multiplication. Moreover, they argue that the efficiency of this effect is higher than reported thus far for any other materials; with this finding therefore the extraordinary properties of perovskite receive a new boost!De Weerd, who successfully defended her PhD thesis based on this and other research last week, says: ""Until now, carrier multiplication had not been reported for perovskites. That we have now found it is of great fundamental impact on this upcoming material. For example, this shows that perovskites can be used to construct very efficient photodetectors, and in the future perhaps solar cells.""Story Source:Materials provided by Osaka University. ",0.0882417,0.146549,0.116481,0.164539,0.159114,0.134709,0.154985,0,0
486,Metal leads to the desired configuration,"Scientists at the University of Basel have found a way to change the spatial arrangement of bipyridine molecules on a surface. These potential components of dye-sensitized solar cells form complexes with metals and thereby alter their chemical conformation. The results of this interdisciplinary collaboration between chemists and physicists from Basel were recently published in the scientific journal ACS Omega.Dye-sensitized solar cells have been considered a sustainable alternative to conventional solar cells for many years, even if their energy yield is not yet fully satisfactory. The efficiency can be increased with the use of tandem solar cells, where the dye-sensitized solar cells are stacked on top of each other.The way in which the dye, which absorbs sunlight, is anchored to the semiconductor plays a crucial role in the effectiveness of these solar cells. However, the anchoring of the dyes on nickel oxide surfaces -- which are particularly suitable for tandem dye-sensitized cells -- is not yet sufficiently understood.Binding on surfacesOver the course of an interdisciplinary collaboration, scientists from the Swiss Nanoscience Institute and the Departments of Physics and Chemistry at the University of Basel investigated how single bipyridine molecules bind to nickel oxide and gold surfaces.Bipyridine crystals served as an anchor molecule for dye-sensitized cells on a semiconductor surface. This anchor binds the metal complexes, which in turn can then be used to bind the various dyes.With the aid of scanning probe microscopes, the investigation determined that initially the bipyridine molecules bind flat to the surface in their trans configuration. The addition of iron atoms and an increase in temperature cause a rotation around a carbon atom in the bipyridine molecule and thus leads to the formation of the cis configuration.""The chemical composition of the cis and trans configuration is the same, but their spatial arrangement is very different. ""The change in configuration can be clearly distinguished on the basis of scanning probe microscope measurements,"" confirms experimental physicist Professor Ernst Meyer.Metal complexes in a modified configurationThis change in spatial arrangement is the result of formation of a metal complex, as confirmed by the scientists through their examination of the bipyridine on a gold surface.During the preparation of the dye-sensitized solar cells, these reactions take place in a solution. However, the examination of individual molecules and their behavior is only possible with the use of scanning probe microscopes in vacuum.""This study allowed us to observe for the first time how molecules that are firmly bound to a surface change their configuration,"" summarizes Meyer. ""This enables us to better understand how anchor molecules behave on nickel oxide surfaces.""Story Source:Materials provided by University of Basel. ",0.106511,0.265497,0.168155,0.149193,0.15357,0.152031,0.177146,0,0
487,Boosting the efficiency of silicon solar cells,"The efficiency of a solar cell is one of its most important parameters. It indicates what percentage of the solar energy radiated into the cell is converted into electrical energy. The theoretical limit for silicon solar cells is 29.3 percent due to physical material properties. In the journal Materials Horizons, researchers from Helmholtz-Zentrum Berlin (HZB) and international colleagues describe how this limit can be abolished. The trick: they incorporate layers of organic molecules into the solar cell. These layers utilise a quantum mechanical process known as singlet exciton fission to split certain energetic light (green and blue photons) in such a way that the electrical current of the solar cell can double in that energy range.The principle of a solar cell is simple: per incident light particle (photon) a pair of charge carriers (exciton) consisting of a negative and a positive charge carrier (electron and hole) is generated. These two opposite charges can move freely in the semiconductor. When they reach the charge-selective electrical contacts, one only allows positive charges to pass through, the other only negative charges. A direct electrical current is therefore generated, which can be used by an external consumer.The team around HZB researcher, Prof. Klaus Lips, gives now a solution to build the solar cell in such a way that certain high energy photons are used to generate two pairs of charge carrier simultaneously. The effect they used resides in some organic crystals and is known as ""singlet exciton fission"" (SF). In order for this multiplier effect to be possible, charge carrier pairs have to fulfill certain quantum physical conditions: all their spins have to be parallel meaning charge carrier pairs called triplet exciton.These triplet excitons are quite durable and very strongly bound together. The challenge was to split apart them at an interface to silicon. This unbinds the positive and negative charge carriers, permitting them to contribute to the solar cell's current.In a pioneering experiment the researchers have shown the splitting to be possible. ""With implementing this concept successfully, we can make a silicon solar cell with a maximum quantum efficiency of 200 percent (double the normal limit), and a theoretical efficiency limit of around 40 percent,"" says Dr. Rowan MacQueen, an Australian researcher who joined the HZB team two years ago and is realizing the charge carrier multiplier solar cell at the HZB.In the recently-reported work, the HZB researchers integrated a 100 nanometer thick layer of singlet fission-capable tetracene crystals into the surface of a silicon solar cell. Using spectroscopic investigations, triplet charge carrier pairs in the thin tetracene layer were detected, a signature of singlet fission. ""The challenge was to separate the triplet pairs at the silicon interface without significantly disrupting the current flow of the silicon solar cell,"" explains Klaus Lips, since a poorly conducting organic layer borders on the well conducting silicon layer, a situation that could greatly impede the current flow.The splitting succeeds with an additionally introduced organic conductor called PEDOT:PSS meaning another organic layer is necessary. ""The interfaces play a special role in this structure,"" says Dr. MacQueen, which is why the researchers used X-ray light from the BESSY II@HZB synchrotron to study the interface properties. They then fabricated a series of working tetracene-silicon solar cells. A key finding was that the addition of the organic layer did not impede the electrical performance of the silicon cell, which is critical for producing an efficient device.The electrical performance of the first silicon singlet fission solar cell showed that tetracene absorbs the blue-green portion of the light, while low-energy photons are absorbed by the silicon. Using a simulation, the researchers were able to estimate that currently about 5-10 percent of the triplet pairs generated in the tetracene layer could be added to the output power.For Klaus Lips this is a great success but he is already considering follow-up experiments: ""With this solar cell structure, we have shown that the approach works in principle, and delivered a workhorse design. And we already know what we have to do to increase the yield of separated triplet excitons to up to 200 percent.""Story Source:Materials provided by Helmholtz-Zentrum Berlin fer Materialien und Energie. ",0.0924019,0.150867,0.132387,0.163079,0.172094,0.156945,0.155627,0,0
488,Emissions-free energy system saves heat from the summer sun for winter,"A research group from Chalmers University of Technology, Sweden, has made great, rapid strides towards the development of a specially designed molecule which can store solar energy for later use. These advances have been presented in four scientific articles this year, with the most recent being published in the highly ranked journal Energy & Environmental Science.Around a year ago, the research team presented a molecule that was capable of storing solar energy. The molecule, made from carbon, hydrogen and nitrogen, has the unique property that when it is hit by sunlight, it is transformed into an energy-rich isomer -- a molecule which consists of the same atoms, but bound together in a different way.This isomer can then be stored for use when that energy is later needed -- for example, at night or in winter. It is in a liquid form and is adapted for use in a solar energy system, which the researchers have named MOST (Molecular Solar Thermal Energy Storage). In just the last year, the research team have made great advances in the development of MOST.""The energy in this isomer can now be stored for up to 18 years. And when we come to extract the energy and use it, we get a warmth increase which is greater than we dared hope for,"" says the leader of the research team, Kasper Moth-Poulsen, Professor at the Department of Chemistry and Chemical Engineering.The research group have developed a catalyst for controlling the release of the stored energy. The catalyst acts as a filter, through which the liquid flows, creating a reaction which warms the liquid by 63 degrees Celsius. If the liquid has a temperature of 20C when it pumps through the filter, it comes out the other side at 83C. At the same time, it returns the molecule to its original form, so that it can be then reused in the warming system.During the same period, the researchers also learned to improve the design of the molecule to increase its storage abilities so that the isomer can store energy for up to 18 years. This was a crucial improvement, as the focus of the project is primarily chemical energy storage.Furthermore, the system was previously reliant on the liquid being partly composed of the flammable chemical toluene. But now the researchers have found a way to remove the potentially dangerous toluene and instead use just the energy storing molecule.Taken together, the advances mean that the energy system MOST now works in a circular manner. First, the liquid captures energy from sunlight, in a solar thermal collector on the roof of a building. Then it is stored at room temperature, leading to minimal energy losses. When the energy is needed, it can be drawn through the catalyst so that the liquid heats up. It is envisioned that this warmth can then be utilised in, for example, domestic heating systems, after which the liquid can be sent back up to the roof to collect more energy -- all completely free of emissions, and without damaging the molecule.""We have made many crucial advances recently, and today we have an emissions-free energy system which works all year around,"" says Kasper Moth-Poulsen.The solar thermal collector is a concave reflector with a pipe in the centre. It tracks the sun's path across the sky and works in the same way as a satellite dish, focusing the sun's rays to a point where the liquid leads through the pipe. It is even possible to add on an additional pipe with normal water to combine the system with conventional water heating.The next steps for the researchers are to combine everything together into a coherent system.""There is a lot left to do. We have just got the system to work. Now we need to ensure everything is optimally designed,"" says Kasper Moth-Poulsen.The group is satisfied with the storage capabilities, but more energy could be extracted, Kasper believes. He hopes that the research group will shortly achieve a temperature increase of at least 110 degrees Celsius and thinks the technology could be in commercial use within 10 years.The research is funded by the Knut and Alice Wallenberg Foundation and the Swedish Foundation for Strategic Research.Story Source:Materials provided by Chalmers University of Technology. ",0.0836385,0.131595,0.111032,0.156876,0.114621,0.132304,0.150163,0,0
489,Supersizing solar cells: Researchers print module six times bigger than previous largest,"A perovskite solar module the size of an A4 sheet of paper, which is nearly six times bigger than 10x10 cm2 modules of that type reported before, has been developed by Swansea University researchers, by using simple and low-cost printing techniques.The breakthrough shows that the technology works at a larger scale, not just in the lab, which is crucial for encouraging industry to take it up.Each of the many individual cells forming the module is made of perovskite, a material of increasing interest to solar researchers as it can be made more easily and cheaply than silicon, the most commonly-used material for solar cells.Perovskite solar cells have also proved to be highly efficient, with scores for power conversion efficiency (PCE) -- the amount of light striking a cell that it converts into electricity -- as high as 22% on small lab samples.The team work for the SPECIFIC Innovation and Knowledge Centre led by Swansea University. They used an existing type of cell, a Carbon Perovskite Solar Cell (C-PSC), made of different layers -- titania, zirconia and carbon on top -- which are all printable.Though their efficiency is lower than other perovskite cell types, C-PSCs do not degrade as quickly, having already proved over 1 year's stable operation under illumination.The Swansea team's breakthrough comes from the optimisation of the printing process on glass substrates as large as an A4 sheet of paper. They ensured the patterned layers were perfectly aligned through a method called registration, well-known in the printing industry.The entire fabrication process was carried out in air, at ambient conditions, without requiring the costly high-vacuum processes which are needed for silicon manufacture.The Swansea team achieved good performance for their modules:- up to 6.3% power conversion efficiency (PCE) when assessed against the ""1 sun"" standard, i.e. full simulated sunlight. This is world-leading for a C-PSC device of this size.- 11% PCE at 200 lux, roughly equivalent to light levels in an average living room- 18% PCE at 1000 lux, equating to light levels in an average supermarket.The high efficiency ratings under indoor lighting conditions demonstrate that this technology has potential not only for energy generation outdoors but also for powering small electronic devices -- such as smartphones and sensors -- indoors.Dr Francesca De Rossi, technology transfer fellow at Swansea University's SPECIFIC Innovation and Knowledge Centre, said:""Our work shows that perovskite solar cells can deliver good performance even when produced on a larger scale than reported so far within the scientific community. This is vital in making it economical and appealing for industry to manufacture them.The key to our success was the screen printing process. We optimised this to avoid defects caused by printing such large areas. Accurate registration of layers and patterning the blocking layer helped improve connections between cells, boosting overall performance.There is more work still to do, for example on increasing the active area -- the percentage of the substrate surface that is actually used for producing power. We are already working on it.But this is an important breakthrough by our team, which can help pave the way for the next generation of solar cells.""Story Source:Materials provided by Swansea University. ",0.0735855,0.114649,0.10374,0.14062,0.124125,0.115569,0.119796,0,0
490,"Bionic mushrooms' fuse nanotech, bacteria and fungi ","In their latest feat of engineering, researchers at Stevens Institute of Technology have taken an ordinary white button mushroom from a grocery store and made it bionic, supercharging it with 3D-printed clusters of cyanobacteria that generate electricity and swirls of graphene nanoribbons that can collect the current.The work, reported in the Nov. 7 issue of Nano Letters, may sound like something straight out of Alice in Wonderland, but the hybrids are part of a broader effort to better improve our understanding of cells biological machinery and how to use those intricate molecular gears and levers to fabricate new technologies and useful systems for defense, healthcare and the environment.""In this case, our system -- this bionic mushroom -- produces electricity,"" said Manu Mannoor, an assistant professor of mechanical engineering at Stevens. ""By integrating cyanobacteria that can produce electricity, with nanoscale materials capable of collecting the current, we were able to better access the unique properties of both, augment them, and create an entirely new functional bionic system.""Cyanobacteria's ability to produce electricity is well known in bioengineering circles. However, researchers have been limited in using these microbes in bioengineered systems because cyanobacteria do not survive long on artificial bio-compatible surfaces. Mannoor and Sudeep Joshi, a postdoctoral fellow in his lab, wondered if white button mushrooms, which naturally host a rich microbiota but not cyanobacteria specifically, could provide the right environment -- nutrients, moisture, pH and temperature -- for the cyanobacteria to produce electricity for a longer period.Mannoor and Joshi showed that the cyanobacterial cells lasted several days longer when placed on the cap of a white button mushroom versus a silicone and dead mushroom as suitable controls. ""The mushrooms essentially serve as a suitable environmental substrate with advanced functionality of nourishing the energy producing cyanobacteria,"" says Joshi. ""We showed for the first time that a hybrid system can incorporate an artificial collaboration, or engineered symbiosis, between two different microbiological kingdoms.""Mannoor and Joshi used a robotic arm-based 3D printer to first print an ""electronic ink"" containing the graphene nanoribbons. This printed branched network serves as an electricity-collecting network atop the mushroom's cap by acting like a nano-probe -- to access bio-electrons generated inside the cyanobacterial cells. Imagine needles sticking into a single cell to access electrical signals inside it, explains Mannoor.Next, they printed a"" bio-ink"" containing cyanobacteria onto the mushroom's cap in a spiral pattern intersecting with the electronic ink at multiple contact points. At these locations, electrons could transfer through the outer membranes of the cyanobacteria to the conductive network of graphene nanoribbons. Shining a light on the mushrooms activated cyanobacterial photosynthesis, generating a photocurrent.In addition to the cyanobacteria living longer in a state of engineered symbiosis, Mannoor and Joshi showed that the amount of electricity these bacteria produce can vary depending on the density and alignment with which they are packed, such that the more densely packed together they are, the more electricity they produce. With 3D printing, it was possible to assemble them so as to boost their electricity-producing activity eight-fold more than the casted cyanobacteria using a laboratory pipette.Recently, a few researchers have 3D printed bacterial cells in different spatial geometrical patterns, but Mannoor and Joshi, as well as co-author Ellexis Cook, are not only the first to pattern it to augment their electricity-generating behavior but also integrate it to develop a functional bionic architecture.""With this work, we can imagine enormous opportunities for next-generation bio-hybrid applications,"" Mannoor says. ""For example, some bacteria can glow, while others sense toxins or produce fuel. By seamlessly integrating these microbes with nanomaterials, we could potentially realize many other amazing designer bio-hybrids for the environment, defense, healthcare and many other fields.""Story Source:Materials provided by Stevens Institute of Technology. ",0.0926144,0.141252,0.171947,0.203571,0.146885,0.166184,0.177575,0,0
491,"Simple stickers may save lives of patients, athletes and lower medical costs ","Heart surgery can be traumatic for patients. Having to continuously monitor your status without a doctor when you are back home can be even scarier. Imagine being able to do that with a simple sticker applied to your body.Purdue University researchers have advanced a sticker solution moving it several steps closer to reality. The research was recently published in ACS Advanced Materials and Interfaces.""For the first time, we have created wearable electronic devices that someone can easily attach to their skin and are made out of paper to lower the cost of personalized medicine,"" said Ramses Martinez, a Purdue assistant professor of industrial engineering and biomedical engineering, who led the research team.Their technology aligns with Purdue's Giant Leaps celebration, acknowledging the university's global advancements made in health as part of Purdue's 150th anniversary. This is one of the four themes of the yearlong celebration's Ideas Festival, designed to showcase Purdue as an intellectual center solving real-world issues.The ""smart stickers"" are made of cellulose, which is both biocompatible and breathable. They can be used to monitor physical activity and alert a wearer about possible health risks in real time.Health professionals could use the Purdue stickers as implantable sensors to monitor the sleep of patients because they conform to internal organs without causing any adverse reactions. Athletes could also use the technology to monitor their health while exercising and swimming.These stickers are patterned in serpentine shapes to make the devices as thin and stretchable as skin, making them imperceptible for the wearer.Since paper degrades fast when it gets wet and human skin is prone to be covered in sweat, these stickers were coated with molecules that repel water, oil, dust and bacteria. Each sticker costs about a nickel to produce and can be made using printing and manufacturing technologies similar to those used to print books at high speed.""The low cost of these wearable devices and their compatibility with large-scale manufacturing techniques will enable the quick adoption of these new fully disposable, wearable sensors in a variety of health care applications requiring single-use diagnostic systems,"" Martinez said.The technology is patented through the Purdue Office of Technology Commercialization. They are continuing to look for partners to test and commercialize their technology.A YouTube video is available at http://bit.ly/EPED-Purdue.Story Source:Materials provided by Purdue University. ",0.103793,0.143871,0.17107,0.224692,0.166471,0.16152,0.185819,0,0
492,Engineers develop process to 3-D print cells to produce human tissue such as ligaments and tendons,"With today's technology, we can 3-D-print sculptures, mechanical parts, prosthetics, even guns and food. But a team of University of Utah biomedical engineers have developed a method to 3-D-print cells to produce human tissue such as ligaments and tendons, a process that will greatly improve a patient's recovery. A person with a badly damaged ligament, tendon, or ruptured disc could simply have new replacement tissue printed and ultimately implanted in the damaged area, according to a new paper published in the Journal of Tissue Engineering, Part C: Methods.""It will allow patients to receive replacement tissues without additional surgeries and without having to harvest tissue from other sites, which has its own source of problems,"" says University of Utah biomedical engineering assistant professor Robby Bowles, who co-authored the paper along with former U biomedical engineering master's student, David Ede.The 3-D-printing method, which took two years to research, involves taking stem cells from the patient's own body fat and printing them on a layer of hydrogel to form a tendon or ligament which would later grow in vitro in a culture before being implanted. But it's an extremely complicated process because that kind of connective tissue is made up of different cells in complex patterns. For example, cells that make up the tendon or ligament must then gradually shift to bone cells so the tissue can attach to the bone.""This is a technique in a very controlled manner to create a pattern and organizations of cells that you couldn't create with previous technologies,"" Bowles says of the printing process. ""It allows us to very specifically put cells where we want them.""To do that, Bowles and his team worked with Salt Lake City-based company, Carterra, Inc., which develops microfluidic devices for medicine. Researchers used a 3-D printer from Carterra typically used to print antibodies for cancer screening applications. But Bowles' team developed a special printhead for the printer that can lay down human cells in the controlled manner they require. To prove the concept, the team printed out genetically-modified cells that glow a fluorescent color so they can visualize the final product.Currently, replacement tissue for patients can be harvested from another part of the patient's body or sometimes from a cadaver, but they may be of poor quality. Spinal discs are complicated structures with bony interfaces that must be recreated to be successfully transplanted. This 3-D-printing technique can solve those problems.Bowles, who specializes in musculoskeletal research, said the technology currently is designed for creating ligaments, tendons and spinal discs, but ""it literally could be used for any type of tissue engineering application,"" he says. It also could be applied to the 3-D printing of whole organs, an idea researchers have been studying for years. Bowles also says the technology in the printhead could be adapted for any kind of 3-D printer.Story Source:Materials provided by University of Utah. ",0.0826411,0.143753,0.148948,0.210012,0.142691,0.143161,0.187612,0,0
493,3-D bioprinting of living structures with built-in chemical sensors,"An international team of researchers led by Professor Michael Kehl at the Department of Biology, University of Copenhagen has just published a breakthrough in 3D bioprinting. Together with German colleagues at the Technical University of Dresden (Centre for Translational Bone, Joint and Soft Tissue Research), Professor Kehls group implemented oxygen sensitive nanoparticles into a gel material that can be used for 3D printing of complex, biofilm and tissue like structures harboring living cells as well as built-in chemical sensors. The work has just been published in the leading materials science journal, Advanced Functional Materials.Kehl explains: ""3D printing is a wide spread techniques for producing object in plastic, metal, and other abiotic materials. Likewise, living cells can be 3D printed in biocompatible gel materials (bioinks) and such 3D bioprinting is a rapidly developing field, e.g. in biomedical studies, where stem cells are cultivated in 3D printed constructs mimicking the complex structure of tissue and bones. Such attempts lack on line monitoring of the metabolic activity of cells growing in bioprinted constructs; currently, such measurements largely rely on destructive sampling. We have developed a patent pending solution to this problem.""The group developed a functionalized bioink by implementing luminescent oxygen sensitive nanoparticles into the print matrix. When blue light excites the nanoparticles, they emit red luminescent light in proportion to the local oxygen concentration -- the more oxygen the less red luminescence. The distribution of red luminescence and thus oxygen across bioprinted living structures can be imaged with a camera system. This allows for on-line, non-invasive monitoring of oxygen distribution and dynamics that can be mapped to the growth and distribution of cells in the 3D bioprinted constructs without the need for destructive sampling.Kehl continues: ""It is important that the addition of nanoparticles doesn't change the mechanical properties of the bioink, e.g. to avoid cell stress and death during the printing process. Furthermore, the nanoparticles should not inhibit or interfere with the cells. We have solved these challenges, as our method shows good biocompatibility and can be used with microalgae as well as sensitive human cell lines.""The recently published study demonstrates how bioinks functionalized with sensor nanoparticles can be calibrated and used e.g. for monitoring algal photosynthesis and respiration as well as stem cell respiration in bioprinted structures with one or several cell types.""This is a breakthrough in 3D bioprinting. It is now possible to monitor the oxygen metabolism and microenvironment of cells on line, and non-invasively in intact 3D printed living structures"" says Prof. Kehl. ""A key challenge in growing stem cells in larger tissue- or bone-like structures is to ensure a sufficient oxygen supply for the cells. With our development, it is now possible to visualize the oxygen conditions in 3D bioprinted structures, which e.g. enables rapid testing and optimization of stem cell growth in differently designed constructs.""The team is very interested in exploring new collaborations and applications of their developments.Kehl ends: ""3D bioprinting with functionalized bioinks is a new powerful technology that can be applied in many other research fields than biomedicine. It is e.g. extremely inspiring to combine such advanced materials science and sensor technology with my research in microbiology and biophotonics, where we currently employ 3D bioprinting to study microbial interactions and photobiology.""Story Source:Materials provided by Faculty of Science - University of Copenhagen. ",0.0739122,0.176603,0.155607,0.177146,0.136307,0.120724,0.180718,0,0
494,Easy to use 3D bioprinting technique creates lifelike tissues from natural materials,"Bioengineers at the University of California San Diego have developed a 3D bioprinting technique that works with natural materials and is easy to use, allowing researchers of varying levels of technical expertise to produce lifelike organ tissue models.As a proof of concept, the UC San Diego team used their method to create blood vessel networks capable of keeping a breast cancer tumor alive outside the body. They also created a model of a vascularized human gut. The work was published recently in Advanced Healthcare Materials.The goal isn't to make artificial organs that can be implanted in the body, researchers said, but to make easy-to-grow human organ models that can be studied outside the body or used for pharmaceutical drug screening.""We want to make it easier for everyday scientists -- who may not have the specialization required for other 3D printing techniques -- to make 3D models of whatever human tissues they're studying,"" said first author Michael Hu, a bioengineering Ph.D. student at the UC San Diego Jacobs School of Engineering. ""The models would be more advanced than standard 2D or 3D cell cultures, and more relevant to humans when it comes to testing new drugs, which is currently done on animal models.""""You don't need anything complicated to adopt this into your lab,"" said Prashant Mali, a bioengineering professor at the UC San Diego Jacobs School of Engineering the study's senior author. ""Our hope is that multiple labs will be able to work with this and experiment with this. The more it gets adopted, the more impact it could have.""The method is simple. To make a living blood vessel network, for example, researchers first digitally design a scaffold using Autodesk. Using a commercial 3D printer, the researchers print the scaffold out of a water soluble material called polyvinyl alcohol. They then pour a thick coating -- made of natural materials -- over the scaffold, let it cure and solidify, and then flush out the scaffold material inside to create hollow blood vessel channels. Next, they coat the insides of the channels with endothelial cells, which are the cells that line the insides of blood vessels. The last step is to flow cell culture media through the vessels to keep the cells alive and growing.The vessels are made of natural materials found in the body such as fibrinogen, a compound found in blood clots, and Matrigel, a commercially available form of actual mammalian extracellular matrix.Finding the right materials was one of the biggest challenges, said bioengineering undergraduate student Xin Yi (Linda) Lei, a co-author on the study. ""We wanted to use materials that were natural rather than synthetic, so we could make something as close to what's in the body as possible. They also needed to be able to work with our 3D printing method.""""We can use these everyday biologically derived materials to make ex vivo tissues that are vascularized,"" said Mali. ""And that's an important aspect if we want to make tissues that can be sustained for very long periods of time outside the body.""Staying aliveIn one set of experiments, the researchers used the printed blood vessels to keep breast cancer tumor tissues alive outside the body. They extracted pieces of tumors from mice and then embedded some of the pieces in the printed blood vessel networks. Other pieces were kept in a standard 3D cell culture. After three weeks, the tumor tissues encapsulated in the blood vessel prints had stayed alive. Meanwhile, those in the standard 3D cell culture had mostly died off.""Our hope is that we can apply our system to make tumor models that can be used to test anti-cancer drugs outside the body,"" said Hu, who is particularly interested in studying breast cancer tumor models. ""Breast cancer is one of the most common cancers -- it has one of the largest portions of research dedicated to it and one of the largest panels of pharmaceuticals being developed for it. So any models we can make would be useful to more people.""In another set of experiments, the researchers created a vascularized gut model. The structure consisted of two channels. One was a straight tube lined with intestinal epithelial cells to mimic the gut. The other was a blood vessel channel (lined with endothelial cells) that spiraled around the gut channel. The aim was recreate a gut surrounded by a blood vessel network. Each channel was then fed with media optimized for its cells. Within two weeks, the channels has started taking on more lifelike morphologies. For example, the gut channel had started to sprout villi, which are the tiny finger-like projections lining the inside of the intestinal wall.""With this type of strategy, we can start to make complex, long living systems in an ex vivo setting. In the future, this could perhaps supplant the use of animals to make these systems, which is what's being done right now,"" said Mali.""This was a proof of concept showing we can culture different types of cells together, which is important if we want to model multi-organ interactions in the body. In a single print, we can create two distinct local environments, each keeping a different type of cell alive, and placed close enough together so that they can interact,"" said Hu.Moving forward, the team is working on extending and refining this technique. Future work will focus on optimizing the printed blood vessels and developing vascularized tumor models that more closely mimic those in the body.Story Source:Materials provided by University of California - San Diego. ",0.061021,0.117925,0.129221,0.195734,0.106411,0.111195,0.16821,0,0
495,"Origami, 3D printing merge to make complex structures in one shot ","By merging the ancient art of origami with 21st century technology, researchers have created a one-step approach to fabricating complex origami structures whose light weight, expandability, and strength could have applications in everything from biomedical devices to equipment used in space exploration. Until now, making such structures has involved multiple steps, more than one material, and assembly from smaller parts.""What we have here is the proof of concept of an integrated system for manufacturing complex origami. It has tremendous potential applications,"" said Glaucio H. Paulino, a professor at the School of Civil and Environmental Engineering at the Georgia Institute of Technology and a leader in the growing field of origami engineering, or using the principles of origami, mathematics and geometry to make useful things. Last fall Georgia Tech became the first university in the country to offer a course on origami engineering, which Paulino taught.The researchers used a relatively new kind of 3D printing called Digital Light Processing (DLP) to create groundbreaking origami structures that are not only capable of holding significant weight but can also be folded and refolded repeatedly in an action similar to the slow push and pull of an accordion. When Paulino first reported these structures, or ""zippered tubes,"" in 2015, they were made of paper and required gluing. In the current work, the zippered tubes -- and complex structures made out of them -- are composed of one plastic (a polymer) and do not require assembly.The work was reported in a recent issue of Soft Matter, a journal published by the Royal Society of Chemistry. The primary authors are Paulino; H. Jerry Qi, a professor in Georgia Tech's George W. Woodruff School of Mechanical Engineering; and Daining Fang of Peking University and the Beijing Institute of Technology. Other authors are Zeang Zhao, a visiting student at Georgia Tech now at Peking University; Qiang Zhang of Peking University; and Xiao Kuang and Jiangtao Wu of Georgia Tech.An Emerging TechnologyThere are many different types of 3D printing technologies. The most familiar, inkjet, has been around for some 20 years. But until now, it has been difficult to create 3D-printed structures with the intricate hollow features associated with complex origami because removing the supporting materials necessary to print these structures is challenging. Further, unlike paper, the 3D-printed materials could not be folded numerous times without breaking.Enter DLP and some creative engineering. According to Qi, a leader in the emerging field collaborating with Fang's group at Peking University, DLP has been in the lab for a while, but commercialization only began about five years ago. Unlike other 3D printing techniques, it creates structures by printing successive layers of a liquid resin that is then cured, or hardened, by ultraviolet light.For the current work, the researchers first developed a new resin that, when cured, is very strong. ""We wanted a material that is not only soft, but can also be folded hundreds of times without breaking,"" said Qi. The resin, in turn, is key to an equally important element of the work: tiny hinges. These hinges, which occur along the creases where the origami structure folds, allow folding because they are made of a thinner layer of resin than the larger panels of which they are part. (The panels make up the bulk of the structure.)Together the new resin and hinges worked. The team used DLP to create several origami structures ranging from the individual origami cells that the zippered tubes are composed of to a complex bridge composed of many zippered tubes. All were subjected to tests that showed they were not only capable of carrying about 100 times the weight of the origami structure, but also could be repeatedly folded and unfolded without breaking. ""I have a piece that I printed about six months ago that I demonstrate for people all the time, and it's still fine,"" said Qi.What's Next?What's next? Among other things, Qi is working to make the printing even easier while also exploring ways to print materials with different properties. Meanwhile, Paulino's team recently created a new origami pattern on the computer that he is excited about but that he has been unable to physically make because it is so complex. ""I think the new system could bring it to life,"" he said.Story Source:Materials provided by Georgia Institute of Technology. ",0.0839054,0.139341,0.162745,0.173903,0.131992,0.137885,0.153212,0,0
496,"3D bioprinting technique could create artificial blood vessels, organ tissue ","University of Colorado Boulder engineers have developed a 3D printing technique that allows for localized control of an object's firmness, opening up new biomedical avenues that could one day include artificial arteries and organ tissue.The study, which was recently published in the journal Nature Communications, outlines a layer-by-layer printing method that features fine-grain, programmable control over rigidity, allowing researchers to mimic the complex geometry of blood vessels that are highly structured and yet must remain pliable.The findings could one day lead to better, more personalized treatments for those suffering from hypertension and other vascular diseases.""The idea was to add independent mechanical properties to 3D structures that can mimic the body's natural tissue,"" said Xiaobo Yin, an associate professor in CU Boulder's Department of Mechanical Engineering and the senior author of the study. ""This technology allows us to create microstructures that can be customized for disease models.""Hardened blood vessels are associated with cardiovascular disease, but engineering a solution for viable artery and tissue replacement has historically proven challenging.To overcome these hurdles, the researchers found a unique way to take advantage of oxygen's role in setting the final form of a 3D-printed structure.""Oxygen is usually a bad thing in that it causes incomplete curing,"" said Yonghui Ding, a postdoctoral researcher in Mechanical Engineering and the lead author of the study. ""Here, we utilize a layer that allows a fixed rate of oxygen permeation.""By keeping tight control over oxygen migration and its subsequent light exposure, Ding said, the researchers have the freedom to control which areas of an object are solidified to be harder or softer -- all while keeping the overall geometry the same.""This is a profound development and an encouraging first step toward our goal of creating structures that function like a healthy cell should function,"" Ding said.As a demonstration, the researchers printed three versions of a simple structure: a top beam supported by two rods. The structures were identical in shape, size and materials, but had been printed with three variations in rod rigidity: soft/soft, hard/soft and hard/hard. The harder rods supported the top beam while the softer rods allowed it to fully or partially collapse.The researchers repeated the feat with a small Chinese warrior figure, printing it so that the outer layers remained hard while the interior remained soft, leaving the warrior with a tough exterior and a tender heart, so to speak.The tabletop-sized printer is currently capable of working with biomaterials down to a size of 10 microns, or about one-tenth the width of a human hair. The researchers are optimistic that future studies will help improve the capabilities even further.""The challenge is to create an even finer scale for the chemical reactions,"" said Yin. ""But we see tremendous opportunity ahead for this technology and the potential for artificial tissue fabrication.""Story Source:Materials provided by University of Colorado at Boulder. Original written by Trent Knoss. ",0.0545472,0.103686,0.144075,0.179539,0.099407,0.110128,0.151484,0,0
497,"3D printers have 'fingerprints,' a discovery that could help trace 3D-printed guns ","Like fingerprints, no 3D printer is exactly the same.That's the takeaway from a new University at Buffalo-led study that describes what's believed to be the first accurate method for tracing a 3D-printed object to the machine it came from.The advancement, which the research team calls ""PrinTracker,"" could ultimately help law enforcement and intelligence agencies track the origin of 3D-printed guns, counterfeit products and other goods.""3D printing has many wonderful uses, but it's also a counterfeiter's dream. Even more concerning, it has the potential to make firearms more readily available to people who are not allowed to possess them,"" says the study's lead author Wenyao Xu, PhD, associate professor of computer science and engineering in UB's School of Engineering and Applied Sciences.The study will be presented in Toronto at the Association for Computing Machinery's Conference on Computer and Communications Security, which runs from Oct. 15-19. It includes coauthors from Rutgers University and Northeastern University.To understand the method, it's helpful to know how 3D printers work. Like a common inkjet printer, 3D printers move back-and-forth while ""printing"" an object. Instead of ink, a nozzle discharges a filament, such as plastic, in layers until a three-dimensional object forms.Each layer of a 3D-printed object contains tiny wrinkles -- usually measured in submillimeters -- called in-fill patterns. These patterns are supposed to be uniform. However, the printer's model type, filament, nozzle size and other factors cause slight imperfections in the patterns. The result is an object that does not match its design plan.For example, the printer is ordered to create an object with half-millimeter in-fill patterns. But the actual object has patterns that vary 5 to 10 percent from the design plan. Like a fingerprint to a person, these patterns are unique and repeatable. As a result, they can be traced back to the 3D printer.""3D printers are built to be the same. But there are slight variations in their hardware created during the manufacturing process that lead to unique, inevitable and unchangeable patterns in every object they print,"" Xu says.To test PrinTracker, the research team created five door keys each from 14 common 3D printers -- 10 fused deposition modeling (FDM) printers and four stereolithography (SLA) printers.With a common scanner, the researchers created digital images of each key. From there, they enhanced and filtered each image, identifying elements of the in-fill pattern. They then developed an algorithm to align and calculate the variations of each key to verify the authenticity of the fingerprint.Having created a fingerprint database of the 14 3D printers, the researchers were able to match the key to its printer 99.8 percent of the time. They ran a separate series of tests 10 months later to determine if additional use of the printers would affect PrinTracker's ability to match objects to their machine of origin. The results were the same.The team also ran experiments involving keys damaged in various ways to obscure their identity. PrinTracker was 92 percent accurate in these tests.Xu likens the technology to the ability to identify the source of paper documents, a practice used by law enforcement agencies, printer companies and other organizations for decades. While the experiments did not involve counterfeit goods or firearms, Xu says PrinTracker can be used to trace any 3D-printed object to its printer.""We've demonstrated that PrinTracker is an effective, robust and reliable way that law enforcement agencies, as well as businesses concerned about intellectual property, can trace the origin of 3D-printed goods,"" Xu says.Story Source:Materials provided by University at Buffalo. Original written by Cory Nealon. ",0.0887153,0.107034,0.150264,0.228807,0.125264,0.151859,0.159782,0,0
498,New NIST method measures 3D polymer processing precisely,"Recipes for three-dimensional (3D) printing, or additive manufacturing, of parts have required as much guesswork as science. Until now.Resins and other materials that react under light to form polymers, or long chains of molecules, are attractive for 3D printing of parts ranging from architectural models to functioning human organs. But it's been a mystery what happens to the materials' mechanical and flow properties during the curing process at the scale of a single voxel. A voxel is a 3D unit of volume, the equivalent of a pixel in a photo.Now, researchers at the National Institute of Standards and Technology (NIST) have demonstrated a novel light-based atomic force microscopy (AFM) technique -- sample-coupled-resonance photorheology (SCRPR) -- that measures how and where a material's properties change in real time at the smallest scales during the curing process.""We have had a ton of interest in the method from industry, just as a result of a few conference talks,"" NIST materials research engineer Jason Killgore said. He and his colleagues have now published the technique in the journal Small Methods.3D printing, or additive manufacturing, is lauded for flexible, efficient production of complex parts but has the disadvantage of introducing microscopic variations in a material's properties. Because software renders the parts as thin layers and then reconstructs them in 3D before printing, the physical material's bulk properties no longer match those of the printed parts. Instead, the performance of fabricated parts depends on printing conditions.NIST's new method measures how materials evolve with submicrometer spatial resolution and submillisecond time resolution -- thousands of times smaller-scale and faster than bulk measurement techniques. Researchers can use SCRPR to measure changes throughout a cure, collecting critical data for optimizing processing of materials ranging from biological gels to stiff resins.The new method combines AFM with stereolithography, the use of light to pattern photo-reactive materials ranging from hydrogels to reinforced acrylics. A printed voxel may turn out uneven due to variations in light intensity or the diffusion of reactive molecules.AFM can sense rapid, minute changes in surfaces. In the NIST SCRPR method, the AFM probe is continuously in contact with the sample. The researchers adapted a commercial AFM to use an ultraviolet laser to start the formation of the polymer (""polymerization"") at or near the point where the AFM probe contacts the sample.The method measures two values at one location in space during a finite timespan. Specifically, it measures the resonance frequency (the frequency of maximum vibration) and quality factor (an indicator of energy dissipation) of the AFM probe, tracking changes in these values throughout the polymerization process. These data can then be analyzed with mathematical models to determine material properties such as stiffness and damping.The method was demonstrated with two materials. One was a polymer film transformed by light from a rubber into a glass. Researchers found that the curing process and properties depended on exposure power and time and were spatially complex, confirming the need for fast, high-resolution measurements. The second material was a commercial 3D printing resin that changed from liquid into solid form in 12 milliseconds. A rise in resonance frequency seemed to signal polymerization and increased elasticity of the curing resin. Therefore, researchers used the AFM to make topographic images of a single polymerized voxel.Surprising the researchers, interest in the NIST technique has extended well beyond the initial 3D printing applications. Companies in the coatings, optics and additive manufacturing fields have reached out, and some are pursuing formal collaborations, NIST researchers say.Story Source:Materials provided by National Institute of Standards and Technology (NIST). ",0.0931281,0.128247,0.158087,0.168051,0.153878,0.13297,0.155333,0,0
499,New 3D-printed cement paste gets stronger when it cracks -- just like structures in nature,"What if the inherent weaknesses of a material actually made houses and buildings stronger during wildfires and earthquakes?Purdue University researchers have 3D-printed cement paste, a key ingredient of the concrete and mortar used to build various elements of infrastructure, that gets tougher under pressure like the shells of arthropods such as lobsters and beetles. The technique could eventually contribute to more resilient structures during natural disasters.""Nature has to deal with weaknesses to survive, so we are using the 'built-in' weaknesses of cement-based materials to increase their toughness,"" said Jan Olek, a professor in Purdue's Lyles School of Civil Engineering.The idea would be to use designs inspired by arthropod shells to control how damage spreads between the printed layers of a material, like trying to break a bunch of uncooked spaghetti noodles as opposed to a single noodle.""The exoskeletons of arthropods have crack propagation and toughening mechanisms that we can reproduce in 3D-printed cement paste,"" said Pablo Zavattieri, Purdue professor of civil engineering.3D-printed cement-based materials -- such as cement paste, mortar and concrete -- would give engineers more control over design and performance, but technicalities have stood in the way of scaling them up.Purdue engineers are the first to use 3D printing to create bioinspired structures using cement paste, as shown in a published paper and the frontispiece for an upcoming print issue of the journal Advanced Materials.""3D printing has removed the need for creating a mold for each type of design, so that we can achieve these unique properties of cement-based materials that were not possible before,"" said Jeffrey Youngblood, Purdue professor of materials engineering.The team is also using micro-CT scans to better understand the behavior of hardened 3D-printed cement-based materials and take advantage of their weak characteristics, such as pore regions found at the ""interfaces"" between the printed layers, which promote cracking. This finding was recently presented at the 1st RILEM International Conference on Concrete and Digital Fabrication.""3D printing cement-based materials provides control over their structure, which can lead to the creation of more damage and flaw-tolerant structural elements like beams or columns,"" said Mohamadreza ""Reza"" Moini, a Purdue Ph.D. candidate of civil engineering.The team was initially inspired by the mantis shrimp, which conquers its prey with a ""dactyl club"" appendage that grows tougher on impact through twisting cracks that dissipate energy and prevent the club from falling apart.Some of the bioinspired cement paste elements designed and fabricated by the team using 3D printing techniques include the ""honeycomb,"" ""compliant"" and ""Bouligand"" designs, called ""architectures.""Each of these architectures allowed for new behaviors in a 3D-printed element once hardened. The Bouligand architecture, for example, takes advantage of weak interfaces to make a material more crack-resistant, whereas the compliant architecture makes cement-based elements act like a spring, even though they are made of brittle material.The team plans to explore other ways that cement-based elements could be designed for building more resilient structures.The work was funded by the National Science Foundation (Grant No. CMMI 1562927).Watch a YouTube video at https://youtu.be/Q0MvUdZoejk.Story Source:Materials provided by Purdue University. ",0.0821545,0.122653,0.264008,0.18677,0.146528,0.141567,0.174154,0,0
500,A 3-D-printed phantom head Developed for testing 7T MRI imaging,"Phantoms are not just ghostly figures of our imagination, they are also numerical or physical models that represent human characteristics and provide an inexpensive way to test electromagnetic applications. Sossena Wood, a bioengineering PhD candidate at the University of Pittsburgh, has developed a realistic phantom head for magnetic resonance research in the Swanson School of Engineering.Wood started her tenure at Pitt as an undergraduate student in the Department of Electrical and Computer Engineering where she met Tamer Ibrahim, an associate professor of bioengineering. She began research in his lab, the Radiofrequency (RF) Research Facility, during her senior year and is now finishing her dissertation incorporating similar research as a graduate student in the Department of Bioengineering.Ibrahim envisioned designing a 3D printed phantom head to use with the uniquely designed ultrahigh field technology in his lab. ""In the RF Research Facility, we use a whole-body 7 Tesla magnetic resonance imager (7T MRI), which is one of the strongest clinical human MRI devices in the world,"" said Ibrahim. 7T ultrahigh field technology is a powerful tool, but unfortunately, there are a few setbacks that come with this type of imaging.""As you move from lower to higher fields, the images produced become less uniform and localized heating becomes more prevalent,"" explained Ibrahim. ""We wanted to develop an anthropomorphic phantom head to help us better understand these issues by providing a safer way to test the imaging. We use the device to analyze, evaluate, and calibrate the MRI systems and instrumentation before testing new protocols on human subjects.""Researchers are currently using numerical simulations to study the effect of electromagnetic (EM) fields on biological tissues at varying frequencies. Wood said, ""EM numerical modeling has been a standard when analyzing these interactions, and we wanted to create a phantom that resembled the human form for use in validating the EM modeling, thereby providing a more realistic environment for testing.""Before Wood could print the 3D structure, she had to do computational work to build the digital blueprint for the model. She started with a 3T MRI dataset of a healthy male, which she characterized by segmentation and broke into eight tissue compartments, a feature that differentiates her model from other basic phantom heads. According to Wood, these compartments help improve image accuracy by acting as a sort of ""speed bump"" for the field.After the computational preparations, Wood used an MRI scanner to produce a 3D-digital image of healthy male's head and ran her model through computer-aided design, which is software used to create, modify, analyze, and optimize a design.The next step was to print the prototype, which took three semesters to complete. ""We used a plastic developed by DSM Somose for our printing material because it allowed us to create durable and detailed parts with a similar conductivity to the human body,"" said Wood. ""To help the model further mimic a real environment, we created filling ports on the prototype where we can deposit fluids that resemble various tissue types.""Now that Wood has a fully printed anthropomorphic phantom head, she is able to assemble it and begin testing. The phantom has many applications including testing to see if certain implants are able to go inside of an MRI or detecting the temperature rise in different tissues based on various RF instrumentation.""With MR imaging, the power from the RF exposure is transformed into heat in the patient's tissue, which can have detrimental effects on the patient's health, especially with implants if not monitored by the scanner"" explained Wood. ""With our phantom head, we can test the safety of our imaging by putting probes inside of certain regions of the head and measuring the effects,"" said Ibrahim.Ibrahim and Wood hope that this model will eventually be developed commercially and provide others with the ability to pursue research without relying on human testing.Story Source:Materials provided by University of Pittsburgh. ",0.0780315,0.0949253,0.142885,0.193558,0.119588,0.13713,0.158907,0,0
501,Programmable materials: Hydrogels capable of complex movement created,"Living organisms expand and contract soft tissues to achieve complex, 3-D movements and functions, but replicating those movements with human-made materials has proven challenging.A University of Texas at Arlington researcher recently published groundbreaking research in Nature Communications that shows promise in finding a solution.Kyungsuk Yum, an assistant professor in UTA's Materials Science and Engineering Department, and his doctoral student, Amirali Nojoomi, have developed a process by which 2-D hydrogels can be programmed to expand and shrink in a space- and time-controlled way that applies force to their surfaces, enabling the formation of complex 3-D shapes and motions.This process could potentially transform the way soft engineering systems or devices are designed and fabricated. Potential applications for the technology include bioinspired soft robotics, artificial muscles -- which are soft materials that change their shapes or move in response to external signals as our muscles do -- and programmable matter. The concept is also applicable to other programmable materials.""We studied how biological organisms use continuously deformable soft tissues such as muscle to make shapes, change shape and move because we were interested in using this type of method to create dynamic 3-D structures,"" Yum said.His approach uses temperature-responsive hydrogels with local degrees and rates of swelling and shrinking. Those properties allow Yum to spatially program how the hydrogels swell or shrink in response to temperature change using a digital light 4-D printing method he developed that includes three dimensions plus time.Using this method, Yum can print multiple 3-D structures simultaneously in a one-step process. Then, he mathematically programs the structures' shrinking and swelling to form 3-D shapes, such as saddle shapes, wrinkles and cones, and their direction.He also has developed design rules based on the concept of modularity to create even more complex structures, including bioinspired structures with programmed sequential motions. This makes the shapes dynamic so they can move through space. He also can control the speed at which the structures change shape and thus create complex, sequential motion, such as how a stingray swims in the ocean.""Unlike traditional additive manufacturing, our digital light 4-D printing method allows us to print multiple, custom-designed 3-D structures simultaneously. Most importantly, our method is very fast, taking less than 60 seconds to print, and thus highly scalable.""""Dr. Yum's approach to creating programmable 3D structures has the potential to open many new avenues in bioinspired robotics and tissue engineering. The speed with which his approach can be applied, as well as its scalability, makes it a unique tool for future research and applications,"" Meletis said.Story Source:Materials provided by University of Texas at Arlington. Original written by Jeremy Agor. ",0.0659103,0.0890388,0.137158,0.180124,0.107601,0.120059,0.159751,0,0
502,Pushing 'print' on large-scale piezoelectric materials First ever large-scale 2D surface deposition of piezoelectric material,"Researchers have developed a revolutionary method to 'print' large-scale sheets of two dimensional piezoelectric material, opening new opportunities for piezo-sensors and energy harvesting.Importantly, the inexpensive process allows the integration of piezoelectric components directly onto silicon chips.Until now, no 2D piezoelectric material has been manufactured in large sheets, making it impossible to integrate into silicon chips or use in large-scale surface manufacturing.This limitation meant that piezo accelerometer devices -- such as vehicle air bag triggers or the devices that recognise orientation changes in mobile phones -- have required separate, expensive components to be embedded onto silicon substrates, adding significant manufacturing costs.Now, FLEET researchers at RMIT University in Melbourne have demonstrated a method to produce large-scale 2D gallium phosphate sheets, allowing this material to be formed at large scales in low-cost, low-temperature manufacturing processes onto silicon substrates, or any other surface.Gallium phosphate (GaPO4) is an important piezoelectric material commonly used in pressure sensors and microgram-scale mass measurement, particularly in high temperatures or other harsh environments.""As so often in science, this work builds on past successes,"" lead researcher Professor Kourosh Kalantar-zadeh explains. ""We adopted the liquid-metal material deposition technique we developed recently to create 2D films of GaPO4 through an easy, two-step process.""Professor Kalantar-zadeh, now Professor of Chemical Engineering at UNSW, led the team that developed the new method while Professor of Electronic Engineering at RMIT University. The work was materialised as a result of significant contribution from RMIT's Dr Torben Daeneke and extreme persistence and focus shown by the first author of the work, PhD researcher Nitu Syed.The revolutionary new method allows easy, inexpensive growth of large-area (several centimetres), wide-bandgap, 2D GaPO4 nanosheets of unit cell thickness.It is the first demonstration of strong, out-of-plane piezoelectricity of the popular piezoelectric material.THE TWO-STEP PROCESSAPPLICATIONSThe new process is simple, scalable, low-temperature and cost effective, significantly expanding the range of materials available to industry at such scales and quality.The process is suitable for the synthesis of free standing GaPO4 nanosheets. The low temperature synthesis method is compatible with a variety of electronic device fabrication procedures, providing a route for the development of future 2D piezoelectric materials.This simple, industry-compatible procedure to print large surface area 2D piezoelectric films onto any substrate offers tremendous opportunities for the development of piezo-sensors and energy harvesters.PIEZOELECTRIC MATERIALSThese are materials that can convert applied mechanical force or strain into electrical energy. Such materials form the basis of sound and pressure sensors, embedded devices that are powered by vibration or bending, and even the simple 'piezo' lighter used for gas BBQs and stovetops.Piezoelectric materials can also take advantage of the small voltages generated by tiny mechanical displacement, vibration, bending or stretching to power miniaturised devices.THE MATERIAL: GALLIUM PHOSPHATE (GaPO4)Gallium phosphate is a quartz-like crystal used in piezoelectric applications such as pressure sensors since the late 1980s, and particularly valued in high-temperature applications. Because it does not naturally crystallise in a stratified structure and hence cannot be exfoliated using conventional methods, its use to date has been limited to applications that rely on carving the crystal from its bulk.Story Source:Materials provided by ARC Centre of Excellence in Future Low-Energy Electronics Technologies. ",0.110858,0.129645,0.126268,0.151697,0.154402,0.154085,0.146453,0,0
503,"At last, a simple 3-D printer for metal ","Used to produce three-dimensional objects of almost any type, across a range of industries, including healthcare, aviation and engineering, 3D printed materials have come of age during the last decade. Research published in the journal Materials Today demonstrates a new approach to 3D printing to fuse metallic filaments made from metallic glass into metallic objects.Jan Schroers, Professor of Mechanical Engineering and Materials Science at Yale University and Desktop Metal, Inc., in Burlington, Massachusetts, USA, along with colleagues point out that 3D printing of thermoplastics is highly advanced, but the 3D printing of metals is still challenging and limited. The reason being that metals generally don't exist in a state that they can be readily extruded.""We have shown theoretically in this work that we can use a range of other bulk metallic glasses and are working on making the process more practical- and commercially-usable to make 3D printing of metals as easy and practical as the 3D printing of thermoplastics,"" said Prof. Schroers.Unlike conventional metals, bulk metallic glasses (BMGs) have a super-cooled liquid region in their thermodynamic profile and are able to undergo continuous softening upon heating -- a phenomenon that is present in thermoplastics, but not conventional metals. Prof. Schroers and colleagues have thus shown that BMGs can be used in 3D printing to generate solid, high-strength metal components under ambient conditions of the kind used in thermoplastic 3D printing.The new work could side-step the obvious compromises in choosing thermoplastic components over metal components, or vice-versa, for a range of materials and engineering applications. Additive manufacturing of metal components has been developed previously, where a powder bed fusion process is used, however this exploits a highly-localized heating source, and then solidification of a powdered metal shaped into the desired structure. This approach is costly and complicated and requires unwieldy support structures that are not distorted by the high temperatures of the fabrication process.The approach taken by Prof. Schroers and colleagues simplifies additive manufacturing of metallic components by exploiting the unique-amongst-metals softening behavior of BMGs. Paired with this plastic like characteristics are high strength and elastic limits, high fracture toughness, and high corrosion resistance. The team has focused on a BMG made from zirconium, titanium, copper, nickel and beryllium, with alloy formula: Zr44Ti11Cu10Ni10Be25. This is a well-characterized and readily available BMG material.The team used amorphous rods of 1 millimeter (mm) diameter and of 700mm length. An extrusion temperate of 460 degrees Celsius is used and an extrusion force of 10 to 1,000 Newtons to force the softened fibers through a 0.5mm diameter nozzle. The fibers are then extruded into a 400eC stainless steel mesh wherein crystallization does not occur until at least a day has passed, before a robotically controlled extrusion can be carried out to create the desired object.When asked what challenges remain toward making BMG 3D printing a wide-spread technique, Prof. Schroers added, ""In order to widely use BMG 3D printing, practical BMG feedstock available for a broad range of BMGs has to be made available. To use the fused filament fabrication commercially, layer-to-layer bonding has to be more reliable and consistent.""Story Source:Materials provided by Elsevier. ",0.0745858,0.139389,0.128181,0.127626,0.129853,0.116811,0.142575,0,0
504,Sound can be used to print droplets that couldn't be printed before,"Harvard University researchers have developed a new printing method that uses sound waves to generate droplets from liquids with an unprecedented range of composition and viscosity. This technique could finally enable the manufacturing of many new biopharmaceuticals, cosmetics, and food and expand the possibilities of optical and conductive materials.""By harnessing acoustic forces, we have created a new technology that enables myriad materials to be printed in a drop-on-demand manner,"" said Jennifer Lewis, the Hansjorg Wyss Professor of Biologically Inspired Engineering at the Harvard John A. Paulson School of Engineering and Applied Sciences and the senior author of the paper.Lewis is also a Core Faculty Member at the Wyss Institute for Biologically Inspired Engineering and the Jianming Yu Professor of Arts and Sciences at Harvard.The research is published in Science Advances.Liquid droplets are used in many applications from printing ink on paper to creating microcapsules for drug delivery. Inkjet printing is the most common technique used to pattern liquid droplets, but it's only suitable for liquids that are roughly 10 times more viscous than water. Yet many fluids of interest to researchers are far more viscous. For example, biopolymer and cell-laden solutions, which are vital for biopharmaceuticals and bioprinting, are at least 100 times more viscous than water. Some sugar-based biopolymers could be as viscous as honey, which is 25,000 times more viscous than water.The viscosity of these fluids also changes dramatically with temperature and composition, makes it even more difficult to optimize printing parameters to control droplet sizes.""Our goal was to take viscosity out of the picture by developing a printing system that is independent from the material properties of the fluid,"" said Daniele Foresti, first author of the paper, the Branco Weiss Fellow and Research Associate in Materials Science and Mechanical Engineering at SEAS and the Wyss Institute.To do that, the researchers turned to acoustic waves.Thanks to gravity, any liquid can drip -- from water dripping out of a faucet to the century-long pitch drop experiment. With gravity alone, droplet size remains large and drop rate difficult to control. Pitch, which has a viscosity roughly 200 billion times that of water, forms a single drop per decade.To enhance drop formation, the research team relies on generating sound waves. These pressure waves have been typically used to defy gravity, as in the case of acoustic levitation. Now, the researchers are using them to assist gravity, dubbing this new technique acoustophoretic printing.The researchers built a subwavelength acoustic resonator that can generate a highly confined acoustic field resulting in a pulling force exceeding 100 times the normal gravitation forces (1 G) at the tip of the printer nozzle -- that's more than four times the gravitational force on the surface of the sun.This controllable force pulls each droplet off of the nozzle when it reaches a specific size and ejects it towards the printing target. The higher the amplitude of the sound waves, the smaller the droplet size, irrespective of the viscosity of the fluid.""The idea is to generate an acoustic field that literally detaches tiny droplets from the nozzle, much like picking apples from a tree,"" said Foresti.The researchers tested the process on a wide range of materials from honey to stem-cell inks, biopolymers, optical resins and, even, liquid metals. Importantly, sound waves don't travel through the droplet, making the method safe to use even with sensitive biological cargo, such as living cells or proteins.""Our technology should have an immediate impact on the pharmaceutical industry,"" said Lewis. ""However, we believe that this will become an important platform for multiple industries.""""This is an exquisite and impactful example of the breadth and reach of collaborative research,"" said Dan Finotello, director of NSF's MRSEC program. ""The authors have developed a new printing platform using acoustic-forces, which, unlike in other methods, are material-independent and thus offer tremendous printing versatility. The application space is limitless.""The Harvard Office of Technology Development has protected the intellectual property relating to this project and is exploring commercialization opportunities.This research was co-authored by Katharina Kroll, Robert Amissah, Francesco Sillani, Kimberly Homan and Dimos Poulikakos. It was funded by Society in Science through the Branco Weiss Fellowship and the National Science Foundation through Harvard MRSEC.Video: https://www.youtube.com/watch?v=FCbxfe9F6fsStory Source:Materials provided by Harvard John A. Paulson School of Engineering and Applied Sciences. Original written by Leah Burrows. ",0.097699,0.115512,0.111579,0.149009,0.11559,0.131905,0.143753,0,0
505,Researchers 3D print prototype for 'bionic eye',"A team of researchers at the University of Minnesota have, for the first time, fully 3D printed an array of light receptors on a hemispherical surface. This discovery marks a significant step toward creating a ""bionic eye"" that could someday help blind people see or sighted people see better.The research is published today in Advanced Materials, a peer-reviewed scientific journal covering materials science. The author also holds the patent for 3D-printed semiconducting devices.""Bionic eyes are usually thought of as science fiction, but now we are closer than ever using a multimaterial 3D printer,"" said Michael McAlpine, a co-author of the study and University of Minnesota Benjamin Mayhugh Associate Professor of Mechanical Engineering.Researchers started with a hemispherical glass dome to show how they could overcome the challenge of printing electronics on a curved surface. Using their custom-built 3D printer, they started with a base ink of silver particles. The dispensed ink stayed in place and dried uniformly instead of running down the curved surface. The researchers then used semiconducting polymer materials to print photodiodes, which convert light into electricity. The entire process takes about an hour.McAlpine said the most surprising part of the process was the 25 percent efficiency in converting the light into electricity they achieved with the fully 3D-printed semiconductors.""We have a long way to go to routinely print active electronics reliably, but our 3D-printed semiconductors are now starting to show that they could potentially rival the efficiency of semiconducting devices fabricated in microfabrication facilities,"" McAlpine said. ""Plus, we can easily print a semiconducting device on a curved surface, and they can't.""McAlpine and his team are known for integrating 3D printing, electronics, and biology on a single platform. They received international attention a few years ago for printing a ""bionic ear."" Since then, they have 3D printed life-like artificial organs for surgical practice, electronic fabric that could serve as ""bionic skin,"" electronics directly on a moving hand, and cells and scaffolds that could help people living with spinal cord injuries regain some function.McAlpine's drive to create a bionic eye is a little more personal.""My mother is blind in one eye, and whenever I talk about my work, she says, 'When are you going to print me a bionic eye?'"" McAlpine said.McAlpine says the next steps are to create a prototype with more light receptors that are even more efficient. They'd also like to find a way to print on a soft hemispherical material that can be implanted into a real eye.McAlpine's research team includes University of Minnesota mechanical engineering graduate student Ruitao Su, postdoctoral researchers Sung Hyun Park, Shuang-Zhuang Guo, Kaiyan Qiu, Daeha Joung, Fanben Meng, and undergraduate student Jaewoo Jeong.The research was funded by the National Institute of Biomedical Imaging and Bioengineering of the National Institutes of Health (Award No. 1DP2EB020537), The Boeing Company, and the Minnesota Discovery, Research, and InnoVation Economy (MnDRIVE) Initiative through the State of Minnesota.Related video: https://www.youtube.com/watch?v=U2_zhpXZkS0Story Source:Materials provided by University of Minnesota. ",0.0910462,0.130251,0.156055,0.203031,0.157347,0.153069,0.173235,0,0
506,One step closer to bioengineered replacements for vessels and ducts Researchers bioprint complex tubular tissues to replace dysfunctional vessels and ducts in the body,"A team of Brigham and Women's Hospital researchers have developed a way to bioprint tubular structures that better mimic native vessels and ducts in the body. The 3-D bioprinting technique allows fine-tuning of the printed tissues' properties, such as number of layers and ability to transport nutrients. These more complex tissues offer potentially viable replacements for damaged tissue. The team describes its new approach and results in a paper published on Aug. 23 in Advanced Materials.""The vessels in the body are not uniform,"" said Yu Shrike Zhang, PhD, senior author on the study and an associate bioengineer in BWH's Department of Medicine. ""This bioprinting method generates complex tubular structures that mimic those in the human system with higher fidelity than previous techniques.""Many disorders damage tubular tissues: arteritis, atherosclerosis and thrombosis damage blood vessels, while urothelial tissue can suffer inflammatory lesions and deleterious congenital anomalies.To make the 3D bioprinter's ""ink,"" the researchers mixed the human cells with a hydrogel, a flexible structure composed of hydrophilic polymers. They optimized the chemistry of the hydrogel to allow the human cells to proliferate, or ""seed,"" throughout the mixture.Next, they filled the cartridge of a 3D bioprinter with this bio-ink. They fitted the bioprinter with a custom nozzle that would allow them to continuously print tubular structures with up to three layers. Once the tubes were printed, the researchers demonstrated their ability to transport nutrients by perfusing fluids.The researchers found that they could print tissues mimicking both vascular tissue and urothelial tissue. They mixed human urothelial and bladder smooth muscle cells with the hydrogel to form the urothelial tissue. To print the vascular tissue, they used a mixture of human endothelial cells, smooth muscle cells and the hydrogel.The printed tubes had varying sizes, thicknesses and properties. According to Zhang, structural complexity of bioprinted tissue is critical to its viability as a replacement for native tissue. That's because natural tissues are complex. For instance, blood vessels are comprised of multiple layers, which in turn are made up of various cell types.The team plans to continue preclinical studies to optimize the bio-ink composition and 3D-printing parameters before testing for safety and effectiveness.""We're currently optimizing the parameters and biomaterial even further,"" said Zhang. ""Our goal is to create tubular structures with enough mechanical stability to sustain themselves in the body.""Story Source:Materials provided by Brigham and Women's Hospital. ",0.114068,0.191765,0.174231,0.215197,0.183456,0.151042,0.22352,0,0
507,Novel process to 3-D print interconnected layers of 2-D graphene,"Researchers from Virginia Tech and Lawrence Livermore National Laboratory have developed a novel way to 3D print complex objects of one of the highest-performing materials used in the battery and aerospace industries.Previously, researchers could only print this material, known as graphene, in 2D sheets or basic structures. But Virginia Tech engineers have now collaborated on a project that allows them to 3D print graphene objects at a resolution an order of magnitude greater than ever before printed, which unlocks the ability to theoretically create any size or shape of graphene.Because of its strength -- graphene is one of the strongest materials ever tested on Earth -- and its high thermal and electricity conductivity, 3D printed graphene objects would be highly coveted in certain industries, including batteries, aerospace, separation, heat management, sensors, and catalysis.Graphene is a single layer of carbon atoms organized in a hexagonal lattice. When graphene sheets are neatly stacked on top of each other and formed into a three-dimensional shape, it becomes graphite, commonly known as the ""lead"" in pencils.Because graphite is simply packed-together graphene, it has fairly poor mechanical properties. But if the graphene sheets are separated with air-filled pores, the three-dimensional structure can maintain its properties. This porous graphene structure is called a graphene aerogel.""Now a designer can design three-dimensional topology comprised of interconnected graphene sheets,"" said Xiaoyu ""Rayne"" Zheng, assistant professor with the Department of Mechanical Engineering in the College of Engineering and director of the Advanced Manufacturing and Metamaterials Lab. ""This new design and manufacturing freedom will lead to optimization of strength, conductivity, mass transport, strength, and weight density that are not achievable in graphene aerogels.""Zheng, also an affiliated faculty member of the Macromolecules Innovation Institute, has received grants to study nanoscale materials and scale them up to lightweight and functional materials for applications in aerospace, automobiles, and batteries.Previously, researchers could print graphene using an extrusion process, sort of like squeezing toothpaste, but that technique could only create simple objects that stacked on top of itself.""With that technique, there's very limited structures you can create because there's no support and the resolution is quite limited, so you can't get freeform factors,"" Zheng said. ""What we did was to get these graphene layers to be architected into any shape that you want with high resolution.""This project began three years ago when Ryan Hensleigh, lead author of the article and now a third-year Macromolecular Science and Engineering Ph.D. student, began an internship at the Lawrence Livermore National Laboratory in Livermore, California. Hensleigh started working with Zheng, who was then a member of the technical staff at Lawrence Livermore National Laboratory. When Zheng joined the faculty at Virginia Tech in 2016, Hensleigh followed as a student and continued working on this project.To create these complex structures, Hensleigh started with graphene oxide, a precursor to graphene, crosslinking the sheets to form a porous hydrogel. Breaking the graphene oxide hydrogel with ultrasound and adding light-sensitive acrylate polymers, Hensleigh could use projection micro-stereolithography to create the desired solid 3D structure with the graphene oxide trapped in the long, rigid chains of acrylate polymer. Finally, Hensleigh would place the 3D structure in a furnace to burn off the polymers and fuse the object together, leaving behind a pure and lightweight graphene aerogel.""It's a significant breakthrough compared to what's been done,"" Hensleigh said. ""We can access pretty much any desired structure you want."" The key finding of this work, which was recently published with collaborators at Lawrence Livermore National Laboratory in the journal Materials Horizons, is that the researchers created graphene structures with a resolution an order of magnitude finer than ever printed. Hensleigh said other processes could print down to 100 microns, but the new technique allows him to print down to 10 microns in resolution, which approaches the size of actual graphene sheets.""We've been able to show you can make a complex, three-dimensional architecture of graphene while still preserving some of its intrinsic prime properties,"" Zheng said. ""Usually when you try to 3D print graphene or scale up, you lose most of their lucrative mechanical properties found in its single sheet form.""Story Source:Materials provided by Virginia Tech. ",0.137348,0.198389,0.209846,0.22075,0.180448,0.191867,0.207141,0,0
508,"Specially prepared paper can bend, fold or flatten on command ","One of the oldest, most versatile and inexpensive of materials -- paper -- seemingly springs to life, bending, folding or flattening itself, by means of a low-cost actuation technology developed at Carnegie Mellon University's Human-Computer Interaction Institute.A thin layer of conducting thermoplastic, applied to common paper with an inexpensive 3D printer or even painted by hand, serves as a low-cost, reversible actuator. When an electrical current is applied, the thermoplastic heats and expands, causing the paper to bend or fold; when the current is removed, the paper returns to a pre-determined shape.""We are reinventing this really old material,"" said Lining Yao, assistant professor in the HCII and director of the Morphing Matter Lab, who developed the method with her team. ""Actuation truly turns paper into another medium, one that has both artistic and practical uses.""Post-doctoral researcher Guanyun Wang, former research intern Tingyu Cheng and other members of Yao's Morphing Matter Lab have designed basic types of actuators, including some based on origami and kirigami forms. These enable the creation of structures that can turn themselves into balls or cylinders. Or, they can be used to construct more elaborate objects, such as a lamp shade that changes its shape and the amount of light it emits, or an artificial mimosa plant with leaf petals that sequentially open when one is touched.In June, more than 50 students in a workshop at Zhejiang University in Hangzhou, China, used the paper actuation technology to create elaborate pop-up books, including interpretations of famous artworks, such as Van Gogh's Starry Night and Sunflowers.The printed paper actuator will be exhibited Sept. 6-10 at the Ars Electronica Festival in Linz, Austria; Sept. 13-30 at Bozar Centre for the Fine Arts in Brussels, Belgium; and from October through March at Hyundai Motorstudio in Beijing, China. Yao's group presented the technology in April at CHI 2018, the Conference on Human Factors in Computing Systems, in Montreal.""Most robots -- even those that are made of paper -- require an external motor,"" said Wang, a CMU Manufacturing Futures Initiative fellow. ""Ours do not, which creates new opportunities, not just for robotics, but for interactive art, entertainment and home applications.""Creating a paper actuator is a relatively simple process, Cheng said. It employs the least expensive type of 3D printer, a so-called FDM printer that lays down a continuous filament of melted thermoplastic. The researchers use an off-the-shelf printing filament -- graphene polyactide composite -- that conducts electricity.The thermoplastic actuator is printed on plain copy paper in a thin layer, just half a millimeter thick. The actuator is then heated in an oven or with a heat gun and the paper is bent or folded into a desired shape and allowed to cool. This will be the default shape of the paper. Electrical leads can then be attached to the actuator; applying electrical current heats the actuator, causing the thermoplastic to expand and thus straighten the paper. When the current is removed, the paper automatically returns to its default shape.Yao said the researchers are refining this method, changing the printing speed or the width of the line of thermoplastic to achieve different folding or bending effects. They have also developed methods for printing touch sensors, finger sliding sensors and bending angle detectors that can control the paper actuators.More work remains to be done. Actuation is slow, which Yao and her team hope to address with some material engineering -- using papers that are more heat conductive and developing printing filaments that are customized for use in actuators. The same actuation used for paper might also be used for plastics and fabrics.In addition to Yao, Wang and Cheng, authors of the CHI research paper are Youngwook Do and Byoungkwon An, HCII research affiliates; Jianzhe Gu, a Ph.D. student in HCII; Humphrey Yang, a master's student in the CMU School of Architecture, and Ye Tao, a visiting scholar from Zhejiang University.Story Source:Materials provided by Carnegie Mellon University. ",0.0933605,0.122417,0.152133,0.175077,0.135946,0.151303,0.165963,0,0
509,"Water bottles, other recycled 3D printing materials could avoid military supply snags ","Soldiers on the battlefield or at remote bases often have to wait weeks for vital replacement parts. Now scientists report they have found a way to fabricate many of these parts within hours under combat conditions using water bottles, cardboard and other recyclable materials found on base as starting materials for 3D printing. They say this 'game-changing' advance could improve operational readiness, reduce dependence on outside supply chains and enhance safety.The researchers are presenting their work today at the 256th National Meeting & Exposition of the American Chemical Society (ACS).e ""Ideally, soldiers wouldn't have to wait for the next supply truck to receive vital equipment,"" Nicole Zander, Ph.D., says. ""Instead, they could basically go into the cafeteria, gather discarded water bottles, milk jugs, cardboard boxes and other recyclable items, then use those materials as feedstocks for 3D printers to make tools, parts and other gadgets.""Supplying combat troops with food, fuel, ammunition and repair parts is a monumental task, requiring thousands of support staff, contractors and manufacturers. In all, the U.S. Department of Defense has an inventory of 5 million items distributed through eight distinct supply chains, according to the U.S. Government Accountability Office. However, few of these items are stockpiled at front-line locations, meaning that troops in those areas can experience occasional shortages of important materials. Many of these units have 3D printers that can produce spare parts and other equipment, but they rely on conventional feedstocks, such as commercially available plastic filaments, that must be requisitioned, and they can take days, weeks or even months to arrive.Recently, Zander, U.S. Marine Corps Capt. Anthony Molnar and colleagues at the U.S. Army Research Laboratory explored the possibility of using recycled polyethylene terephthalate (PET) plastic as a starting material for 3D printers. PET plastics, found in water and soda bottles, are common waste materials found around bases, and the researchers realized that this material could be a viable feedstock. They determined that PET filaments, produced by recycling, were just as strong and flexible as commercially available filaments for 3D printers. In tests, the team used recycled PET filaments to print a vehicle radio bracket, a long-lead-time military part. This process required about 10 water bottles and took about two hours to complete.Initially, the researchers determined that other types of plastic, such as polypropylene (PP), used in yogurt or cottage cheese containers, or polystyrene (PS), used for plastic utensils, were not practical for use in 3D printing. Undeterred, the team sought to strengthen PP by mixing it with cardboard, wood fibers and other cellulose waste materials found on military bases to create new composite filaments. In addition, the very brittle PS was blended with ductile PP to generate a strong and flexible filament.The researchers used a process called solid-state shear pulverization to generate composite PP/cellulose filaments. In this process, shredded plastic and paper, cardboard or wood flour was pulverized in a twin-screw extruder to generate a fine powder that was then melt-processed into 3D printing filaments. After testing using dynamic mechanical analysis, the scientists concluded that the new composites had improved mechanical properties, and they could be used to make strong 3D-printed materials.Zander's team is building a mobile recycling trailer that will enable specially trained soldiers to fabricate 3D-printing filaments from plastic waste. She is also exploring ways to print materials from plastic pellets instead of filaments, which could help soldiers quickly produce larger 3D-printed parts and machinery.""We still have a lot to learn about how to best process these materials and what kinds of additives will improve their properties,"" Zander says. ""We're just scratching the surface of what we can ultimately do with these discarded plastics.""Video: https://www.youtube.com/watch?v=8WnN9y-tFukStory Source:Materials provided by American Chemical Society. ",0.0850999,0.142227,0.147432,0.163471,0.148961,0.128547,0.148819,0,0
510,World's first-ever 4D printing for ceramics,"A research team at City University of Hong Kong (CityU) has achieved a ground-breaking advancement in materials research by successfully developing the world's first-ever 4D printing for ceramics, which are mechanically robust and can have complex shapes. This could turn a new page in the structural application of ceramics.Ceramic has a high melting point, so it is difficult to use conventional laser printing to make ceramics. The existing 3D-printed ceramic precursors, which are usually difficult to deform, also hinder the production of ceramics with complex shapes.To overcome these challenges, the CityU team has developed a novel ""ceramic ink,"" which is a mixture of polymers and ceramic nanoparticles. The 3D-printed ceramic precursors printed with this novel ink are soft and can be stretched three times beyond their initial length. These flexible and stretchable ceramic precursors allow complex shapes, such as origami folding. With proper heat treatment, ceramics with complex shapes can be made.The team was led by Professor Jian Lu, Vice-President (Research and Technology) and Chair Professor of Mechanical Engineering, who is a distinguished materials scientist with research interests ranging from fabricating nanomaterials and advanced structural materials to the computational simulation of surface engineering.With the development of the elastic precursors, the research team has achieved one more breakthrough by developing two methods of 4D printing of ceramics.4D printing is conventional 3D printing combined with the additional element of time as the fourth dimension, where the printed objects can re-shape or self-assemble themselves over time with external stimuli, such as mechanical force, temperature, or a magnetic field.In this research, the team made use of the elastic energy stored in the stretched precursors for shape morphing. When the stretched ceramic precursors are released, they undergo self-reshaping. After heat treatment, the precursors turn into ceramics.The resultant elastomer-derived ceramics are mechanically robust. They can have a high compressive strength-to-density ratio (547 MPa on 1.6 g cm-3 microlattice), and they can come in large sizes with high strength compared to other printed ceramics.""The whole process sounds simple, but it's not,"" said Professor Lu. ""From making the ink to developing the printing system, we tried many times and different methods. Like squeezing icing on a cake, there are a lot of factors that can affect the outcome, ranging from the type of cream and the size of the nozzle, to the speed and force of squeezing, and the temperature.""It took more than two and a half years for the team to overcome the limitations of the existing materials and to develop the whole 4D ceramic printing system.In the first shaping method, a 3D-printed ceramic precursor and substrate were first printed with the novel ink. The substrate was stretched using a biaxial stretching device, and joints for connecting the precursor were printed on it. The precursor was then placed on the stretched substrate. With the computer-programmed control of time and the release of the stretched substrate, the materials morphed into the designed shape.In the second method, the designed pattern was directly printed on the stretched ceramic precursor. It was then released under computer-programming control and underwent the self-morphing process.The innovation was published in the latest issue of top academic journal Science Advances under the title ""Origami and 4D printing of elastomer-derived ceramic structures."" All research team members are from CityU, including Dr LIU Guo, Research Assistant, Dr ZHAO Yan, Senior Research Associate, and Dr WU Ge, Research Fellow.""With the versatile shape-morphing capability of the printed ceramic precursors, its application can be huge!"" said Professor Lu. One promising application will be for electronic devices. Ceramic materials have much better performance in transmitting electromagnetic signals than metallic materials. With the arrival of 5G networks, ceramic products will play a more important role in the manufacture of electronic products. The artistic nature of ceramics and their capability to form complex shapes also provide the potential for consumers to tailor-make uniquely designed ceramic mobile phone back plates.Furthermore, this innovation can be applied in the aero industry and space exploration. ""Since ceramic is a mechanically robust material that can tolerate high temperatures, the 4D-printed ceramic has high potential to be used as a propulsion component in the aerospace field,"" said Prof Lu.Riding on the breakthrough in material and 4D-printing technique advancement, Prof Lu said the next step is to enhance the mechanical properties of the material, such as reducing its brittleness.Story Source:Materials provided by City University of Hong Kong. ",0.0989868,0.175728,0.135548,0.169655,0.155489,0.141174,0.172124,0,0
511,Color effects from transparent 3D printed nanostructures,"Most of the objects we see are colored by pigments, but using pigments has disadvantages: such colors can fade, industrial pigments are often toxic, and certain color effects are impossible to achieve. The natural world, however, also exhibits structural coloration, where the microstructure of an object causes various colors to appear. Peacock feathers, for instance, are pigmented brown, but -- because of long hollows within the feathers -- reflect the gorgeous, iridescent blues and greens we see and admire. Recent advances in technology have made it practical to fabricate the kind of nanostructures that result in structural coloration, and computer scientists from the Institute of Science and Technology Austria (IST Austria) and the King Abdullah University of Science and Technology (KAUST) have now created a computational tool that automatically creates 3D-print templates for nanostructures that correspond to user-defined colors. Their work demonstrates the great potential for structural coloring in industry, and opens up possibilities for non-experts to create their own designs. This project will be presented at this year's top computer graphics conference, SIGGRAPH 2018, by first author and IST Austria postdoc Thomas Auzinger. This is one of five IST Austria presentations at the conference this year.The changing colors of a chameleon and the iridescent blues and greens of the morpho butterfly, among many others in nature, are the result of structural coloration, where nanostructures cause interference effects in light, resulting in a variety of colors when viewed macroscopically. Structural coloration has certain advantages over coloring with pigments (where particular wavelengths are absorbed), but until recently, the limits of technology meant fabricating such nanostructures required highly specialized methods. New ""direct laser writing"" set-ups, however, cost about as much as a high-quality industrial 3D printer, and allow for printing at the scale of hundreds of nanometers (hundred to thousand time thinner than a human hair), opening up possibilities for scientists to experiment with structural coloration.So far, scientists have primarily experimented with nanostructures that they had observed in nature, or with simple, regular nanostructural designs (e.g. row after row of pillars). Thomas Auzinger and Bernd Bickel of IST Austria, together with Wolfgang Heidrich of KAUST, however, took an innovative new approach that differs in several key ways. First, they solve the inverse design task: the user enters the color they want to replicate, and then the computer creates a nanostructure pattern that gives that color, rather than attempting to reproduce structures found in nature. Moreover, ""our design tool is completely automatic,"" says Thomas Auzinger. ""No extra effort is required on the part of the user.""Second, the nanostructures in the template do not follow a particular pattern or have a regular structure; they appear to be randomly composed -- a radical break from previous methods, but one with many advantages. ""When looking at the template produced by the computer I cannot tell by the structure alone, if I see a pattern for blue or red or green,"" explains Auzinger. ""But that means the computer is finding solutions that we, as humans, could not. This free-form structure is extremely powerful: it allows for greater flexibility and opens up possibilities for additional coloring effects."" For instance, their design tool can be used to print a square that appears red from one angle, and blue from another (known as directional coloring).Finally, previous efforts have also stumbled when it came to actual fabrication: the designs were often impossible to print. The new design tool, however, guarantees that the user will end up with a printable template, which makes it extremely useful for the future development of structural coloration in industry. ""The design tool can be used to prototype new colors and other tools, as well as to find interesting structures that could be produced industrially,"" adds Auzinger. Initial tests of the design tool have already yielded successful results. ""It's amazing to see something composed entirely of clear materials appear colored, simply because of structures invisible to the human eye,"" says Bernd Bickel, professor at IST Austria, ""we're eager to experiment with additional materials, to expand the range of effects we can achieve.""""It's particularly exciting to witness the growing role of computational tools in fabrication,"" concludes Auzinger, ""and even more exciting to see the expansion of 'computer graphics' to encompass physical as well as virtual images.""Story Source:Materials provided by Institute of Science and Technology Austria. ",0.0590565,0.0878724,0.114798,0.171296,0.0964641,0.106645,0.128347,0,0
512,New manufacturing technique could improve common problem in printing technology,"A new manufacturing technique developed by researchers from Binghamton University, State University at New York may be able to avoid the ""coffee ring"" effect that plagues inkjet printers.The outer edges of the ring that a coffee mug leaves behind are darker than the inside of the ring. That's because the solute is separated from the liquid during the evaporation process. That's what's called the coffee ring effect.This same effect can happen with printers as well. When printing a text document, the text itself consists of an outline of the letters and the inside of the letters. While it may not be visible to the untrained eye, the outlines are actually darker than the inside. This happens during the drying process, just like the coffee ring effect, but researchers have wanted to find a way to remove this difference in pigmentation.Assistant Professor Xin Yong, Professor Tim Singler and Associate Professor Paul Chiarot from Binghamton University's Mechanical Engineering Department recently made a discovery that could eliminate the difference between the outline and the inside that happens during evaporation.They published their work, titled ""Interfacial Targeting of Sessile Droplets Using Electrospray,"" in the journal Langmuir.The team of experimentalists and theorists worked together to study the flow inside and on the surface of drying droplets to understand more about the coffee ring effect and how to avoid it. Using a unique technique called electrospray, they applied a high voltage to a liquid to produce an aerosol to add nanoparticles to the droplets. Nanoparticles are often useful for researchers due to their small size and large surface area.This technique allowed for a more even dispersal of ink and stopped the coffee ring effect.""Not only does this study help us understand how to avoid the coffee ring effect, it also tells us more about the phenomena that occur during evaporation that lead to the coffee ring effect,"" said Yong.In the research paper, the team said, ""To our knowledge, we are among the first to use electrospray for this purpose to explore interfacial particle transport and to elucidate the role of surfactants in governing particle motion and deposit structure.""While this difference in quality may not affect the standard user's print job, it will have a substantial effect on the capabilities of additive manufacturing and biotechnology, where printing films in a uniform way is extremely important.Story Source:Materials provided by Binghamton University. ",0.131942,0.161212,0.189482,0.220971,0.177314,0.199985,0.198185,0,0
513,3-D inks that can be erased selectively Scientists develop method for the specific degradation of laser-written microstructures,"3D printing by direct laser writing enables production of micro-meter-sized structures for many applications, from biomedicine to microelectronics to optical metamaterials. Researchers of Karlsruhe Institute of Technology (KIT) have now developed 3D inks that can be erased selectively. This allows specific degradation and reassembly of highly precise structures on the micrometer and nanometer scales. The new photoresists are presented in the journal Nature Communications.3D printing is gaining importance, as it allows for the efficient manufacture of complex geometries. A very promising method is direct laser writing: a computer-controlled focused laser beam acts as a pen and produces the desired structure in a photoresist. In this way, three-dimensional structures with details in the sub-micrometer range can be produced. ""The high resolution is very attractive for applications requiring very precise filigree structures, such as in biomedicine, microfluidics, microelectronics or for optical metamaterials,"" say Professor Christopher Barner-Kowollik, Head of the Macromolecular Architectures Group of KIT's Institute for Chemical Technology and Polymer Chemistry (ITCP) and of the Soft Matter Materials Group of Queensland University of Technology (QUT) in Brisbane, Australia, and Dr. Eva Blasco of the ITCP of KIT. Over a year ago, KIT researchers already succeeded in expanding the possibilities of direct laser writing: the working groups of Professor Martin Wegener at the Institute of Applied Physics (APH) and the Institute of Nanotechnology (INT) of KIT and of Professor Christopher Barner-Kowollik developed an erasable ink for 3D printing. Thanks to reversible binding, the building blocks of the ink can be separated again.Now, the scientists from Karlsruhe and Brisbane have largely refined their development. They have developed several inks, in different colors so to speak, that can be erased independently of each other. This enables selective and sequential degradation and reassembly of the laser-written microstructures. In case of highly complex constructions, for instance, temporary supports can be produced and removed again later on. It may also be possible to add or remove parts to or from three-dimensional scaffolds for cell growth, the objective being to observe how the cells react to such changes. Moreover, the specifically erasable 3D inks allow for the exchange of damaged or worn parts in complex structures.When producing the cleavable photoresists, the researchers were inspired by degradable biomaterials. The photoresists are based on silane compounds that can be cleaved easily. Silanes are silicon-hydrogen compounds. The scientists used specific atom substitution for preparing the photoresists. In this way, microstructures can be degraded specifically under mild conditions without structures with other material properties being damaged. This is the major advantage over formerly used erasable 3D inks. New photoresists also contain the monomer pentaerythritol triacrylate that significantly enhances writing without affecting cleavability.Story Source:Materials provided by Karlsruher Institut fer Technologie (KIT). ",0.0891965,0.14367,0.184213,0.17481,0.148309,0.144704,0.166147,0,0
514,Bose-Einstein condensate generated in space for the first time Physicists put in place groundwork for accurately testing Einstein's equivalence principle,"A team of scientists from Germany has succeeded in creating a Bose-Einstein condensate for the first time in space on board a research rocket. On January 23, 2017 at 3:30 a.m. Central European Time, the MAIUS-1 mission was launched into space from the Esrange Space Center in Sweden. After a detailed analysis, the results have been published recently in the journal Nature. The Bose-Einstein condensate, an ultracold gas, can be used as a starting point for performing important measurements in zero gravity. During the approximately 15-minute rocket flight, the scientists managed to conduct approximately 100 experiments with regard to the generation and characterization of the Bose-Einstein condensate and its suitability as a basis for high-precision interferometry.Ultracold quantum gases are employed as a source for a variety of quantum technological experiments. Scientists can use them to measure the Earth's gravitational field, detect gravitational waves, and test Einstein's equivalence principle with high precision. As these experiments are usually limited in accuracy by how long the ultracold atoms can be observed, performing them in space promises to increase their sensitivity significantly, because the observation times are no longer limited by the gravitational acceleration on Earth.Led by Leibniz University Hannover, a team of scientists from eleven German universities and research institutions, among them a group led by Professor Patrick Windpassinger of the Institute of Physics at Johannes Gutenberg University Mainz (JGU), developed and constructed the scientific payload of the MAIUS-1 mission and collected data during its flight. ""The success of the MAIUS mission not only proves that the laser system we developed is highly reliable in extreme environments away from the laboratory. It also demonstrates that quantum technologies like those pursued in initiatives run by the German Federal Ministry of Education and Research (BMBF) and in the EU Quantum Flagship program have already reached a very high level of technological maturity,"" said Windpassinger, commenting on the mission.The experiments employ lasers and magnetic fields to capture an atomic gas, which is composed of rubidium atoms in this case, and cool it to temperatures within a few billionths of a degree above absolute zero. As a result, the scientists can observe the condensate for several hundred milliseconds and use it to take measurements. Absolute zero is around minus 273.15 degrees Celsius.Ultracold atom generation can be reproduced reliablyIn their paper published in Nature, the scientists demonstrate that their technique for generating and transporting ultracold atoms is highly reproducible. In particular, the movements of the atoms in the magnetic trap, generated by a so-called atom chip, closely matched predictions. Being able to transport them in this way would permit their utilization in more advanced cooling schemes.""This should enable us to achieve even lower temperatures, down to a few trillionths of a degree above absolute zero or even lower, in future missions,"" Dr. Andre Wenzlawski, a member of the Windpassinger group, pointed out. At such extremely cold temperatures, the speed of the atoms is reduced drastically. It is thus hoped that in future missions they will be observable for several seconds, opening up the possibility of precision measurements at accuracies not possible on Earth.The research team has thus demonstrated the viability of their basic method when it comes to experimentally measuring the Earth's gravitational field, detecting gravitational waves, and testing Einstein's equivalence principle.Follow-up missions MAIUS-2 and MAIUS-3 are plannedThe researchers will pursue the last-mentioned application in the follow-up missions MAIUS-2 and MAIUS-3. These are scheduled for two more rocket launches in 2020 and 2021, in which, in addition to ultracold rubidium atoms, potassium atoms will also be studied. Comparing the gravitational acceleration of the two kinds of atoms should allow to measure the equivalence principle in the future.Similarly, the findings will be used to plan BECCAL, a joint project to be undertaken by NASA and the German Aerospace Center (DLR) that will involve an experiment using ultracold quantum gases on the International Space Station (ISS). Professor Patrick Windpassinger's group at Mainz University will also form part of this team.The MAIUS-1 high-altitude research rocket mission was implemented as a joint project by Leibniz University Hannover, the University of Bremen, Johannes Gutenberg University Mainz, Universitet Hamburg, Humboldt-Universitet zu Berlin, the Ferdinand-Braun-Institut, Leibniz-Institut fer Hechstfrequenztechnik (FBH) in Berlin, TU Darmstadt, Ulm University, and the German Aerospace Center (DLR). Financing for the project was arranged by DLR Space Mission Management with funds provided by the German Federal Ministry for Economic Affairs and Energy on the basis of a resolution of the German Bundestag.Story Source:Materials provided by Johannes Gutenberg Universitaet Mainz. ",0.112622,0.103612,0.0957825,0.125163,0.103018,0.14033,0.12453,0,0
515,Dandelion seeds reveal newly discovered form of natural flight Why the plant is among the best fliers in the natural world,"The extraordinary flying ability of dandelion seeds is possible thanks to a form of flight that has not been seen before in nature, research has revealed.The discovery, which confirms the common plant among the natural world's best fliers, shows that movement of air around and within its parachute-shaped bundle of bristles enables seeds to travel great distances -- often a kilometre or more, kept afloat entirely by wind power.Researchers from the University of Edinburgh carried out experiments to better understand why dandelion seeds fly so well, despite their parachute structure being largely made up of empty space.Their study revealed that a ring-shaped air bubble forms as air moves through the bristles, enhancing the drag that slows each seed's descent to the ground.This newly found form of air bubble -- which the scientists have named the separated vortex ring -- is physically detached from the bristles and is stabilised by air flowing through it.The amount of air flowing through, which is critical for keeping the bubble stable and directly above the seed in flight, is precisely controlled by the spacing of the bristles.This flight mechanism of the bristly parachute underpins the seeds' steady flight. It is four times more efficient than what is possible with conventional parachute design, according to the research.Researchers suggest that the dandelion's porous parachute might inspire the development of small-scale drones that require little or no power consumption. Such drones could be useful for remote sensing or air pollution monitoring.The study, published in Nature, was funded by the Leverhulme Trust and the Royal Society.Dr Cathal Cummins, of the University of Edinburgh's Schools of Biological Sciences and Engineering, who led the study, said: ""Taking a closer look at the ingenious structures in nature -- like the dandelion's parachute -- can reveal novel insights. We found a natural solution for flight that minimises the material and energy costs, which can be applied to engineering of sustainable technology.""Story Source:Materials provided by University of Edinburgh. ",0.188119,0.149868,0.183434,0.191288,0.148762,0.221131,0.176538,0,0
516,Engineers study hovering bats and hummingbirds in Costa Rica,"Each sunrise in Las Cruces, Costa Rica, River Ingersoll's field team trekked into the jungle to put the finishing touches on nearly invisible nets. A graduate student in the lab of David Lentink, assistant professor of mechanical engineering at Stanford University, Ingersoll needed these delicate nets to catch, study and release the region's abundant hummingbirds and bats -- the only two vertebrates with the ability to hover in place.""We're really interested in how hovering flight evolved,"" said Ingersoll. ""Nectar bats drink from flowers like hummingbirds do, so we want to see if there's any similarities or differences between these two different taxa.""Ingersoll's nets worked, and he ended up examining over 100 individual hummingbirds and bats, covering 17 hummingbird and three bat species, during his field study, the results of which the group published in Science Advances.Through a combination of high-speed camera footage and aerodynamic force measurements, he and his fellow researchers found that hummingbirds and bats hover in very different ways. Yet they also found that nectar bats' hovering shares some similarities with hummingbird hovering -- which fruit bats do not share. This suggests that they evolved a different method to hover compared with other bats in order to drink nectar.In addition to learning more about bats and hummingbirds, Lentink and others can apply what they learned to engineering problems, such as designing flying robots. Engineers have already created robots inspired by hummingbirds and bats but haven't known which of the natural counterparts of these robots hover most effectively.Watching every featherIt is simple to imagine how a flying animal supports itself by flapping downward, but in order to avoid exaggerated bobbing up and down, hovering animals must maintain this support while flapping upward as well. Hummingbirds and bats accomplish this feat by twisting their wings backward on the upstroke, continuously pushing air downward to keep them steadily aloft.""If you look amongst vertebrates, there are two that can hover in a sustained way,"" said Lentink. ""Those are hummingbirds and nectar bats. And you'll find both in the neotropics, like Costa Rica.""To study these subjects, Ingersoll collaborated with a long-standing bird banding project run by Stanford ecologists in Las Cruces. Borrowing birds and bats from their project, he placed each animal in a flight chamber outfitted with aerodynamic force sensors at the top and bottom of the chamber -- equipment developed by Lentink's lab to measure extremely small changes in vertical force at 10,000 times per second. These plates are so sensitive that they captured the vertical forces produced by every twist and flutter of hummingbirds that weighed as little as 2.4 grams.By synching those force measurements with multiple high-speed cameras recording at 2,000 frames per second, the researchers could isolate any moment of their subjects' flights to see how the lift they were generating related to the shape of their wings.""I'd sit and wait for the hummingbird to feed at the flower. Once it was feeding, I would trigger the cameras and the force measurements and we'd get four seconds of footage of the hummingbird flapping at the flower,"" said Ingersoll.After their short stint in the flight chamber, Ingersoll returned the birds and bats to where they were caught and released them. The whole process took between one and three hours.Different ways to hoverThe researchers found that the bats and hummingbirds all exerted a similar amount of energy relative to their weight during these flights but that the hummingbirds, fruit bats and nectar bats all hovered in very different ways. The hummingbirds hovered in a more aerodynamically efficient way than the bats -- the hummingbirds generated more lift relative to drag. In comparing wing shapes, the researchers found this efficiency is likely because the hummingbirds invert their wings more easily. Although the bats struggled with turning over their wings, they exerted a comparable amount of energy because they have bigger wings and larger strokes.The researchers were surprised to find that nectar bats, which sidle up to flowers like hummingbirds, generated more upward force when the wings were lifting than fruit bats. Looking at their wing shape, the researchers found that nectar bats can twist their wings much more than fruit bats on the upstroke. So nectar bats' hovering form is like a blend of fruit bats' and hummingbirds' hovering.The researchers plan to build on these findings as part of their work on flapping robots and drones but Lentink also sees potential for more work beyond the lab.""When Rivers proposed to do this study in Costa Rica, a field study was something I'd never hoped for. Now, he really inspired me,"" said Lentink. ""There are about 10,000 species of birds and most of them have never been studied. It sounds like too big a study to embark on but that's what I dream about.""Additional co-authors are Lukas Haizmann from the University of Bremen, who visited the Lentink Lab for his internship and master's thesis research. Lentink is also a member of Stanford Bio-X and the Stanford Neurosciences Institute, and an affiliate of the Stanford Woods Institute for the Environment. This work was funded by the National Science Foundation and the King Abdulaziz City for Science and Technology Center of Excellence for Aeronautics and Astronautics at Stanford.Video: https://www.youtube.com/watch?v=fSgQ9HRE9MQStory Source:Materials provided by Stanford University. Original written by Taylor Kubota. ",0.224271,0.15162,0.176565,0.241455,0.171561,0.235713,0.209289,0,0
517,"Drones offer ability to find, ID and count marine megafauna ","New research from North Carolina State University demonstrates that consumer-grade drones are effective tools for monitoring marine species across multiple sites in the wild. The work shows that the technology can be a valuable platform for scientists and conservationists interested in studying populations of sharks, rays, sea turtles and other marine megafauna.""We found that drones can be used to count and make species-level identifications of marine species, particularly in shallow marine environments,"" says Enie Hensel, a Ph.D. candidate at NC State and first author of a paper on the work.""Demonstrating the viability of drones for this work matters, because these are inexpensive tools for collecting accurate abundance estimates,"" Hensel says. ""And those estimates are important for both informing the development of conservation efforts and for assessing the effectiveness of those efforts.""Drone surveys are also a good way to monitor shallow water, megafauna species because they are not intrusive,"" Hensel says. ""More traditional monitoring methods -- such as boat surveys or gill nets -- are more invasive, and have the potential to harm individuals or alter their movement patterns.""Previous studies using drones to monitor marine species have focused on single sites. The recent work from NC State evaluated multiple sites, demonstrating that drones can be used to assess environmental variables that may be responsible for population differences between locations. For example, drones may be used to help target conservation efforts on sites that have the most value in terms of supporting specific species.The researchers also showed that drones are effective at sites with varying degrees of water clarity.To assess the effectiveness of the drones, researchers placed fake sharks underwater at two sites with different water clarity. Drone footage allowed researchers to identify all of the decoys at both sites.""We chose grey shark decoys because they would be the most difficult to spot in these environments, but we were able to spot them all,"" Hensel says.In field testing, researchers were also able to make species-level identifications of lemon, nurse and bonnethead sharks, as well as southern stingrays and spotted eagle rays. The drone footage also allowed researchers to identify sea turtles, though they had difficulty differentiating between hawksbill and green sea turtles.""One reason we chose these sites, all of which were on Great Abaco Island in The Bahamas, was because The Bahamas are interested in using several of the sites as a pilot for a managed conservation effort,"" Hensel says. ""Our surveys provide baseline data for marine megafauna abundances within these newly established parks and we show that drones offer a new management tool for the park service of The Bahamas. And, of course, the technology certainly opens doors for us to explore a range of conservation issues.""Story Source:Materials provided by North Carolina State University. ",0.117843,0.156929,0.204585,0.251866,0.148402,0.179452,0.199697,0,0
518,"Smart skin' simplifies spotting strain in structures Invention can use fluorescing carbon nanotubes to reveal stress in aircraft, structures","Thanks to one peculiar characteristic of carbon nanotubes, engineers will soon be able to measure the accumulated strain in an airplane, a bridge or a pipeline -- or just about anything -- over the entire surface or down to microscopic levels.They'll do so by shining a light onto structures coated with a two-layer nanotube film and protective polymer. Strain in the surface will show up as changes in the wavelengths of near-infrared light emitted from the film and captured by a miniaturized hand-held reader. The results will show engineers and maintenance crews whether structures like bridges or aircraft have been deformed by stress-inducing events or regular wear and tear.Like a white shirt under an ultraviolet light, single-wall carbon nanotubes fluoresce, a property discovered in 2002 in the lab of Rice chemist Bruce Weisman. In a basic research project a few years later, the group showed that stretching a nanotube changes the color of its fluorescence.When Weisman's results came to the attention of Rice civil and environmental engineer Satish Nagarajaiah -- who had been working independently on similar ideas using Raman spectroscopy, but at the macro scale, since 2003 -- he suggested collaborating to turn that scientific phenomenon into a useful technology for strain sensing.Now, Nagarajaiah and Weisman and have published a pair of important papers about their ""smart skin"" project. The first appears in Structural Control & Health Monitoring, and introduces the latest iteration of the technology they first revealed in 2012.It describes a method of depositing the microscopic nanotube-sensing film separately from a protective top layer. Color changes in the nanotube emission indicate the amount of strain in the underlying structure. The researchers say it enables two-dimensional mapping of accumulated strain that can't be achieved by any other non-contact method.The second paper, in the Journal of Structural Engineering, details the results of testing smart skin on metal specimens with irregularities where stress and strain are often concentrated.""The project started out as pure science about nanotube spectroscopy, and led to the proof-of-principle collaborative work that showed we could measure the strain of the underlying substrate by checking the spectrum of the film in one place,"" Weisman said. ""That suggested the method could be expanded to measure whole surfaces. What we've shown now is a lot closer to that practical application.""Since the initial report, the researchers have refined the composition and preparation of the film and its airbrush-style application, and also developed scanner devices that automatically capture data from multiple programmed points. Unlike conventional sensors that only measure strain at one point along one axis, the smart film can be selectively probed to reveal strain in any direction and location.The two-layer film is only a few microns thick, a fraction of the width of a human hair, and barely visible on a transparent surface. ""In our initial films, the nanotube sensors were mixed into the polymer,"" Nagarajaiah said. ""Now that we've separated the sensing and the protective layers, the nanotube emission is clearer and we can scan at a much higher resolution. That lets us capture significant amounts of data rather quickly.""The researchers tested smart skin on aluminum bars under tension with either a hole or a notch to represent the places where strain tends to build. Measuring these potential weak spots in their unstressed state and then again after applying stress showed dramatic changes in strain patterns pieced together from point-by-point surface mapping.""We know where the high-stress regions of the structure are, the potential points of failure,"" Nagarajaiah said. ""We can coat those regions with the film and scan them in the healthy state, and then after an event like an earthquake, go back and re-scan to see whether the strain distribution has changed and the structure is at risk.""In their tests, the researchers said the measured results were a close match to strain patterns obtained through advanced computational simulations. Readings from the smart skin allowed them to quickly spot distinctive patterns near the high-stress regions, Nagarajaiah said. They were also able to see clear boundaries between regions of tensile and compressive strain.""We measured points 1 millimeter apart, but we can go 20 times smaller when necessary without sacrificing strain sensitivity,"" Weisman said. That's a leap over standard strain sensors, which only provide readings averaged over several millimeters, he said.The researchers see their technology making initial inroads in niche applications, like testing turbines in jet engines or structural elements in their development stages. ""It's not going to replace all existing technologies for strain measurement right away,"" Weisman said. ""Technologies tend to be very entrenched and have a lot of inertia.""But it has advantages that will prove useful when other methods can't do the job,"" he said. ""I expect it will find use in engineering research applications, and in the design and testing of structures before they are deployed in the field.""With their smart skin refined, the researchers are working toward developing the next generation of the strain reader, a camera-like device that can capture strain patterns over a large surface all at once.Story Source:Materials provided by Rice University. ",0.0654343,0.099313,0.137597,0.138874,0.1137,0.103012,0.143795,0,0
519,Fleets of drones could aid searches for lost hikers System allows drones to cooperatively explore terrain under thick forest canopies where GPS signals are unreliable,"Finding lost hikers in forests can be a difficult and lengthy process, as helicopters and drones can't get a glimpse through the thick tree canopy. Recently, it's been proposed that autonomous drones, which can bob and weave through trees, could aid these searches. But the GPS signals used to guide the aircraft can be unreliable or nonexistent in forest environments.In a paper being presented at the International Symposium on Experimental Robotics conference next week, MIT researchers describe an autonomous system for a fleet of drones to collaboratively search under dense forest canopies. The drones use only onboard computation and wireless communication -- no GPS required.Each autonomous quadrotor drone is equipped with laser-range finders for position estimation, localization, and path planning. As the drone flies around, it creates an individual 3-D map of the terrain. Algorithms help it recognize unexplored and already-searched spots, so it knows when it's fully mapped an area. An off-board ground station fuses individual maps from multiple drones into a global 3-D map that can be monitored by human rescuers.In a real-world implementation, though not in the current system, the drones would come equipped with object detection to identify a missing hiker. When located, the drone would tag the hiker's location on the global map. Humans could then use this information to plan a rescue mission.""Essentially, we're replacing humans with a fleet of drones to make the search part of the search-and-rescue process more efficient,"" says first author Yulun Tian, a graduate student in the Department of Aeronautics and Astronautics (AeroAstro).The researchers tested multiple drones in simulations of randomly generated forests, and tested two drones in a forested area within NASA's Langley Research Center. In both experiments, each drone mapped a roughly 20-square-meter area in about two to five minutes and collaboratively fused their maps together in real-time. The drones also performed well across several metrics, including overall speed and time to complete the mission, detection of forest features, and accurate merging of maps.Exploring and mappingOn each drone, the researchers mounted a LIDAR system, which creates a 2-D scan of the surrounding obstacles by shooting laser beams and measuring the reflected pulses. This can be used to detect trees; however, to drones, individual trees appear remarkably similar. If a drone can't recognize a given tree, it can't determine if it's already explored an area.The researchers programmed their drones to instead identify multiple trees' orientations, which is far more distinctive. With this method, when the LIDAR signal returns a cluster of trees, an algorithm calculates the angles and distances between trees to identify that cluster. ""Drones can use that as a unique signature to tell if they've visited this area before or if it's a new area,"" Tian says.This feature-detection technique helps the ground station accurately merge maps. The drones generally explore an area in loops, producing scans as they go. The ground station continuously monitors the scans. When two drones loop around to the same cluster of trees, the ground station merges the maps by calculating the relative transformation between the drones, and then fusing the individual maps to maintain consistent orientations.""Calculating that relative transformation tells you how you should align the two maps so it corresponds to exactly how the forest looks,"" Tian says.In the ground station, robotic navigation software called ""simultaneous localization and mapping"" (SLAM) -- which both maps an unknown area and keeps track of an agent inside the area -- uses the LIDAR input to localize and capture the position of the drones. This helps it fuse the maps accurately.The end result is a map with 3-D terrain features. Trees appear as blocks of colored shades of blue to green, depending on height. Unexplored areas are dark but turn gray as they're mapped by a drone. On-board path-planning software tells a drone to always explore these dark unexplored areas as it flies around. Producing a 3-D map is more reliable than simply attaching a camera to a drone and monitoring the video feed, Tian says. Transmitting video to a central station, for instance, requires a lot of bandwidth that may not be available in forested areas.More efficient searchingA key innovation is a novel search strategy that let the drones more efficiently explore an area. According to a more traditional approach, a drone would always search the closest possible unknown area. However, that could be in any number of directions from the drone's current position. The drone usually flies a short distance, and then stops to select a new direction.""That doesn't respect dynamics of drone [movement],"" Tian says. ""It has to stop and turn, so that means it's very inefficient in terms of time and energy, and you can't really pick up speed.""Instead, the researchers' drones explore the closest possible area while considering their speed and direction and maintaining a consistent velocity. This strategy -- where the drone tends to travel in a spiral pattern -- covers a search area much faster. ""In search and rescue missions, time is very important,"" Tian says.In the paper, the researchers compared their new search strategy with a traditional method. Compared to that baseline, the researchers' strategy helped the drones cover significantly more area, several minutes faster and with higher average speeds.One limitation for practical use is that the drones still must communicate with an off-board ground station for map merging. In their outdoor experiment, the researchers had to set up a wireless router that connected each drone and the ground station. In the future, they hope to design the drones to communicate wirelessly when approaching one another, fuse their maps, and then cut communication when they separate. The ground station, in that case, would only be used to monitor the updated global map.Co-authors on the paper are: Katherine Liu, a PhD student in MIT's Computer Science and Artificial Intelligence Laboratory (CSAIL) and AeroAstro; Kyel Ok, a PhD student in CSAIL and the Department of Electrical Engineering and Computer Science; Loc Tran and Danette Allen of the NASA Langley Research Center; Nicholas Roy, an AeroAstro professor and CSAIL researcher; and Jonathan P. How, the Richard Cockburn Maclaurin Professor of Aeronautics and Astronautics.Story Source:Materials provided by Massachusetts Institute of Technology. Original written by Rob Matheson. ",0.115134,0.103759,0.144849,0.254106,0.126355,0.163586,0.169525,0,0
520,New air-filled fiber bundle could make endoscopes smaller Innovative optical fibers poised to expand endoscopy to new wavelengths and diagnoses,"Researchers have fabricated a new kind of air-filled optical fiber bundle that could greatly improve endoscopes used for medical procedures like minimally invasive surgeries or bronchoscopies. The new technology might also lead to endoscopes that produce images using infrared wavelengths, which would allow diagnostic procedures that are not possible with endoscopes today.Endoscopes use bundles of optical fibers to transmit images from inside the body. Light falling on one end of the fiber bundle travels through each fiber to the far end, allowing a picture to be carried in the form of thousands of spots that are much like the pixels that make up a digital picture.Optical fibers consist of an inner core and an outer cladding with different optical properties, which traps the light inside and allows it to travel down the fiber. Rather than using cores and claddings made of two types of glass like most fiber bundles, the new bundles use an array of glass cores surrounded by hollow glass capillaries filled with air that act as the cladding.In The Optical Society (OSA) journal Optics Letters, researchers show that their new fiber bundles, which they call air-clad imaging fibers, maintain the resolution of the best commercial imaging fibers at double the wavelength range that the commercial fibers can be used. The new fiber could be used to create endoscopes that are smaller or have higher resolutions than those available today.""Higher resolution is always helpful to clinicians carrying out endoscopic procedures, but the most sensitive jobs, such as those in the brain, usually require the thinnest instruments,"" said the paper's first author, Harry Wood of the University of Bath. ""These instruments are usually so narrow that the imaging fiber contains too few cores to make a clear image. Our air-clad bundles allow more fibers to be packed into a smaller diameter and so will likely be particularly useful in these situations.""In addition to applications in medical diagnostics and treatment, the new fiber could prove useful for industrial applications such as monitoring the contents of hazardous machines or imaging the inside of oil and mineral drills.Combining air and glassWhen a bundle of fibers contains a greater number of cores within a given cross section area, it will produce more detailed images in the same way that a camera with more pixels creates higher resolution images. However, if the cores are too small and close together, light can leak from one to another and the image becomes blurry.""The honeycomb structure we developed combines glass and air to contain light far more tightly in the cores than traditional imaging fibers that use two types of glass,"" said Wood. ""This allows us to bring the cores closer together than ever before possible, or squeeze in longer wavelengths of light, without the blurring that would be seen with conventional approaches.""The fact that the new fibers work well with wavelengths further into the infrared portion of the spectrum could allow the development of endoscopes that image fluorescent markers that emit at these wavelengths. Infrared light also can be used to image cells that are embedded more deeply within tissue than can be imaged with visible wavelengths.""There are fluorescent marker probes that emit light of specific wavelengths in response to certain bacteria or immune cells,"" said Wood. ""These could be very effective at highlighting disease in the lung, for example, but we can currently use only one or two such probes in the wavelength range that is offered by today's endoscope technology.""Comparing fiber performanceTo test the imaging fibers, the researchers made an air-clad fiber bundle that matched the resolution of a leading commercial fiber because it had the same spacing between cores. They were able to incorporate more than 11,000 cores into the fiber by stacking multiple smaller honeycomb structures together.The researchers point out that the principle behind the new fibers has been known for years but that fabrication approaches, especially for fibers with air gaps, have just recently advanced to the point where these fibers could be made.The researchers used their new air-clad fiber bundle and the commercial fiber to image a standard test target image. ""We were delighted to find that the air-clad fiber functioned well beyond the wavelength range our visible camera could detect,"" said Wood. ""When we changed to an infrared camera, we saw that the fiber created a clear image at double the wavelength that the commercial fiber reached.""Story Source:Materials provided by The Optical Society. ",0.110709,0.120175,0.179744,0.210715,0.185714,0.135934,0.165699,0,0
521,Volcanic ash impact on air travel could be reduced says new research,"Manchester-based Volcanologists have developed a method and camera that could help reduce the dangers, health risks and travel impacts of ash plumes during a volcanic eruption.Ash is sometimes seen as a secondary danger in volcanoes when compared to more visual hazards, such as lava and pyroclastic flows. However, ash can have a major impact on human lives and infrastructure, such as the 2010 eruption of the Icelandic volcano, Eyjafjallajekull, which brought international aviation across Europe to a standstill.This research, which is being published in Scientific Reports, uses a new kind camera, developed at The University of Manchester, to measure the flow and speed of ash falling out of a volcanic plume.The researchers measure how ash particles interact with sunlight and, specifically, how they change the polarisation of sunlight, in a similar way to how polarised sunglasses work. This wouldn't have been possible without the development of the new ""AshCam"" which is the first camera of its kind in the world.During a volcanic eruption heavy ash fall can lead to building collapse, potentially injuring or killing those inside. Plus exposure to ash can cause irritation to the nose, throat and eyes, as well as aggravating pre-existing health conditions such as asthma.But ash is also a major danger to other critical human infrastructure, including electrical, water and transportation networks, especially air travel -- such as the 2010 Icelandic eruption.Chair in Volcanology at The University of Manchester, Professor Mike Burton, said: ""Volcanic ash is a primary product of an explosive volcanic eruption which generally poses a threat to human health and infrastructure.""The Icelandic eruption in 2010 highlighted to the whole world that ash-rich volcanic eruptions can have a major impact on the global economy through air space closures designed to minimise the risk of jet engine failure due to ash clogging.""The team visited Santiaguito Lava Dome, part of Santa Maria volcano in Guatemala, and measured a series of explosions with the camera. Santiaguito produces a number of these explosions every day, approximately one every two hours, making it the ideal place to test out new equipment, techniques and research models.Professor Burton added: ""Models of ash dispersion are key to forecasting the concentrations of ash during an eruption, which ultimately determine which air space is closed. Our research helps to measure the dynamics of ash fallout during an eruption. This gives us new insights into volcanic ash dynamics, providing an important step towards improved models of ash dispersion.""Previous studies have used ground-based UV cameras for the observation of volcanic ash or measured black carbon particles in ship emissions. However, these methods were unable to distinguish ash from other particles in the plume, making it difficult to measure the flow of the ash.This is where ""AshCam"" is different. It is made up of commercial cameras already available on the market that have been adapted with special filters that can identify ash more easily using sunlight.PhD Researcher, Benjamin Esse, from Manchester's School of Earth and Environmental Sciences, explains: ""AshCam is small, lightweight and relatively cheap, making it ideal for use in volcanic environments where larger, more expensive equipment is unable to be deployed.""This is an exciting development as it offers volcanologists a tool that allows them to easily measure the dynamics of ash plumes using only sunlight. The results from these measurements can be used to inform ash dispersal models, potentially improving their accuracy and effectiveness in mitigating the risks posed by ash plumes.""Story Source:Materials provided by University of Manchester. ",0.142276,0.195043,0.216619,0.218425,0.165211,0.197018,0.209566,0,0
522,Unmasking corrosion to design better protective thin films for metals Researchers from three universities team up to analyze oxide films at atomic level,"Corrosion is an age-old problem that is estimated to cost about $1 trillion a year, or about 5 percent of the U.S. gross domestic product. Corrosion of metals can be particularly bad, but fortunately they are normally protected from catastrophic damage by naturally forming, super-thin oxide films.Traditionally, these protective films have been viewed as simple oxides of well-anticipated compounds, but new work from scientists at Northwestern University, the University of Virginia and the University of Wisconsin-Madison reveals dramatic new insights into these oxide films.Using state-of-the-art experimental techniques and theoretical modeling, the scientists were able to analyze oxide films at the atomic level, deciphering how the atoms are arranged in the oxides.Their findings? The protective films develop new structures and compositions that depend on how fast the oxide film grows. The study's authors say their findings could provide clues about how to make the protective films better -- perhaps much, much better.It's a breakthrough that could have implications for everything from nuts and bolts to high-technology batteries and turbine engines.""This changes many things about how we understand these oxide films and opens the door to drastically new ways of protecting metals,"" said Laurence Marks, professor of materials science and engineering at Northwestern's McCormick School of Engineering, who led the study. ""We now know that there are ways to predict the chemical composition of these films, something we can exploit so the protective films last much longer.""The study was published today (Oct. 3) by the journal Physical Review Letters.""We now have more routes than ever to control and tune oxides to protect materials,"" said John Scully, the Charles Henderson Chaired Professor and Chair of the Department of Materials Science and Engineering at the University of Virginia and one of the study's authors.""This provides key information about how to design new materials that will corrode far less,"" said Northwestern's Peter Voorhees, another of the study's authors. Voorhees is the Frank C. Engelhart Professor of Materials Science and Engineering at Northwestern Engineering.The team studied, in detail, the oxides that form on alloys composed of nickel and chromium, which are widely used in a variety of products, such as the heating elements of a household toaster or in aircraft engines.These oxides are also used for applications when there is water present, such as in dental implants. It has long been known that these oxides work when hot and resist corrosion in the mouth because of the formation of an oxide of chromium. It was assumed that the nickel formed a separate oxide, or in some cases dissolved away in the body. The team found something unexpected -- that the oxide was not just chromium and oxygen, but instead contained a very large number of nickel atoms.Why? Because the nickel atoms do not have time to escape from the oxide, becoming captured inside it. The fraction that is captured depends upon how fast the oxide grows. If it grows very slowly, the nickel atoms can escape. If it grows very fast, they cannot.This occurs both when the metals are reacting with oxygen from the air at high temperatures, as well as when they are reacting with water in ships or in dental implants. The atoms that are captured in the oxide change many of the properties, the study's authors say.The findings mean it is possible to deliberately trap atoms into these oxides in new ways, and thus change how they behave.""We are close to the limits of what we can do with aircraft engines, as one example,"" said John Perepezko, the IBM-Bascom Professor of Materials Science and Engineering at the University of Wisconsin-Madison and another of the study's authors. ""This new vision of protective oxide formation leads to many new ways one could build better engines.""Story Source:Materials provided by Northwestern University. ",0.0814248,0.168888,0.153188,0.165507,0.118773,0.138684,0.164811,0,0
523,Novel flying robot mimics rapid insect flight,"A novel insect-inspired flying robot, developed by TU Delft researchers from the Micro Air Vehicle Laboratory (MAVLab), is presented in Science (14 September 2018). Experiments with this first autonomous, free-flying and agile flapping-wing robot -- carried out in collaboration with Wageningen University & Research -- improved our understanding of how fruit flies control aggressive escape manoeuvres. Apart from its further potential in insect flight research, the robot's exceptional flight qualities open up new drone applications.Flying animals both power and control flight by flapping their wings. This enables small natural flyers such as insects to hover close to a flower, but also to rapidly escape danger, which everyone has witnessed when trying to swat a fly. Animal flight has always drawn the attention of biologists, who not only study their complex wing motion patterns and aerodynamics, but also their sensory and neuro-motor systems during such agile manoeuvres. Recently, flying animals have also become a source of inspiration for robotics researchers, who try to develop lightweight flying robots that are agile, power-efficient and even scalable to insect sizes.Novel highly agile flying robot TU Delft researchers from the MAVLab have developed a novel insect-inspired flying robot; so far unmatched in its performance, and yet with a simple and easy-to-produce design. As in flying insects, the robot's flapping wings, beating 17 times per second, not only generate the lift force needed to stay airborne but also control the flight via minor adjustments in the wing motion. Inspired by fruit flies, the robot's control mechanisms have proved to be highly effective, allowing it not only to hover on the spot and fly in any direction but also be very agile.'The robot has a top speed of 25 km/h and can even perform aggressive manoeuvres, such as 360-degree flips, resembling loops and barrel rolls', says Mat?j Karesek, the first author of the study and main designer of the robot. 'Moreover, the 33 cm wingspan and 29 gram robot has, for its size, excellent power efficiency, allowing 5 minutes of hovering flight or more than a 1 km flight range on a fully charged battery.'Research on fruit fly escape manoeuvresApart from being a novel, autonomous micro-drone, the robot's flight performances, combined with its programmability also make it well suited for research into insect flight. To this end, TU Delft has collaborated with Wageningen University. 'When I first saw the robot flying, I was amazed at how closely its flight resembled that of insects, especially when manoeuvring. I immediately thought we could actually employ it to research insect flight control and dynamics', says Prof. Florian Muijres from the Experimental Zoology group of Wageningen University & Research. Due to Prof. Muijres' previous work on fruit flies, the team decided to program the robot to mimic the hypothesized control actions of these insects during high-agility escape manoeuvres, such as those used when we try to swat them.The manoeuvres performed by the robot closely resembled those observed in fruit flies. The robot was even able to demonstrate how fruit flies control the turn angle to maximize their escape performance. 'In contrast to animal experiments, we were in full control of what was happening in the robot's ""brain."" This allowed us to identify and describe a new passive aerodynamic mechanism that assists the flies, but possibly also other flying animals, in steering their direction throughout these rapid banked turns', adds Karesek.Potential for future applicationsThe MAVLab has been developing insect-inspired flying robots for over 10 years within the DelFly project. The MAVLab scientific leader, Prof. Guido de Croon, says: 'Insect-inspired drones have a high potential for novel applications, as they are light-weight, safe around humans and are able to fly more efficiently than more traditional drone designs, especially at smaller scales. However, until now, these flying robots had not realized this potential since they were either not agile enough -- such as our DelFly II -- or they required an overly complex manufacturing process.' The robot in this study, named the DelFly Nimble, builds on established manufacturing methods, uses off-the-shelf components, and its flight endurance is long enough to be of interest for real-world applications.The DelFly Nimble will be further developed within the TTW project, 'As nimble as a bee', which is a collaboration between TU Delft and Wageningen University, funded by the Dutch science foundation NWO.Video: https://www.youtube.com/watch?time_continue=6&v=CEhu-FePBC0Story Source:Materials provided by Delft University of Technology. ",0.179527,0.122863,0.13756,0.222488,0.133219,0.207831,0.203383,0,0
524,NASA-funded rocket to view sun with X-ray vision,"Without special instrumentation, the Sun looks calm and inert. But beneath that placid faeade are countless miniature explosions called nanoflares.These small but intense eruptions are born when magnetic field lines in the Sun's atmosphere tangle up and stretch until they break like a rubber band. The energy they release accelerates particles to near lightspeed and according to some scientists, heats the solar atmosphere to its searing million-degree Fahrenheit temperature.All of this happens in colors of light so extreme that the human eye can't see them. Nanoflares aren't visible -- at least not to the naked eye.Finding the traces of nanoflares requires X-ray vision, and scientists have been hard at work developing the best tools for the job. The latest advance in this project is represented by NASA's Focusing Optics X-ray Solar Imager, or FOXSI mission, soon to take its third flight from the White Sands Missile Range in White Sands, New Mexico, no earlier than Sept. 7.FOXSI is a sounding rocket mission. Derived from the nautical term ""to sound,"" meaning to measure, sounding rockets make brief 15-minute journeys above Earth's atmosphere for a peek at space before falling back to the ground. Smaller, cheaper and faster to develop than large-scale satellite missions, sounding rockets offer a way for scientists to test their latest ideas and instruments -- and achieve rapid results.FOXSI will travel 190 miles up, above the shield of Earth's atmosphere, to stare directly at the Sun and search for nanoflares using its X-ray vision.""FOXSI is the first instrument built specially to image high-energy X-rays from the Sun by directly focusing them,"" said Lindsay Glesener, space physicist at the University of Minnesota in Minneapolis and principal investigator for the mission. ""Other instruments have done this for other astronomical objects, but FOXSI is so far the only instrument to optimize especially for the Sun.""The Sun tells its story in layers of light, each of which reveals what's happening at different temperatures. For example, the sunlight that our eyes can see is primarily from the Sun's photosphere, which is approximately 10,000 degrees Fahrenheit. But there's much more going on outside the bounds of human vision. X-ray light, in particular, reveals processes that heat plasma to millions of degrees Fahrenheit, like the most violent explosions at the cores of nanoflares.But high-quality views of X-rays from the Sun don't come easy. Unlike visible light, X-rays are hard to focus; they are largely unaffected by the lenses and mirrors used in conventional telescopes. Previous X-ray missions had to make do without focused light.""In the past we generally used cleverly selected masks to block out some part of the incoming X-rays,"" said Sem Krucker, space physicist at the University of California, Berkeley, and principal investigator for FOXSI's two previous flights. ""This does not result in very high-quality images, but it nevertheless gave us crucial information on the most energetic part of solar flares.""To focus the X-rays, the FOXSI team used extremely hard, smooth surfaces tilted to a small angle (less than half a degree) that would gently corral incoming X-ray light to a point of focus.""Thanks to these telescopes we can now make focused X-ray images of our Sun"" said Krucker. ""These images have a much-improved image quality at a much higher sensitivity.""This will be FOXSI's third flight -- its first was in 2012, during which it successfully viewed a small solar flare in progress, and its second in 2014, when it detected the best evidence at the time of X-ray emission from nanoflares. The third mission follows up on this discovery, but this time it includes a new telescope designed for imaging lower-energy, so-called soft X-rays as well.""Including the soft X-ray telescope gives us more precise temperatures,"" said Glesener, allowing the team to spot nanoflare signatures that would be missed with the hard X-ray telescopes alone. In addition, several other performance improvements have been made to produce more accurate, higher-resolution images.FOXSI's third flight will also be the first led by Glesener, who was a graduate student, and then the project manager, for the previous two flights led by Krucker.""This kind of training and project inheritance is common in sounding rocket programs,"" said Glesener. ""They are designed to grow and mature scientific leaders as well as hardware!""FOXSI is a collaboration between NASA and the Japan Aerospace Exploration Agency, and has co-investigators from the University of Minnesota; University of California at Berkeley; NASA's Goddard and Marshall space flight centers in Greenbelt, Maryland, and Huntsville, Alabama, respectively; the University of Tokyo; Nagoya University; the National Astronomical Observatory of Japan; and Tokyo University of Science. FOXSI is supported through NASA's Sounding Rocket Program at the agency's Wallops Flight Facility in Virginia. NASA's Heliophysics Division manages the sounding rocket program.Story Source:Materials provided by NASA/Goddard Space Flight Center. Original written by Miles Hatfield. ",0.110661,0.0902927,0.112554,0.135017,0.102616,0.121869,0.122316,0,0
525,The low impact of the high-speed train on international tourism,"At the height of the tourist season, a study by the Applied Economics & Management, Research Group, based at the University of Seville, is a pioneering analysis of the relationship between the high-speed train and tourism in Europe, in contrast with tourism's relationship with the plane.For the Economics Professor, Jose Ignacio Castillo Manzano, there is undeniably a complementary relationship between air travel and the high-speed train, which would justify the development of joint strategies, starting with rail connections between airports and railway stations with high-speed connections, and going as far as joint plane and high-speed train tickets, as are already sold by one airline. However, and although both means of transport favour tourism, European experience indicates that their influence is very different.The plane has a close and direct relationship with both national and international tourism. Additionally, not only is it related to a higher volume of visitors, but there is also a relationship with longer stays, especially for international tourism.In contrast, according to Castillo Manzano, ""the relationship the high-speed train has is mostly with national tourism, and it lacks any significant influence on international tourism."" For the professor, in the case of Spain, ""a larger presence of foreign tourists on the AVE in Spain would act as a mere optical illusion on the supposed relevance of this means of transport on international tourism as, really, the great majority of these tourists have come via the many and cheap flight connections that our airports offer. If the AVE network did not exist, these tourists would instead travel around the country using the greater number of and more frequent domestic flight connections that would exist if the AVE wasn't there."" According to this study, there is not even any empirical evidence that, thanks to high-speed train connections, foreign tourists extend their stays in the country.Of course, the relationship of the high-speed train with national tourism is much closer and more positive than the plane's. But, for Castillo Manzano, the share of earnings that are generated by our high-speed train in the fomentation of domestic tourism remains to be studied. Giving as an example the first AVE line between Madrid and Seville, he explains that ""although there is no doubt that this was very important in Seville being able to attract many more tourists from the centre of the peninsula, especially in the nineties, while the planned high-speed train network has been developed, incorporating new cities, it is very probable that the more significant part of the money earned goes to Madrid. Doubtlessly, what has happened is a significant improvement in access facilities from our country's main cities to the capital. Thanks to the AVE, Madrid is now the easiest place to organise a national conference, a work meeting or for ordinary Spanish people to have a weekend break to, for example, see a musical or a new exhibition at the Prado. However, tourists that come from Madrid do not only head for Seville, rather they visit different cities on the AVE.""On the other hand, the study also concludes that those countries with a lower per capita income and lower prices in the tourist sector are those that attract more foreign tourists, whereas the more developed a country is, the more national tourism it generates. So, for the professor, encouraging the economic development of a country is also a magnificent policy for promoting domestic tourism.In contrast, if we are speaking about attracting foreign tourism, for Castillo good airport management and infrastructure is fundamental. ""There are few more effective tourism policies than the setting of optimal airport taxes that favour the opening of new routes and increased flight frequency and combat the highly seasonal nature of the tourism industry."" In this way, ""the good working of the pairing of transport and tourism is the best guarantee of the future of the sector, hence the need to contribute to finding long-term solutions to problems related to transport that threaten, as with the taxi sector, systemic delays at some airports, or labour problems as experienced by Ryanair.""Story Source:Materials provided by University of Seville. ",0.161311,0.136671,0.210193,0.204671,0.16129,0.211901,0.180683,0,0
526,"In the race of life, the tortoise beats the hare every time Research shows that, when speed is averaged throughout a lifetime, the fastest animals and machines are actually the slowest","Over the long-run, the race will indeed go to the slower, steadier animal.""The fable of 'The Tortoise and the Hare' is a metaphor about life, not a story about a race,"" said Adrian Bejan, the J.A. Jones Professor of Mechanical Engineering at Duke University. ""We see in animal life two starkly different lifestyles -- one with nearly steady feeding and daily sleep and another with short bursts of intermittent feeding interspersed with day-long siestas. Both of these patterns are the rhythms of living that Aesop taught.""In the iconic parable, Aesop tells of a race between a fast but often-distracted hare and a slow but relentless tortoise. Readers are supposed to be surprised when the tortoise manages to defeat the hare, coining the phrase ""slow and steady wins the race."" But according to Bejan's new analysis, they shouldn't be.Published on August 27 in the journal Scientific Reports, Bejan analyzes the reported speeds of animals based on land, air and water. The results show that some of the world's fastest animals are actually some of the slowest when their movements are averaged throughout their lifetimes.Bejan then goes on to demonstrate that this counterintuitive result is also true of the modern aviation industry. With data from hundreds of historical airplane models in hand, Bejan shows that the general trend in their design is for size and speed to increase hand-in-hand.Except, that is, for the modern jet fighter.Just like in the animal world, the jet fighter may be faster than other airplanes in short bursts, but it spends much of its time grounded. When averaged over its service lifetime, jet fighters are surprisingly slow when compared to models designed for transport or reconnaissance.The study emerged as a consequence of a previous paper that used Bejan's constructal theory to show that all animals' speeds tend to rise along with their body mass and adhere to a similar ratio. For example, the stride frequency of running vertebrates bears the same relationship to the animals' mass as does the rate at which fish swim. Similarly, the velocity of runners conforms to the same principles as the speed of birds in flight. These models can be used to predict future evolutionary trends and design directions for aircraft and other vehicles.""When I would give speeches on this topic, somebody would always bring up outliers to this principle such as the cheetah as counterexamples,"" said Bejan. ""But this study shows that these 'outliers' are to be expected and, when looked at over their lifetimes, are not so different from their lumbering cousins after all.""Story Source:Materials provided by Duke University. ",0.132076,0.0957998,0.138216,0.175482,0.106845,0.191753,0.149607,0,0
527,Researchers turn tracking codes into 'clouds' to authenticate genuine 3-D printed parts,"The worldwide market for 3D-printed parts is a $5 billion business with a global supply chain involving the internet, email, and the cloud -- creating a number of opportunities for counterfeiting and intellectual property theft. Flawed parts printed from stolen design files could produce dire results: experts predict that by 2021, 75 percent of new commercial and military aircraft will fly with 3D-printed engine, airframe, and other components, and the use of AM in the production of medical implants will grow by 20 percent per year over the next decade.A team at NYU Tandon School of Engineering has found a way to prove the provenance of a part by employing QR (Quick Response) codes in an innovative way for unique device identification. In the latest issue of Advanced Engineering Materials, the researchers describe a method for converting QR codes, bar codes, and other passive tags into three-dimensional features hidden in such a way that they neither compromise the part's integrity nor announce themselves to counterfeiters who have the means to reverse engineer the part.Noted materials researcher Nikhil Gupta, an associate professor of mechanical engineering at NYU Tandon; Fei Chen, a doctoral student under Gupta; and joint NYU Tandon and NYU Abu Dhabi researchers Nektarios Tsoutsos, Michail Maniatakos and Khaled Shahin, detail how they exploited the layer-by-layer AM printing process to turn QR codes into a game of 3D chess. Gupta's team developed a scheme that ""explodes"" a QR code within a computer-assisted design (CAD) file so that it presents several false faces -- dummy QR tags -- to a micro-CT scanner or other scanning device. Only a trusted printer or end user would know the correct head-on orientation for the scanner to capture the legitimate QR code image.""By converting a relatively simple two-dimensional tag into a complex 3D feature comprising hundreds of tiny elements dispersed within the printed component, we are able to create many 'false faces,' which lets us hide the correct QR code from anyone who doesn't know where to look,"" Gupta said.The team tested different configurations -- from distributing a code across just three layers of the object, to fragmenting the code into up to 500 tiny elements -- on thermoplastics, photopolymers, and metal alloys, with several printing technologies commonly employed in the industry.Chen, the study's lead author, said that after embedding QR codes in such simple objects as cubes, bars, and spheres, the team stress-tested the parts, finding that the embedded features had negligible impact on structural integrity.""To create typical QR code contrasts that are readable to a scanner you have to embed the equivalent of empty spaces,"" she explained. ""But by dispersing these tiny flaws over many layers we were able to keep the part's strength well within acceptable limits.""Tsoutsos and Maniatakos explored threat vectors to determine which AM sectors are best served by this security technology, a step that Gupta said was crucial in the research.""You need to be cost efficient and match the solution to the threat level,"" he explained. ""Our innovation is particularly useful for sophisticated, high-risk sectors such as biomedical and aerospace, in which the quality of even the smallest part is critical.""A 2016 article by Gupta and a team of researchers that included Maniatakos and Tsoutsos in JOM, The Journal of the Minerals, Metals & Materials Society explored how flaws caused by printing orientation and insertion of fine defects could be foci for AM cyber-attacks. The paper was the most-read engineering research that year among Springer's over 245 engineering journals. In a paper last year in Materials and Design, Gupta detailed methods of inserting undetectable flaws within CAD files so that only a trusted printer could correctly produce the parts.The Office of Naval Research helped fund the work.Story Source:Materials provided by NYU Tandon School of Engineering. ",0.0680288,0.0862682,0.142753,0.220346,0.127496,0.111898,0.158551,0,0
528,Spinning heat shield for future spacecraft,"A University of Manchester PhD student has developed a prototype flexible heat shield for spacecraft that could reduce the cost of space travel and even aid future space missions to Mars.Heat shields are essentially used as the brakes to stop spacecraft burning up and crashing on entry and re-entry into a planet's atmosphere. This design is the first in the world to utilise centrifugal forces that stiffen lightweight materials to prevent burnup.Current spacecraft heat shield methods include huge inflatables and mechanically deployed structures that are often heavy and complicated to use.Rui Wu, from Manchester's School of Mechanical, Aerospace and Civil Engineering, says as well as being lightweight in design is prototype is also ""self-regulating."" This means there is no need for any additional machinery, reducing the weight of spacecraft even further and allowing for low-cost scientific research and recovery of rocket parts.He says: ""Spacecraft for future missions must be larger and heavier than ever before, meaning that heat shields will become increasingly too large to manage.""To address this demand Wu and his team have developed a flexible heat shield that is shaped like a skirt and spins like a sycamore seed. Planets with atmospheres, such as Earth and Mars, allow spacecraft to utilise aerodynamic drag to slow down and the prototype's design uses this to enable atmospheric entry.""This is similar to high board diving, where the drag from water decelerates your body before you reach the bottom of the swimming pool,"" Wu explains.The fast entry into Earth's atmosphere generates so much heat -- over 10,000 ?C -- that the air around the spacecraft can burn into plasma. For safe atmospheric entry, spacecraft need a front end, or shield, that tolerates high heat as well as an aerodynamic shape that generates drag.However, Unlike Earth, the Martian atmosphere is very thin. ""If Earth re-entry is like diving into thick honey, Mars entry would be like diving into water,"" Wu says.To carry heavy equipment and astronauts, a high drag area is needed. When entering Earth's or Mars' atmospheres, spacecraft require highly designed shields to avoid burnup, generate drag, and support heavy loads.Wu's design potentially solves both issues.The prototype is made of a flexible material that allows for easy storage on board spacecraft. This material, while foldable, is strong and has a high temperature tolerance.The shield is also stitched along a special pattern that allows it to spin up during flight, inducing centrifugal force.Wu sees his design helping with space-based scientific research and rescue missions in the future. He adds: ""More and more research is being conducted in space, but this is usually very expensive and the equipment has to share a ride with other vehicles.""Since this prototype is lightweight and flexible enough for use on smaller satellites, research could be made easier and cheaper. The heat shield would also help save cost in recovery missions, as its high induced drag reduces the amount of fuel burned upon re-entry.""Story Source:Materials provided by University of Manchester. ",0.201809,0.114888,0.142302,0.145413,0.141475,0.198437,0.143041,0,0
529,Mini antimatter accelerator could rival the likes of the Large Hadron Collider,"Researchers have found a way to accelerate antimatter in a 1000x smaller space than current accelerators, boosting the science of exotic particles.The new method could be used to probe more mysteries of physics, like the properties of the Higgs boson and the nature of dark matter and dark energy, and provide more sensitive testing of aircraft and computer chips.The method has been modelled using the properties of existing lasers, with experiments planned soon. If proven, the technology could allow many more labs around the world to conduct antimatter acceleration experiments.Particle accelerators in facilities such as the Large Hadron Collider (LHC) in CERN and the Linac Coherent Light Source (LCLS) at Stanford University in the United States, speed up elementary particles like protons and electrons.These accelerated particles can be smashed together, as in the LHC, to produce particles that are more elementary, like the Higgs boson, which gives all other particles mass.They can also be used to generate x-ray laser light, such as in the LCLS, which is used to image extremely fast and small process, like photosynthesis.However, to get to these high speeds, the accelerators need to use equipment that is at least two kilometres long. Previously, researchers at Imperial College London had invented a system that could accelerate electrons using equipment only meters long.Now a researcher at Imperial has invented a method of accelerating the antimatter version of electrons -- called positrons -- in a system that would be just centimetres long.The accelerator would require a type of laser system that currently covers around 25 square metres, but that is already present in many physics labs. Dr Aakash Sahai, from the Department of Physics at Imperial reported his method today in the Physical Review Journal for Accelerators and Beams.He said: ""With this new accelerator method, we could drastically reduce the size and the cost of antimatter acceleration. What is now only possible by using large physics facilities at tens of million-dollar costs could soon be possible in ordinary physics labs.""""The technologies used in facilities like the Large Hadron Collider or the Linac Coherent Light Source have not undergone significant advances since their invention in the 1950s. They are expensive to run, and it may be that we will soon have all we can get out of them.""A new generation of compact, energetic and cheap accelerators of elusive particles would allow us to probe new physics -- and allow many more labs worldwide to join the effort.""While the method is currently undergoing experimental validation, Dr Sahai is confident it will be possible to produce a working prototype within a couple of years, based on the Department's previous experience creating electron beams using a similar method.The method uses lasers and plasma -- a gas of charged particles -- to produce, concentrate positrons and accelerate them to create a beam. This centimetre-scale accelerator could use existing lasers to accelerate positron beams with tens of millions of particles to the same energy as reached over two kilometres at the Stanford accelerator.Colliding electron and positron beams could have implications in fundamental physics. For example, they could create a higher rate of Higgs bosons than the LHC can, allowing physicists to better study its properties. They could also be used to look for new particles thought to exist in a theory called 'supersymmetry', which would fill in some gaps in the Standard Model of particle physics.The positron beams would also have practical applications. Currently, when checking for faults and fracture risks in materials such as aircraft bodies, engine blades and computer chips, x-rays or electron beams are used. Positrons interact in a different way with these materials than x-rays and electrons, providing another dimension to the quality control process.Dr Sahai added: ""It is particularly gratifying to do this work at Imperial, where our lab's namesake -- Professor Patrick Blackett -- won a Nobel Prize for his invention of methods to track exotic particles like antimatter. Professor Abdus Salam, another Imperial academic, also won a Nobel Prize for the validation of his theory of weak force made possible only using a pre-LHC positron-electron collider machine at CERN. It's wonderful to attempt to carry on this legacy.""Story Source:Materials provided by Imperial College London. Original written by Hayley Dunning. ",0.0787015,0.102501,0.114412,0.147123,0.11714,0.137361,0.122856,0,0
530,Nanofiber carpet could lead to new sticky or insulating surfaces,"Inspired by the extraordinary characteristics of polar bear fur, lotus leaves and gecko feet, engineering researchers have developed a new way to make arrays of nanofibers that could bring us coatings that are sticky, repellent, insulating or light emitting, among other possibilities.""This is so removed from anything I've ever seen that I would have thought it was impossible,"" said Joerg Lahann, a professor of chemical engineering at the University of Michigan and senior author of the study in the journal Science.Researchers at U-M and the University of Wisconsin made the somewhat serendipitous discovery, which revealed a new and powerful method for making arrays of fibers that are hundreds of times thinner than a human hair.Polar bear hairs are structured to let light in while keeping heat from escaping. Water-repelling lotus leaves are coated with arrays of microscopic waxy tubules. And the nanoscale hairs on the bottoms of gravity-defying gecko feet get so close to other surfaces that atomic forces of attraction come into play. Researchers looking to mimic these superpowers and more have needed a way to create the minuscule arrays that do the work.""Fundamentally, this is a completely different way of making nanofiber arrays,"" Lahann said.The researchers have shown that their nanofibers repelled water like lotus leaves. They grew straight and curved fibers and tested how they stuck together like Velcro -- finding that clockwise and counterclockwise twisted fibers knitted together more tightly than two arrays of straight fibers.They also experimented with optical properties, making a material that glowed. They believe it will be possible to make a structure that works like polar bear fur, with individual fibers structured to channel light.But molecular carpets weren't the original plan. Lahann's group was working with that of Nicholas Abbott, at the time a professor of chemical engineering at UW-Madison, to put thin films of chain-like molecules, called polymers, on top of liquid crystals. Liquid crystals are best known for their use in displays such as televisions and computer screens. They were trying to make sensors that could detect single molecules.Lahann brought the expertise in producing thin films while Abbott led the design and production of the liquid crystals. In typical experiments, Lahann's group evaporates single links in the chain and coaxes them to condense onto surfaces. But the thin polymer films sometimes didn't materialize as expected.""The discovery reinforces my view that the best advances in science and engineering occur when things don't go as planned,"" Abbott said. ""You just have to be alert and view failed experiments as opportunities.""Instead of coating the top of the liquid crystal, the links slipped into the fluid and connected with each other on the glass slide. The liquid crystal then guided the shapes of the nanofibers growing up from the bottom, creating nanoscale carpets.""A liquid crystal is a relatively disordered fluid, yet it can template the formation of nanofibers with remarkably well-defined lengths and diameters,"" Abbott said.And they didn't only make straight strands. Depending on the liquid crystal, they could generate curved fibers, like microscopic bananas or staircases.""We have a lot of control over the chemistry, the type of fibers, the architecture of the fibers and how we deposit them,"" Lahann said. ""This really adds a lot of complexity to the way we can engineer surfaces now; not just with thin two-dimensional films but in three dimensions.""Story Source:Materials provided by University of Michigan. ",0.0859573,0.143933,0.14675,0.167635,0.142668,0.120737,0.154697,0,0
531,What happens when materials take tiny hits High-speed camera shows incoming particles cause damage by briefly melting surfaces as they strike,"When tiny particles strike a metal surface at high speed -- for example, as coatings being sprayed or as micrometeorites pummeling a space station -- the moment of impact happens so fast that the details of process haven't been clearly understood, until now.A team of researchers at MIT has just accomplished the first detailed high-speed imaging and analysis of the microparticle impact process, and used that data to predict when the particles will bounce away, stick, or knock material off the surface and weaken it. The new findings are described in a paper appearing today in the journal Nature Communications.Mostafa Hassani-Gangaraj, an MIT postdoc and the paper's lead author, explains that high-speed microparticle impacts are used for many purposes in industry, for example, for applying coatings, cleaning surfaces, and cutting materials. They're applied in a kind of superpowered version of sandblasting that propels the particles at supersonic speeds. Such blasting with microparticles can also be used to strengthen metallic surfaces. But until now these processes have been controlled without a solid understanding of the underlying physics of the process.""There are many different phenomena that can take place"" at the moment of impact, Hassani-Gangaraj says, but now for the first time the researchers have found that a brief period of melting upon impact plays a crucial role in eroding the surface when the particles are moving at speeds above a certain threshold.That's important information because the rule of thumb in industrial applications is that higher velocities will always lead to better results. The new findings show that this is not always the case, and ""we should be aware that there is this region at the high end"" of the range of impact velocities, where the effectiveness of the coating (or strengthening) declines instead of improving, Hassani-Gangaraj says. ""To avoid that, we need to be able to predict"" the speed at which the effects change.The results may also shed light on situations where the impacts are uncontrolled, such as when wind-borne particles hit the blades of wind turbines, when microparticles strike spacecraft and satellites, or when bits of rock and grit carried along in a flow of oil or gas erode the walls of pipelines. ""We want to understand the mechanisms and exact conditions when these erosion processes can happen,"" Hassani-Gangaraj says.The challenge of measuring the details of these impacts was twofold. First, the impact events take place extremely quickly, with particles travelling at upward of one kilometer per second (three or four times faster than passenger jet airplanes). And second, the particles themselves are so tiny, about a tenth of the thickness of a hair, that observing them requires very high magnification as well. The team used a microparticle impact testbed developed at MIT, which can record impact videos with frame rates of up to 100 million frames per second, to perform a series of experiments that have now clearly delineated the conditions that determine whether a particle will bounce off a surface, stick to it, or erode the surface by melting.For their experiments, the team used tin particles of about 10 micrometers (hundred thousandths of a meter) in diameter, accelerated to speeds ranging up to 1 kilometer per second and hitting a tin surface. The particles were accelerated using a laser beam that instantly evaporates a substrate surface and ejects the particles in the process. A second laser beam was used to illuminate the flying particles as they struck the surface.Previous studies had relied on post-mortem analysis -- studying the surface after the impact has taken place. But that did not allow for an understanding of the complex dynamics of the process. It was only the high-speed imaging that revealed that melting of both the particle and the surface took place at the moment of impact, in the high-speed cases.The team used the data from these experiments to develop a general model to predict the response of particles of a given size travelling at a given speed, says David Veysset, a staff researcher at MIT and co-author of the paper. So far, he says, they have used pure metals, but the team plans further tests using alloys and other materials. They also intend to test impacts at a variety of angles other than the straight-down impacts tested so far. ""We can extend this to every situation where erosion is important,"" he says. The aim is to develop ""one function that can tell us whether erosion will happen or not.""That could help engineers ""to design materials for erosion protection, whether it's in space or on the ground, wherever they want to resist erosion,"" Veysset says.The team included senior author Christopher Schuh, professor and head of the Department of Materials Science and Engineering, and Keith Nelson, professor of chemistry. The work was supported by the U.S. Department of Energy, the U.S. Army Research Office, and the Office of Naval research.Story Source:Materials provided by Massachusetts Institute of Technology. ",0.0735993,0.0826168,0.13143,0.120041,0.0807796,0.129126,0.113856,0,0
532,An important step towards completely secure quantum communication networks,"The quest for a secure information network is on. Researchers have recently succeeded in boosting the storage time of quantum information, using a small glass container filled with room temperature atoms, taking an important step towards a secure quantum encoded distribution network.Sending information in optical fibres over long distances in the classical regimeSending information over long distances can be done by encoding messages into light pulses, and sending them through optical fibres. But there is loss in fibres, so amplification is necessary along the way. Repeaters amplify the light pulses at specific intervals along the line and Voila! -- transatlantic communication is possible. But there is a problem: It is not completely secure. The information can be picked up, and even if it is encoded, codes can be broken.Distributing information in the quantum regimeWhat happens when sending quantum information, is slightly different. The information itself is not actually travelling, but is teleported via entanglement distributed in the network. The sender has one half of the entanglement, and the receiver has the other half. Entanglement is much easier to create over short distances, so the line between sender and receiver is segmented and entanglement is created between each beginning and end of the segments. If each segment is capable of storing the entanglement, the line operator can wait until entanglement is created in all segments and then perform entanglement swaps at the joints to extend the entanglement to the full distance between sender and receiver. So storage is critical -- and this is why the improvement of storage time now done by the researchers is so important. Only when entanglement is in place in the entire length of the line, the actual communication can take place. Along the way, it is totally inaccessible for anyone else, as the delicate quantum information self-destructs immediately if you try to eavesdrop or manipulate it in any way.We need many quantum repeatersThe storage time comes into the picture, as it actually takes some time for the information to travel in the fibres. The delicate quantum entanglement has to be stored, waiting its turn to travel through the optical fibre. It makes very good sense to aim for a system that operates at room temperature, because of the scale of such networks. If quantum repeaters have to be deployed for app. every 10 km of communication line, the benefits of a simple setup, working at room temperature, are tremendous. The researchers at the Niels Bohr Institute have managed to boost this crucial lifespan of the quantum state at room temperature to about a quarter of a millisecond, and in this period of time, the light can travel roughly 50 km in the fibre. ""So, 50 km -- it is still not very far, if you want to send regional quantum information, but it is way longer than what has previously been achieved with atoms at room temperature,"" says Karsten Dideriksen, PhD student on the project.The actual technologyThe technique itself consists of a small glass container, filled with Cesium atoms, in which the researchers are able to load, store and retrieve single photons (particles of light) from, the quantum states necessary for the repeater. This technique improves the life span of the quantum states at room temperature a hundred times. Simplicity is key, as one has to imagine this technology, once developed to its full potential, spread out across the globe as quantum repeaters in our information networks.The immediate perspective is, as mentioned, storage for use in secure quantum information networks, but other options like generation of on-demand single photons for quantum computing are on the table.Story Source:Materials provided by Faculty of Science - University of Copenhagen. ",0.105425,0.133545,0.148832,0.256827,0.177787,0.157696,0.182851,0,0
533,New device widens light beams by 400 times Broadening possibilities in science and technology,"By using light waves instead of electric current to transmit data, photonic chips -- circuits for light -- have advanced fundamental research in many areas from timekeeping to telecommunications. But for many applications, the narrow beams of light that traverse these circuits must be substantially widened in order to connect with larger, off-chip systems. Wider light beams could boost the speed and sensitivity of medical imaging and diagnostic procedures, security systems that detect trace amounts of toxic or volatile chemicals and devices that depend on the analysis of large groupings of atoms.Scientists at the National Institute of Standards and Technology (NIST) have now developed a highly efficient converter that enlarges the diameter of a light beam by 400 times. NIST physicist Vladimir Aksyuk and his colleagues, including researchers from the University of Maryland NanoCenter in College Park, Maryland, and Texas Tech University in Lubbock, described their work in the journal Light: Science and Applications.The slab maintains the narrow width of the light in the vertical (top-to- bottom) dimension, but it provides no such constraints for the lateral, or sideways, dimension. As the gap between the waveguide and the slab is gradually changed, the light in the slab forms a precisely directed beam 400 times wider than the approximately 300 nm diameter of the original beam.In the second stage of the expansion, which enlarges the vertical dimension of the light, the beam traveling through the slab encounters a diffraction grating. This optical device has periodic rulings or lines, each of which scatters light. The team designed the depth and spacing of the rulings to vary so that the light waves combine, forming a single wide beam directed at nearly a right angle to the chip's surface.Importantly, the light remains collimated, or precisely parallel, throughout the two-stage expansion process, so that it stays on target and does not spread out. The area of the collimated beam is now large enough to travel the long distance needed to probe the optical properties of large diffuse groupings of atoms.Working with a team led by John Kitching of NIST in Boulder, Colorado, the researchers have already used the two-stage converter to successfully analyze the properties of some 100 million gaseous rubidium atoms as they jumped from one energy level to another. That's an important proof-of-concept because devices based on interactions between light and atomic gasses can measure quantities such as time, length and magnetic fields and have applications in navigation, communications and medicine.""Atoms move very quickly, and if the beam monitoring them is too small, they move in and out of the beam so fast that it becomes difficult to measure them,"" said Kitching. ""With large laser beams, the atoms stay in the beam for longer and allow for more precise measurement of the atomic properties,"" he added. Such measurements could lead to improved wavelength and time standards.Story Source:Materials provided by National Institute of Standards and Technology (NIST). ",0.0868644,0.120546,0.165071,0.154096,0.144466,0.131678,0.140221,0,0
534,Atomic jet: First lens for extreme-ultraviolet light developed,"Scientists from the Max Born Institute (MBI) have developed the first refractive lens that focuses extreme ultraviolet beams. Instead of using a glass lens, which is non-transparent in the extreme-ultraviolet region, the researchers have demonstrated a lens that is formed by a jet of atoms. The results, which provide novel opportunities for the imaging of biological samples on the shortest timescales, were published in Nature.A tree trunk partly submerged in water appears to be bent. Since hundreds of years people know that this is caused by refraction, i.e. the light changes its direction when traveling from one medium (water) to another (air) at an angle. Refraction is also the underlying physical principle behind lenses which play an indispensable role in everyday life: They are a part of the human eye, they are used as glasses, contact lenses, as camera objectives and for controlling laser beams.Following the discovery of new regions of the electromagnetic spectrum such as ultraviolet (UV) and X-ray radiation, refractive lenses were developed that are specifically adapted to these spectral regions. Electromagnetic radiation in the extreme-ultraviolet (XUV) region is, however, somewhat special. It occupies the wavelength range between the UV and X-ray domains, but unlike the two latter types of radiation, it can only travel in vacuum or strongly rarefied gases. Nowadays XUV beams are widely used in semiconductor lithography as well as in fundamental research to understand and control the structure and dynamics of matter. They enable the generation of the shortest human made light pulses with attosecond durations (an attosecond is one billionth of a billionth of a second). However, in spite of the large number of XUV sources and applications, no XUV lenses have existed up to now. The reason is that XUV radiation is strongly absorbed by any solid or liquid material and simply cannot pass through conventional lenses.In order to focus XUV beams, a team of MBI researchers have taken a different approach: They replaced a glass lens with that formed by a jet of atoms of a noble gas, helium. This lens benefits from the high transmission of helium in the XUV spectral range and at the same time can be precisely controlled by changing the density of the gas in the jet. This is important in order to tune the focal length and minimize the spot sizes of the focused XUV beams.In comparison to curved mirrors that are often used to focus XUV radiation, these gaseous refractive lenses have a number of advantages: A 'new' lens is constantly generated through the flow of atoms in the jet, meaning that problems with damages are avoided. Furthermore, a gas lens results in virtually no loss of XUV radiation compared to a typical mirror. ""This is a major improvement, because the generation of XUV beams is complex and often very expensive,"" Dr. Bernd Schuette, MBI scientist and corresponding author of the publication, explains.In the work the researchers have further demonstrated that an atomic jet can act as a prism breaking the XUV radiation into its constituent spectral components. This can be compared to the observation of a rainbow, resulting from the breaking of the Sun light into its spectral colors by water droplets, except that the 'colors' of the XUV light are not visible to a human eye.The development of the gas-phase lenses and prisms in the XUV region makes it possible to transfer optical techniques that are based on refraction and that are widely used in the visible and infrared part of the electromagnetic spectrum, to the XUV domain. Gas lenses could e.g. be exploited to develop an XUV microscope or to focus XUV beams to nanometer spot sizes. This may be applied in the future, for instance, to observe structural changes of biomolecules on the shortest timescales.Story Source:Materials provided by Forschungsverbund Berlin. ",0.0723521,0.108154,0.11443,0.136949,0.121408,0.118792,0.129134,0,0
535,Hard limits on the postselectability of optical graph states,"Since the discovery of quantum mechanics, in the early 20th century, physicists have relied on optics to test its fundamentals.Even today, linear quantum optics -- the physics of how single photons behave in mirrors, waveplates, and beamsplitters -- leads the way in terms of observations of multi-party entanglement, tests of quantum nonlocality, and addressing fundamental questions about the nature of reality itself.Light notoriously avoids interaction. One beam of light does not readily affect anything about a second beam -- they simply add up, through interference, and go about their business.To date, quantum mechanical tests have relied on our ability to produce states of light in which, when all the photons are measured, a subset of the measurement patterns can be sifted out -- those in which a desired interaction has occurred. Physicists call this technique 'postselection'.New work by a team at the University of Bristol's Centre for Quantum Photonics has uncovered fundamental limits on the quantum operations which can be carried out with postselection. As physicists build bigger and bigger quantum states of light, fewer and fewer entangled states are reachable using postselection alone.The Bristol team found that as the complexity of the postselection scheme increases, the desired interacting state, which at first is easy to sift from the larger state, starts to behave indistinguishably from the noise, making postselection impossible.Each photon can carry a quantum bit, or 'qubit', of quantum information, for applications ranging from quantum computing to quantum communications. An important class of entangled states are the 'graph states', so called because their entanglement can be visualised as connections between the qubit nodes of a graph.Applying their postselectability heuristics to graph states, the researchers catalogued which graphs of up to nine qubits are postselectable, finding these to be fewer than one fifth of the total. This fraction is expected to drop severely for larger quantum systems, limiting the kinds of entanglement that can be reached with today's quantum photonic technology, and strengthening the call for new technologies to generate and entangle photons.The work is published today in the journal Quantum Science and Technology.Jeremy Adcock, lead author of the new work, said: ""Even though our rules for postselection show that most states are off limits, they also tell us how to build experiments of maximum complexity.""Dr Joshua Silverstone, who led the project, and is a Leverhulme Early Career Fellow at Bristol, added: ""People have known about problems with postselection for many years, but it's remarkable that only now can we see through to its fundamental limits.""""Postselection still has some fight left in it, but this work should really make people think about modern approaches to optical quantum tech.""Story Source:Materials provided by University of Bristol. ",0.10114,0.127448,0.14177,0.227033,0.155573,0.164725,0.175519,0,0
536,Safer and cheaper 3D medical imaging,"A new study led by The Australian National University (ANU) has discovered a promising way to significantly lower doses of X-rays that has the potential to revolutionise 3D medical imaging and make screening for early signs of disease much cheaper and safer.The research team, which involved the European Synchrotron Radiation Facility and Monash University, built upon an unconventional imaging approach known as ""ghost imaging"" to take 3D X-ray images of an object's interior that is opaque to visible light.Lead researcher Dr Andrew Kingston said the study was the first to achieve 3D X-ray imaging using the ghost imaging approach, which has the potential to make 3D medical imaging much cheaper, safer and more accessible.""The beauty of using the ghost imaging technique for 3D imaging is that most of the X-ray dose is not even directed towards the object you want to capture -- that's the ghostly nature of what we're doing,"" said Dr Kingston from the ANU Research School of Physics and Engineering.""There's great potential to significantly lower doses of X-rays in medical imaging with 3D ghost imaging and to really improve early detection of diseases like breast cancer.""Too much radiation from medical x-ray imaging can increase cancer risk, which limits how often patients can be tested with CT systems, 3D mammography for breast cancer screening and other 3D X-ray approaches.""A variation of our approach doesn't require an X-ray camera at all, just a sensor -- this would make a 3D medical imaging setup much cheaper,"" Dr Kingston said.The proof-of-concept approach took a 3D ghost image of a simple object of 5.6mm diameter at a relatively low resolution of about 0.1mm.The researchers devised a new ghost imaging measurement system that used a series of X-ray beams with patterns.Each beam was then split into two identical beams. The pattern was recorded in the primary beam, which acted as a reference since it never passed through the object that the researchers were imaging. The secondary beam passed through the object, with only the total X-ray transmission measured by a single sensor.The researchers then used a computer to create a 2D X-ray projection image of the object from these measurements.This process was repeated with the object at different orientations to construct a 3D image.""Our most important innovation is to extend this 2D concept to achieve 3D imaging of the interior of objects that are opaque to visible light,"" Dr Kingston said.""3D X-ray ghost imaging, or ghost tomography, is a completely new field, so there's an opportunity for the scientific community and industry to work together to explore and develop this exciting innovation.""Co-researcher Professor David Paganin from Monash University said the team's achievement could be compared to the early days of electron microscopes, which could only achieve a magnification of 14 times.""This result was not as good as could be obtained with even the crudest of glass lenses using visible light,"" he said.""However, the microscope using electrons rather than light had the potential -- realised only after decades of subsequent development -- to see individual atoms, which are much tinier than an ordinary microscope using visible light can see.""Story Source:Materials provided by Australian National University. ",0.109779,0.160992,0.193645,0.230998,0.166311,0.166096,0.19646,0,0
537,Big results from small solutions: New method for analyzing metalloproteins,"A new method only needs a tiny liquid sample to analyze metalloproteins. This breakthrough was achieved by a research team led by Associate Professor Eiji Ohmichi and Tsubasa Okamoto at the Kobe University Graduate School of Science. The findings were published on November 26 in Applied Physics Letters.Metalloproteins (also known as metal-binding proteins) play vital roles in our bodies for oxygen transport and storage, electron transport, oxidation and reduction. In many cases, the metal ions in these proteins are the active centers for these activities, so by identifying the exact state of these ions, we can understand the mechanisms behind their functions.An experimental method called electron paramagnetic resonance (EPR) can be used to measure the state of electron ions in proteins. Effective EPR techniques require a certain amount of specimen volume for sensitive measurements. However, many metalloproteins are difficult to isolate and refine, so we can only obtain small samples.Conventional EPR measurements detect the electromagnetic waves absorbed by metal ions. The notable feature of this study is the use of a trampoline-shaped device called a nanomembrane. In EPR the electron spin transitions to a high-energy state by absorbing electromagnetic waves, but at the same time the spin direction reverses, and the magnetic properties of the metal ions also change. Before the experiment the research team attached tiny magnets to the nanomembrane, so the changes in the force of attraction between the magnets and the metal ions are transformed into a force on the nanomembrane, and this EPR signal is detected. Since the nanomembrane is very thin (just 100 nm (=0.1 ?m) we can sensitively measure small changes in force that accompany EPR absorption.The solution specimen is placed in a solution cell directly above the membrane. The cell volume is just 50?L(=0.05 cc), and the team adds about 1-10?L(0.001-0.01 cc) of solution for measurement. In order to prevent the solution from evaporating, the cell is covered with a resin lid. In this method the thin and fragile nanomembrane is independent from the solution cell, making it easy to switch specimens.In order to evaluate the performance of this setup, the team carried out EPR measurement over a high-frequency (over 0.1 THz) for an iron-containing protein called myoglobin and its model complex hemin chloride. The team succeeded in detecting EPR signals across a wide wave frequency (0.1-0.35 THz) for a 50 mM concentration, 2?L hemin chloride solution. They also observed a characteristic EPR signal for a 8.8 mM, 10?L specimen of myoglobin solution. A great advantage of this method is the ability to measure across a wide frequency range, making it applicable for metalloproteins with a variety of magnetic properties.Professor Ohmichi comments: ""This new method makes it possible to determine to a detailed level the state of the metal ions in a tiny amount of metalloprotein solution. We may be able to apply the method to metalloproteins that previously could not be measured. For example, in our metabolisms, a metalloprotein called peroxidase plays a crucial role by converting hydrogen peroxide into water, making it harmless, but the details of the mechanism for this reactive process are still unclear. The results from this study can potentially be applied as a leading analysis method to shed light on this sort of vital phenomenon.""Story Source:Materials provided by Kobe University. ",0.0693438,0.131285,0.11905,0.123101,0.106837,0.119396,0.121596,0,0
538,Insight into the brain's hidden depths: Scientists develop minimally invasive probe,"This could be a major step towards a better understanding of the functions of deeply hidden brain compartments, such as the formation of memories, as well as related dysfunctions, including Alzheimer's disease. Researchers from the Leibniz Institute of Photonic Technology (Leibniz IPHT) in Jena and the University of Edinburgh have succeeded in using a hair-thin fibre endoscope to gain insights into hardly-accessible brain structures. For the first time, scientists are able to achieve high-resolution observations of neuronal structures inside deep brain areas of living mice -- by the use of the most minimally invasive endoscopic probe reported thus far. This study has been published in the current issue of the journal ""Light: Science & Applications.""Using a hair-thin optical fibre, the researchers can look into deep brain areas of a living mouse as if through a keyhole. Recently introduced methods for holographic control of light propagation in complex media enable the use of a multimode fibre as an imaging tool. Based on this new approach, the scientists designed a compact system for fluorescence imaging at the tip of a fibre, offering a much smaller footprint as well as enhanced resolution compared to conventional endoscopes based on fibre bundles or graded-index lenses.""We are very excited to see our technology making its first steps towards practical applications in neuroscience,"" says Dr Sergey Turtaev from Leibniz IPHT, lead author of the paper. ""For the first time, we have shown that it is possible to examine deep brain regions of a living animal model in a minimally invasive way and to achieve high-resolution images at the same time,"" adds IPHT scientist Dr Ivo T. Leite. Sergey and Ivo work in the research group for Holographic Endoscopy led by IPHT scientist Prof. Tome ?iemer, who developed the holographic method for imaging through a single fibre. Using this approach, the research team succeeded in obtaining images of brain cells and neuronal processes in the visual cortex and hippocampus of living mice with resolution approaching one micrometre (i.e. one thousand times smaller than a millimetre). Detailed observations within these areas are crucial for research into sensory perception, memory formation, and severe neuronal diseases such as Alzheimer's. Current investigation methods are strongly invasive, such that it is not possible to observe neuronal networks in these inner regions at work without massive destruction of the surrounding tissue -- usual endoscopes composed of hundreds of optical fibres are too large to penetrate such sensitive brain regions, while the neuronal structures are too tiny to be visualised by non-invasive imaging methods such as magnetic resonance imaging (MRI).""This minimally invasive approach will enable neuroscientists to investigate functions of neurons in deep structures of the brain of behaving animals: without perturbing the neuronal circuits in action, it will be possible to reveal the activity of these neuronal circuits while the animal is exploring an environment or learning a new task,"" explains project partner Dr Nathalie Rochefort from the University of Edinburgh.Building up on this work, the research team now wants to address the current challenges of neuroscience, which will entail the delivery of advanced microscopy techniques through single fibre endoscopes. ""Under the ""Photonics for Life"" flag of the Leibniz-IPHT and in the scope of the European Research Council funded project LIFEGATE, we will strive hard to prepare more significant advancements on this result, essentially funnelling the most advanced methods of modern microscopy deep inside the tissues of living and functioning organisms."" concludes Prof. Tome ?iemer.Story Source:Materials provided by Leibniz-Institute of Photonic Technology. ",0.0881102,0.126192,0.165379,0.24751,0.152238,0.141475,0.174907,0,0
539,New technique to make objects invisible proposed,"In recent years, invisibility has become an area of increasing research interest due to advances in materials engineering. This research work by the UEx, which has been published in Scientific Reports by the Nature Group, has explored the electromagnetic properties of specific materials which can make certain objects invisible when they are introduced into its interior, in the manner of fillers. Normally, artificial materials known as metamaterials, or materials with high dielectric or magnetic constants, are usedThis idea for attaining invisibility using filler materials instead of external layers, was inspired by the Final-Year Project of Alberto Serna and Luis Molina, undergraduate students of Telecommunications at the UEx. ""Although all of us think of the invisibility cloak of Harry Potter and this is the model that other scientists have used to invisibilize objects, up to now, the idea of fillers, previously suggested in the novel The Invisible Man by H.G. Wells, in which Griffin becomes invisible by injecting himself with a lightening agent,"" explains the researcher Alberto Serna, who is currently working on his doctoral thesis in the Telecommunications Group at the UEx .Serna clarifies that ""the majority of the techniques with which cloaks of invisibility are developed harness the extraordinary properties of certain materials to make light circumvent the object to be made invisible."" Nevertheless, this model cannot be implemented using fillers, because the object is exposed to the light and therefore forced to interact with it. ""We have used a different technique, plasmonic cloaking, which makes the object and the filler jointly invisible,"" says Serna from Italy, where he is currently on a research stay.In this way, the method makes it possible to achieve invisibility from the interior of an object without using any external device. In addition, invisibility using fillers allows the object to interact with its environment without being hampered by the external cloaking. The technique is valid for objects of small size, and the bandwidths achieved are still narrow, but the investigators believe the scope for further improvements is promising.New applicationsLuis Landesa, who led this work, contends that the idea of fillers opens up a new array of applications ""because the fact that an object can ""see"" the outside without hindrance from external layers is novel and promising."" The researchers suggest applications which range from using non-solid materials to uses in communications and bioengineering. A typical example of the utility of invisibility is in invisible microscopic probes which do not perturb the device to be measured; with the use of fillers, in addition, the reading itself would not be altered, this being the problem posed by invisibility cloaks.Story Source:Materials provided by University of Extremadura. ",0.0964594,0.116997,0.139364,0.218756,0.130876,0.162127,0.171495,0,0
540,Advances in cellular microscopy: Transparent fruit flies,"The nervous system of an animal can be studied by cutting it up into thin layers -- however this inevitably leads to the destruction of the cellular structures in the tissue. Analyzing complex nerve connections is then hardly possible. The far more elegant method is the so called optical ""clearing"" of the various tissues using chemical processes that make the animal transparent. Interesting structures in the tissue can be selectively marked and analyzed.At the Vienna University of Technology, a clearing method has now been developed that can be applied to insects, which is a particularly difficult task. With an improved light-sheet microscope (a so-called ultramicroscope), it is now possible to image large nerve tissue samples and take high-resolution pictures of complex neural networks that have been labeled with fluorescent molecules. The new method has been published in the journal Nature Communications.Fluorescent Molecules""We can learn a lot about the nervous systems of animals by using genetic engineering to insert special molecules into the nerve tissues, which can then be made to fluoresce,"" says Marko Pende, a PhD student at TU Wien (Vienna). The big question is how these fluorescent molecules can be imaged without damaging the tissue.One method that has been used with great success is ultramicroscopy. Transparent tissue is illuminated with a laser beam, which is widened by special optical elements, creating a two-dimensional flat surface of light. This surface penetrates the tissue and illuminates those fluorescent molecules that lie exactly in that plane. Layer by layer, the tissue can be analyzed with this light-sheet, creating a three-dimensional model from the two-dimensional frames on the computer.""We focused on the fruit fly Drosophila melanogaster because it is of particular interest for research into the nervous system. Unfortunately, it is particularly difficult to develop a suitable clearing method for insects, ""explains Marko Pende.""For the tissue to become transparent, it has to be treated with special chemicals, and in insect tissues these chemicals have always destroyed the fluorescent molecules until now."" In addition, insect tissue contains chitin, which can hardly be made transparent. Also, Drosophila has particularly robust pigments in it its eyes.The team at TU Wien (Vienna), together with the University of Vienna and the Medical University, succeeded in finding a way to make Drosophila flies completely transparent without destroying the fluorescent marker molecules. This was achieved with the help of improved chemical mixtures. ""It is an important step forward for the Drosophila research community,"" says Prof. Thomas Hummel from the Department of Neurobiology (University of Vienna).The pictures were made possible by pioneering optical research by Saiedeh Saghafi (TU Wien). She was able to significantly improve the ultramicroscope: The light-sheet, with which the plane is illuminated layer by layer, used to be about 10 microns thick. The improved ultramicroscope now produces uniform light-sheets of only 3 ?m thickness over a large area. In addition, the microscope was equipped with an additional lens, which changes the focal point, much like to a pair of glasses: ""So far, we could only focus on the outer area of the tissue, now we can take a centimeter-deep look into the tissue and still get sharp images,"" says Prof. Hans Ulrich Dodt, Head of the Department of Bioelectronics (TU Wien). ""It will enable impressive, high-resolution images that will give us important insights into the way the Drosophila nervous system works.""The Connectome and Fruit Fly BehaviorThe new technique should now help to study the so-called ""connectome"" of Drosophila. The connectome is the arrangement of interconnections throughout the nervous system, the ""electrical circuit diagram"" of the animal. This circuit diagram can then be related to behavioral patterns of Drosophila.In addition, Drosophila is ideal for analyzing genes that lead to neurodegenerative diseases in humans, such as Alzheimer's and Parkinson's disease. Transparent flies now provide a unique opportunity to understand the complex changes in various areas of the nervous system during neurodegeneration.Story Source:Materials provided by Vienna University of Technology. ",0.0967974,0.132143,0.135648,0.193749,0.14719,0.145046,0.174449,0,0
541,"Meta-surface corrects for chromatic aberrations across all kinds of lenses The single-layer surface of nanostructures can be incorporated into commercial optical systems, from simple to complex","Today's optical systems -- from smartphone cameras to cutting-edge microscopes -- use technology that hasn't changed much since the mid-1700s. Compound lenses, invented around 1730, correct the chromatic aberrations that cause lenses to focus different wavelengths of light in different spots. While effective, these multi-material lenses are bulky, expensive, and require precision polishing or molding and very careful optical alignment. Now, a group of researchers at the Harvard John A. Paulson School of Engineering and Applied Sciences (SEAS) is asking: Isn't it time for an upgrade?SEAS researchers have developed a so-called metacorrector, a single-layer surface of nanostructures that can correct chromatic aberrations across the visible spectrum and can be incorporated into commercial optical systems, from simple lenses to high-end microscopes. The metacorrector eliminated chromatic aberrations in a commercial lens across the entire visible light spectrum. The device also works for the super-complex objectives with as many as 14 conventional lenses, used in high-resolution microscopes.The research is described in Nano Letters.""Our metacorrector technology can work in tandem with traditional refractive optical components to improve performance while significantly reducing the complexity and footprint of the system, for a wide range of high-volume applications"" said Federico Capasso, the Robert L. Wallace Professor of Applied Physics and Vinton Hayes Senior Research Fellow in Electrical Engineering at SEAS and senior author of the paper.In previous research, Capasso and his team demonstrated that metasurfaces, arrays of nanopillars spaced less than a wavelength apart, can be used to manipulate the phase, amplitude and polarization of light and enable new, ultra-compact optical devices, including flat lenses. This research uses the same principles to tune and control the effective refractive index of each nanopillar so that all wavelengths are brought by the metacorrector to the same focal point.""You can imagine light as different packets being delivered at different speeds as it propagates in the nanopillars. We have designed the nanopillars so that all these packets arrive at the focal spot at the same time and with the same temporal width,"" said Wei Ting Chen, a Research Associate in Applied Physics at SEAS and first author of the paper.""Using metacorrectors is fundamentally different from conventional methods of aberration correction, such as cascading refractive optical components or using diffractive elements, since it involves nanostructure engineering,"" said Alexander Zhu, a graduate student at SEAS and co-author of the study. ""This means we can go beyond the material limitations of lenses and have much better performances.""Next, the researchers aim to increase efficiency for high-end and miniature optical devices.Harvard's Office of Technology Development has protected the intellectual property relating to this project and is exploring commercialization opportunities.Story Source:Materials provided by Harvard John A. Paulson School of Engineering and Applied Sciences. Original written by Leah Burrows. ",0.0760695,0.109663,0.120753,0.190368,0.132632,0.134598,0.153918,0,0
542,When AI and optoelectronics meet: Researchers take control of light properties,"Using machine-learning and an integrated photonic chip, researchers from INRS (Canada) and the University of Sussex (UK) can now customize the properties of broadband light sources. Also called ""supercontinuum,"" these sources are at the core of new imaging technologies and the approach proposed by the researchers will bring further insight into fundamental aspects of light-matter interactions and ultrafast nonlinear optics. The work is published in the journal Nature Communications on November 20, 2018.In Professor Roberto Morandotti's laboratory at INRS, researchers were able to create and manipulate intense ultrashort pulse patterns, which are used to generate a broadband optical spectrum. In recent years, the development of laser sources featuring intense and ultrashort laser pulses -- that led to the Nobel Prize in Physics in 2018 -- along with ways to spatially confine and guide light propagation (optical fibre and waveguides) gave rise to optical architectures with immense power. With these new systems, an array of possibilities emerges, such as the generation of supercontinua, i.e extended light spectra generated through intense light-matter interactions.Such powerful and complex optical systems, and their associated processes, currently form the building blocks of widespread applications spanning from laser science and metrology to advanced sensing and biomedical imaging techniques. To keep pushing the limits of these technologies, more tailoring capability of the light properties is needed. With this work, the international research team unveils a practical and scalable solution to this issue.Dr Benjamin Wetzel (University of Sussex), principal investigator of this research led by Prof. Roberto Morandotti (INRS) and Prof. Marco Peccianti (University of Sussex), demonstrated that diverse patterns of femtosecond optical pulses can be prepared and judiciously manipulated. ""We have taken advantage of the compactness, stability and sub-nanometer resolution offered by integrated photonic structures to generate reconfigurable bunches of ultrashort optical pulses,"" explains Dr Wetzel. ""The exponential scaling of the parameter space obtained yields to over 1036 different configurations of achievable pulse patterns, more than the number of stars estimated in the universe,"" he concludes.With such a large number of combinations to seed an optical system known to be highly sensitive to its initial conditions, the researchers have turned to a machine-learning technique in order to explore the outcome of light manipulation. In particular, they have shown that the control and customization of the output light is indeed efficient, when conjointly using their system and a suitable algorithm to explore the multitude of available light pulse patterns used to tailor complex physical dynamics.These exciting results will impact fundamental as well as applied research in a number of fields, as a large part of the current optical systems rely on the same physical and nonlinear effects as the ones underlying supercontinuum generation. The work by the international research team is thus expected to seed the development of other smart optical systems via self-optimization techniques, including the control of optical frequency combs (Nobel 2005) for metrology applications, self-adjusting lasers, pulse processing and amplification (Nobel 2018) as well as the implementation of more fundamental approaches of machine-learning, such as photonic neural network systems.Story Source:Materials provided by Institut national de la recherche scientifique - INRS. ",0.0736831,0.0980397,0.10638,0.190746,0.147769,0.122538,0.135519,0,0
543,New non-mechanical laser steering technology,"Scientists at the U.S. Naval Research Laboratory have recently demonstrated a new nonmechanical chip-based beam steering technology that offers an alternative to costly, cumbersome and often unreliable and inefficient mechanical gimbal-style laser scanners.The chip, known as a steerable electro-evanescent optical refractor, or SEEOR, takes laser light in the mid-wavelength infrared (MWIR) as an input and steers the beam in two dimensions at the output without the need for mechanical devices -- demonstrating improved steering capability and higher scan speed rates than conventional methods.""Given the low size, weight and power consumption and continuous steering capability, this technology represents a promising path forward for MWIR beam-steering technologies,"" said Jesse Frantz, research physicist, NRL Optical Sciences Division. ""Mapping in the MWIR spectral range demonstrates useful potential in a variety of applications, such as chemical sensing and monitoring emissions from waste sites, refineries, and other industrial facilities.""The SEEOR is based on an optical waveguide -- a structure that confines light in a set of thin layers with a total thickness of less than a tenth that of a human hair. Laser light enters through one facet and moves into the core of the waveguide. Once in the waveguide, a portion of the light is located in a liquid crystal (LC) layer on top of the core. A voltage applied to the LC through a series of patterned electrodes changes the refractive index (in effect, the speed of light within the material), in portions of the waveguide, making the waveguide act as a variable prism. Careful design of the waveguides and electrodes allow this refractive index change to be translated to high speed and continuous steering in two dimensions.SEEORs were originally developed to manipulate shortwave infrared (SWIR) light -- the same part of the spectrum used for telecommunications -- and have found applications in guidance systems for self-driving cars.""Making a SEEOR that works in the MWIR was a major challenge,"" Frantz said. ""Most common optical materials do not transmit MWIR light or are incompatible with the waveguide architecture, so developing these devices required a tour de force of materials engineering.""To accomplish this, the NRL researchers designed new waveguide structures and LCs that are transparent in the MWIR, new ways to pattern these materials, and new ways to induce alignment in the LCs without absorbing too much light. This development combined efforts across multiple NRL divisions including the Optical Sciences Division for MWIR materials, waveguide design and fabrication, and the Center for Bio/Molecular Science and Engineering for synthetic chemistry and liquid crystal technology.The resulting SEEORs were able to steer MWIR light through an angular range of 14ee0.6e. The researchers are now working on ways to increase this angular range and to extend the portion of the optical spectrum where SEEORs work even further.Story Source:Materials provided by Naval Research Laboratory. ",0.103922,0.141528,0.147641,0.180086,0.179056,0.149343,0.16064,0,0
544,"Gut-on-a-chip' system shows intestinal barrier disruption is the onset initiator of gut inflammation 'Once the gut barrier has been damaged, probiotics can be harmful just like any other bacteria'","The first study investigating the mechanism of how a disease develops using human organ-on-a-chip technology has been successfully completed by engineers at The University of Texas at Austin.Researchers from the Cockrell School of Engineering were able to shed light on a part of the human body -- the digestive system -- where many questions remain unanswered. Using their ""gut inflammation-on-a-chip"" microphysiological system, the research team confirmed that intestinal barrier disruption is the onset initiator of gut inflammation.The study also includes evidence that casts doubt on the conventional wisdom of taking probiotics -- live bacteria that are considered good for gut health and found in supplements and foods such as yogurt -- on a regular basis. According to the findings, the benefits of probiotics depend on the vitality of one's intestinal epithelium, or the gut barrier, a delicate single-cell layer that protects the rest of the body from other potentially harmful bacteria found in the human gut.""By making it possible to customize specific conditions in the gut, we could establish the original catalyst, or onset initiator, for the disease,"" said Hyun Jung Kim, assistant professor in the department of biomedical engineering who led the study. ""If we can determine the root cause, we can more accurately determine the most appropriate treatment.""The findings are published this week in Proceedings of the National Academy of Sciences.Until now, organs-on-chips, which are microchips lined by living human cells to model various organs from the heart and lungs to the kidneys and bone marrow, solely served as accurate models of organ functionality in a controlled environment. This is the first time that a diseased organ-on-a-chip has been developed and used to show how a disease develops in the human body -- in this case, the researchers examined gut inflammation.""Once the gut barrier has been damaged, probiotics can be harmful just like any other bacteria that escapes into the human body through a damaged intestinal barrier,"" said Woojung Shin, a biomedical engineering Ph.D. candidate who worked with Kim on the study. ""When the gut barrier is healthy, probiotics are beneficial. When it is compromised, however, they can cause more harm than good. Essentially, 'good fences make good neighbors.' ""Shin plans to develop more customized human intestinal disease models such as for inflammatory bowel disease or colorectal cancer in order to identify how the gut microbiome controls inflammation, cancer metastasis and the efficacy of cancer immunotherapy.Kim is a leading researcher in the development of human organs-on-chips. He developed the first human gut-on-a-chip in 2012 at Harvard University's Wyss Institute for Biologically Inspired Engineering.This research was supported in part by the Alternatives in Scientific Research of The International Foundation for Ethical Research Graduate Fellowship and the National Research Foundation of Korea.Story Source:Materials provided by University of Texas at Austin. ",0.0876375,0.148159,0.192382,0.224842,0.148947,0.143948,0.190884,0,0
545,Making light work of quantum computing,"Light may be the missing ingredient in making usable quantum silicon computer chips, according to an international study featuring a University of Queensland researcher.The team has engineered a silicon chip that can guide single particles of light -- photons -- along optical tracks, encoding and processing quantum-bits of information known as 'qubits'.Professor Timothy Ralph from UQ's Centre for Quantum Computation and Communication Technology said that the use of photons in this way could increase the number and types of tasks that computers can help us with.""Current computers use a binary code -- comprising ones and zeroes -- to transfer information, but quantum computers have potential for greater power by harnessing the power of qubits,"" Professor Ralph said.""Qubits can be one and zero at the same time, or can link in much more complicated ways -- a process known as quantum entanglement -- allowing us to process enormous amounts of data at once.""The real trick is creating a quantum computing device that is reprogrammable and can be made at low cost.""The experiment, conducted primarily at the University of Bristol, proved that it is possible to fully control two qubits of information within a single integrated silicon chip.""What this means is that we've effectively created a programmable machine that can accomplish a variety of tasks.""And since it's a very small processor and can be built out of silicon, it might be able to be scaled in a cost-effective way,"" he said.""It's still early days, but we've aimed to develop technology that is truly scalable, and since there's been so much research and investment in silicon chips, this innovation might be found in the laptops and smartphones of the future.""A surprising result of the experiment is that the quantum computing machine has become a research tool in its own right.""The device has now been used to implement several different quantum information experiments using almost 100,000 different reprogrammed settings,"" Professor Ralph said.""This is just the beginning; we're just starting to see what kind of exponential change this might lead to.""Story Source:Materials provided by University of Queensland. ",0.071575,0.110829,0.112274,0.235857,0.149703,0.116711,0.157117,0,0
546,Semiconductor quantum transistor opens the door for photon-based computing,"Transistors are tiny switches that form the bedrock of modern computing; billions of them route electrical signals around inside a smartphone, for instance.Quantum computers will need analogous hardware to manipulate quantum information. But the design constraints for this new technology are stringent, and today's most advanced processors can't be repurposed as quantum devices. That's because quantum information carriers, dubbed qubits, have to follow different rules laid out by quantum physics.Scientists can use many kinds of quantum particles as qubits, even the photons that make up light. Photons have added appeal because they can swiftly shuttle information over long distances, and they are compatible with fabricated chips. However, making a quantum transistor triggered by light has been challenging because it requires that the photons interact with each other, something that doesn't ordinarily happen on its own.Now, researchers at the University of Maryland's A. James Clark School of Engineering and Joint Quantum Institute (JQI) -- led by Professor of Electrical and Computer Engineering, JQI Fellow, and Institute for Research in Electronics and Applied Physics Affiliate Edo Waks -- have cleared this hurdle and demonstrated the first single-photon transistor using a semiconductor chip. The device, described in the July 6 issue of Science, is compact; roughly one million of these new transistors could fit inside a single grain of salt. It is also fast and able to process 10 billion photonic qubits every second.""Using our transistor, we should be able to perform quantum gates between photons,"" says Waks. ""Software running on a quantum computer would use a series of such operations to attain exponential speedup for certain computational problems.The photonic chip is made from a semiconductor with numerous holes in it, making it appear much like a honeycomb. Light entering the chip bounces around and gets trapped by the hole pattern; a small crystal called a quantum dot sits inside the area where the light intensity is strongest. Analogous to conventional computer memory, the dot stores information about photons as they enter the device. The dot can effectively tap into that memory to mediate photon interactions -- meaning that the actions of one photon affect others that later arrive at the chip.""In a single-photon transistor the quantum dot memory must persist long enough to interact with each photonic qubit,"" says Shuo Sun, lead author of the new work and postdoctoral research fellow at Stanford University who was a UMD grad student at the time of the research. ""This allows a single photon to switch a bigger stream of photons, which is essential for our device to be considered a transistor.""To test that the chip operated like a transistor, the researchers examined how the device responded to weak light pulses that usually contained only one photon. In a normal environment, such dim light might barely register. However, in this device, a single photon gets trapped for a long time, registering its presence in the nearby dot.The team observed that a single photon could, by interacting with the dot, control the transmission of a second light pulse through the device. The first light pulse acts like a key, opening the door for the second photon to enter the chip. If the first pulse didn't contain any photons, the dot blocked subsequent photons from getting through. This behavior is similar to a conventional transistor where a small voltage controls the passage of current through its terminals. Here, the researchers successfully replaced the voltage with a single photon and demonstrated that their quantum transistor could switch a light pulse containing around 30 photons before the quantum dot's memory ran out.Waks says that his team had to test different aspects of the device's performance prior to getting the transistor to work. ""Until now, we had the individual components necessary to make a single photon transistor, but here we combined all of the steps into a single chip,"" he says.Sun says that with realistic engineering improvements their approach could allow many quantum light transistors to be linked together. The team hopes that such speedy, highly connected devices will eventually lead to compact quantum computers that process large numbers of photonic qubits.Story Source:Materials provided by University of Maryland. ",0.0995769,0.127526,0.122187,0.222303,0.203484,0.14283,0.164039,0,0
547,Chip upgrade helps bee-size drones navigate Low-power design will allow devices as small as a honeybee to determine their location while flying,"Researchers at MIT, who last year designed a tiny computer chip tailored to help honeybee-sized drones navigate, have now shrunk their chip design even further, in both size and power consumption.The team, co-led by Vivienne Sze, associate professor in MIT's Department of Electrical Engineering and Computer Science (EECS), and Sertac Karaman, the Class of 1948 Career Development Associate Professor of Aeronautics and Astronautics, built a fully customized chip from the ground up, with a focus on reducing power consumption and size while also increasing processing speed.The new computer chip, named ""Navion,"" which they are presenting this week at the Symposia on VLSI Technology and Circuits, is just 20 square millimeters -- about the size of a LEGO minifigure's footprint -- and consumes just 24 milliwatts of power, or about one-thousandth the energy required to power a lightbulb.Using this tiny amount of power, the chip is able to process in real-time camera images at up to 171 frames per second, as well as inertial measurements, both of which it uses to determine where it is in space. The researchers say the chip can be integrated into ""nanodrones"" as small as a fingernail, to help the vehicles navigate, particularly in remote or inaccessible places where global positioning satellite data is unavailable.The chip design can also be run on any small robot or device that needs to navigate over long stretches of time on a limited power supply.""I can imagine applying this chip to low-energy robotics, like flapping-wing vehicles the size of your fingernail, or lighter-than-air vehicles like weather balloons, that have to go for months on one battery,"" says Karaman, who is a member of the Laboratory for Information and Decision Systems and the Institute for Data, Systems, and Society at MIT. ""Or imagine medical devices like a little pill you swallow, that can navigate in an intelligent way on very little battery so it doesn't overheat in your body. The chips we are building can help with all of these.""Sze and Karaman's co-authors are EECS graduate student Amr Suleiman, who is the lead author; EECS graduate student Zhengdong Zhang; and Luca Carlone, who was a research scientist during the project and is now an assistant professor in MIT's Department of Aeronautics and Astronautics.A flexible chipIn the past few years, multiple research groups have engineered miniature drones small enough to fit in the palm of your hand. Scientists envision that such tiny vehicles can fly around and snap pictures of your surroundings, like mosquito-sized photographers or surveyors, before landing back in your palm, where they can then be easily stored away.But a palm-sized drone can only carry so much battery power, most of which is used to make its motors fly, leaving very little energy for other essential operations, such as navigation, and, in particular, state estimation, or a robot's ability to determine where it is in space.""In traditional robotics, we take existing off-the-shelf computers and implement [state estimation] algorithms on them, because we don't usually have to worry about power consumption,"" Karaman says. ""But in every project that requires us to miniaturize low-power applications, we have to now think about the challenges of programming in a very different way.""In their previous work, Sze and Karaman began to address such issues by combining algorithms and hardware in a single chip. Their initial design was implemented on a field-programmable gate array, or FPGA, a commercial hardware platform that can be configured to a given application. The chip was able to perform state estimation using 2 watts of power, compared to larger, standard drones that typically require 10 to 30 watts to perform the same tasks. Still, the chip's power consumption was greater than the total amount of power that miniature drones can typically carry, which researchers estimate to be about 100 milliwatts.To shrink the chip further, in both size and power consumption, the team decided to build a chip from the ground up rather than reconfigure an existing design. ""This gave us a lot more flexibility in the design of the chip,"" Sze says.Running in the worldTo reduce the chip's power consumption, the group came up with a design to minimize the amount of data -- in the form of camera images and inertial measurements -- that is stored on the chip at any given time. The design also optimizes the way this data flows across the chip.""Any of the images we would've temporarily stored on the chip, we actually compressed so it required less memory,"" says Sze, who is a member of the Research Laboratory of Electronics at MIT. The team also cut down on extraneous operations, such as the computation of zeros, which results in a zero. The researchers found a way to skip those computational steps involving any zeros in the data. ""This allowed us to avoid having to process and store all those zeros, so we can cut out a lot of unnecessary storage and compute cycles, which reduces the chip size and power, and increases the processing speed of the chip,"" Sze says.Through their design, the team was able to reduce the chip's memory from its previous 2 megabytes, to about 0.8 megabytes. The team tested the chip on previously collected datasets generated by drones flying through multiple environments, such as office and warehouse-type spaces.""While we customized the chip for low power and high speed processing, we also made it sufficiently flexible so that it can adapt to these different environments for additional energy savings,"" Sze says. ""The key is finding the balance between flexibility and efficiency."" The chip can also be reconfigured to support different cameras and inertial measurement unit (IMU) sensors.From these tests, the researchers found they were able to bring down the chip's power consumption from 2 watts to 24 milliwatts, and that this was enough to power the chip to process images at 171 frames per second -- a rate that was even faster than what the datasets projected.The team plans to demonstrate its design by implementing its chip on a miniature race car. While a screen displays an onboard camera's live video, the researchers also hope to show the chip determining where it is in space, in real-time, as well as the amount of power that it uses to perform this task. Eventually, the team plans to test the chip on an actual drone, and ultimately on a miniature drone.This research was supported, in part, by the Air Force Office of Scientific Research, and by the National Science Foundation.Story Source:Materials provided by Massachusetts Institute of Technology. Original written by Jennifer Chu. ",0.0772055,0.0726931,0.10472,0.24528,0.116809,0.127572,0.142802,0,0
548,RNA microchips: A new chapter in ribonucleic acid synthesis,"Ribonucleic acid (RNA) is, along with DNA and protein, one of the three primary biological macromolecules and was probably the first to arise in early life forms. In the ""RNA world""hypothesis, RNA is able to support life on its own because it can both store information and catalyze biochemical reactions. Even in modern life, the most complex molecular machines in all cells, the ribosomes, are made largely of RNA. Chemists at the Faculty of Chemistry of the University of Vienna and at McGill University have developed a new synthetic approach that allows RNA to be chemically synthesized about a million times more efficiently than previously possible.RNA is ubiquitous in cells. It is responsible for shuttling information out of the nucleus, regulating gene expression and synthesizing proteins. Some RNA molecules, particularly in bacteria, also catalyze biochemical reactions and sense environmental signals.The chemical synthesis of DNA and RNA goes back to the early days of molecular biology, particularly the efforts by Nobel Laureate Har Gobind Khorana in the early 1960s to decipher the genetic code. Over the years, the chemistry has improved considerably but RNA synthesis has remained much more difficult and slow due to the need for an additional protecting group on the 2'-hydroxy of the ribose sugar of RNA. Chemists at Department of Inorganic Chemistry of the Faculty of Chemistry of the University of Vienna and at McGill University have now been able to bring RNA synthesis a large step forward.In order to increase the synthesis efficiency, the chemists joined two key concepts: photolithographic fabrication technology from semiconductor manufacture and the development of a new protecting group.First, the chemists adapted the photolithographic fabrication technology from the semiconductor chip industry, commonly used for integrated circuit manufacture, for the chemical synthesis of RNA. Biological photolithography makes it possible to produce RNA chips with a density of up to one million sequences per square centimeter. Instead of using far ultraviolet light, which is used in the production of computer chips for silicon etching and doping, the researchers use UV-A light. ""Shortwave ultraviolet light has a very destructive effect on RNA, so we are limited to UV-A light in the synthesis"" explains Mark Somoza, of the Institute of Inorganic Chemistry.In addition to the innovative use of photolithography, the researchers were also able to develop a new protecting group for the RNA 2'-hydroxyl group that is compatible with photolithographic synthesis. The new protecting group is acetal levulinyl ester (ALE), which also gives very high yields (over 99 percent) in the coupling reactions between the added RNA monomers in the extension of the RNA strand. ""The combination of high-synthesis yield and ease of handling makes it possible to foresee the preparation of longer, and functional, RNA molecules on microchips"" said Jory Lietard, post-doc of the group of Mark Somoza.Story Source:Materials provided by University of Vienna. ",0.0853467,0.192024,0.130824,0.173076,0.165422,0.126576,0.150786,0,0
549,"Inside these fibers, droplets are on the move ","Microfluidics devices are tiny systems with microscopic channels that can be used for chemical or biomedical testing and research. In a potentially game-changing advance, MIT researchers have now incorporated microfluidics systems into individual fibers, making it possible to process much larger volumes of fluid, in more complex ways. In a sense, the advance opens up a new ""macro"" era of microfluidics.Traditional microfluidics devices, developed and used extensively over the last couple of decades, are manufactured onto microchip-like structures and provide ways of mixing, separating, and testing fluids in microscopic volumes. Medical tests that only require a tiny droplet of blood, for example, often rely on microfluidics. But the diminutive scale of these devices also poses limitations; for example, they generally aren't useful for procedures that need larger volumes of liquid to detect substances present in minute amounts.A team of MIT researchers found a way around that, by making microfluidic channels inside fibers. The fibers be made as long as needed to accommodate larger throughput, and they offer great control and flexibility over the shapes and dimensions of the channels. The new concept is described in a paper appearing this week in the journal Proceedings of the National Academy of Sciences, written by MIT graduate student Rodger Yuan, professors Joel Voldman and Yoel Fink, and four others.A multidisciplinary approachThe project came about as a result of a ""speedstorming"" event (an amalgam of brainstorming and speed dating, an idea initiated by Professor Jeffrey Grossman) that was instigated by Fink when he was director of MIT's Research Laboratory of Electronics. The events are intended to help researchers develop new collaborative projects, by having pairs of students and postdocs brainstorm for six minutes at a time and come up with hundreds of ideas in an hour, which are ranked and evaluated by a panel. In this particular speedstorming session, students in electrical engineering worked with others in materials science and microsystems technology to develop a novel approach to cell sorting using a new class of multimaterial fibers.Yuan explains that, although microfluidic technology has been extensively developed and widely used for processing small amounts of liquid, it suffers from three inherent limitations related to the devices' overall size, their channel profiles, and the difficulty of incorporating additional materials such as electrodes.Because they are typically made using chip-manufacturing methods, microfluidic devices are limited to the size of the silicon wafers used in such systems, which are no more than about 8 inches across. And the photolithography methods used to make such chips limit the shapes of the channels; they can only have square or rectangular cross sections. Finally, any additional materials, such as electrodes for sensing or manipulating the channels' contents, must be individually placed in position in a separate process, severely limiting their complexity.""Silicon chip technology is really good at making rectangular profiles, but anything beyond that requires really specialized techniques,"" says Yuan, who carried out the work as part of his doctoral research. ""They can make triangles, but only with certain specific angles."" With the new fiber-based method he and his team developed, a variety of cross-sectional shapes for the channels can be implemented, including star, cross, or bowtie shapes that may be useful for particular applications, such as automatically sorting different types of cells in a biological sample.In addition, for conventional microfluidics, elements such as sensing or heating wires, or piezoelectric devices to induce vibrations in the sampled fluids, must be added at a later processing stage. But they can be completely integrated into the channels in the new fiber-based system.A shrinking profileLike other complex fiber systems developed over the years in the laboratory of co-author Yoel Fink, professor of materials science and engineering and head of the Advanced Functional Fabrics of America (AFFOA) consortium, these fibers are made by starting with an oversized polymer cylinder called a preform. These preforms contain the exact shape and materials desired for the final fiber, but in much larger form -- which makes them much easier to make in very precise configurations. Then, the preform is heated and loaded into a drop tower, where it is slowly pulled through a nozzle that constricts it to a narrow fiber that's one-fortieth the diameter of the preform, while preserving all the internal shapes and arrangements.In the process, the material is also elongated by a factor of 1,600, so that a 100-millimeter-long (4-inch-long) preform, for example, becomes a fiber 160 meters long (about 525 feet), thus dramatically overcoming the length limitations inherent in present microfluidic devices. This can be crucial for some applications, such as detecting microscopic objects that exist in very small concentrations in the fluid -- for example, a small number of cancerous cells among millions of normal cells.""Sometimes you need to process a lot of material because what you're looking for is rare,"" says Voldman, a professor of electrical engineering who specializes in biological microtechnology. That makes this new fiber-based microfluidics technology especially appropriate for such uses, he says, because ""the fibers can be made arbitrarily long,"" allowing more time for the liquid to remain inside the channel and interact with it.While traditional microfluidics devices can make long channels by looping back and forth on a small chip, the resulting twists and turns change the profile of the channel and affect the way the liquid flows, whereas in the fiber version these can be made as long as needed, with no changes in shape or direction, allowing uninterrupted flow, Yuan says.The system also allows electrical components such as conductive wires to be incorporated into the fiber. These can be used for example to manipulate cells, using a method called dielectrophoresis, in which cells are affected differently by an electric field produced between two conductive wires on the sides of the channel.With these conductive wires in the microchannel, one can control the voltage so the forces are ""pushing and pulling on the cells, and you can do it at high flow rates,"" Voldman says.As a demonstration, the team made a version of the long-channel fiber device designed to separate cells, sorting dead cells from living ones, and proved its efficiency in accomplishing this task. With further development, they expect to be able to perform more subtle discrimination between cell types, Yuan says.""For me this was a wonderful example of how proximity between research groups at an interdisciplinary lab like RLE leads to groundbreaking research, initiated and led by a graduate student. We the faculty were essentially dragged in by our students,"" Fink says.The researchers emphasize that they do not see the new method as a substitute for present microfluidics, which work very well for many applications. ""It's not meant to replace; it's meant to augment"" present methods, Voldman says, allowing some new functions for particular uses that have not previously been possible.Story Source:Materials provided by Massachusetts Institute of Technology. Original written by David L. Chandler. ",0.04806,0.0768079,0.0874895,0.13625,0.0895645,0.0880285,0.103731,0,0
550,Miniature magnetic swimming devices to revolutionize diagnostics and drug delivery,"Scientists have created miniature magnetic swimming devices -- which mimic the appearance of sperm cells -- that could revolutionise disease treatment by swimming drugs to specific areas of the body.The devices, which measure as small as one millimetre long, consist of a magnetic head and flexible tail that allows them to 'swim' to a specific location when activated by a magnetic field.Researchers at the University of Exeter, who designed the devices and magnetic control mechanism, have also created a mathematical model that allows them to predict their behaviour in different environment, such as microfluidic channels or complex liquids.The researchers believe that the devices could be used to deliver drugs to specific areas of the body, and so dramatically improving treatment time and success.They also believe that the devices could revolutionise the wider field of microfluidics, which focuses on moving liquids through extremely narrow channels.Their research is currently focused on implementing microscopic prototypes and the researchers have already successfully demonstrated swimmers comparable to the size of red blood cells.The research is published in the Physics of Fluids journal.Professor Feodor Ogrin, principal investigator at the University of Exeter said: ""Developing this technology could radically change the way we do medicine. The swimmers could one day be used to direct drugs to the right areas of the body by swimming through blood vessels.""We also envisage microscopic versions of the device being used on 'lab-on-a-chip' technology, where complex procedures normally conducted in a laboratory, such as diagnosing disease, are conducted on a simple chip. This would drastically reduce the time taken before treatments can be implemented, potentially saving lives.""Similar devices have previously been made using more complex and expensive techniques. This is the first swimmer that could be made on an industrial scale, thus paving a way for making cheap and disposable microfluidic chips.Professor Ogrin added: ""In future, diagnosing many diseases before getting treatment could be as simple as putting a drop of blood on a chip in a doctor's office. This is especially useful for diseases like sepsis, where symptoms progress from mild to life-threatening before such tests can be conducted.""By modifying the length of the tail and the strength of the magnetic field applied the researchers were able to find the optimum length for speed, and controllability -- allowing them to cause the device to move in the direction of, or perpendicular to the magnetic field.Microfluidics very often relies on using high-pressure pumps to move liquids, as they become extremely viscous in such small channels. The researchers demonstrate that the swimming device can be easily modified to act as a pump, stirrer or a valve in such technology. This could revolutionise the field, providing a simple and efficient way of manipulating liquids at a microscale.Story Source:Materials provided by University of Exeter. ",0.078513,0.102955,0.122169,0.186215,0.108722,0.135821,0.165916,0,0
551,"New test kit for quick, accurate and low-cost screening of diseases ","A multidisciplinary team of researchers at the National University of Singapore (NUS) has developed a portable, easy-to-use device for quick and accurate screening of diseases. This versatile technology platform called enVision (enzyme-assisted nanocomplexes for visual identification of nucleic acids) can be designed to detect a wide range of diseases -- from emerging infectious diseases (e.g. Zika and Ebola) and high-prevalence infections (e.g. hepatitis, dengue, and malaria) to various types of cancers and genetic diseases.enVision takes between 30 minutes to one hour to detect the presence of diseases, which is two to four times faster than existing infection diagnostics methods. In addition, each test kit costs under S$1 -- 100 times lower than the current cost of conducting similar tests.""The enVision platform is extremely sensitive, accurate, fast, and low-cost. It works at room temperature and does not require heaters or special pumps, making it very portable. With this invention, tests can be done at the point-of-care, for instance in community clinics or hospital wards, so that disease monitoring or treatment can be administered in a timely manner to achieve better health outcomes,"" said team leader Assistant Professor Shao Huilin from the Biomedical Institute for Global Health Research and Technology (BIGHEART) and Department of Biomedical Engineering at NUS. Asst Prof Shao is also an investigator with the Institute of Molecular and Cell Biology (IMCB) under the Agency for Science, Technology and Research (A*STAR).Superior sensitivity and specificity compared to clinical gold standardThe research team used the human papillomavirus (HPV), the key cause of cervical cancer, as a clinical model to validate the performance of enVision. In comparison to clinical gold standard, this novel technology has demonstrated superior sensitivity and specificity.""enVision is not only able to accurately detect different subtypes of the same disease, it is also able to spot differences within a specific subtype of a given disease to identify previously undetectable infections,"" Asst Prof Shao added.Bringing the lab to the patientIn addition, test results are easily visible -- the assay turns from colourless to brown if a disease is present -- and could also be further analysed using a smartphone for quantitative assessment of the amount of pathogen present. This makes enVision an ideal solution for personal healthcare and telemedicine.""Conventional technologies -- such as tests that rely on polymerase chain reaction to amplify and detect specific DNA molecules -- require bulky and expensive equipment, as well as trained personnel to operate these machines. With enVision, we are essentially bringing the clinical laboratory to the patient. Minimal training is needed to administer the ,test and interpret the results, so more patients can have access to effective, lab-quality diagnostics that will substantially improve the quality of care and treatment,"" said Dr Nicholas Ho, a researcher from NUS BIGHEART and A*STAR's IMCB, and co-first author of the study.Versatile point-of-care diagnostic deviceIn this study, Asst Prof Shao and her team developed patented DNA molecular machines that can recognise genetic material of different diseases and perform different functions. These molecular machines form the backbone of the enVision platform.The novel platform adopts a 'plug-and-play' modular design and uses microfluidic technology to reduce the amount of samples and biochemical reagents required as well as to optimise the technology's sensitivity for visual readouts.""The enVision platform has three key steps -- target recognition, target-independent signal enhancement, and visual detection. It employs a unique set of molecular switches, composed of enzyme-DNA nanostructures, to accurately detect, as well as convert and amplify molecular information into visible signals for disease diagnosis,"" explained Dr Lim Geok Soon, a researcher from NUS BIGHEART and A*STAR's IMCB, and co-first author of the study.Each test is housed in a tiny plastic chip that is preloaded with a DNA molecular machine that is designed to recognise disease-specific molecules. The chip is then placed in a common signal cartridge that contains another DNA molecular machine responsible for producing visual signals when disease-specific molecules are detected.Multiple units of the same test chip -- to test different patient samples for the same disease -- or a collection of test chips to detect different diseases could be mounted onto the common cartridge.""Having a target-independent signal enhancement step frees up the design possibilities for the recognition element. This allows enVision to be programmed as a biochemical computer with varying signals for different combinations of target pathogens. This can be very useful to monitor populations for multiple diseases like dengue and malaria simultaneously, or testing for highly mutable pathogens like the flu with high sensitivity and specificity,"" said Dr Ho.Future workAsst Prof Shao and her team took about a year and a half to develop the enVision platform. Building on the current work, the research team is developing a sample preparation module -- for extraction and treatment of DNA material -- to be integrated with the enVision platform to enhance point-of-care application. In addition, the research team foresees that the smartphone app could include more advanced image correction and analysis algorithms to further improve its performance for real-world application.This research work was published in prestigious scientific journal Nature Communications in August 2018, and featured as an Editors' Highlight by the journal.Story Source:Materials provided by National University of Singapore. ",0.0629732,0.116913,0.10871,0.189948,0.116776,0.111117,0.135485,0,0
552,Autonomous gene expression control nanodevice will contribute to medical care Scientists construct integrated gene logic-chips called 'gene nanochips',"Gene expression is a fundamental of life, where each cell switches on and off specific genes. Thus, an autonomous device that could control the on-off switching would have great value in medical care.Synthetic genetic circuits are a technology to control gene expression and program cells to perform desired functions. Therefore, increasing the complexity of the genetic circuit will allow us to control cell fates more accurately.However, the complexity of genetic circuits remains low. This is because, in conventional reaction-diffusion systems, the enzymes and substrates are provided separately, and non-specific binding of the enzymes to the substrates causes unintended crosstalk among the different circuits.Osaka University-led researchers, in a joint research project with The University of Tokyo, Kyoto University, and Waseda University, constructed integrated gene logic-chips called ""gene nanochips."" Using integrated factors on the nanochips, these self-contained nanochips can switch genes on and off within a single chip, preventing unintended crosstalk.The researchers showed the autonomous responses of the nanochips in artificial cells: environmental sensing, information computation and product output at the single-chip level. Their research results were published in Nature Nanotechnology.DNA nanotechnology is a versatile method used to construct custom structures and to control precise molecular layouts. The researchers used a rectangular sheet (90 nm wide, 60 nm deep, 2 nm high), and integrated enzyme, RNA polymerase (RNAP, an enzyme that synthesizes RNA from a DNA template), and multiple target-gene substrates.The nano-layout ability of DNA nanotechnology allows the researcher to rationally design gene expression levels by changing the intermolecular distances between the enzyme and the target genes, thus affecting the collision efficiency and subsequent reaction.The researchers further integrated sensors. Ideally, a sensor that is capable of detecting any type of signal should have minimal design limitations. However, conventional methods have suffered from several limitations (e.g., materials). This is because, in conventional genetic circuits, the sensor is part of the substrate of the enzyme (e.g., DNA in transcription; see Note for detail).Conversely, in this study, the sensor part was independent of the enzymatic reaction. Thus, the researchers can use any sensor materials that change the effective intermolecular distance on signal recognition, allowing the construction of various sensors responding to distinct signals (microRNAs, chemical compounds, proteins and light). Moreover, by combining and integrating sensors responding to distinct signals, the researchers have succeeded in photo-reprogramming of the genetic circuits.Finally, the nanochip allowed the researchers to simplify the construction of a genetic circuit that responded to an artificial cell, a water-in-oil droplet, and they could compute its miRNA profile, by simply mixing the orthogonal chips, expanding the power of the genetic circuit.Corresponding author Hisashi Tadakuma says, ""All factors necessary for transcription reactions are on this integrated nanochip, so environmental sensing, information computation, and product output can be completed at the single-chip level. In the near future, autonomous nanochips will be useful in maintaining the cell in the healthy state through controlling gene expression spatially and temporally, which will embody the ideal of the saying 'prevention is best cure'. ""Story Source:Materials provided by Osaka University. ",0.0795365,0.14666,0.131007,0.224014,0.174588,0.125982,0.162813,0,0
553,Low bandwidth? Use more colors at once New ultrathin optic cavities allow simultaneous color production on an electronic chip,"The rainbow is not just colors -- each color of light has its own frequency. The more frequencies you have, the higher the bandwidth for transmitting information.Only using one color of light at a time on an electronic chip currently limits technologies based on sensing changes in scattered color, such as detecting viruses in blood samples, or processing airplane images of vegetation when monitoring fields or forests.Putting multiple colors into service at once would mean deploying multiple channels of information simultaneously, broadening the bandwidth of not only today's electronics, but also of the even faster upcoming ""nanophotonics"" that will rely on photons -- fast and massless particles of light -- rather than slow and heavy electrons to process information with nanoscale optical devices.IBM and Intel have already developed supercomputer chips that combine the higher bandwidth of light with traditional electronic structures.As researchers engineer solutions for eventually replacing electronics with photonics, a Purdue University-led team has simplified the manufacturing process that allows utilizing multiple colors at the same time on an electronic chip instead of a single color at a time.The researchers also addressed another issue in the transition from electronics to nanophotonics: The lasers that produce light will need to be smaller to fit on the chip.""A laser typically is a monochromatic device, so it's a challenge to make a laser tunable or polychromatic,"" said Alexander Kildishev, associate professor of electrical and computer engineering at Purdue University. ""Moreover, it's a huge challenge to make an array of nanolasers produce several colors simultaneously on a chip.""This requires downsizing the ""optical cavity,"" which is a major component of lasers. For the first time, researchers from Purdue, Stanford University and the University of Maryland embedded so-called silver ""metasurfaces"" -- artificial materials thinner than light waves -- in nanocavities, making lasers ultrathin.""Optical cavities trap light in a laser between two mirrors. As photons bounce between the mirrors, the amount of light increases to make laser beams possible,"" Kildishev said. ""Our nanocavities would make on-a-chip lasers ultrathin and multicolor.""Currently, a different thickness of an optical cavity is required for each color. By embedding a silver metasurface in the nanocavity, the researchers achieved a uniform thickness for producing all desired colors. Their findings appear in Nature Communications.""Instead of adjusting the optical cavity thickness for every single color, we adjust the widths of metasurface elements,"" Kildishev said.Optical metasurfaces could also ultimately replace or complement traditional lenses in electronic devices.""What defines the thickness of any cell phone is actually a complex and rather thick stack of lenses,"" Kildishev said. ""If we can just use a thin optical metasurface to focus light and produce images, then we wouldn't need these lenses, or we could use a thinner stack.""A patent has been filed for this technology. The work was supported by the Air Force Office of Scientific Research (AFOSR) MURI grant FA9550-14-1-0389 and the Defense Advanced Research Projects Agency's Defense Sciences Office Extreme Optics and Imaging (EXTREME) program, Award HR00111720032.Story Source:Materials provided by Purdue University. Original written by Kayla Wiles. ",0.0839492,0.112836,0.140337,0.214134,0.189267,0.122447,0.159691,0,0
554,Artificial placenta created in the laboratory,"The placenta has an essential and highly complex task: it must ensure the exchange of important substances between the mother and her unborn child, whilst simultaneously blocking other substances from passing through. Until now, it has not been fully understood what the permeability of the placenta depends on -- after all, it is incredibly difficult to investigate its function in humans directly.For this reason, TU Wien (Vienna) has now produced an artificial placenta model that very closely resembles the natural organ. Using a specially developed femtosecond laser-based 3D printing process, it is possible to produce customized hydrogel membranes directly within microfluidic chips, which are then populated with placenta cells. This means it is now possible to provide clarity in some vital research issues, such as the exchange of glucose between mother and child.Complex substance exchange between mother and child""The transport of substances through biological membranes plays an important role in various areas of medicine,"" says Prof. Aleksandr Ovsianikov of the Institute of Materials Science and Technology at TU Wien. ""These include the blood-brain barrier, ingestion of food in the stomach and intestine, and also the placenta.""There are, for example, numerous studies showing that diseases in the mother such as diabetes can have an impact on the unborn child. High blood pressure can also affect the transport of substances to the fetus. Until now, however, it has been almost impossible to investigate the way in which the many parameters involved interact in such cases.Special chip with biological partition from the 3D printerResearchers at TU Wien are therefore working on replicating organ structures on compact chips in order to investigate important aspects of their function under controlled conditions. ""Our chip consists of two areas -- one represents the fetus, the other the mother,"" explains Denise Mandt, who worked on the project as part of her thesis. ""We use a special 3D printing process to produce a partition between them -- the artificial placenta membrane.""TU Wien has been working on high-resolution 3D printing of this kind for years, with great success: it involves the use of materials which can be solidified with the help of laser beams. This allows the desired 3D structures to be created point by point with a resolution in the micrometre range. ""In our case it involves a hydrogel with good biocompatibility,"" Aleksandr Ovsianikov explains. ""Based on the model of the natural placenta, we produce a surface with small, curved villi. The placenta cells can then colonise it, creating a barrier very similar to the natural placenta.""The organ on a chip""This 'organ-on-a-chip' technology is a revolutionary approach in biomedicine, which has generated a great deal of interest in clinical diagnostics, biotechnology and pharmaceutics in recent years,"" says Prof. Peter Ertl, head of the cell chip research group which played a key role in the project. ""The creation of human mini organs on a chip should allow the development of patient-specific therapeutic approaches, and also represents a vital method for replacing animal experiments.""On the chip, important biological parameters can be closely monitored, such as the pressure, temperature, geometry and nutrient supply of the mini organs, as well as the administration of medications. This makes it possible to accurately observe disease progression and cure rates.Initial tests have already shown that the artificial placenta on the chip does in fact behave in a similar way to a natural placenta: small molecules are allowed to pass through, while large ones are held back. The model is now intended to be used specifically to investigate important aspects of nutrient transport from the mother to the fetus.Story Source:Materials provided by Vienna University of Technology. ",0.0740949,0.124683,0.135009,0.196865,0.137263,0.114304,0.1578,0,0
555,Trapping light that doesn't bounce off track for faster electronics,"Replacing traditional computer chip components with light-based counterparts will eventually make electronic devices faster due to the wide bandwidth of light.A new protective metamaterial ""cladding"" prevents light from leaking out of the very curvy pathways it would travel in a computer chip.Because processing information with light can be more efficient than with electrons used in current devices, there is good reason to confine light onto a chip. But light and the bits of information it carries tend to leak and scatter out of the tiny components that must fit on a chip.A Purdue University-led effort has built a novel cladding along the highways for light travel, called waveguides, to prevent information leaks -- particularly around sharp bends where light bounces off track and scatters. Information then gets lost or jumbled rather than communicated throughout a device. Preventing this could facilitate the integration of photonic with electric circuitry, increasing communication speed and reducing power consumption.""We want the bits of information that we are sending in the waveguide to travel along tight bends and simultaneously not be lost as heat. This is a challenge,"" said Zubin Jacob, Purdue assistant professor of electrical and computer engineering.What makes the waveguide cladding so unique is anisotropy, meaning that the cladding design enables light to travel at different velocities in different directions. By controlling the anisotropy of the cladding, the researchers prevented light from leaking off track into other waveguides where ""crosstalk,"" or mixing, of information would occur. Instead, bits of information carried by light bounce off by ""total internal reflection"" and stay strongly confined within a waveguide.""The waveguide we made is an extreme skin-depth structure, which means that any leakage that does happen will be really small,"" said Saman Jahani, Purdue graduate research assistant in electrical and computer engineering. ""This approach can pave the way for dense photonic integration on a computer chip without worrying about light leakage.""Story Source:Materials provided by Purdue University. ",0.141396,0.144419,0.198701,0.244848,0.227236,0.180543,0.200855,0,0
556,"Nano-sized traps show promise in diagnosing pathogenic bacterial infections New device could quickly detect disease-causing bacteria, save lives","A new type of ""lab on a chip"" developed by McGill University scientists has the potential to become a clinical tool capable of detecting very small quantities of disease-causing bacteria in just minutes.The device designed by Sara Mahshid, Assistant Professor in the Department of Bioengineering at McGill, is made of nano-sized ""islands,"" about one tenth of the thickness of a single human hair, which act as bacterial traps or snares.In collaboration with colleagues from the University of Toronto, Professor Mahshid's team was able to demonstrate that the system is capable of analyzing very small volumes of culture media containing bacteria such as E. coli and a strain of S. aureus resistant to methicillin, an antibiotic used to treat bacterial infections.Bacterial infections are blamed for 700,000 deaths a year, and successful treatment of many disease-causing infections depends largely on rapid detection. Unfortunately, it sometimes takes several days to confirm a diagnosis with the tools currently available to doctors.""Speed is of the essence because some bacterial infections can cause serious health problems and sometimes lead to death,"" Mahshid says. ""With a fluorescent microscope, the device we've developed can confirm the presence of bacteria in just a few minutes. I hope one day clinicians will use our device to deliver faster diagnostics, start treatment much more quickly and, ultimately, save lives.""Mahshid and her team, who just published their work in the journal Small, now hope to test their device on clinical samples, a necessary step before doctors are able to use such a device in a hospital setting. Theoretically, this new lab-on-a-chip, which is relatively inexpensive and easy to make, could also analyze samples from urine, blood or nasal swabs.Story Source:Materials provided by McGill University. ",0.101574,0.130162,0.177848,0.230265,0.141437,0.167708,0.18633,0,0
557,Soundwave-surfing droplets leave no traces New digital microfluidics platform uses soundwaves and oil to avoid contamination for reusable lab-on-a-chip devices,"Engineers at Duke University have developed a way to manipulate, split and mix droplets of biological fluids by having them surf on acoustic waves in oil. The technology could form the basis of a small-scale, programmable, rewritable biomedical chip that is completely reusable for disparate purposes from on-site diagnostics to laboratory-based research.The study appears on July 26 in the journal Nature Communications.Automated fluid handling has driven the development of many scientific fields. Robotic pipetting systems have, for example, revolutionized the preparation of sequencing libraries, clinical diagnostics and large-scale compound screening. While ubiquitous in the modern biomedical research and pharmaceutical industries, these systems are bulky, expensive and do not handle small volumes of liquids well.Lab-on-a-chip systems have been able to fill this space to some extent, but most are hindered by one major drawback -- surface absorption. Because these devices rely on solid surfaces, the samples being transported inevitably leave traces of themselves that can lead to contamination.""There are a lot of protein-laden fluids and certain reagents that tend to stick to the chips that are handling them,"" said Tony Jun Huang, the William Bevan Professor of Mechanical Engineering and Materials Science at Duke. ""This is especially true of biological samples like undiluted blood, sputum and fecal samples. Our technology is well-suited for processing these difficult samples.""The new lab-on-a-chip platform uses a thin layer of inert, immiscible oil to stop droplets from leaving behind any trace of themselves. Just below the oil, a series of piezoelectric transducers vibrate when electricity is passed through them. Just like the surface of a subwoofer, these vibrations create sound waves in the thin layer of oil above them.By carefully controlling the sound waves, the researchers create vertical vortexes that form small dimples in the oil to either side of the active transducer. These dimples can hold droplets with volumes ranging from one nanoliter to 100 microliters and pass them along the surface of the oil as the sound waves are modulated and different transducers are activated.The droplets are effectively surfing on tiny soundwaves.""Our contactless liquid-handling mechanism inherently eliminates cross-contamination associated with surface adsorption and the need for surface modification,"" Huang said. ""It enables reusable paths for the droplets to be dynamically processed on arbitrary routes without cross-talk between each other, exponentially increasing the allowable number of combinations of reagent inputs on the same device.""Huang next wants to take this proof-of-concept demonstration and create a fully automated lab-on-a-chip platform that can handle complex operations with dozens of droplets simultaneously. He's planning to collaborate with peers at Duke for various applications in biology and medicine.Story Source:Materials provided by Duke University. ",0.0965362,0.126308,0.140133,0.174293,0.140796,0.136894,0.160376,0,0
558,Engineers develop world's most efficient semiconductor for thermal management New material draws heat away from hotspots much faster than current materials,"Working to address ""hotspots"" in computer chips that degrade their performance, UCLA engineers have developed a new semiconductor material, defect-free boron arsenide, that is more effective at drawing and dissipating waste heat than any other known semiconductor or metal materials.This could potentially revolutionize thermal management designs for computer processors and other electronics, or for light-based devices like LEDs.The study was recently published in Science and was led by Yongjie Hu, UCLA assistant professor of mechanical and aerospace engineering.Computer processors have continued to shrink down to nanometer sizes where today there can be billions of transistors are on a single chip. This phenomenon is described under Moore's Law, which predicts that the number of transistors on a chip will double about every two years. Each smaller generation of chips helps make computers faster, more powerful and able to do more work. But doing more work also means they're generating more heat.Managing heat in electronics has increasingly become one of the biggest challenges in optimizing performance. High heat is an issue for two reasons. First, as transistors shrink in size, more heat is generated within the same footprint. This high heat slows down processor speeds, in particular at ""hotspots"" on chips where heat concentrates and temperatures soar. Second, a lot of energy is used to keep those processors cool. If CPUs did not get as hot in the first place, then they could work faster and much less energy would be needed to keep them cool.The UCLA study was the culmination of several years of research by Hu and his students that included designing and making the materials, predictive modeling, and precision measurements of temperatures.The defect-free boron arsenide, which was made for the first time by the UCLA team, has a record-high thermal conductivity, more than three-times faster at conducting heat than currently used materials, such as silicon carbide and copper, so that heat that would otherwise concentrate in hotspots is quickly flushed away.""This material could help greatly improve performance and reduce energy demand in all kinds of electronics, from small devices to the most advanced computer data center equipment,"" Hu said. ""It has excellent potential to be integrated into current manufacturing processes because of its semiconductor properties and the demonstrated capability to scale-up this technology. It could replace current state-of-the-art semiconductor materials for computers and revolutionize the electronics industry.""The study's other authors are UCLA graduate students in Hu's research group: Joonsang Kang, Man Li, Huan Wu, and Huuduy Nguyen.In addition to the impact for electronic and photonics devices, the study also revealed new fundamental insights into the physics of how heat flows through a material.""This success exemplifies the power of combining experiments and theory in new materials discovery, and I believe this approach will continue to push the scientific frontiers in many areas, including energy, electronics, and photonics applications,"" Hu said.The research was funded by the National Science Foundation, the Air Force Office of Scientific Research, the American Chemical Society's Petroleum Research Fund, UCLA's Sustainable LA Grand Challenge, and the Anthony and Jeanne Pritzker Family Foundation.Story Source:Materials provided by UCLA Samueli School of Engineering. ",0.0674854,0.106507,0.112036,0.179694,0.127133,0.114662,0.143726,0,0
559,"Biosensor chip detects single nucleotide polymorphism wirelessly, with higher sensitivity ","A team led by the University of California San Diego has developed a chip that can detect a type of genetic mutation known as a single nucleotide polymorphism (SNP) and send the results in real time to a smartphone, computer, or other electronic device. The chip is at least 1,000 times more sensitive at detecting an SNP than current technology.The advance, published July 9 in Advanced Materials, could lead to cheaper, faster and portable biosensors for early detection of genetic markers for diseases such as cancer.An SNP is the change in a single nucleotide base (A, C, G or T) in the DNA sequence. It is the most common type of genetic mutation. While most SNPs have no discernible effect on health, some are associated with increased risk of developing pathological conditions such as cancer, diabetes, heart disease, neurodegenerative disorders, autoimmune and inflammatory diseases.Traditional SNP detection methods have several limitations: they have relatively poor sensitivity and specificity; they require amplification to get multiple copies for detection; they require the use of bulky instruments; and they cannot work wirelessly.The new DNA biosensor developed by the UC San Diego-led team is a wireless chip that's smaller than a fingernail and can detect an SNP that's present in picomolar concentrations in solution.""Miniaturized chip-based electrical detection of DNA could enable in-field and on-demand detection of specific DNA sequences and polymorphisms for timely diagnosis or prognosis of pending health crises, including viral and bacterial infection-based epidemics,"" said Ratnesh Lal, professor of bioengineering, mechanical engineering and materials science at the UC San Diego Jacobs School of Engineering.The chip essentially captures a strand of DNA containing a specific SNP mutation and then produces an electrical signal that is sent wirelessly to a mobile device. It consists of a graphene field effect transistor with a specially engineered piece of double stranded DNA attached to the surface. This piece of DNA is bent near the middle and shaped like a pair of tweezers. One side of these so-called ""DNA-tweezers"" codes for a specific SNP. Whenever a DNA strand with that SNP approaches, it binds to that side of the DNA-tweezers, opening them up and creating a change in electrical current that is detected by the graphene field effect transistor.The project is led by Lal and involves teams at the Institute of Engineering in Medicine at UC San Diego, Chinese Academy of Sciences in China, University of Pennsylvania, Max Planck Institute for Biophysical Chemistry in Germany, and Inner Mongolia Agricultural University in China.DNA strand displacementWhat drives this technology is a molecular process called DNA strand displacement -- when a DNA double helix exchanges one of its strands for a new complementary strand. In this case, the DNA-tweezers swap one their strands for one with a particular SNP.This is possible due to the particular way the DNA-tweezers are engineered. One of the strands is a ""normal"" strand that is attached to the graphene transistor and contains the complementary sequence for a specific SNP. The other is a ""weak"" strand in which some of the nucleotides are replaced with a different molecule to weaken its bonds to the normal strand. A strand containing the SNP is able to bind more strongly to the normal strand and displace the weak strand. This leaves the DNA-tweezers with a net electric charge that can be easily detected by the graphene transistor.New and improved SNP detection chipThis work builds upon the first label- and amplification-free electronic SNP detection chip that Lal's team previously developed in collaboration with Gennadi Glinksy, a research scientist at the UC San Diego Institute of Engineering in Medicine, and other UC San Diego researchers. The new chip has added wireless capability and is at least 1,000 times more sensitive than its predecessor.What makes the new chip so sensitive is the design of the DNA-tweezers. When the SNP-containing strand binds, it opens up the DNA-tweezers, changing their geometry so that they become almost parallel to the graphene surface. This brings the net electric charge of the DNA close to the graphene surface, giving a larger signal. In contrast, the DNA probe built into the previous chip has a structure that cannot be brought closer to the graphene surface, so it generates a weaker signal upon binding an SNP-containing strand.Next steps include designing array chips to detect up to hundreds of thousands of SNPs in a single test. Future studies will involve testing the chip on blood and other bodily fluid samples taken from animals or humans.Story Source:Materials provided by University of California - San Diego. ",0.121302,0.173665,0.176941,0.224445,0.19757,0.169797,0.196874,0,0
560,Closing the gap: On the road to terahertz electronics Asymmetric plasmonic antennas deliver femtosecond pulses for fast optoelectronics,"A team headed by the TUM physicists Alexander Holleitner and Reinhard Kienberger has succeeded for the first time in generating ultrashort electric pulses on a chip using metal antennas only a few nanometers in size, then running the signals a few millimeters above the surface and reading them in again a controlled manner.Classical electronics allows frequencies up to around 100 gigahertz. Optoelectronics uses electromagnetic phenomena starting at 10 terahertz. This range in between is referred to as the terahertz gap, since components for signal generation, conversion and detection have been extremely difficult to implement.The TUM physicists Alexander Holleitner and Reinhard Kienberger succeeded in generating electric pulses in the frequency range up to 10 terahertz using tiny, so-called plasmonic antennas and run them over a chip. Researchers call antennas plasmonic if, because of their shape, they amplify the light intensity at the metal surfaces.Asymmetric antennasThe shape of the antennas is important. They are asymmetrical: One side of the nanometer-sized metal structures is more pointed than the other. When a lens-focused laser pulse excites the antennas, they emit more electrons on their pointed side than on the opposite flat ones. An electric current flows between the contacts -- but only as long as the antennas are excited with the laser light.""In photoemission, the light pulse causes electrons to be emitted from the metal into the vacuum,"" explains Christoph Karnetzky, lead author of the Nature work. ""All the lighting effects are stronger on the sharp side, including the photoemission that we use to generate a small amount of current.""Ultrashort terahertz signalsThe light pulses lasted only a few femtoseconds. Correspondingly short were the electrical pulses in the antennas. Technically, the structure is particularly interesting because the nano-antennas can be integrated into terahertz circuits a mere several millimeters across.In this way, a femtosecond laser pulse with a frequency of 200 terahertz could generate an ultra-short terahertz signal with a frequency of up to 10 terahertz in the circuits on the chip, according to Karnetzky.The researchers used sapphire as the chip material because it cannot be stimulated optically and, thus, causes no interference. With an eye on future applications, they used 1.5-micron wavelength lasers deployed in traditional internet fiber-optic cables.An amazing discoveryHolleitner and his colleagues made yet another amazing discovery: Both the electrical and the terahertz pulses were non-linearly dependent on the excitation power of the laser used. This indicates that the photoemission in the antennas is triggered by the absorption of multiple photons per light pulse.""Such fast, nonlinear on-chip pulses did not exist hitherto,"" says Alexander Holleitner. Utilizing this effect he hopes to discover even faster tunnel emission effects in the antennas and to use them for chip applications.The experiments were funded by the European Research Council (ERC) as part of the ""NanoREAL"" project and the DFG Cluster of Excellence ""Nanosystems Initiative Munich"" (NIM).Story Source:Materials provided by Technical University of Munich (TUM). ",0.141143,0.150144,0.136219,0.182087,0.228758,0.192318,0.158873,0,0
561,"Automated robotic device for faster blood testing New technology could speed hospital work, enhance health care","Rutgers researchers have created an automated blood drawing and testing device that provides rapid results, potentially improving the workflow in hospitals and other health-related institutions to allow health care practitioners to spend more time treating patients.A study describing the fully automated device is published online in the journal TECHNOLOGY.""This device represents the holy grail in blood testing technology,"" said Martin L. Yarmush, senior author of the study and Paul & Mary Monroe Endowed Chair & Distinguished Professor in the Department of Biomedical Engineering at Rutgers University-New Brunswick. ""Integrating miniaturized robotic and microfluidic (lab-on-a-chip) systems, this technology combines the breadth and accuracy of traditional blood drawing and laboratory testing with the speed and convenience of point-of-care testing.""Diagnostic blood testing is the most commonly performed clinical procedure in the world, and it influences most of the medical decisions made in hospitals and laboratories. But the success rate of manually drawing blood samples depends on clinicians' skill and patient physiology, and nearly all test results come from centralized labs that handle large numbers of samples and use labor-intensive analytical techniques.So, a Rutgers biomedical engineering research team created a device that includes an image-guided robot for drawing blood from veins, a sample-handling module and a centrifuge-based blood analyzer. Their device provides highly accurate results from a white blood cell test, using a blood-like fluid spiked with fluorescent microbeads. The testing used artificial arms with plastic tubes that served as blood vessels. The device could provide rapid test results at bedsides or in ambulances, emergency rooms, clinics and doctors' offices.""When designing the system, our focus was on creating a modular and expandable device,"" said Max Balter, who led the study and holds a doctorate in biomedical engineering from Rutgers. ""With our relatively simple chip design and analysis techniques, the device can be extended to incorporate a broader panel of tests in the future.""Story Source:Materials provided by Rutgers University. ",0.118493,0.145427,0.182533,0.244325,0.161949,0.188139,0.206022,0,0
562,New laser makes silicon 'sing',"Yale scientists have created a new type of silicon laser that uses sounds waves to amplify light. A study about the discovery appears June 8 in the online edition of the journal Science.In recent years, there has been increasing interest in translating optical technologies -- such as fiber optics and free-space lasers -- into tiny optical or ""photonic"" integrated circuits. Using light rather than electricity for integrated circuits permits sending and processing information at speeds that would be impossible with conventional electronics. Researchers say silicon photonics -- optical circuits based on silicon chips -- are one of the leading platforms for such technologies, thanks to their compatibility with existing microelectronics.""We've seen an explosion of growth in silicon photonic technologies the past few of years,"" said Peter Rakich, an associate professor of applied physics at Yale who led the research. ""Not only are we beginning to see these technologies enter commercial products that help our data centers run flawlessly, we also are discovering new photonic devices and technologies that could be transformative for everything from biosensing to quantum information on a chip. It's really an exciting time for the field.""The researchers said this rapid growth has created a pressing need for new silicon lasers to power the new circuits -- a problem that has been historically difficult due to silicon's indirect bandgap. ""Silicon's intrinsic properties, although very useful for many chip-scale optical technologies, make it extremely difficult to generate laser light using electrical current,"" said Nils Otterstrom, a graduate student in the Rakich lab and the study's first author. ""It's a problem that's stymied scientists for more than a decade. To circumvent this issue, we need to find other methods to amplify light on a chip. In our case, we use a combination of light and sound waves.""The laser design corrals amplified light within a racetrack shape -- trapping it in circular motion. ""The racetrack design was a key part of the innovation. In this way, we can maximize the amplification of the light and provide the feedback necessary for lasing to occur,"" Otterstrom said.To amplify the light with sound, the silicon laser uses a special structure developed in the Rakich lab. ""It's essentially a nanoscale waveguide that's is designed to tightly confine both light and sound waves and maximize their interaction,"" Rakich said.""What's unique about this waveguide is that there are two distinct channels for light to propagate,"" added Eric Kittlaus, a co-author of the study and a graduate student in the Rakich lab. ""This allows us to shape the light-sound coupling in a way that permits remarkably robust and flexible laser designs.""Without this type of structure, the researchers explained, amplification of light using sound would not be possible in silicon. ""We've taken light-sound interactions that were virtually absent in these optical circuits, and have transformed them into the strongest amplification mechanism in silicon,"" Rakich said. ""Now, we're able to use it for new types of laser technologies no one thought possible 10 years ago.""Otterstrom said there were two main challenges in developing the new laser: ""First, designing and fabricating a device where the amplification outpaces the loss, and then figuring out the counter-intuitive dynamics of this system,"" he said. ""What we observe is that while the system is clearly an optical laser, it also generates very coherent hypersonic waves.""The research team said these properties may lead to a number of potential applications ranging from integrated oscillators to new schemes for encoding and decoding information. ""Using silicon, we can create a multitude of laser designs, each with unique dynamics and potential applications,"" said co-author Ryan Behunin, an assistant professor at Northern Arizona University and a former member of the Rakich lab. ""These new capabilities dramatically expand our ability to control and shape light in silicon photonic circuits.""Story Source:Materials provided by Yale University. ",0.0806459,0.106846,0.119886,0.174281,0.173744,0.114696,0.138945,0,0
563,Scientists use RFID chips to track biological samples,"Radio frequency identification (RFID) chips are used today for everything from paying for public transit to tracking livestock to stopping shoplifters. But now, researchers in the U.S. and Japan want to use them for something else: keeping track of organoids, samples of human tissue that mimic pieces of organs and are grown from stem cells. The organoids the researchers embedded with RFID chips functioned normally and withstood extreme conditions, suggesting that they could be a useful way to organize and identify the large quantities of organoids that are often needed in experimental situations. The work appears May 31 in the journal iScience.""This kind of multidisciplinary approach potentially offers a disruptive way forward: the idea is to combine organoids with digital technologies to advance drug testing and transplantation,"" says senior author Takanori Takebe, a clinician and researcher at Cincinnati Children's Hospital Medical Center, Tokyo Medical and Dental University, and Yokohama City University.Human organoids are a promising avenue for research into human development and disease because they replicate the structure, function, and phenotype of our organs in miniature in the lab. Grown from human induced pluripotent stem cells, they divide, differentiate, and self-assemble according to the growth programs of their corresponding organs. And particularly in medicine, they can illustrate the effects of certain drugs on our organs in ways that more traditional cell cultures cannot.The idea of embedding microchips into human organoids seemed like a natural fit to Takebe, who has worked extensively with RFID chips in health contexts. The chips could be used to sense, record, and track interesting changes live in large quantities of organoids at once -- and because cells self-assemble into 3D structures during the growth process of an organoid, he thought it might be possible for the microchips to be naturally integrated into the organoids as they were growing. ""Introducing the chips by forceful methods like injection is extremely toxic to organoids, so we leveraged the organoid's natural self-cavitation power to integrate the microchips so that we could prevent tissue damage and destruction,"" he says.To test this procedure, he and his team grew hybrid liver organoids containing cheap, commercially available RFID chips the size of grains of sand. They found that 95% of their 96 test organoids successfully incorporated the chip. The organoids were undamaged by the procedure: they were shaped normally, secreted normal liver proteins, and transported bile as expected. ""There were almost no differences, surprisingly,"" says Takebe.The RFID chips, which are known for their durability, also functioned as expected. Hybrid organoids grown from stem cells of donors with fatty liver disease could be identified by RFID in a pool of organoids from a variety of donors. And the chips also stood up to a number of tests of the kinds of conditions they might need to survive in order to be useful in research: they and their organoids still functioned normally after being frozen and thawed for cryopreservation, at temperatures of nearly minus 200 degrees Celsius, after being embedded in paraffin, and at a range of different pHs.Takebe acknowledges that there are still limitations with this approach. More work needs to be done to scale up the production of these hybrid organoids, and he and his team are currently working to develop a system that could scan the radio frequency and fluorescence of an organoid at the same time. He also hopes that other kinds of microchips could be integrated into organoids in the future and that RFID chips with sensing technologies could be used to record real-time data about the organoids. ""My lab's focus is completely biological, so some of these challenges are things we cannot resolve alone. But with collaboration between experts in different fields, and especially given how rapidly technology is evolving, I believe that we can and will solve them,"" he says.This work was supported by the Cincinnati Children's Research Foundation, the Digestive Disease Research Core Center at CCHMC, the Just-In-Time Core Grant Program at CCTST, a PRESTO grant from Japan Science and Technology Agency (JST), the State of Ohio, the Ohio Development Services Agency, Ohio Third Frontier, and Cincinnati Children's Hospital Medical Center's Innovation Fund.Story Source:Materials provided by Cell Press. ",0.0635525,0.0998229,0.134383,0.19639,0.117694,0.110305,0.144425,0,0
564,Scientists use a photonic quantum simulator to make virtual movies of molecules vibrating,"Scientists have shown how an optical chip can simulate the motion of atoms within molecules at the quantum level, which could lead to better ways of creating chemicals for use as pharmaceuticals.An optical chip uses light to process information, instead of electricity, and can operate as a quantum computing circuit when using single particles of light, known as photons. Data from the chip allows a frame-by-frame reconstruction of atomic motions to create a virtual movie of a molecule's quantum vibrations, which is what lies at the heart of the research published today in Nature.These findings are the result of a collaboration between researchers at the University of Bristol, MIT, IUPUI, Nokia Bell Labs, and NTT. As well as paving the way for more efficient pharmaceutical developments, the research could prompt new methods of molecular modelling for industrial chemists.When lasers were invented in the 1960s, experimental chemists had the idea of using them to break apart molecules. However, the vibrations within molecules rapidly redistribute the laser energy before the intended molecular bond is broken. Controlling the behaviour of molecules requires an understanding of how they vibrate at the quantum level. But modelling these dynamics requires massive computational power, beyond what we can expect from coming generations of supercomputers.The Quantum Engineering and Technology Labs at Bristol have pioneered the use of optical chips, controlling single photons of light, as basic circuitry for quantum computers. Quantum computers are expected to be exponentially faster than conventional supercomputers at solving certain problems. Yet constructing a quantum computer is a highly challenging long-term goal.As reported in Nature, the team demonstrated a new route to molecular modelling that could become an early application of photonic quantum technologies. The new methods exploit a similarity between the vibrations of atoms in molecules and photons of light in optical chips.Bristol physicist Dr Anthony Laing, who led the project, explained: ""We can think of the atoms in molecules as being connected by springs. Across the whole molecule, the connected atoms will collectively vibrate, like a complicated dance routine. At a quantum level, the energy of the dance goes up or down in well-defined levels, as if the beat of the music has moved up or down a notch. Each notch represents a quantum of vibration.""Light also comes in quantised packets called photons. Mathematically, a quantum of light is like a quantum of molecular vibration. Using integrated chips, we can control the behaviour of photons very precisely. We can program a photonic chip to mimic the vibrations of a molecule.""We program the chip, mapping its components to the structure of a particular molecule, say ammonia, then simulate how a particular vibrational pattern evolves over some time interval. By taking many time intervals, we essentially build up a movie of the molecular dynamics.""First author Dr Chris Sparrow, who was a student on the project, spoke of the simulator's versatility: ""The chip can be reprogrammed in a few seconds to simulate different molecules. In these experiments we simulated the dynamics of ammonia and a type of formaldehyde, and other more exotic molecules. We simulated a water molecule reaching thermal equilibrium with its environment, and energy transport in a protein fragment.""In this type of simulation, because time is a controllable parameter, we can immediately jump to the most interesting points of the movie. Or play the simulation in slow motion. We can even rewind the simulation to understand the origins of a particular vibrational pattern.""Joint first author, Dr Enrique Marten-Lopez, now a Senior Researcher with Nokia Bell Labs, added: ""We were also able to show how a machine learning algorithm can identify the type of vibration that best breaks apart an ammonia molecule. A key feature of the photonic simulator that enables this is its tracking of energy moving through the molecule, from one localised vibration to another. Developing these quantum simulation techniques further has clear industrial relevance.""The photonic chip used in the experiments was fabricated by Japanese Telecoms company NTT.Dr Laing explained the main directions for the future of the research: ""Scaling up the simulators to a size where they can provide an advantage over conventional computing methods will likely require error correction or error mitigation techniques. And we want to further develop the sophistication of molecular model that we use as the program for the simulator. Part of this study was to demonstrate techniques that go beyond the standard harmonic approximation of molecular dynamics. We need to push these methods to increase the real-world accuracy of our models.""This approach to quantum simulation uses analogies between photonics and molecular vibrations as a starting point. This gives us a head start in being able to implement interesting simulations. Building on this, we hope that we can realise quantum simulation and modelling tools that provide a practical advantage in the coming years.""Story Source:Materials provided by University of Bristol. ",0.0587755,0.114516,0.0958457,0.183389,0.112777,0.10826,0.145034,0,0
565,Lung-on-a-chip simulates pulmonary fibrosis New biotech could reduce time and cost of developing medicine for deadly lung disease,"Developing new medicines to treat pulmonary fibrosis, one of the most common and serious forms of lung disease, is not easy.One reason: it's difficult to mimic how the disease damages and scars lung tissue over time, often forcing scientists to employ a hodgepodge of time-consuming and costly techniques to assess the effectiveness of potential treatments.Now, new biotechnology reported in the journal Nature Communications could streamline the drug-testing process.The innovation relies on the same technology used to print electronic chips, photolithography. Only instead of semiconducting materials, researchers placed upon the chip arrays of thin, pliable lab-grown lung tissues -- in other words, its lung-on-a-chip technology.""Obviously it's not an entire lung, but the technology can mimic the damaging effects of lung fibrosis. Ultimately, it could change how we test new drugs, making the process quicker and less expensive,"" says lead author Ruogang Zhao, PhD, assistant professor in the Department of Biomedical Engineering at the University at Buffalo.The department is a multidisciplinary unit formed by UB's School of Engineering and Applied Sciences and the Jacobs School of Medicine and Biomedical Sciences at UB.With limited tools for fibrosis study, scientists have struggled to develop medicine to treat the disease. To date, there are only two drugs -- pirfenidone and nintedanib -- approved by the U.S. Food and Drug Administrations that help slow its progress.However, both drugs treat only one type of lung fibrosis: idiopathic pulmonary fibrosis. There are more than 200 types of lung fibrosis, according to the American Lung Association, and fibrosis also can affect other vital organs, such as the heart, liver and kidney.Furthermore, the existing tools do not simulate the progression of lung fibrosis over time -- a drawback that has made the development of medicine challenging and relatively expensive. Zhao's research team, which included past and present students, as well as a University ofToronto collaborator, created the lung-on-a-chip technology to help address these issues.Using microlithography, the researchers printed tiny, flexible pillars made of a silicon-based organic polymer. They then placed the tissue, which acts like alveoli (the tiny air sacs in the lungs that allow us to consume oxygen), on top of the pillars.Researchers induced fibrosis by introducing a protein that causes healthy lung cells to become diseased, leading to the contraction and stiffening of the engineered lung tissue. This mimics the scarring of the lung alveolar tissue in people who suffer from the disease.The tissue contraction causes the flexible pillars to bend, allowing researchers to calculate the tissue contraction force based on simple mechanical principles.Researchers tested the system's effectiveness with pirfenidone and nintedanib. While each drug works differently, the system showed the positive results for both, suggesting the lung-on-a-chip technology could be used to test a variety of potential treatments for lung fibrosis.The research was supported by the National Institutes of Health; the UB School of Engineering and Applied Sciences; the Jacobs School of Medicine and Biomedical Sciences at UB; and the Clinical and Translational Science Institute at UB.Story Source:Materials provided by University at Buffalo. Original written by Cory Nealon. ",0.137408,0.216332,0.203539,0.246581,0.202028,0.18453,0.232252,0,0
566,First chip-scale broadband optical system that can sense molecules in mid-infrared range System could lead to a spectroscopy lab-on-a-chip for real-time sensing in microseconds,"Researchers at Columbia Engineering have demonstrated, for the first time, a chip-based dual-comb spectrometer in the mid-infrared range, that requires no moving parts and can acquire spectra in less than 2 microseconds. The system, which consists of two mutually coherent, low-noise, microresonator-based frequency combs spanning 2600 nm to 4100 nm, could lead to the development of a spectroscopy lab-on-a-chip for real-time sensing on the nanosecond time scale.""Our results show the broadest optical bandwidth demonstrated for dual-comb spectroscopy on an integrated platform,"" said Alexander Gaeta, David M. Rickey Professor of Applied Physics and of Materials Science and senior author of the study, published May 14 in Nature Communications.Creating a spectroscopic sensing device on a chip that can realize real-time, high-throughput detection of trace molecules has been challenging. A few months ago, teams led by Gaeta and Michal Lipson, Higgins Professor of Electrical Engineering, were the first to miniaturize dual-frequency combs by putting two frequency comb generators on a single millimeter-sized chip. They have been working on broadening the frequency span of the dual combs, and on increasing the resolution of the spectrometer by tuning the lines of the comb.In this current study, the researchers focused on the mid-infrared (mid-IR) range, which, because its strong molecular absorption is typically 10 to 1,000 times greater than those in the visible or near-infrared, is ideal for detecting trace molecules. The mid-IR range effectively covers the ""fingerprint"" of many molecules.The team performed mid-IR dual-comb spectroscopy using two silicon nanophotonic devices as microresonators. Their integrated devices enabled the direct generation of broadband mid-infrared light and fast acquisition speeds for characterizing molecular absorption.""Our work is a critical advance for chip-based dual-comb spectroscopy for liquid/solid phase studies,"" said Mengjie Yu, lead author of the paper and a PhD student in Gaeta's lab. ""Our chip-scale broadband optical system, essentially a photonic lab-on-a-chip, is well-suited for identification of chemical species and could find a wide range of applications in chemistry, biomedicine, material science, and industrial process control.""Story Source:Materials provided by Columbia University School of Engineering and Applied Science. ",0.115208,0.168836,0.126392,0.216807,0.201392,0.160393,0.17796,0,0
567,Strain directs spin waves Contributing to ultra-low power spin wave ICs,"It is becoming harder to respond to the demands of the rapidly-growing information society due to failures caused by increased chip temperatures in the ever-more integrated chips used by the latest electronic devices based on semiconductor materials. Therefore, the development of spin wave integrated circuits (ICs), which can perform information processing with minimal generation of heat by manipulating spin only rather than moving electrons, has been gaining attention. Within this field, spin waves transmitted through a magnetic insulator film have the advantage that energy loss is small and long-distance transmission is possible. On the other hand, in order to transmit spin waves within a magnetic insulator film, it was previously necessary to attach relatively large permanent magnet parts to the magnetic insulator film, which was a problem for realizing spin wave ICs.Taichi Goto at the Toyohashi University of Technology and Caroline Ross of the Massachusetts Institute of Technology and others collaborated to create a single-crystalline yttrium iron garnet (YIG) film, that is well-known as a magnetic insulator, on multiple substrates, and transmit the spin waves. Then, the influence of the magnitude of the stress in the magnetic insulator film on a spin wave was studied. As a result, it was revealed that if the stress magnitude is large, spin waves are transmitted even if the attached permanent magnets are small (or weak). This is because if there is stress in the magnetic insulator film, it has the same effect as placing weak permanent magnets in close proximity.According to Assistant Professor Goto, ""YIG is one of the most noteworthy materials of late, and new devices and new phenomena using this technology, including spin waves, are being discovered one after another. Among these discoveries, we are leading the world in developing spin wave ICs using YIG. In the past, the relationship between the static magnetic response created by stress and the dynamic response that indicates the behavior of spin waves in YIG film was not well understood. This important development piece was what we wanted to put in place with this research.""Takuya Yoshimoto, a Research Fellow of the Japan Society for the Promotion of Science (JSPS), who worked on forming the samples, commented, ""This research has yielded an equation that represents the relationship between stress and spin waves in magnetic insulator films. This is not only a very important step towards the realization of spin wave ICs but also accelerates the R&D on high frequency magnetic properties in the GHz band including spin waves and magnetic materials in the nano and micro scales.""In this research, a YIG thin film with a thickness of about 100 nm was formed on three garnet substrates with the same garnet structure as YIG but different lattice constants using pulsed laser deposition, and was used to investigate crystal structure, crystal strain, and stress magnitude. A pair of electrodes for exciting and detecting spin waves was formed on the fabricated YIG using electron beam lithography, and the relationship between the external magnetic field and the propagation frequency of the spin wave was measured. The dispersion equation of the spin wave including the change in magnetic anisotropy due to crystal strain was calculated, and it was confirmed that the calculated results were almost equal to the measured results. Also, by changing the magnitude of the generated strain, the size of the magnet required to excite the spin wave was able to be reduced by about 2.5 times compared to the case without strain. As a result, the whole spin wave IC can be miniaturized, and the device can be fabricated on a chip. In the future, the research team will apply the spin wave multi-input/output phase interference device of this technique to real spin wave devices, with the initial aim of demonstrating the function of a spin wave IC fabricated on a chip.Story Source:Materials provided by Toyohashi University of Technology. ",0.141191,0.139784,0.155774,0.172668,0.178057,0.229618,0.168048,0,0
568,New 3D printer can create complex biological tissues Device could help advance regenerative medicine,"A UCLA Samueli-led team has developed a specially adapted 3D printer to build therapeutic biomaterials from multiple materials. The advance could be a step toward on-demand printing of complex artificial tissues for use in transplants and other surgeries.""Tissues are wonderfully complex structures, so to engineer artificial versions of them that function properly, we have to recreate their complexity,"" said Ali Khademhosseini, who led the study and is UCLA's Levi James Knight, Jr., Professor of Engineering at the UCLA Samueli School of Engineering. ""Our new approach offers a way to build complex biocompatible structures made from different materials.""The study was published in Advanced Materials.The technique uses a light-based process called stereolithography, and it takes advantage of a customized 3D printer designed by Khademhosseini that has two key components. The first is a custom-built microfluidic chip -- a small, flat platform similar in size to a computer chip -- with multiple inlets that each ""prints"" a different material. The other component is a digital micromirror, an array of more than a million tiny mirrors that each moves independently.The researchers used different types of hydrogels -- materials that, after passing through the printer, form scaffolds for tissue to grow into. The micromirrors direct light onto the printing surface, and the illuminated areas indicate the outline of the 3D object that's being printed. The light also triggers molecular bonds to form in the materials, which causes the gels to firm into solid material. As the 3D object is printed, the mirror array changes the light pattern to indicate the shape of each new layer.The process is the first to use multiple materials for automated stereolithographic bioprinting -- an advance over conventional stereolithographic bioprinting, which only uses one type of material. While the demonstration device used four types of bio-inks, the study's authors write that the process could accommodate as many inks as needed.The researchers first used the process to make simple shapes, such as pyramids. Then, they made complex 3D structures that mimicked parts of muscle tissue and muscle-skeleton connective tissues. They also printed shapes mimicking tumors with networks of blood vessels, which could be used as biological models to study cancers. They tested the printed structures by implanting them in rats. The structures were not rejected.Story Source:Materials provided by UCLA Samueli School of Engineering. ",0.0807112,0.128252,0.131898,0.193124,0.142035,0.130641,0.171664,0,0
569,Waterloo chemists create faster and more efficient way to process information Moore's Law extended with the use of light,"University of Waterloo chemists have found a much faster and more efficient way to store and process information by expanding the limitations of how the flow of electricity can be used and managed.In a recently released study, the chemists discovered that light can induce magnetization in certain semiconductors -- the standard class of materials at the heart of all computing devices today.""These results could allow for a fundamentally new way to process, transfer, and store information by electronic devices, that is much faster and more efficient than conventional electronics.""For decades, computer chips have been shrinking thanks to a steady stream of technological improvements in processing density. Experts have, however, been warning that we'll soon reach the end of the trend known as Moore's Law, in which the number of transistors per square inch on integrated circuits double every year.""Simply put, there's a physical limit to the performance of conventional semiconductors as well as how dense you can build a chip,"" said Pavle Radovanovic, a professor of chemistry and a member of the Waterloo Institute for Nanotechnology. ""In order to continue improving chip performance, you would either need to change the material transistors are made of -- from silicon, say to carbon nanotubes or graphene -- or change how our current materials store and process information.""Radovanovic's finding is made possible by magnetism and a field called spintronics, which proposes to store binary information within an electron's spin direction, in addition to its charge and plasmonics, which studies collective oscillations of elements in a material.""We've basically magnetized individual semiconducting nanocrystals (tiny particles nearly 10,000 times smaller than the width of a human hair) with light at room temperature,"" said Radovanovic. ""It's the first time someone's been able to use collective motion of electrons, known as plasmon, to induce a stable magnetization within such a non-magnetic semiconductor material.""In manipulating plasmon in doped indium oxide nanocrystals Radovanovic's findings proves that the magnetic and semiconducting properties can indeed be coupled, all without needing ultra-low temperatures (cryogens) to operate a device.He anticipates the findings could initially lead to highly sensitive magneto-optical sensors for thermal imaging and chemical sensing. In the future, he hopes to extend this approach to quantum sensing, data storage, and quantum information processing.Story Source:Materials provided by University of Waterloo. ",0.0621413,0.111601,0.0939083,0.144838,0.121909,0.124542,0.128249,0,0
570,Chemical sensing chip sniffs out cocaine within minutes New technology could one day lead to portable drug detectors,"What if you could test for cocaine, opioids and marijuana as quickly as a breathalyzer identifies alcohol?A new, low-cost chemical sensing chip brings us one step closer to this technology, which has long been on the wish list of police officers and others looking to monitor drug use and curb dangerous driving.The chip could be integrated into a handheld, portable device for detecting drugs in biological samples such as blood, breath, urine or spit.""Currently, there is a great demand for on-site drug testing,"" says Qiaoqiang Gan, PhD, associate professor of electrical engineering in the University at Buffalo School of Engineering and Applied Sciences. ""The high-performance chip we designed was able to detect cocaine within minutes in our experiments. It's also inexpensive: It can be produced using raw materials that cost around 10 cents, and the fabrication techniques we used are also low-cost.""""In the future, we are hoping to also use this technology to detect other drugs, including marijuana,"" he adds. ""The widening legalization of marijuana raises a lot of societal issues, including the need for a system to quickly test drivers for drug use.""Gan developed the new chip with a team that included first authors Jun Gao, a research associate of Material Sciences at Fudan University in China, and Nan Zhang, a PhD candidate at UB, along with colleagues from the UB Department of Electrical Engineering; the UB Research Institute on Addictions; and the UB Department of Community Health and Health Behavior in the UB School of Public Health and Health Professions.The study, funded by the National Science Foundation, was published on May 7 in the journal Small Methods.Detecting the chemical fingerprint of cocaine and other drugsThe new chip is an engineered nanostructure that traps light at the edges of gold and silver nanoparticles. When biological or chemical molecules land on the chip's surface, some of the captured light interacts with the molecules and is ""scattered"" into light of new energies. This effect occurs in recognizable patterns that act as fingerprints, revealing information about what compounds are present.Because all chemicals -- cocaine, opioids, and active ingredients in marijuana -- have their unique light-scattering signatures, researchers can use the technology to quickly identify a wide range of chemicals.This sensing method is called surface-enhanced Raman spectroscopy (SERS), and it's not new. But the chip that Gan's team developed is noteworthy for its high performance and low cost.""SERS holds a lot of promise for rapid detection of drugs and other chemicals, but the materials required to perform the sensing are usually quite expensive,"" Zhang says. ""The chips used for SERS are typically fabricated using expensive methods, such as lithography, which creates specific patterns on a metal substrate. We created our chip by depositing various thin layers of materials on a glass substrate, which is cost-effective and suitable for industrial-scale production.""An optical layer cakeThe new chip -- a structure known as a metasurface -- resembles a layer cake, with several horizontal layers of material sitting atop one another.Specifically, the technology consists of a sheet of dielectric material (e.g. silicon dioxide, aluminum oxide, etc.) sandwiched between a silver mirror (the base of the chip) and a hybrid nanomaterial made from gold and silver nanoparticles (the chip's active surface).This architecture is ideal for SERS: Molecules of cocaine or other substances fall into the tiny spaces between the nanoparticles on the chip's surface. Then, when the structure is exposed to light for testing, the silver mirror and the dielectric layer act as an ""optical cavity,"" manipulating light in a way that increases the number of photons at the surface of the chip. Ultimately, this intensifies the scattering signature of compounds being sensed, which improves detection.The technology has a long shelf life, with researchers finding that it performed well after a year in storage. The chip's longevity is due in part to its surface design: The gold nanoparticles, which are deposited last, help to shield the silver nanoparticles from the air, which prevents oxidization, degradation and tarnishing.""With our structure, we can realize both high performance and stable performance over time,"" Gan says.The next step in the research is to install the chip in a simple, portable testing device. This technology would first run blood, breath, urine or saliva through a purification process that extracts specific molecules, such as cocaine or other drugs. Then, any chemicals captured through this procedure would be transferred to the chip for detection and identification.Story Source:Materials provided by University at Buffalo. ",0.0716274,0.143027,0.122802,0.172424,0.140534,0.110704,0.146501,0,0
571,A prosthetic arm that decodes phantom limb movements,"About 75% of amputees exhibit mobility of their phantom limb. Using this information, in collaboration with physicians, researchers from CNRS and Aix-Marseille University have developed a prototype capable of detecting these movements and activating a prosthetic arm. The prosthesis does not require any surgery and patients do not need training. The results are published on November 29, 2018 in Frontiers in Bioengineering and Biotechnology.Most amputees feel sensations where their missing limb used to be, hence the name ""phantom limb."" In a previous study, researchers had shown that more than 75% of amputees can voluntarily move their phantom limb. The execution of these ""phantom"" movements, such as for instance finger pinching, the hand making a fist, or the wrist rotating or flexing, are always associated with specific muscle contractions in the stump. In people with an upper arm amputation, these contractions concern muscle groups that are not connected with the joints used before the amputation, as if muscle reinnervation had spontaneously occurred, without surgery.The team of researchers has developed a natural approach of prosthesis control that exploits this phenomenon. For the prototype, the researchers created algorithms capable of recognizing muscle activity generated by mobilization of the phantom limb and reproduction of the detected movement by the prosthesis: intuitive control, without training or surgery.In the tests, two transhumeral amputees used this type of control to act with a prosthesis not worn but placed near their arm stump. The results were very encouraging. They showed that the participants could control the prosthesis and achieve the task after only a few minutes of familiarization with the system, despite long action periods. This research is very promising, since arm amputees often have difficulty in controlling their prostheses effectively, to a point that many give it up.The researchers are continuing their work by moving on to tests on prostheses being worn, while also contributing to increasing knowledge about the phenomenon of the phantom limb, whose mechanisms are not yet properly understood. These scientists have also shown the need to reconsider the phenomenon of phantom limbs, generally taboo, often attributed to mourning the lost limb, and mainly considered from the angle of pain.Story Source:Materials provided by CNRS. ",0.138684,0.149207,0.178633,0.27194,0.157526,0.213234,0.247894,0,0
572,Machine learning masters the fingerprint to fool biometric systems Synthetic fingerprints can spoof smartphone fingerprint sensors,"Fingerprint authentication systems are a widely trusted, ubiquitous form of biometric authentication, deployed on billions of smartphones and other devices worldwide. Yet a new study from New York University Tandon School of Engineering reveals a surprising level of vulnerability in these systems. Using a neural network trained to synthesize human fingerprints, the research team evolved a fake fingerprint that could potentially fool a touch-based authentication system for up to one in five people.Much the way that a master key can unlock every door in a building, these ""DeepMasterPrints"" use artificial intelligence to match a large number of prints stored in fingerprint databases and could thus theoretically unlock a large number of devices. The research team was headed by NYU Tandon Associate Professor of Computer Science and Engineering Julian Togelius and doctoral student Philip Bontrager, the lead author of the paper, who presented it at the IEEE International Conference of Biometrics: Theory, Applications and Systems, where it won the Best Paper Award.The work builds on earlier research led by Nasir Memon, professor of computer science and engineering and associate dean for online learning at NYU Tandon. Memon, who coined the term ""MasterPrint,"" described how fingerprint-based systems use partial fingerprints, rather than full ones, to confirm identity. Devices typically allow users to enroll several different finger images, and a match for any saved partial print is enough to confirm identity. Partial fingerprints are less likely to be unique than full prints, and Memon's work demonstrated that enough similarities exist between partial prints to create MasterPrints capable of matching many stored partials in a database. Bontrager and his collaborators, including Memon, took this concept further, training a machine-learning algorithm to generate synthetic fingerprints as MasterPrints. The researchers created complete images of these synthetic fingerprints, a process that has twofold significance. First, it is yet another step toward assessing the viability of MasterPrints against real devices, which the researchers have yet to test; and second, because these images replicate the quality of fingerprint images stored in fingerprint-accessible systems, they could potentially be used to launch a brute force attack against a secure cache of these images.""Fingerprint-based authentication is still a strong way to protect a device or a system, but at this point, most systems don't verify whether a fingerprint or other biometric is coming from a real person or a replica,"" said Bontrager. ""These experiments demonstrate the need for multi-factor authentication and should be a wake-up call for device manufacturers about the potential for artificial fingerprint attacks."" This research has applications in fields beyond security. Togelius noted that their Latent Variable Evolution method used here to generate fingerprints can also be used to make designs in other industries -- notably game development. The technique has already been used to generate new levels in popular video games.Story Source:Materials provided by NYU Tandon School of Engineering. ",0.0548662,0.0757524,0.103732,0.331278,0.100068,0.109628,0.161184,0,0
573,The subtle science of wok tossing,"Wok tossing is essential for making a good fried rice -- or so claim a group of researchers presenting new work at the American Physical Society's Division of Fluid Dynamics 71st Annual Meeting, which will take place Nov. 18-20 at the Georgia World Congress Center in Atlanta, Georgia.The researchers, led by David Hu, a fluid mechanics professor at the Georgia Institute of Technology, and his doctoral student Hungtang Ko, became interested in the physics of cooking and noticed that relatively little attention had been paid to Chinese cuisine. Seeking to satiate their curiosity, they focused on stir-fry, which lies at the heart of the Chinese culinary style, dating back at least to the Sui Dynasty, around A.D. 600. The team set out to isolate the key elements of this venerable cooking technique.As a first step, Ko videotaped experienced chefs as they made fried rice at two stir-fry restaurants in Taiwan. Restaurant customers assumed that Ko was making a TV show, never suspecting that he was engaged in a serious scientific investigation. After Ko returned to Georgia Tech, he and Hu carefully tracked the movements of the wok during the roughly two-minute cooking process, identifying continuously repeating cycles lasting about a third of a second each. They broke down each of these 0.32-second cycles into four distinct phases composed of two oscillating motions: a translational motion, in which the wok moves toward and away from the chef, and a rotational or ""seesaw"" motion, in which the wok is tilted backward and forward. These two periodic motions share the same frequency but are slightly out of phase.The net effect, said Hu, is like ""flipping pancakes or juggling with rice. The key thing is that the rice must leave the wok to cool, because the wok is very hot"" -- up to 1,200 degrees Celsius. So, the chef is constantly flinging the rice into the air, catching it, mixing it, and then flinging the ingredients again, until the concoction is cooked and perfectly browned but not burned.""Once we came up with a mathematical model for fried rice cooking,"" Ko said, ""we realized it could lend itself rather easily to a robotic design."" Previous attempts at stir-fry robots succeeded in mixing the ingredients by shaking or rotation, added Hu, ""but none were able to throw the rice, which meant they couldn't cook at the high temperatures needed to produce nicely carbonized grains.""Fried rice cooking is an extremely fast-paced and strenuous activity, with top chefs working at the limits of human capabilities. ""If there was an automated way of doing this,"" Hu noted, ""it could be very useful.""Story Source:Materials provided by American Physical Society. ",0.135311,0.17148,0.171695,0.215653,0.166486,0.190964,0.206009,0,0
574,"Prototype of robot dog nose Animal stem cells used to create an 'e-nose' for detecting explosives, drugs and other contraband","Every day, thousands of trained K9 dogs sniff out narcotics, explosives and missing people across the United States. These dogs are invaluable for security, but they're also very expensive and they can get tired.Duke researchers have taken the first steps toward building an artificial ""robot nose"" device made from living mouse cells that officers could use instead of dogs.The researchers have developed a prototype based on odor receptors grown from the genes of mice that respond to target odors, including the smells of cocaine and explosives. Their work appeared earlier this month in Nature Communications.It turns out, there are a couple of very big differences between testing things in a lab dish and testing them in an actual nose.""This idea of an artificial nose has been present for a long time,"" said senior study author Hiroaki Matsunami, a professor of molecular genetics and microbiology in the Duke School of Medicine. ""The receptors were identified in the 1990s, but there are significant technical hurdles to produce all these receptors and monitor the activity so that we can use that in an artificial device.""""E-noses"" that exist now use various chemical compounds to detect smells instead of receptor stem cells, Matsunami said. He said those devices are ""not as good as a trained dog.""""The idea is that by using the actual, living receptors, maybe we can develop a device similar to animals,"" Matsunami said. ""Nobody has achieved that yet, but this study is moving toward that goal.""Human, dog and mouse genomes contain around 20,000 genes, which contain instructions to create proteins that smell, taste, feel, move and do everything that our bodies do. About 5 percent of mouse genes have been identified as instructions to make odor receptors, Matsunami said. In contrast, humans only use about 2 percent of their genes to make odor receptors.""These animals invest a lot of resources for this purpose,"" Matsunami said. ""Mice and rats are very good smellers; we just don't use mice for detecting explosives in real life. There are some practical problems to do that.""The first step of the study was to identify the best odor receptors to respond to target odors like cocaine or marijuana.The researchers created a liquid medium primed with molecules that could light up from reactions. Next they copied about 80 percent of the odor receptors from mice, and mixed those receptors with seven target odor chemicals in the medium.They measured the resulting luminescence and chose the best-performing odor receptors for the second part of the study, which monitored receptor activation in real time.Previous research had done this by exposing selected receptors to odor chemicals in a liquid. But there are several differences between the petri dish and the nose. For one, we rarely submerge our noses into liquid baths of odor chemicals. Instead, our noses detect smells from wafting perfumes or stenches borne on the air. And our noses are full of mucus.So, for the second half of the study, which was supported by the National Institute of Health grants DC014423 and DC016224 and the Defense Advanced Research Project Agency RealNose Project, they attempted to mimic how we use our noses by exposing odorants to vapors and a few enzymes.The researchers tested the receptors they had identified against two odor vapors for this study.""We only tested two of them in the paper, but it's showing the proof of principle of how it can be used,"" Matsunami said.The researchers hope they can fine-tune the device to test all receptors against many different smells.""We have a panel of receptors so we can monitor how different receptors respond differently to various smells, including ones that are similar to each other in chemical structure or ones that might be related to real-world use, like something associated to explosives or drugs,"" Matsunami said.The researchers also tested various enzymes that one might find in mucus to see how they aided or impeded reactions. This process is more true-to-life than vapor molecules directly interacting with odor receptors.""You'd think when we smell a chemical, the chemical would bind to the chemical receptor in the nose, but actually it's not so simple,"" Matsunami said. ""When the chemical dissolves in the nasal mucus before binding to the receptor, it might be converted to another chemical by enzymes in the nasal mucus.""Mucus is an unknown frontier in understanding how we smell. Reconstructing the key components of nasal mucus may be the next step toward building an artificial nose, according to the paper.""It's not like our paper will be immediately applied to a portable device used in the airport soon, but this is an important step forward to show that it is possible,"" Matsunami said. ""We can more clearly see what kind of hurdles to pass in order for the community to create such a device.""Story Source:Materials provided by Duke University. Original written by Yen Duong. ",0.11897,0.180809,0.162862,0.244188,0.164758,0.16404,0.210196,0,0
575,Artificial sensor mimics human sense of touch,"A team of researchers have developed an artificial tactile sensor that mimics the ability of human skin to detect surface information, such as shapes, patterns and structures. This may be one step closer to making electronic devices and robots that can perceive sensations such as roughness and smoothness.""Mimicking the human senses is one of the most popular areas of engineering, but the sense of touch is notoriously difficult to replicate,"" says Kwonsik Shin, engineer at Korea's Daegu Gyeongbuk Institute of Science and Technology (DGIST) and lead author of the study published in IEEE/ASME Transactions on Mechatronics.Not only do humans simultaneously sense multiple features of their environment, such as pressure, temperature, vibration, tension and shear force, but we also detect psychological parameters such as roughness, smoothness, hardness and pain. Detecting precise surface information is a crucial first step towards replicating psychological sensations of touch.To tackle this challenge, DGIST researchers teamed up with colleagues from ASML Korea, Dongguk University-Seoul, Sungkyunkwan University and the University of Oxford. They developed a device capable of measuring surface textures with high accuracy. The sensor is made from piezoelectric materials -- highly sensitive materials that can generate electrical power as a response to applied stress. These materials have similar properties to skin.The new sensor has several advantages over existing artificial sensors. First, it can detect signals through both touch and sliding. This mimics the two ways humans sense surface characteristics: by poking it or running our fingers over it. Most artificial sensors use a single method. Second, it consists of an array of multiple receptors, meaning that sliding speed can be calculated using the time interval between two receptor signals and the distance between them. Most robot fingers use a single receptor, requiring an external speedometer.The researchers tested their sensor by pressing stamps shaped like a square, triangle or dome against the sensor surface. They also added soft material to the sensor to see if it could measure depth, thus sensing in three dimensions. The sensor produced different voltages depending on the shape of the stamp. The results show that the sensor has high spatial resolution and can represent the surface characteristics of certain objects, such as the width and pitch, with high accuracy. However, at present, the sensor cannot distinguish between shapes perfectly in 3D.In the future, the sensor could be incorporated into electronic devices, such as robots or smart phones, to improve their ability to detect surface textures.Story Source:Materials provided by DGIST (Daegu Gyeongbuk Institute of Science and Technology). ",0.0927441,0.105038,0.14261,0.173086,0.14997,0.143489,0.167847,0,0
576,Photo recognition that keeps personal interests private,"From just a quick snapshot on a smartphone, image recognition technology can provide a wealth of information to help shoppers find in-store bargains and inform tourists of the name of a landmark. But these photos may be giving away more information about users' preferences and tendencies than they want to share.Researchers at Osaka University have proposed an encryption-free framework for preserving users' privacy when they use photo-based information services.Artificial intelligence, such as deep learning, has dramatically improved the performance of image recognition. Users can send a photo to a server, which identifies the content using an image recognizer and returns relevant information. This is advantageous to shoppers, tourists, and others, but the results can disclose private information, such as a user's current location. The server can also use identifiers from the smartphone to link current results with past results to build a location history that contains even more private information: ""Photos reflect private aspects of their owner, such as interests, preferences, and tendencies,"" explains co-author Naoko Nitta, ""which can be leaked by web-based image recognition services. To address this problem, we developed an encryption-free framework for privacy-preserving image recognition called EnfPire.""To use the framework, the user extracts a feature from the photo. EnfPire transforms the feature before it's sent to the server. Because the server cannot uniquely identify the transformed image, it returns a set of candidates to the user, who compares them with the original feature using a simple recognizer. ""With our framework, the provider of the photo-recognition services is unable to receive enough information for unique image recognition, while the user obtains the correct recognition result and its related service information,"" says lead author Kazuaki Nakamura.EnfPire successfully abstracts location information, but this is not sufficient to protect the user's history, which could still be approximated from geographical relationships between results. So, the research team proposed a countermeasure by which dummy requests are automatically sent from the smartphone to the server, which returns results based on the dummy requests that are automatically removed from the device without the user being aware of the process. The dummy features are chosen carefully so that the server does not identify them as such.In real-world experiments, EnfPire degraded the server's recognition accuracy from 99.8% to 41.4%, but the user's accuracy was 86.9%. ""We expect this framework to make a major contribution to research, development, and application of safe and secure artificial intelligence,"" adds senior author Noboru Babaguchi.Story Source:Materials provided by Osaka University. ",0.0972052,0.111972,0.162436,0.385682,0.172682,0.142063,0.206584,0,0
577,"Simple, mass production of giant vesicles using a porous silicone material ","A technique to generate large amounts of giant vesicle (liposome) dispersion has been developed. The technique involves adsorbing a lipid into a silicone porous material resembling a ""marshmallow-like gel"" and then squeezing it out like a sponge by impregnating a buffer solution.This work was conducted by Dr. Gen Hayase (Assistant Professor) from the Tohoku University Frontier Research Institute for Interdisciplinary Sciences, and Dr. Shinichiro Nomura (Associate Professor) from the Graduate School of Engineering Robotics, Department of Robotics at Tohoku University. This simple method is expected to be applied not only as a tool for cell research requiring giant vesicles but also for medical and cosmetic purposes.Giant vesicle (GV), a liposome consisting of a phospholipid membrane, has a structure similar to live cells and is several micrometers in size. For this reason, it has been used for various biochemical experiments such as the introduction of functional molecules into living cells, and also made into a vessel of artificial cells and molecular robotic research.Many methods have been proposed for the preparation of GV dispersions, but in the lab, the method of producing large quantities of several hundred milliliters at one time has been limited. In 2011, Nomura discovered that GVs were generated by adsorbing lipid into porous polydimethylsiloxane (PDMS) and squeezing it in a buffer solution.The practical development of the GV generation method and improvement of its efficiency is achieved by using a pore-controlled silicone composition flexible macroporous material, which can be described as a ""marshmallow-like gel"" given its consistency.The microstructure of MG is composed of a skeleton with a diameter of several micrometers and a pore size of several tens of micrometers, and the fine skeleton surface has a smooth hydrophobic quality. When MG absorbs chloroform containing phospholipids and vacuum drying is done, phospholipids are thought to orient the hydrophobic group toward the pore side of the MG skeleton/hydrophilic group.If buffer solution is impregnated here, GV is formed in the pores immediately. The GV dispersion can be removed by squeezing out this MG with a buffer solution. ""We also found that the hydrophilic molecular group can be included in GV with high efficiency by just adding molecules to the buffer in advance"" said Dr. Hayase.As a demonstration, a marshmallow-like gel with lipid impregnated was packed into a commercially available ""coffee pot"" and pressed; GV dispersion was produced. Since it is possible to generate GV even with such a simple procedure, it is expected that the method will be useful as a way of GV generation.The MG is relatively easy to produce, and it is possible to generate vesicles many times with the same sample. This method might be useful as both an artificial cell research tool and also have applications for medical and cosmetic products that can be generated on the spot, such as DIY. The research team is now developing a simpler MG fabrication method, which will be announced in a year.Story Source:Materials provided by Tohoku University. ",0.0922606,0.193998,0.171043,0.180819,0.14793,0.14602,0.183009,0,0
578,Robotic arm may help to rehabilitate chronic stroke victims,"New research published in Frontiers in Neurology finds that robotic arm rehabilitation in chronic stroke patients with aphasia, the loss of ability to understand or express speech, may promote speech and language function recovery.Robotic arm rehabilitation is a commonly-used intervention for treating impaired motor function in the arm, wrist, or shoulder subsequent to stroke. The robotic arm rehabilitation in this study targeted the right arm, as the participants had each suffered a left hemisphere stroke leading to a deficit in motor function on their right side. Individuals with left hemisphere strokes affecting motor function are also likely to have deficits in speech and language processing, and the present study investigated whether those individuals may improve in their speech and language performance following treatment aimed at the domain of motor function. The research team observed small but consistent improvement on measures assessing speech articulation and overall language processing in aphasia.This research was led Adam Buchwald, Associate Professor of Communicative Sciences and Disorders at NYU Steinhardt's School of Culture, Education, and Human Development, as well as Carolyn Falconer-Horne, a recent Ph.D. graduate of the department.""While this is an initial finding that should be interpreted cautiously, it remains exciting to consider the possibility that stroke rehabilitation in one domain would improve performance in another domain, said Buchwald. ""It remains possible that some treatment approaches encourage plasticity and reorganization that can span multiple domains, most likely those with similar neural substrates. Further testing of these combined effects could lead to breakthroughs in our approach to stroke rehabilitation for individuals with complex deficits affecting mobility, speech and language processing, and other cognitive domains.""The research team was assembled following an initial observation by Weill Cornell Medicine's Dylan Edwards, Ph.D., P.T., that participants may be speaking better following 12 weeks of robotic arm treatment. Falconer-Horne, along with Buchwald and Edwards, designed a battery to evaluate possible changes in participant performance that formed the basis of the analyses in this paper.Story Source:Materials provided by New York University. ",0.112263,0.135755,0.187137,0.286287,0.151796,0.176765,0.238392,0,0
579,Elephant trunks form joints to pick up small objects. Research could be translated to robotics,"Understanding how elephants use their trunks to pick up small objects could lead to robots designed with flexible hands or grippers, according to a new study that includes Rochester Institute of Technology research.RIT scientist Scott Franklin collaborated with David Hu at Georgia Institute of Technology to understand how elephants apply force with their trunk to manipulate different sized objects. Hu led the study that appears in the Oct. 24 issue of the Journal of the Royal Society Interface.Elephants graze nearly non-stop and eat about 440 pounds of food per day, according to the authors. Elephant trunks, which lack bones, form kinks or joints to compress small pieces of food into an efficient bite-sized masses.Staff at Zoo Atlanta conducted videotaped experiments involving the zoo's popular 35-year-old female African elephant named ""Kelly."" Her handlers offered her rutabaga and carrot cubes of different sizes and piles of wheat bran on a force plate. The experiment measured the force the elephant applied to the piles of food and imaged the shape of her trunk when grabbing each object.Franklin studied the video and data provided by Hu's team that suggested the elephant applied more force when attempting to pick up a pile of small particles. The behavior had a characteristic shape Franklin recognized from his earlier work applying the weakest link theory to granular materials. The chance for a weakest link increases when a pile consists of many individual particles, and the elephant stabilized the weakest link in the mound of food by applying more force, according to Franklin.""It seems counterintuitive that you put more weight on something that you're trying to pick up, but the reason that makes sense is that when you push down on this food, because it's particulate, it forces all the food to squeeze together even more,"" said Franklin, a professor of physics in RIT's School of Physics and Astronomy. ""The more tightly the food is squeezed together, the more likely the friction between them will be enough to allow you to pick it up.""The elephant trunk could be a model for researchers designing robotic hands or grippers with the finesse to pick up different objects, varying in material and quantity, he said.""It is very difficult to develop a gripper that is flexible enough to pick up a variety of objects, for example a single pen or a pile of pens, or a cube of Jell-O,"" Franklin said. ""The elephant trunk is a single thing; it doesn't change shape at all, but it is able to pick up food of different sizes, weights and masses, so the idea is that this will give us insight and information into how nature has solved this problem of how to pick up multiple things and then we can try to reproduce it.""Story Source:Materials provided by Rochester Institute of Technology. ",0.113645,0.154217,0.175305,0.220011,0.151877,0.1773,0.19171,0,0
580,Small flying robots haul heavy loads,"A closed door is just one of many obstacles that poses no barrier to a new type of flying, micro, tugging robot called a FlyCroTug. Outfitted with advanced gripping technologies and the ability to move and pull on objects around it, two FlyCroTugs can jointly lasso the door handle and heave the door open.Developed in the labs of Mark Cutkosky, the Fletcher Jones Chair in the School of Engineering at Stanford University, and Dario Floreano at the ecole Polytechnique Federale de Lausanne in Switzerland, FlyCroTugs are micro air vehicles that the researchers have modified so the vehicles can anchor themselves to various surfaces using adhesives inspired by the feet of geckos and insects, previously developed in Cutkosky's lab.With these attachment mechanisms, FlyCroTugs can pull objects up to 40 times their weight, like door handles in one scenario, or cameras and water bottles in a rescue situation. Similar vehicles can only lift objects about twice their own weight using aerodynamic forces.""When you're a small robot, the world is full of large obstacles,"" said Matthew Estrada, a graduate student at Stanford and lead author of a paper on FlyCroTugs, published Oct. 25 in Science Robotics. ""Combining the aerodynamic forces of our aerial vehicle along with interaction forces that we generate with the attachment mechanisms resulted in something that was very mobile, very forceful and micro as well.""The researchers say the FlyCroTugs' small size means they can navigate through snug spaces and fairly close to people, making them useful for search and rescue. Holding tightly to surfaces as they tug, the tiny robots could potentially move pieces of debris or position a camera to evaluate a treacherous area.Taking a cue from natureAs with most projects in Cutkosky's lab, the FlyCroTugs were inspired by the natural world. Hoping to have an air vehicle that was fast, small and highly maneuverable but also able to move large loads, the researchers looked to wasps.""Wasps can fly rapidly to a piece of food, and then if the thing's too heavy to take off with, they drag it along the ground. So this was sort of the beginning inspiration for the approach we took,"" said Cutkosky, who is a co-author of the paper.The researchers read studies on wasp prey capture and transport, which identify the ratio of flight-related muscle to total mass that determines whether a wasp flies with its prey or drags it. They also followed the lead of the wasp in having different attachment options depending on where the FlyCroTugs land.For smooth surfaces, the robots have gecko grippers, non-sticky adhesives that mimic a gecko's intricate toe structures and hold on by creating intermolecular forces between the adhesive and the surface. For rough surfaces, these robots are equipped with 32 microspines, a series of fishhook-like metal spines that can individually latch onto small pits in a surface.Each FlyCroTug has a winch with a cable and either microspines or gecko adhesive in order to tug. Beyond those fixed features they are otherwise highly modifiable. The location of the grippers can vary depending on the surface where they will be landing, and the researchers can also add parts for ground-based movement, such as wheels. Getting all of these features onto a small air vehicle with twice the weight of a golf ball was no small feat, according to the researchers.""People tend to think of drones as machines that fly and observe the world, but flying insects do many other things -- such as walking, climbing, grasping, building -- and social insects can even cooperate to multiply forces,"" said Floreano, who was senior author on the paper. ""With this work, we show that small drones capable of anchoring to the environment and collaborating with fellow drones can perform tasks typically assigned to humanoid robots or much larger machines.""Interacting with the worldDrones and other small flying robots may seem like all the rage these days but the FlyCroTugs -- with their ability to navigate to remote locations, anchor and pull -- fall into a more specific niche, according to Cutkosky.""There are many laboratories around the world that are starting to work with small drones or air vehicles, but if you look at the ones that are also thinking about how these little vehicles can interact physically with the world, it's a much smaller set,"" he said.The researchers can successfully open a door with two FlyCroTugs. They also had one fly atop a crumbling structure and haul up a camera to see inside. Next, they hope to work on autonomous control and the logistics of flying several vehicles at once.""The tools to create vehicles like this are becoming more accessible,"" said Estrada. ""I'm excited at the prospect of increasingly incorporating these attachment mechanisms into the designer's tool belt, enabling robots to take advantage of interaction forces with their environment and put these to useful ends.""Additional co-authors of this paper are Stefano Mintchev of EPFL and David Christensen, who was at Stanford at the time of the research. This work was funded by the Swiss National Science Foundation, the National Science Foundation, a Swiss Government Excellence Scholarship and the United States Army Research Laboratory MAST program.Story Source:Materials provided by Stanford University. Original written by Taylor Kubota. ",0.0827717,0.0575362,0.101998,0.136005,0.0707104,0.124284,0.117354,0,0
581,New technique reveals limb control in flies -- and maybe robots,"One of the major goals of biology, medicine, and robotics is to understand how limbs are controlled by circuits of neurons working together. And as if that is not complex enough, a meaningful study of limb activity also has to take place while animals are behaving and moving. The problem is that it is virtually impossible to get a complete view of the activity of motor and premotor circuits that control limbs during behavior, in either vertebrates or invertebrates.Scientists from the lab of Pavan Ramdya at EPFL's Brain Mind Institute and Interfaculty Institute of Bioengineering have developed a new method for recording the activity of limb control neural circuits in the popular model organism, the fruit fly Drosophila melanogaster. The method uses an advanced imaging technique called ""two-photon microscopy"" to observe the firing of fluorescently labeled neurons that become brighter when they are active.The scientists focused on the fly's ventral nerve cord, which is a major neural circuit controlling the legs, neck, wings, and two dumbbell-shaped organs that the insect uses to orient itself, called the ""halteres."" But most importantly, they were able to image the fly's ventral nerve cord while the animal was carrying out specific behaviors.The scientists discovered different patterns of activity across populations of neurons in the cord during movement and behavior. Specifically, the researchers looked at grooming and walking, which allowed them to study neurons involved in the fly's ability to walk forward, backwards, or to turn while navigating complex environments.Finally, the team developed a genetic technique that makes it easier to access to the ventral nerve cord. This can help future studies that directly investigate circuits associated with complex limb movements.""I am very excited about our new recording approach,"" says Professor Pavan Ramdya. ""Combined with the powerful genetic tools available for studying the fly, I believe we can rapidly make an impact on understanding how we move our limbs and how we might build robots that move around the world just as effectively as animals.""Story Source:Materials provided by Ecole Polytechnique Federale de Lausanne. ",0.0996178,0.118221,0.133403,0.232664,0.13085,0.149166,0.182794,0,0
582,Looking and listening for signals of navy test explosions off Florida coast,"Underwater explosions detonated by the U.S. Navy to test the sturdiness of ships' hulls have provided seismologists with a test opportunity of their own: how much can we know about an underwater explosion from the seismic and acoustic data it generates?In a study published in the Bulletin of the Seismological Society of America, a team of researchers led by Ross Heyburn of AWE Blacknest in the United Kingdom compared the ""ground-truth"" data available from the Navy on the location, size and depth of these detonations to estimates obtained from seismic stations and hydroacoustic sensors.The explosions conducted off the Florida coast offer a chance to test the capabilities of the International Monitoring System (IMS) of the Comprehensive Test-Ban Treaty Organization, deployed to detect secret nuclear test explosions, say the authors.""Underwater explosions of this size where ground-truth data are available are quite rare,"" said Heyburn. IMS stations ""do record signals from many underwater events, both naturally occurring, like earthquakes, and human-made, like air-gun surveys. However, ground-truth data from the human-made sources are often not available, and usually any underwater explosions observed are much smaller in size than the Florida explosions.""The Navy explosion tests were conducted in 2001, 2008 and 2016 within the Navy's Mayport test area, located 120 kilometers off the coast of Jacksonville, Florida. The experiments tested the hull integrity of the near-shore combat ships the USS Jackson, the USS Milwaukee, the USS Winston Churchill and the USS Mesa Verde. During the experiments, 10,000-pound chemical explosions were towed behind a vessel sailing alongside the combat ships, to test how the combat ship hulls responded to the shock waves produced by nearby detonations equivalent to a TNT charge weight of about 6759 kilograms.The explosions aren't the size of a nuclear test explosion, but they are much larger than many of the other underwater explosions that are available to study, explained Heyburn. ""The explosions described in the paper are much smaller than a typical nuclear test explosion which is typically of the order of kilotons in TNT equivalent charge weight,"" he said. ""For comparison, naval mines and anti-submarine depth charges typically contain explosive charges with TNT equivalent charge weights of the order of hundreds of kilograms.""Heyburn and colleagues analyzed data from the explosions collected by IMS seismic stations around the globe and from a hydrophone station near Ascension Island in the South Atlantic Ocean for the tests in 2008 and 2016. The researchers used the location data provided by the Navy for several of the explosions as a way to improve the accuracy of calculations used to pinpoint the epicenter of explosions without available ground-truth data.Previous research on detonations in the Dead Sea and other underwater test sites has shown that there is a relationship between the local magnitude of an explosion and its charge weight. After taking into account the salinity of the seawater near Florida, Heyburn and colleagues were able to modify the Dead Sea calculations to estimate the Navy charge weights from published estimates of local magnitude for the explosions.Underwater explosions also produce unique seismic features that can be used to identify the source. One such notable feature is the bubble pulse, the underwater pulsing generated by the gas bubble created by the explosion as it expands and contracts in size. The researchers were able to identify the bubble pulse in the seismic records, confirming that the seismic signals were the result of an underwater explosion.The bubble pulse was not observed as clearly at the Ascension Island hydrophone station, and Heyburn and colleagues suggest that the shallow waters off the Florida Coast may have distorted the hydroacoustic signals of the explosion. ""This showed that both the seismic and hydroacoustic data can be important when analyzing signals from underwater explosions,"" said Heyburn.Story Source:Materials provided by Seismological Society of America. ",0.170789,0.147868,0.215756,0.228188,0.167894,0.234322,0.205765,0,0
583,Scientists create synthetic prototissue capable of synchronized beating,"The discovery, published in Nature Materials, is the first chemically programmed approach to producing an artificial tissue. The findings, which could have major health applications in the future, could see chemically programmed synthetic tissue being used to support failing living tissues and to cure specific diseases.The development of synthetic tissue which can mimic the ability of living cells to produce functions such as beating and chemical detoxification has, until now, remained a major synthetic biology challenge.A team, led by Professor Stephen Mann FRS and Dr Pierangelo Gobbo from Bristol's School of Chemistry, chemically programmed artificial synthetic cells known as protocells to communicate and interact with each other in a highly co-ordinated way.The researchers constructed two types of artificial cells each having a protein-polymer membrane but with complementary surface anchoring groups. The team then assembled a mixture of the sticky artificial cells into chemically-linked clusters to produce self-supporting artificial tissue spheroids. By using a polymer that could expand or contract as the temperature was changed below or above 37 degrees Celsius, it was possible to make the artificial tissues undergo sustained beat-like oscillations in size.The team was able to increase the functionality of the artificial tissues by capturing enzymes within their constituent artificial cells. Using various combinations of enzymes, the team was able to modulate the amplitude of the beating and control the movement of chemical signals in and out of the artificial tissues.Professor Stephen Mann FRS, Professor of Chemistry at Bristol and lead author, said: ""Our approach to the rational design and fabrication of prototissues bridges an important gap in bottom-up synthetic biology and should also contribute to the development of new bioinspired materials that work at the interface between living tissues and their synthetic counterparts.""Dr Pierangelo Gobbo, lead author, added: ""Our methodology opens up a route from the synthetic construction of individual protocells to the co-assembly and spatial integration of multi-protocellular structures. In this way, we can combine the specialisation of individual protocell types with the collective properties of the ensemble.""Story Source:Materials provided by University of Bristol. ",0.0986526,0.195419,0.182129,0.247205,0.173556,0.155248,0.223793,0,0
584,"New technique locates robots, soldiers in GPS-challenged areas ","Scientists at the U.S. Army Research Laboratory have developed a novel algorithm that enables localization of humans and robots in areas where GPS is unavailable.According to ARL researchers Gunjan Verma and Dr. Fikadu Dagefu, the Army needs to be able to localize agents operating in physically complex, unknown and infrastructure-poor environments.""This capability is critical to help find dismounted Soldiers and for humans and robotic agents to team together effectively,"" Verma said. ""In most civilian applications, solutions such as GPS work well for this task, and help us, for example, navigate to a destination via our car.""However, noted the researchers, such solutions are not suitable for the military environment.""For example, an adversary may destroy the infrastructure (e.g., satellites) needed for GPS; alternatively, complex environments (e.g., inside a building) are hard for the GPS signal to penetrate,"" Dagefu said. ""This is because complex and cluttered environments impede the straight-line propagation of wireless signals.""Dagefu said that obstacles inside the building, especially when their size is much larger than the wavelength of the wireless signal, weaken the power of the signal (attenuation) and re-direct its flow (called multipath), making a wireless signal very unreliable for communicating information about location.According to the researchers, typical approaches to localization, which use a wireless signal's power or delay (i.e., how long it takes to reach a target from a source), work well in outdoor scenes with minimal obstacles; however, they perform poorly in obstacle-rich scenes.The team of ARL scientists including Dagefu and Verma developed a novel technique for determining the direction of arrival, or DoA, of a radio frequency signal source, which is a fundamental enabler of localization.""The proposed technique is robust to multiple scattering effects, unlike existing methods such as those that rely on the phase or time of arrival of the signal to estimate the DoA,"" Verma said. ""This means even in the presence of occluders that scatter the signal in different directions before it is received by the receiver, the proposed approach can accurately estimate the direction of the source.""The underlying idea is that the gradient of the spatially sampled received signal strength, or RSS, carries information about the source direction.""Extracting the DoA requires a theoretically grounded analysis to obtain a robust estimator in the presence of undesirable propagation phenomena,"" Verma said. ""For example, large obstacles cause the RSS samples nearby to become highly correlated (so-called ""correlated shadowing""). If left uncorrected, this correlation can seriously bias the DoA estimate.""The key invention according to the researchers is an algorithm that statistically models the RSS gradient and controls for spatial outliers and correlations.Importantly, when the signal is extremely noisy, the estimator correctly outputs that no DoA is present, rather than incorrectly estimating an arbitrary direction.The output is an estimated DoA and associated uncertainty.The researchers have validated the approach with several publically available as well as in-house collected measurement datasets at 40MHz and 2.4GHz bands, as well as data from high fidelity simulations.The technique works in conditions of heavy multipath in which classical phase or time of arrival based estimates would fail.In addition to not requiring any fixed infrastructure, the proposed technique also does not rely on any prior training data, knowledge about the environment, multiple antennas, or prior calibration between nodes.Story Source:Materials provided by U.S. Army Research Laboratory. ",0.0680324,0.0720188,0.124865,0.179695,0.120374,0.105864,0.123003,0,0
585,"Forensics: New tool predicts eye, hair and skin color from a DNA sample of an unidentified individual New tool will be used when standard forensic profiling is not helpful","An international team, led by scientists from the School of Science at IUPUI and Erasmus MC University Medical Center Rotterdam in the Netherlands, has developed a novel tool to accurately predict eye, hair and skin color from human biological material -- even a small DNA sample -- left, for example, at a crime scene or obtained from archeological remains. This all-in-one pigmentation profile tool provides a physical description of the person in a way that has not previously been possible by generating all three pigment traits together using a freely available webtool.The tool is designed to be used when standard forensic DNA profiling is not helpful because no reference DNA exists against which to compare the evidence sample.The HIrisPlex-S DNA test system is capable of simultaneously predicting eye, hair and skin color phenotypes from DNA. Users, such as law enforcement officials or anthropologists, can enter relevant data using a laboratory DNA analysis tool, and the webtool will predict the pigment profile of the DNA donor.""We have previously provided law enforcement and anthropologists with DNA tools for eye color and for combined eye and hair color, but skin color has been more difficult,"" said forensic geneticist Susan Walsh from IUPUI, who co-directed the study. ""Importantly, we are directly predicting actual skin color divided into five subtypes -- very pale, pale, intermediate, dark and dark to black -- using DNA markers from the genes that determine an individual's skin coloration. This is not the same as identifying genetic ancestry. You might say it's more similar to specifying a paint color in a hardware store rather than denoting race or ethnicity.""If anyone asks an eyewitness what they saw, the majority of time they mention hair color and skin color. What we are doing is using genetics to take an objective look at what they saw,"" Walsh said.The innovative high-probability and high-accuracy complete pigmentation profile webtool is available online without charge.The study, ""HIrisPlex-S System for Eye, Hair and Skin Colour Prediction from DNA: Introduction and Forensic Developmental Validation,"" is published in the peer-reviewed journal Forensic Science International: Genetics.""With our new HIrisPlex-S system, for the first time, forensic geneticists and genetic anthropologists are able to simultaneously generate eye, hair and skin color information from a DNA sample, including DNA of the low quality and quantity often found in forensic casework and anthropological studies,"" said Manfred Kayser of Erasmus MC, co-leader of the study.Story Source:Materials provided by Indiana University-Purdue University Indianapolis School of Science. ",0.144833,0.19216,0.225374,0.266149,0.185618,0.201033,0.236414,0,0
586,Into the wild for plant genetics Scientists sequence a whole genome on a Welsh mountainside to identify a plant species within hours,"In a paper published today in Scientific Reports (Nature Publishing Group), researchers at the Royal Botanic Gardens, Kew, detail for the first time the opportunities for plant sciences that are now available with portable, real-time DNA sequencing.Kew scientist and co-author of the paper Joe Parker says; ""This research proves that we can now rapidly read the DNA sequence of an organism to identify it with minimum equipment. Rapidly reading DNA anywhere, at will, should become a routine step in many research fields. Despite hundreds of years of taxonomic research, it is still not always easy to work out which species a plant belongs to just by looking at it. Few people could correctly identify all the species in their own gardens.""Over the last forty years, DNA sequencing has revolutionised the scientific world but has remained laboratory-bound. Using current methods, a complete experiment to identify a species, from fieldwork to result, could easily take a scientist months to complete. Species identification is, by nature, a largely a field-based area of pursuit, thereby limiting the pace of discovery and decision making that can depend upon it. Using new technology to identify species quickly and on-site is critical for scientific research, the conservation of biodiversity and in the fight against species crime.In this new study, Kew scientists used the portable DNA sequencer, the MinION from Oxford Nanopore Technologies, to analyse plant species in Snowdonia National Park. This was the first time genomic sequencing of plants has been performed in the field.This technology, commercially launched in 2015, has since been used in Antarctica, in remote regions affected by disease, and on the International Space Station.One of the successes illustrated in the paper is the field identification of two innocuous white flowers, Arabidopsis thaliana and Arabidopsis lyrata ssp. petraea. This was achieved by sequencing random parts of the plants' genomes, avoiding the tricky and time consuming process of targeting specific pieces of DNA which is the more traditional approach for identifying species with DNA.The researchers compared their new data to a freely available database of reference genome sequences to make their identification. Crucially, replicating their experiment in Kew's Jodrell Laboratory with other DNA sequencing methods allowed them to devise sophisticated statistics to understand the useful properties of this new kind of data for the first time.Alexander Papadopulos, Kew scientist and co-author on the paper, says; ""Accurate species identification is essential for evolutionary and ecological research, in the fight against wildlife crime and for monitoring rare and threatened species. Identifying species correctly based on what they look like can be really tricky and needs expertise to be done well. This is especially true for plants when they aren't in flower or when they have been processed into a product. Our experiments show that by sequencing random pieces of the genome in the field it's possible to get very accurate species identification within a few hours of collecting a specimen. More traditional methods need a lot of lab equipment and have often only provided enough information to identify a sample to the genus level.""There are other useful properties of their data too. This field sequenced data can be used to assemble a whole genome sequence, act as a reference database for the species and help understand evolutionary relationships. Currently, the team is exploring the feasibility of rapidly generating a reference sequence database from the incredibly diverse collection of plants help in Kew's living collection and herbarium as well as applications for monitoring plant health.Story Source:Materials provided by Royal Botanic Gardens Kew. ",0.0844852,0.135397,0.140569,0.197487,0.105762,0.164637,0.157559,0,0
587,"Creating 3-D hands to keep us safe, increase security ","Creating a 3-D replica of someone's hand complete with all five fingerprints and breaking into a secure vault sounds like a plot from a James Bond movie. But Michigan State University Distinguished Professor Anil Jain recently discovered this may not be as far-fetched as once thought and wants security companies and the public to be aware.Jain and his biometrics team were studying how to test and calibrate fingerprint scanners commonly used across the globe at police departments, airport immigration counters, banks and even amusement parks. Without a standard life-like 3-D model to test the scanners with, there's no consistent and repeatable way to determine the accuracy of the scans and establish which scanner is better.To test the scanners, they created life-size 3-D hand models complete with all five fingerprints using a high-resolution 3-D printer that can produce the same ridges and valleys as a real finger.""Like any optical device, fingerprint and hand scanners need to be calibrated, but currently there is no standard method for calibrating them,"" said Jain. ""This is the first time a whole hand 3-D target has been created to calibrate fingerprint scanners. As a byproduct of this research we realized a fake 3-D hand, essentially a spoof, with someone's fingerprints, could potentially allow a crook to steal the person's identity to break into a vault, contaminate a crime scene or enter the country illegally.""Now, another application of this technology will be to evaluate the spoof-resistance of commercial fingerprint scanners. We have highlighted a security loophole and the limitations of existing fingerprint scanning technology, now it's up to the scanner manufacturers to design a scanner that is spoof-resistant. The burden is on them to tell whether the finger being placed on the scanner is real human skin or a printed material,"" said Jain.The study aims to design and develop standard models and procedures for consistent and reliable evaluation of fingerprint readers and is funded by the National Institute of Standards and Technology.""We are very pleased with this research and how it is showing the uncertainties in the process and what it can mean for the accuracy of the readers,"" said Nicholas Paulter, Group Leader for the Security Technologies Group at NIST and a co-author of the study. ""The FBI, CIA, military and manufacturers will all be interested in this project.""Along with Jain and Paulter, the study was co-authored by Sunpreet Arora, MSU doctoral student. The paper describing the design and fabrication process of 3-D whole hand models can be accessed here. It received the best paper award at the 15th International Conference of the Biometrics Special Interest Group, 2016.For a video of the model hand in action click here: https://www.youtube.com/watch?v=5eKbAVhlXkoTo access the report, see: http://www.cse.msu.edu/rgroups/biometrics/Publications/Fingerprint/Aroraetal_3DWholeHandTargets_BIOSIG2016_camready.pdfStory Source:Materials provided by Michigan State University. ",0.0928115,0.112069,0.166682,0.264459,0.137534,0.148072,0.172145,0,0
588,"Meet RobERt, the dreaming detective for exoplanet atmospheres ","Machine-learning techniques that mimic human recognition and dreaming processes are being deployed in the search for habitable worlds beyond our solar system. A deep belief neural network, called RobERt (Robotic Exoplanet Recognition), has been developed by astronomers at UCL to sift through detections of light emanating from distant planetary systems and retrieve spectral information about the gases present in the exoplanet atmospheres. RobERt will be presented at the National Astronomy Meeting (NAM) 2016 in Nottingham by Dr Ingo Waldmann on Tuesday 28th June.""Different types of molecules absorb and emit light at specific wavelengths, embedding a unique pattern of lines within the electromagnetic spectrum,"" explained Dr Waldmann, who leads RobERt's development team. ""We can take light that has been filtered through an exoplanet's atmosphere or reflected from its cloud-tops, split it like a rainbow and then pick out the 'fingerprint' of features associated with the different molecules or gases. Human brains are really good at finding these patterns in spectra and label them from experience, but it's a really time consuming job and there will be huge amounts of data.We built RobERt to independently learn from examples and to build on his own experiences. This way, like a seasoned astronomer or a detective, RobERt has a pretty good feeling for what molecules are inside a spectrum and which are the most promising data for more detailed analysis. But what usually takes days or weeks takes RobERt mere seconds.""Deep belief neural networks, or DBNs, were developed more than a decade ago and are commonly used for speech recognition, Internet searches and tracking customer behaviour. RobERt's DBN has three layers of unit processors, or 'neurons'. Information is fed into a bottom layer of 500 neurons, which make an initial filter of the data and pass a subset up to the second layer. Here, 200 neurons refine the selection and pass data up to a third layer of 50 neurons to make the final identification of the gases most likely to be present.To prepare RobERt for his challenge, Waldmann and colleagues at UCL created a total of 85,750 simulated spectra, covering five different types of exoplanet ranging from GJ1214b, a potential ""ocean planet,"" to WASP-12, a hot Jupiter orbiting very close to its star. Each spectrum in the training set contained the fingerprint of a single gas species. RobERt's learning progress was tested at intervals during the training with 'control' spectra. At the end of the training phase, RobERt had a recognition accuracy of 99.7%.""RobERt has learned to take into account factors such as noise, restricted wavelength ranges and mixtures of gases,"" said Waldmann. ""He can pick out components such as water and methane in a mixed atmosphere with a high probability, even when the input comes from the limited wavebands that most space instruments provide and when it contains overlapping features.""RobERt's DBN can also be reversed so that instead of analysing data fed into the system, he can enter a 'dreaming state' in which he can generate full spectra based on his experiences.""Robots really do dream. We can ask RobERt to dream up what he thinks a water spectrum will look like, and he's proved very accurate,"" said Waldmann. ""This dreaming ability has been very useful when trying to identify features in incomplete data. RobERt can use his dream state to fill in the gaps. The James Webb Space Telescope, due for launch in 2018, will tell as more about the atmospheres of exoplanets, and new facilities like Twinkle or ARIEL will be coming online over the next decade that are specifically tailored to characterising the atmospheres of exoplanets. The amount of data these missions will provide will be breathtaking. RobERt will play an invaluable role in helping us to analyse data from these missions and find out what these distant worlds are really like.""Story Source:Materials provided by Royal Astronomical Society (RAS). ",0.0852754,0.103019,0.125615,0.228763,0.117874,0.132299,0.157497,0,0
589,Combining multiple CCTV images could help catch suspects,"Combining multiple poor quality CCTV images into a single, computer-enhanced composite could improve the accuracy of facial recognition systems used to identify criminal suspects, new research suggests.Psychologists from the universities of Lincoln and York, both in the UK, and the University of New South Wales in Australia created a series of pictures using a 'face averaging' technique -- a method which digitally combines multiple images into a single enhanced image, removing variants such as head angles or lighting so that only features that indicate the identity of the person remain.They compared how effectively humans and computer facial recognition systems could identify people from high quality images, pixelated images, and face averages. The results showed that both people and computer systems were better at identifying a face when viewing an average image that combined multiple pixelated images, compared to the original poor-quality images. Computer systems benefited from averaging together multiple images that were already high in quality, and in some cases reached 100 per cent accurate face recognition.The results have implications for law enforcement and security agencies, where low quality, pixelated images are often the only pictures of suspects available to use in investigations. The image averaging method offers a standardised way of using images captured from multiple CCTV cameras to create a digital snapshot which can be better recognised by both people and computer software systems.Dr Kay Ritchie, from the University of Lincoln's School of Psychology, led the study. She said: ""We know that not all CCTV systems have the luxury of high quality cameras, meaning that face identifications are often being made from poor quality images. We have shown that there is a relatively quick and easy way to improve pixelated images of someone's face.""We also know anecdotally that there are lots of different techniques that people can use as investigative tools to improve low-quality images, such as manipulating brightness. Our standardised face averaging method could help in suspect identification from low-quality CCTV footage where images from multiple different cameras are available, for example, from tracking a suspect along a particular route.""In the study, participants were asked to compare a high quality image with either a low quality pixelated image or one created using the image averaging method, and determine whether they depicted the same person or two different people. Results showed that accuracy was significantly higher when viewing an average combining pixelated images, rather than a single pixelated image.The same test images were run through two separate computer recognition programmes, one a smart phone application, and the other a commercial facial recognition system widely used in forensic settings. Both computerised systems showed higher levels of accuracy in identifying a person from average images.Story Source:Materials provided by University of Lincoln. ",0.104175,0.119899,0.178688,0.303305,0.158162,0.170218,0.19597,0,0
590,Switching DNA and RNA on and off,"DNA and RNA are naturally polarised molecules containing electric dipole moments due to the presence of a significant number of charged atoms at neutral pH. Scientists believe that these molecules have an in-built polarity that can be reoriented or reversed fully or in part under an electric field -- a property referred to as bioferroelectricity. However, the mechanism of these properties remains unclear. In a new study published in EPJ E, See-Chuan Yam from the University of Malaya, Kuala Lumpur, Malaysia, and colleagues show that all the DNA and RNA building blocks, or nucleobases, exhibit a non-zero polarisation in the presence of polar atoms or molecules such as amidogen and carbonyl. They have two stable states, indicating that DNA and RNA basically have memory properties, just like a ferroelectric or ferromagnetic material. This is relevant for finding better ways of storing data in DNA and RNA because they have a high capacity for storage and offer a stable storage medium. Such physical properties may play an important role in biological processes and functions. Specifically, these properties could also be extremely useful for possible applications as a biosensor to detect DNA damage and mutation.In this work, the authors employ computational molecular modelling to study the polarisation switching of DNA and RNA using a semi-empirical quantum mechanical approach. To do so, they model the five nucleobases which are the building blocks of DNA and RNA.The authors also make an interesting discovery: that the minimum electric field required for switching the polarisation of a nucleobase is inversely proportional to the ratio of the topological polar surface area (TPSA) to the total surface area (TSA) of a nucleobase. This work may, therefore, also provide valuable insights for understanding the possible existence of ferroelectricity in biomaterials; further, the observed switching mechanism and ferroelectrical properties of DNA and RNA nucleobases could inform the future development of DNA and RNA-based nanomaterials and electronic devices.Story Source:Materials provided by Springer. ",0.121393,0.190278,0.156147,0.201073,0.17005,0.177921,0.192038,0,0
591,NIST builds statistical foundation for next-generation forensic DNA profiling New data will allow forensic scientists to calculate match statistics when using DNA sequencing to solve crimes,"DNA is often considered the most reliable form of forensic evidence, and this reputation is based on the way DNA experts use statistics. When they compare the DNA left at a crime scene with the DNA of a suspect, experts generate statistics that describe how closely those DNA samples match. A jury can then take those match statistics into account when deciding guilt or innocence.These match statistics are reliable because they're based on rigorous scientific research. However, that research only applies to DNA fingerprints, also called DNA profiles, that have been generated using current technology. Now, scientists at the National Institute of Standards and Technology (NIST) have laid the statistical foundation for calculating match statistics when using Next Generation Sequencing, or NGS, which produces DNA profiles that can be more useful in solving some crimes. This research, which was jointly funded by NIST and the FBI, was published in Forensic Science International: Genetics.""If you're working criminal cases, you need to be able to generate match statistics,"" said Katherine Gettings, the NIST biologist who led the study. ""The data we've published will make it possible for labs that use NGS to generate those statistics.""How to Create a DNA ProfileTo generate a DNA profile, forensic labs analyze sections of DNA, called genetic markers, where the genetic code repeats itself, like a word typed over and over again. Those sections are called short tandem repeats, or STRs, and the number of repeats at each marker varies from person to person. The analyst doesn't actually read the genetic sequence inside those markers, but just counts the number of repeats at each one. That yields a series of numbers that, like a long social security number, can be used to identify a person.STR-based profiling was developed in the 1990s, when genetic sequencing was hugely expensive. Today, NGS makes sequencing cost-effective for biomedical research and other applications. NGS can also be used to create forensic DNA profiles that, unlike traditional STR profiles, include the actual genetic sequence inside the markers. That provides a lot more data.That extra data might not be needed because in most cases, STR-based profiles contain more than enough information to reliably identify a suspect. However, if the evidence contains only a minute amount of DNA, or if the DNA has been exposed to the elements and has begun to break down, then the analyst might only get a partial profile, which may not be enough to identify a suspect. In those cases, the extra data in an NGS-based profile might help solve the case.In addition, evidence that contains a mixture of DNA from several people can be difficult to interpret. The extra data in NGS-based profiles can help in those cases as well.Calculating Match StatisticsDNA analysts are able to calculate match statistics for STR-based profiles because scientists have measured how frequently different versions of the markers occur in the population. With those frequencies, you can calculate the chances of randomly encountering a particular DNA profile, just as you can calculate the chances of picking all the right numbers in a lottery.NIST measured those STR gene frequencies years ago using a library of DNA samples from 1,036 individuals. To calculate gene frequencies for NGS-based profiles, Gettings and her co-authors cracked open the freezer that contained the original samples, which were anonymized and donated by people who consented to their DNA being used for research. The scientists generated NGS-based profiles for them by sequencing 27 markers -- the core set of 20 included in most DNA profiles in the U.S. plus seven others. They then calculated the frequencies for the various genetic sequences found at each marker.It might be surprising that scientists can estimate gene frequencies from such a small library of samples. However, the NIST team was measuring frequencies not for the full profiles, but for the individual markers. Since they sequenced 27 markers, with each marker occurring twice per sample, the number of markers tested wasn't 1,036, but more than 55,000.Although NIST has now published the data needed to generate match statistics for NGS-based profiles, other hurdles must still be cleared before the new technology sees widespread use in forensics. For instance, labs will have to develop ways to manage the greater amounts of data produced by NGS. They will also have to implement operating procedures and quality controls for the new technology. Still, while much work remains, said Peter Vallone, the research chemist who leads NIST's forensic genetics research, ""We're laying the foundation for the future.""Story Source:Materials provided by National Institute of Standards and Technology (NIST). ",0.114272,0.154433,0.190093,0.274665,0.16289,0.178748,0.214558,0,0
592,Using bloodstains at crime scenes to determine age of a suspect or victim,"From the spatter analysis made famous in the TV show Dexter to the frequent DNA profiling of CSI and the real cases covered in the FBI Files, blood tests are ubiquitous in forensic science. Now, researchers report in ACS Central Science that a new blood test, which could be performed at a crime scene, could help determine the age of a suspect or victim within just an hour.Despite their usefulness, current blood tests that analyze samples from crime scenes have limitations. Many tests cannot be completed in the field, some take a long time, and others can destroy the sample. Typically, DNA and fingerprint analyses both require that the person of interest is included in a database. And even if the person's information is in a database, some characteristics like age cannot be determined by conventional DNA profiling or fingerprint analysis. To estimate age, one would ideally monitor something that changes over the course of a lifetime. Some components of blood (particularly the structure of hemoglobin that gives blood its characteristic red color) fit the bill. A technique called Raman spectroscopy provides information about the chemical composition and molecular structure of material. Thus, Igor Lednev and colleagues wanted to see whether they could use this method to analyze blood components to determine the ages of victims and suspects.By performing Raman spectroscopy on samples from human donors, the researchers looked at samples from three different age groups: newborns (under one year), adolescents (approximately 11 to 13 years of age) or adults (43 to 68 years old). The researchers could distinguish between the groups -- adolescent and adult samples were differentiated with greater than 99 percent accuracy, and there were no errors in the identifications of the newborn samples. Because the method is a non-destructive technique that requires no sample preparation, the authors say this could be a useful on-site addition to the current cohort of available tests. In the future, they plan to refine their model to provide more specific ages, not just large age ranges, of the donors.Story Source:Materials provided by American Chemical Society. ",0.0937542,0.143833,0.176364,0.232687,0.138978,0.146459,0.187698,0,0
593,"Fingerprints in birefringence Supercharged biomacromolecules can maintain their reordered structures induced, for example, by a fingertip touch","Stimuli-sensitive materials can respond to physical forces with structural phase transitions. This also applies to biopolymer-surfactant mixtures, a study by German and Chinese scientists now reports. Surprisingly, the newly adopted phases persist after removal of the stress and can be detected by a simple optical read-out technology. Biometric fingerprint detection is an attractive application for this technology. The results are published in the journal Angewandte Chemie.Liquid crystals are shape-anisotropic molecules that can adopt distinct ordered phases, depending on the physical conditions. Temperature, pressure, or charge can produce color shifts, dark-light switches, or a birefractive appearance, all of which represent changes in the molecular order. Such transitions can also occur in gels, and even in soaps with micellar transitions. The chemical system developed by Andreas Herrmann at the University of Groningen, the Netherlands, and colleagues at the Chinese Academy of Sciences, is a complex of a supercharged polypeptide with a cationic surfactant. The viscous liquid adopted birefringence patterns after simply being touched, to reveal details such as those of a fingerprint.Seeking to explore the behavior of biological fluids, the scientists designed a series of supercharged polypeptides that form biological soft materials with interesting properties when paired with molecules supplying the opposite charge. The supercharged polypeptides consisted of five amino acid repeating units with one or two negatively charged glutamic acid residues within each unit. As the cationic surfactant, the researchers designed an aromatic azobenzene with a positive charge on one side and a hydrophobic chain on the other. Added together, the polypeptide and the surfactant formed a water-rich polypeptide liquid droplet with an orange hue. In this liquid the scientists found no molecular order, birefringence, or diffraction pattern, and merely an isotropic viscous fluid.A shear force stimulated a different response. Flowing water or the touch of a finger made the sample birefringent, and ordered patterns were evident, the authors reported. These ordered structures resembled the long-range lyotropic liquid crystalline phases typical for surfactant-containing mixtures. Surprisingly, that order persisted, even after removing the shear. A polarized optical microscope detected birefringence patterns that sensitively recorded the texture of the shear-applying tool. In other words, the minutiae, the ridges and lines on the fingertip that make up a fingerprint, were well-represented in the polarization micrographs.This remarkable discovery suggests that the supercharged polypeptide fluid could, in principle, be used for biometric detection. Whereas modern day fingerprint sensors that are not based on ink printing rely on finely adjusted electronics, the scientists present a different setup with microscopic birefringence read-out. However, the exact conditions for the phase transitions in the material and the underlying mechanisms have yet to be explored, the authors remark.Story Source:Materials provided by Wiley. ",0.0785749,0.143094,0.13237,0.131619,0.11644,0.123582,0.137612,0,0
594,Face recognition technology that works in the dark,"Army researchers have developed an artificial intelligence and machine learning technique that produces a visible face image from a thermal image of a person's face captured in low-light or nighttime conditions. This development could lead to enhanced real-time biometrics and post-mission forensic analysis for covert nighttime operations.Thermal cameras like FLIR, or Forward Looking Infrared, sensors are actively deployed on aerial and ground vehicles, in watch towers and at check points for surveillance purposes. More recently, thermal cameras are becoming available for use as body-worn cameras. The ability to perform automatic face recognition at nighttime using such thermal cameras is beneficial for informing a Soldier that an individual is someone of interest, like someone who may be on a watch list.The motivations for this technology -- developed by Drs. Benjamin S. Riggan, Nathaniel J. Short and Shuowen ""Sean"" Hu, from the U.S. Army Research Laboratory -- are to enhance both automatic and human-matching capabilities.""This technology enables matching between thermal face images and existing biometric face databases/watch lists that only contain visible face imagery,"" said Riggan, a research scientist. ""The technology provides a way for humans to visually compare visible and thermal facial imagery through thermal-to-visible face synthesis.""He said under nighttime and low-light conditions, there is insufficient light for a conventional camera to capture facial imagery for recognition without active illumination such as a flash or spotlight, which would give away the position of such surveillance cameras; however, thermal cameras that capture the heat signature naturally emanating from living skin tissue are ideal for such conditions.""When using thermal cameras to capture facial imagery, the main challenge is that the captured thermal image must be matched against a watch list or gallery that only contains conventional visible imagery from known persons of interest,"" Riggan said. ""Therefore, the problem becomes what is referred to as cross-spectrum, or heterogeneous, face recognition. In this case, facial probe imagery acquired in one modality is matched against a gallery database acquired using a different imaging modality.""This approach leverages advanced domain adaptation techniques based on deep neural networks. The fundamental approach is composed of two key parts: a non-linear regression model that maps a given thermal image into a corresponding visible latent representation and an optimization problem that projects the latent projection back into the image space.Details of this work were presented in March in a technical paper ""Thermal to Visible Synthesis of Face Images using Multiple Regions"" at the IEEE Winter Conference on Applications of Computer Vision, or WACV, in Lake Tahoe, Nevada, which is a technical conference comprised of scholars and scientists from academia, industry and government.At the conference, Army researchers demonstrated that combining global information, such as the features from the across the entire face, and local information, such as features from discriminative fiducial regions, for example, eyes, nose and mouth, enhanced the discriminability of the synthesized imagery. They showed how the thermal-to-visible mapped representations from both global and local regions in the thermal face signature could be used in conjunction to synthesize a refined visible face image.The optimization problem for synthesizing an image attempts to jointly preserve the shape of the entire face and appearance of the local fiducial details. Using the synthesized thermal-to-visible imagery and existing visible gallery imagery, they performed face verification experiments using a common open source deep neural network architecture for face recognition. The architecture used is explicitly designed for visible-based face recognition. The most surprising result is that their approach achieved better verification performance than a generative adversarial network-based approach, which previously showed photo-realistic properties.Riggan attributes this result to the fact the game theoretic objective for GANs immediately seeks to generate imagery that is sufficiently similar in dynamic range and photo-like appearance to the training imagery, while sometimes neglecting to preserve identifying characteristics, he said. The approach developed by ARL preserves identity information to enhance discriminability, for example, increased recognition accuracy for both automatic face recognition algorithms and human adjudication.As part of the paper presentation, ARL researchers showcased a near real-time demonstration of this technology. The proof of concept demonstration included the use of a FLIR Boson 320 thermal camera and a laptop running the algorithm in near real-time. This demonstration showed the audience that a captured thermal image of a person can be used to produce a synthesized visible image in situ. This work received a best paper award in the faces/biometrics session of the conference, out of more than 70 papers presented.Riggan said he and his colleagues will continue to extend this research under the sponsorship of the Defense Forensics and Biometrics Agency to develop a robust nighttime face recognition capability for the Soldier.Story Source:Materials provided by U.S. Army Research Laboratory. ",0.0859168,0.100512,0.144996,0.294167,0.134986,0.1426,0.18526,0,0
595,Scientists decry lack of science in 'forensic science' Many forensic methods have never been scientifically validated,"Many of the ""forensic science"" methods commonly used in criminal cases and portrayed in popular police TV dramas have never been scientifically validated and may lead to unjust verdicts, according to an editorial in this week's Proceedings of the National Academy of Sciences.The authors were the six independent scientists who served on the National Commission for Forensic Science (NCFS), a panel set up in 2013 by the U.S. Department of Justice and the National Institute of Standards and Technology. The NCFS was terminated early last year when the DOJ did not renewed its charter.The scientists wrote their editorial as an appeal to the scientific community to help maintain the momentum in putting forensic science on a firmer scientific basis. The co-authors include colleagues from West Virginia University, Cornell University, the Salk Institute for Biological Studies, Brown University and the University of Arizona.""We wanted to alert people that this is a continuing and a major issue: that many of the forensic techniques used today to put people in jail have no scientific backing,"" says senior author Arturo Casadevall, MD, Bloomberg Distinguished Professor and the Alfred & Jill Sommer Professor and Chair of Molecular Microbiology and Immunology at Johns Hopkins Bloomberg School of Public Health.Casadevall and colleagues argue in their essay that the problems with forensic science are rooted in the discipline's traditional close association with prosecutors and law enforcement. ""Almost all publicly funded [forensic] laboratories, whether federal, state, or local, are associated with law enforcement. At the very least, this creates a potential conflict-of-interest,"" they note.Traditional forensic methods aimed at linking suspects to patterns found at the crime scene -- fingerprints, bite-marks, footprints, tire-marks -- were developed by law-enforcement laboratories and matured largely outside of mainstream scientific culture. Once these methods became admissible in criminal cases they continued to be used in court by the principle of judicial precedent. Yet while fingerprint analysis recently has been reformed into a more quantified, objective, empirically based method, most other traditional pattern-matching methods continue to be used despite insufficient understanding of their accuracy and reliability.""The forensic experts might claim, for example, that the fibers isolated from a body 'match' the fibers in a rug that was in the back of the suspect's truck, but generally they won't be able to say what the error rate in their conclusion is"" Casadevall says. ""In other words, they haven't properly characterized the uncertainties involved in concluding that it is a match.""A major catalyst for a re-think of forensic methods was DNA analysis, which entered widespread use in forensics in the 1990s and had an unprecedentedly high accuracy and reliability. DNA analysis overturned many convictions that had been based on traditional, less precise methods, and forced the recognition that these traditional methods were overdue for scientific scrutiny.At the behest of Congress, the National Academy of Sciences (NAS) convened a panel to study the problem, and in 2009 the NAS panel issued a report that was sharply critical of contemporary forensic science. The authors concluded that ""with the exception of nuclear DNA analysis ... no forensic method has been rigorously shown to have the capacity to consistently, and with a high degree of certainty, demonstrate a connection between evidence and a specific individual or source.""The NAS report also recommended the establishment of a research organization that would be dedicated to reforming forensic science and bringing it into mainstream scientific culture. The report urged that this entity be independent of the Department of Justice (DOJ), which understandably has little institutional interest in questioning older methods, since that could lead to the overturning of old verdicts.In the end, though, DOJ prevailed. The NCFS was established to increase communication among academic scientists, judges, prosecutors, lawyers, and the forensic science community, but it was placed under the joint administration of the DOJ and the National Institute of Standards and Technology (NIST). Following the 2016 presidential election, the winds shifted and DOJ declined to renew the commission.""It's a shame -- we had been making a lot of progress,"" Casadevall says. ""Commissioners were realizing that in addressing these questions they might be able to improve their methods and the justice system.""Casadevall sees little chance that the NCFS will be resurrected by the current administration, but he and his colleagues hope that by drawing attention to the problem, they can inspire the scientific and forensic communities to keep pushing for reforms, for example, mandatory empirical testing of all methods that are currently admissible in court.""We're hoping to keep the issue alive,"" he says, ""and we can see that a lot of otherwise educated and well-informed people still don't understand the extent of the problem.""Story Source:Materials provided by Johns Hopkins University Bloomberg School of Public Health. ",0.0710556,0.115114,0.152457,0.215471,0.103934,0.139145,0.159139,0,0
596,New laser technique may help detect chemical warfare in atmosphere,"The Department of Homeland Security could benefit from a reliable, real-time instrument that could scan the atmosphere for toxic agents in order to alert communities to a biological or chemical attack. UCF optics and photonics Professor Konstantin Vodopyanov is developing just such a technology to accomplish that.He has found a new way to use infrared lasers to detect even trace amounts of chemicals in the air. Every chemical is made up of individual molecules that vibrate at their own unique frequency. Vodopyanov has found a way to use lasers to detect these vibrations.The technique is so accurate and sensitive that he can determine if there is a molecule of any chemical present even at concentrations as low as one part per billion. So even if someone tried to hide the toxic chemicals, his technique would be able to detect them.His findings are published online this week in Nature Photonics.""We still have much work ahead,"" he said. ""We are now working on broadening the range of the laser frequencies that can get the job done. If costs can be reduced and the tech made mobile, the applications could be endless.""A similar principle is used in the medical field to detect biomarkers for different kinds of health conditions, including cancer, by taking breath samples.It's possible, Vodopyanov said, because of the rules of physics.""The frequencies of molecules are very distinct, but they are invariant -- here, on a different continent, on a different planet, anywhere,"" Vodopyanov said. ""It is universal. Think of it as a molecular fingerprint. So when we use the laser we can detect these fingerprints with great precision.""The novel approach could open the door for developing non-invasive technology, including sensors, that could be used to detect:To see a video explanation click here: https://youtu.be/PPBZeLLXdoYOther collaborators on the Nature Photonics paper include Andrey Muraviev at UCF's the College of Optics & Photonics, Viktor Smolski of IPG Photonics -- Mid-Infrared Lasers in Birmingham, AL, and Zachary Loparo from UCF's Department of Mechanical and Aerospace Engineering.Story Source:Materials provided by University of Central Florida. Original written by Zenaida Gonzalez Kotala. ",0.121192,0.167792,0.153236,0.183943,0.174298,0.164529,0.168713,0,0
597,Measurement chip detects Legionella Microarray rapid test speeds up detection in case of a Legionella pneumophila outbreak,"In an outbreak of Legionnaires' disease, finding the exact source as quickly as possible is essential to preventing further infections. To date, a standard analysis takes days. Researchers at the Technical University of Munich have now developed a rapid test that achieves the same result in about 35 minutes.Legionella are rod-shaped bacteria that can cause life-threatening pneumonia in humans. They multiply in warm water and can be dispersed into the air via cooling towers, evaporative recooling systems and hot water systems.The most dangerous among the almost 50 species of Legionella is Legionella pneumophila. It is responsible for 80 percent of all infections. When an outbreak occurs, the source of the germs must be identified as soon as possible to prevent further infections.Similar to a paternity test, the origin of the outbreak is confirmed when the germs in the process water of a technical system exactly match those identified in the patient. However, often numerous systems must be tested in the process, and the requisite cultivation for the test takes around ten days.Faster detection with antibodiesMeanwhile there is a rapid test for detecting the Legionella pathogen in the clinic. It identifies compounds of Legionella in the urine of patients. ""Unfortunately, this quick test serves only as a first indication and is not suitable for screening the water of technical systems,"" says PD Dr. Michael Seidel, head of the research group at the Chair of Analytical Chemistry and Water Chemistry of the Technical University of Munich.The team of scientists thus developed a measuring chip in the context of the ""LegioTyper"" project funded by the German Federal Ministry of Education and Research. This chip not only detects the dangerous pathogen Legionella pneumophila but also identifies which of the approximately 20 subtypes is present.Fast, inexpensive and versatileThe foil-based measuring chip uses the microarray analysis platform MCR of the Munich company GWK GmbH. Using 20 different antibodies, the system provides a complete analysis within 34 minutes.""Compared to previous measurements, the new method not only provides a huge speed advantage,"" says Michael Seidel, ""but is also so cheap that we can use the chip in one-time applications.""The system can be deployed for environmental hygiene as well as clinical diagnostics. In combination with a second, DNA-based method, the system can even distinguish between dead and living Legionella pathogens. This allows the success of disinfection measures to be monitored. The project participants will present their system to the public for the first time at the Analytica 2018 trade fair in Munich (Hall 3, Booth 315).Story Source:Materials provided by Technical University of Munich (TUM). ",0.0720237,0.106407,0.141508,0.185687,0.102626,0.13486,0.144634,0,0
598,Smart heat control of microchips,"Technological progress in the electronics sector, such as higher speeds, reduced costs, and smaller sizes, result in entirely new possibilities of automation and industrial production, without which ""Industry 4.0"" would not be feasible. In particular, miniaturization advanced considerably in the last years. Meanwhile, physical flow of a few electrons is sufficient to execute a software. But this progress also has its dark side. Processors for industrial production of less than 10 nm in dimension are highly sensitive. By specific overloading through incorrect control commands, hackers might initiate an artificial aging process that will destroy the processors within a few days. To defend such attacks on industrial facilities in the future, researchers of KIT are now working on a smart self-monitoring system.The new approach is based on identifying thermal patterns during normal operation of processors. ""Every chip produces a specific thermal fingerprint,"" explains Professor Jerg Henkel, who heads the team at the Chair for Embedded Systems (CES). ""Calculations are carried out, something is stored in the main memory or retrieved from the hard disk. All these operations produce short-term heating and cooling in various areas of the processor."" Henkel's team monitored this pattern with sensitive infrared cameras and reproduced changes in the control routine from minimum temperature changes or temporal deviations of a few milliseconds. The setup with infrared cameras was used to demonstrate feasibility of such thermal monitoring. In the future, sensors on the chip are planned to assume the function of the cameras. ""We already have temperature sensors on chips. They are used for overheat protection,"" Jerg Henkel says. ""We will increase the number of sensors and use them for cyber security purposes for the first time."" In addition, the scientists want to equip the chips with neural networks to identify thermal deviations and to monitor the chip in real time.The researchers think that their smart heat control will be applied in industrial facilities first. As mostly static control routines are executed, deviations are easier to identify than in a smartphone, for instance. However, also industry computers are exposed to dynamic threats. ""As soon as the hackers will know that we monitor temperature, they will adapt,"" computer scientist Hussam Amrouch, who works in the team of Jerg Henkel, explains. ""They will write smaller or slower programs, whose heating profiles will be more difficult to identify."" Right from the start, the neural networks will therefore be trained to identify even modified threats.Story Source:Materials provided by Karlsruhe Institute of Technology. ",0.0731712,0.0797907,0.102833,0.247721,0.120432,0.132949,0.146661,0,0
599,Real-time Captcha technique improves biometric authentication,"A new login authentication approach could improve the security of current biometric techniques that rely on video or images of users' faces. Known as Real-Time Captcha, the technique uses a unique ""challenge"" that's easy for humans -- but difficult for attackers who may be using machine learning and image generation software to spoof legitimate users.The Real-Time Captcha requires users to look into their mobile phone's built-in camera while answering a randomly-selected question that appears within a Captcha on the screens of the devices. The response must be given within a limited period of time that's too short for artificial intelligence or machine learning programs to respond. The Captcha would supplement image- and audio-based authentication techniques that can be spoofed by attackers who may be able to find and modify images, video and audio of users -- or steal them from mobile devices.The technique will be described February 19th at the Network and Distributed Systems Security (NDSS) Symposium 2018 in San Diego, Calif. Supported by the Office of Naval Research (ONR) and the Defense Advanced Research Projects Agency (DARPA), the research was conducted by cyber security specialists at the Georgia Institute of Technology.""The attackers now know what to expect with authentication that asks them to smile or blink, so they can produce a blinking model or smiling face in real time relatively easily,"" said Erkam Uzun, a graduate research assistant in Georgia Tech's School of Computer Science and the paper's first author. ""We are making the challenge harder by sending users unpredictable requests and limiting the response time to rule out machine interaction.""As part of efforts to eliminate traditional passwords for logins, mobile devices and online services are moving to biometric techniques that utilize a human face, retina or other biological attribute to verify who is attempting to log in. The iPhone X is designed to unlock with the user's face, for instance, while other systems utilize short video segments of a user nodding, blinking or smiling.In the cat-and-mouse game of cyber security, those biometrics can be spoofed or stolen, which will force companies to find better approaches, said Wenke Lee, a professor in Georgia Tech's School of Computer Science and co-director of the Georgia Tech Institute for Information Security and Privacy.""If the attacker knows that authentication is based on recognizing a face, they can use an algorithm to synthesize a fake image to impersonate the real user,"" Lee said. ""But by presenting a randomly-selected challenge embedded in a Captcha image, we can prevent the attacker from knowing what to expect. The security of our system comes from a challenge that is easy for a human, but difficult for a machine.""In testing done with 30 subjects, the humans were able to respond to the challenges in one second or less. The best machines required between six and ten seconds to decode the question from the Captcha and respond with a faked video and audio. ""This allows us to determine quickly if the response is from a machine or a human,"" Uzun said.The new approach would require login requests to pass four tests: successful recognition of a challenge question from within a Captcha, response within a narrow time window that only humans can meet, and successful matches to both the legitimate user's pre-recorded image and voice.""Using face recognition alone for authentication is probably not strong enough,"" said Lee. ""We want to combine that with Captcha, a proven technology. If you combine the two, that will make face recognition technology much stronger.""Captcha technology -- originally an acronym for ""Completely Automated Public Turing test to tell Computers and Humans Apart"" -- is widely used to prevent bots from accessing forms on websites. It works by taking advantage of a human's superior ability to recognize patterns in images. The Real-Time Captcha approach would go beyond what's required on websites by prompting a response that will produce live video and audio that would then be matched against a user's stored security profile.Captcha challenges might involve recognizing scrambled letters or solving simple math problems. The idea would be to allow humans to respond before machines can even recognize the question.""Making a still image smile or blink takes a machine just a few seconds, but breaking our Captcha changes takes ten seconds or more,"" said Uzun.In trying to improve authentication, the researchers studied image spoofing software and decided to try a new approach, hoping to open a new front in the battle against attackers. The approach moves the attacker's task from that of generating convincing video to breaking a Captcha.""We looked at the problem knowing what the attackers would likely do,"" said Simon Pak Ho Chung, a research scientist in Georgia Tech's School of Computer Science. ""Improving image quality is one possible response, but we wanted to create a whole new game.""The real-time Captcha approach shouldn't significantly change bandwidth requirements since the Captcha image sent to mobile devices is small and authentication schemes were already transmitting video and audio, Chung said.Among the challenges going forward is overcoming the difficulty of recognizing speech in a noisy environment and securing the connection between the device camera and the authenticating server.""For any security mechanism that we develop, we need to worry about the security of the mechanism first,"" Lee said. ""Once you develop security technology, it becomes a target for the attackers, and that certainly applies to biometric technology.""Story Source:Materials provided by Georgia Institute of Technology. ",0.0637192,0.0803209,0.12252,0.35159,0.120146,0.124057,0.170937,0,0
600,How good a match is it? Putting statistics into forensic firearm identification,"On February 14, 1929, gunmen working for Al Capone disguised themselves as police officers, entered the warehouse of a competing gang, and shot seven of their rivals dead. The St. Valentine's Day Massacre is famous not only in the annals of gangland history, but also the history of forensic science. Capone denied involvement, but an early forensic scientist named Calvin Goddard linked bullets from the crime scene to Tommy guns found at the home of one of Capone's men. Although the case never made it to trial-and Capone's involvement was never proved in a court of law-media coverage introduced millions of readers to Goddard and his strange-looking microscope.That microscope had a split screen that allowed Goddard to compare bullets or cartridge cases, the metal cases a gun ejects after firing a bullet, side by side. If markings on the bullets or cases matched, that indicated that they were fired from the same gun. Firearms examiners still use that same method today, but it has an important limitation: After visually comparing two bullets or cartridge cases, the examiner can offer an expert opinion as to whether they match. But they cannot express the strength of the evidence numerically, the way a DNA expert can when testifying about genetic evidence.Now, a team of researchers at the National Institute of Standards and Technology (NIST) has developed a statistical approach for ballistic comparisons that may enable numerical testimony. While other research groups are also working on this problem, the advantages of the NIST approach include a low error rate in initial tests and that it is relatively easy to explain to a jury. The researchers described their approach in Forensic Science International.When comparing two cartridge cases, the NIST method produces a numerical score that describes how similar they are. It also estimates the probability that random effects might cause a false positive match-a concept similar to match probabilities for DNA evidence.""No scientific method has a zero error rate,"" said John Song, a NIST mechanical engineer and the lead author of the study. ""Our goal is to give the examiner a way to estimate the probability of this type of error so the jury can take that into account when deciding guilt or innocence.""The new approach also seeks to transform firearm identification from a subjective method that depends on an examiner's experience and judgement to one that is based on objective measurements. A andmark 2009 report from the National Academy of Sciences and a 2016 report from the President's Council of Advisors on Science and Technology both called for research that would bring about this transformation.The Theory Behind Forensic BallisticsWhen a gun is fired, and the bullet blasts down the barrel, it encounters ridges and grooves that cause it to spin, increasing the accuracy of the shot. Those ridges dig into the soft metal of the bullet, leaving striations. At the same time that the bullet explodes forward, the cartridge case explodes backward with equal force against the mechanism that absorbs the recoil, called the breech face. This stamps an impression of the breech face into the soft metal at the base of the cartridge case, which is then ejected from the gun.The theory behind firearm identification is that microscopic striations and impressions left on bullets and cartridge cases are unique, reproducible, and therefore, like ""ballistic fingerprints"" that can be used to identify a gun. If investigators recover bullets or cartridge cases from a crime scene, forensic examiners can test-fire a suspect's gun to see if it produces ballistic fingerprints that match the evidence.But bullets and cartridge cases that are fired from different guns might have similar markings, especially if the guns were consecutively manufactured. This raises the possibility of a false positive match, which can have serious consequences for the accused.A Statistical ApproachIn 2013, Song and his NIST colleagues developed an algorithm that compares three-dimensional surface scans of the breech face impressions on cartridge cases. Their method, called Congruent Matching Cells, or CMC, divides one of the scanned surfaces into a grid of cells, then searches the other surface for matching cells. The greater the number of matching cells, the more similar the two surfaces, and the more likely they are to have come from the same gun.In their recent study, the researchers scanned 135 cartridge cases that were fired from 21 different 9-millimeter pistols. This produced 433 matching image pairs and 4,812 non-matching pairs. To make the test even more difficult, most of the pistols were consecutively manufactured.The CMC algorithm classified all the pairs correctly. Furthermore, almost all the non-matching pairs had zero matching cells, with a handful having one or two due to random effects. All the matching pairs, on the other hand, had at least 18 matching cells. In other words, the matching and non-matching pairs fell into highly separated distributions based on the number of matching cells.""That separation indicates that the probability of random effects causing a false positive match using the CMC method is very low,"" said co-author and physicist Ted Vorburger.A Better Way to TestifyUsing well-established statistical methods, the authors built a model for estimating the likelihood that random effects would cause a false positive match. Using this method, a firearms expert would be able to testify about how closely the two cartridges match based on the number of matching cells, and also the probability of a random match, similar to the way forensic experts testify about DNA.Although this study did not include enough test-fires to calculate realistic error rates for actual casework, the study has demonstrated the concept. ""The next step is to scale up with much larger and more diverse datasets,"" said Johannes Soons, a NIST mechanical engineer and co-author of the study.With more diverse datasets, researchers will be able to create separate models for different types of guns and ammunition. That would make it possible to estimate random match rates for the various combinations that might be used in a crime.Other groups of researchers are working on ways to express the strength of evidence numerically, not only for firearms but also fingerprints and other types of pattern evidence. Many of those efforts use machine learning and artificial intelligence-based algorithms to compare patterns in the evidence. But it can be difficult to explain how machine-learning algorithms work.""The CMC method can be easily explained to a jury,"" Song said. ""It also appears to produce very low false positive error rates.""Story Source:Materials provided by National Institute of Standards and Technology (NIST). ",0.0790222,0.1173,0.120925,0.2061,0.116207,0.130443,0.148109,0,0
601,A step toward sensitive and fast gluten detection,"For people with celiac disease and gluten-sensitivities, the number of food options in the stores is growing. But current tests for gluten are not finding all of the substance in foods, resulting in some products being labeled ""gluten free"" when they really aren't. Now researchers reporting in ACS Sensors say they have developed a fast gluten detector that has the potential to detect and quantify different sources of gluten than those on the market today.Gluten is an array of proteins found in plants such as wheat, barley and oats. The enzyme-linked immunosorbent assay (ELISA) is the gold standard for sensing the levels of these proteins in foods. But this test is inconsistent, varies by manufacturer and can provide false negatives, which can result in health problems for those who are sensitive. Also, a different ELISA is needed for optimal detection of each type of gluten -- barley, wheat or oat -- as some people can be sensitive to proteins from one source but not another. Because of these limitations, scientists have been seeking alternative methods, such as DNA-based sensors and mass spectrometry, to do this testing. DNA-based sensors do not accurately reflect gluten content, and mass spectrometry, although accurate and sensitive, is costly and requires technical expertise. So, Kevin D. Dorfman, Scott P. White and C. Daniel Frisbie wanted to design a more comprehensive detector.The researchers developed an immunological assay based on floating gate transistors. Their test is in a device that includes tiny microchannels for a sample to move through. If a sample contains gluten, the substance can bind to one of three capture agents, which can be antibodies or a DNA-based aptamer, that specifically latch onto gluten proteins from certain sources. This binding causes a shift in the voltage read-out of the transistor and can provide a chemical fingerprint that tells researchers whether the gluten was from barley or wheat, for example. Compared to ELISA, the newly developed sensor produced results 45 minutes faster due to fewer processing steps and automated sampling. As with ELISA, the detectors could sense less than 20 parts per million of gluten, which is the allotted maximum limit by the U.S. Food and Drug Administration for a ""gluten-free"" designation.Story Source:Materials provided by American Chemical Society. ",0.0950147,0.145205,0.151076,0.191777,0.170248,0.132762,0.154306,0,0
602,"Your smartphone's next trick? Fighting cybercrime Like bullets fired from a gun, photos can be traced to individual smartphones, opening up new ways to prevent identity theft","Not comfortable with Face ID and other biometrics? This cybersecurity advancement may be for you.A University at Buffalo-led team of researchers has discovered how to identify smartphones by examining just one photo taken by the device. The advancement opens the possibility of using smartphones -- instead of body parts -- as a form of identification to deter cybercrime.""Like snowflakes, no two smartphones are the same. Each device, regardless of the manufacturer or make, can be identified through a pattern of microscopic imaging flaws that are present in every picture they take,"" says Kui Ren, the study's lead author. ""It's kind of like matching bullets to a gun, only we're matching photos to a smartphone camera.""The new technology, to be presented in February at the 2018 Network and Distributed Systems Security Conference in California, is not yet available to the public. However, it could become part of the authentication process -- like PIN numbers and passwords -- that customers complete at cash registers, ATMs and during online transactions.For people who've had their personal identification stolen, it could also help prevent cybercriminals from using that information to make purchases in their name, says Ren, PhD, SUNY Empire Innovation Professor in the Department of Computer Science and Engineering in UB's School of Engineering and Applied Sciences.How each camera is uniqueThe study -- ""ABC: Enabling Smartphone Authentication with Built-in Camera"" -- centers on an obscure flaw in digital imaging called photo-response non-uniformity (PRNU).Digital cameras are built to be identical. However, manufacturing imperfections create tiny variations in each camera's sensors. These variations can cause some of sensors' millions of pixels to project colors that are slightly brighter or darker than they should be.Not visible to the naked eye, this lack of uniformity forms a systemic distortion in the photo called pattern noise. Extracted by special filters, the pattern is unique for each camera.First observed in conventional digital cameras, PRNU analysis is common in digital forensic science. For example, it can help settle copyright lawsuits involving photographs.But it hasn't been applied to cybersecurity -- despite the ubiquity of smartphones -- because extracting it had required analyzing 50 photos taken by a camera, and experts though that customers wouldn't be willing to supply that many photos. Plus, savvy cybercriminals can fake the pattern by analyzing images taken with a smartphone that victims post on unsecured websites.Applying the technique to cybersecurityThe study addresses how each of these challenges can be overcome.Compared to a conventional digital camera, the image sensor of a smartphone is much smaller. The reduction amplifies the pixels' dimensional non-uniformity and generates a much stronger PRNU. As a result, it's possible to match a photo to a smartphone camera using one photo instead of the 50 normally required for digital forensics.""I think most people assumed you would need 50 images to identify a smartphone camera. But our research shows that's not the case,"" says Ren, an IEEE (Institute of Electrical and Electronics Engineers) Fellow and an ACM (Association for Computing Machinery) Distinguished Scientist.To prevent forgeries, Ren designed a protocol -- it is part of the authentication process described below -- which detects and stops two types of attacks.How the new security protocol worksThe study discusses how such a system might work. First, a customer registers with a business -- such as a bank or retailer -- and provides that business with a photo that serves as a reference.When a customer initiates a transaction, the retailer asks the customer (likely through an app) to photograph two QR codes (a type of barcode that contains information about the transaction) presented on an ATM, cash register or other screen.Using the app, the customer then sends the photograph back to the retailer, which scans the picture to measure the smartphone's PRNU. The retailer can detect a forgery because the PRNU of the attacker's camera will alter the PRNU component of the photograph.More savvy cybercriminals could potentially remove the PRNU from their device. But Ren's protocol can spot this because the QR codes include an embedded probe signal that will be weakened by the removal process.The transaction is either approved or denied based upon these tests.Results and what's nextThe protocol defeats three of the most common tactics used by cybercriminals: fingerprint forgery attacks, man-in-the-middle attacks and replay attacks. It was 99.5 percent accurate in tests involving 16,000 images and 30 different iPhone 6s smartphones and 10 different Galaxy Note 5s smartphones.Ren plans to lead future experiments on smartphones that include two cameras, which he said could be used to make the forgery attacks more difficult.In addition to Ren, co-authors include Zhongjie Ba (UB), Sixu Piao (UB), Dimitrios Koutsonikolas (UB), Aziz Mohaisen (formerly of UB, and now of the University of Central Florida), and Xinwen Fu (University of Central Florida).Story Source:Materials provided by University at Buffalo. Original written by Cory Nealon. ",0.0779591,0.0902996,0.123974,0.270857,0.152727,0.123752,0.153707,0,0
603,Walk this way: A better way to identify gait differences Osaka University researchers design a gait recognition method that can overcome intra-subject variations by view differences,"Biometric-based person recognition methods have been extensively explored for various applications, such as access control, surveillance, and forensics. Biometric verification involves any means by which a person can be uniquely identified through biological traits such as facial features, fingerprints, hand geometry, and gait, which is a person's manner of walking.Gait is a practical trait for video-based surveillance and forensics because it can be captured at a distance on video. In fact, gait recognition has been already used in practical cases in criminal investigations. However, gait recognition is susceptible to intra-subject variations, such as view angle, clothing, walking speed, shoes, and carrying status. Such hindering factors have prompted many researchers to explore new approaches with regard to these variations.Research harnessing the capabilities of deep learning frameworks to improve gait recognition methods has been geared to convolutional neural network (CNN) frameworks, which take into account computer vision, pattern recognition, and biometrics. A convolutional signal means combining any two of these signals to form a third that provides more information.An advantage of a CNN-based approach is that network architectures can easily be designed for better performance by changing inputs, outputs, and loss functions. Nevertheless, a team of Osaka University-centered researchers noticed that existing CNN-based cross-view gait recognition fails to address two important aspects.""Current CNN-based approaches are missing the aspects on verification versus identification, and the trade-off between spatial displacement, that is, when the subject moves from one location to another,"" study lead author Noriko Takemura explains.Considering these two aspects, the researchers designed input/output architectures for CNN-based cross-view gait recognition. They employed a Siamese network for verification, where an input is a pair of gait features for matching, and an output is genuine (the same subjects) or imposter (different subjects) probability.Notably, the Siamese network architectures are insensitive to spatial displacement, as the difference between a matching pair is calculated at the last layer after passing through the convolution and max pooling layers, which reduces the gait image dimensionality and allows for assumptions to be made about hidden features. They can therefore be expected to have higher performance under considerable view differences. The researchers also used CNN architectures where the difference between a matching pair is calculated at the input level to make them more sensitive to spatial displacement.""We conducted experiments for cross-view gait recognition and confirmed that the proposed architectures outperformed the state-of-the-art benchmarks in accordance with their suitable situations of verification/identification tasks and view differences,"" coauthor Yasushi Makihara says.As spatial displacement is caused not only by view difference but also walking speed difference, carrying status difference, clothing difference, and other factors, the researchers plan to further evaluate their proposed method for gait recognition with spatial displacement caused by other covariates.Story Source:Materials provided by Osaka University. ",0.0954579,0.123412,0.165524,0.339514,0.163596,0.152003,0.238321,0,0
604,A new advanced forensics tool: Recovering erased serial numbers in polymers,"Polymers are highly prized by industry and increasingly used as replacements for metals in the manufacture of e.g. automobile parts and firearms. Such parts are marked with serial numbers, for security and traceability purposes. The numbers may however be partially or completely erased, and although there are techniques for recovering them from metal parts, this is not the case for polymers. But that may be about to change. In an article published in Analytical Chemistry, researchers have demonstrated the potential of a non-destructive method for making abraded serial numbers on polymers visible again.Cedric Parisien, an INRS master's student in energy science and materials, used Raman spectroscopy to reconstruct erased information from a sample of polycarbonate, a polymer that figures prominently in the manufacture of bulletproof products. The technique could be of great benefit to forensics, since it requires no pre-treatment and does not damage the sample.""The stamping technique used to engrave serial numbers creates deformations deep in the material,"" said INRS professor and study coauthor Andreas Ruediger. ""We used Raman spectroscopy to reveal those changes and get a kind of fingerprint for use in recovering erased images, without recourse to thermal, chemical, or other treatments.""Other studies are underway to test the reliability of this new method on various other polymers and materials such as ceramics, both for security applications and in quality control.Story Source:Materials provided by Institut national de la recherche scientifique - INRS. ",0.105717,0.210906,0.170471,0.245154,0.164412,0.160311,0.231979,0,0
605,Mystery of how black widow spiders create steel-strength silk webs further unravelled 'Modified micelle theory' may allow scientists to create equally strong synthetic materials,"Researchers at Northwestern University and San Diego State University (SDSU) have better unraveled the complex process of how black widow spiders transform proteins into steel-strength fibers. This knowledge promises to aid scientists in creating equally strong synthetic materials.Black widow spiders and their relatives, native to temperate climates in North America, Europe, Asia, Australia, Africa and South America, produce an array of silks with exceptional materials properties.Scientists have long known the primary sequence of amino acids that make up some spider silk proteins and understood the structure of the fibers and webs. Previous research theorized that spider silk proteins await the spinning process as nano-size amphiphilic spherical micelles (clusters of water soluble and non-soluble molecules) before being funneled through the spider's spinning apparatus to form silk fibers. However, when scientists attempted to replicate this process, they were unable to create synthetic materials with the strengths and properties of native spider silk fibers.""The knowledge gap was literally in the middle,"" Northwestern's Nathan C. Gianneschi said. ""What we didn't understand completely is what goes on at the nanoscale in the silk glands or the spinning duct -- the storage, transformation and transportation process involved in proteins becoming fibers.""Gianneschi is the Jacob and Rosaline Cohn Professor in the department of chemistry in the Weinberg College of Arts and Sciences and in the departments of materials science and engineering and of biomedical engineering in the McCormick School of Engineering. He and Gregory P. Holland, associate professor in the department of chemistry and biochemistry at SDSU and the author of more than 40 papers on spider silk, are the paper's co-corresponding authors.The research will be published online the week of Oct. 22 in the Proceedings of the National Academy of Sciences (PNAS).Utilizing complementary, state-of-the-art techniques -- nuclear magnetic resonance (NMR) spectroscopy, the same technology utilized in MRI, at SDSU, followed by electron microscopy at Northwestern -- the research team was able to more closely see inside the protein gland where the silk fibers originate, revealing a much more complex, hierarchical protein assembly.This ""modified micelles theory"" concludes that spider silk proteins do not start out as simple spherical micelles, as previously thought, but instead as complex, compound micelles. This unique structure is potentially required to create the black widow spider's impressive fibers.""We now know that black widow spider silks are spun from hierarchical nano-assemblies (200 to 500 nanometers in diameter) of proteins stored in the spider's abdomen, rather than from a random solution of individual proteins or from simple spherical particles,"" Holland said.If duplicated, ""the practical applications for a material like this are essentially limitless,"" Holland said, and could include high-performance textiles for military, first responders and athletes; building materials for cable bridges and other construction; environmentally friendly replacements for plastics; and biomedical applications.""One cannot overstate the potential impact on materials and engineering if we can synthetically replicate this natural process to produce artificial fibers at scale,"" said Gianneschi, who also is the associate director of the International Institute for Nanotechnology and a member of the Simpson Querrey Institute and the Chemistry of Life Processes Institute at Northwestern. ""Simply put, it would be transformative.""Story Source:Materials provided by Northwestern University. ",0.10541,0.191223,0.216001,0.193266,0.155144,0.171049,0.191098,0,0
606,Engineered sand zaps storm water pollutants Technology could help provide local source of drinking water for parched communities,"University of California, Berkeley, engineers have created a new way to remove contaminants from storm water, potentially addressing the needs of water-stressed communities that are searching for ways to tap the abundant and yet underused source of fresh drinking water.Using a mineral-coated sand that reacts with and destroys organic pollutants, the researchers have discovered that the engineered sand could help purify storm water percolating into underground aquifers, creating a safe and local reservoir of drinking water for parched communities.""The way we treat storm water, especially in California, is broken. We think of it as a pollutant, but we should be thinking about it as a solution,"" said Joseph Charbonnet, a graduate student in civil and environmental engineering at UC Berkeley. ""We have developed a technology that can remove contamination before we put it in our drinking water in a passive, low-cost, non-invasive way using naturally-occurring minerals.""As rain water rushes over our roofs, lawns and streets, it can pick up a slew of nasty chemicals such as herbicides, pesticides, toxic metals, car oil and even dog poop. Excess storm water can also overwhelm sewer systems and flood streets and basements. Not surprisingly, cities often discharge this polluted water into neighboring rivers and streams as quickly as possible.Directing storm water through sand into underground aquifers may be an ideal solution for gathering water in cities with Mediterranean climates like Los Angeles, Charbonnet said. Like giant rain barrels, aquifers can be filled during periods of intense rainfall and then store water until it is needed in the dry season.Cities are already using storm water reclamation on smaller scales through constructs such as bioswales and rain gardens, which funnel storm water through sand or mulch to remove debris and prevent surface runoff. In the Sun Valley neighborhood of Los Angeles, Charbonnet and his adviser, David Sedlak, are working with the local community to transform a 46-acre gravel pit into a wetland and water infiltration system for storm water.""Before we built the buildings, roads and parking lots that comprise our cities, rainwater would percolate into the ground and recharge groundwater aquifers,"" said Sedlak, professor of civil and environmental engineering at UC Berkeley and co-director of the Berkeley Water Center. ""As utilities in water stressed regions try to figure out how to get urban storm water back into the ground, the issue of water quality has become a major concern. Our coated sands represent an inexpensive, new approach that can remove many of the contaminants that pose risks to groundwater systems where storm water is being infiltrated.""Although the coated sand doesn't remove all types of contaminants, it may be used in conjunction with other water purification systems to remove many of the contaminants that water picks up, Sedlak said.The team details the finding Aug. 30 in the journal Environmental Science & Technology.To create the coated sand, Charbonnet mixed plain sand with two forms of manganese that react to form manganese oxide. This harmless mineral binds to organic chemicals such as herbicides, pesticides, and the endocrine-disrupting bisphenol-A (BPA) and breaks them down into smaller pieces that are usually less toxic and more biodegradable.""Manganese oxides are something that soil scientists identified 30 or 40 years ago as having these really interesting properties, but we are one of the first groups to use it in engineered ways to help unlock this water source,"" Charbonnet said.The manganese oxide-coated sand, which is a dull brown color, is safe and environmentally friendly. ""I guarantee that you have some manganese oxide on your shoe right now because it is ubiquitous in the soil,"" Charbonnet said.Charbonnet tested the sand by percolating simulated storm water, which contained a low concentration of BPA, through columns of the material. The coated sand initially removed nearly all of the BPA, but lost its effectiveness over time. However, the manganese oxide could be ""recharged"" by bathing the sand in a solution containing a low concentration of chlorine. Recharging the sand restored all of the manganese oxide's initial reactivity.""If you have to come in every year or two and dig up this sand and replace it, that is incredibly labor intensive, so in order to make this useful for community stakeholders it's really important that this stuff can be regenerated in place,"" Charbonnet said.Charbonnet estimates that it would take about two days to recharge a half-meter-deep layer of sand using 25 parts per million of chlorine in water, the concentration used to treat wastewater.In the next phase of the experiment, the team is performing field tests in Sonoma County using storm water from a local creek.Story Source:Materials provided by University of California - Berkeley. Original written by Kara Manke. ",0.0705078,0.124381,0.334823,0.159048,0.120319,0.139097,0.158745,0,0
607,Humanmade mangroves could get to the 'root' of the problem for threats to coastal areas,"With threats of sea level rise, storm surge and other natural disasters, researchers from Florida Atlantic University's College of Engineering and Computer Science are turning to nature to protect humans from nature. They are developing innovative ways to guard coastlines and prevent scouring and erosion from waves and storms using bioinspired materials that mimic mangrove trees found along shores, rivers and estuaries in the tropics and subtropics. Growing from a tangle of roots that twist their way out of the mud, mangrove trees naturally protect shorelines, shelter coastal ecosystem habitats and provide important water filtration. In many cases, these roots trap sediments flowing down rivers and off the land, helping to stabilize the coastline.Certain mangrove root systems even have the ability to dissipate tidal energy through unique hydrological flows and divert the energy of water in different directions reducing risk of coastal damage. Yet, to date, few studies have examined the fluid dynamics such as flow structure and drag force on mangrove roots.For a study, published in the American Physical Society's journal, Physical Review Fluids, researchers singled out the red mangrove tree (Rhizophora mangle) from more than 80 different species of mangroves, because of its robust network of roots that can withstand extreme environmental conditions. The red mangrove provided the researchers with an ideal model for bioinspired shoreline applications.""Because of their strong structures, mangroves have survived for more than 8,000 years,"" said Amirkhosro Kazemi, Ph.D., lead author of the study and a post-doctoral fellow in FAU's Department of Ocean and Mechanical Engineering , who was awarded a Link Foundation fellowship and is working with Oscar Curet, Ph.D., co-author and an assistant professor in the department. ""What is truly amazing about mangroves is that they can adjust to changes in rising sea levels by forming upward structures through a natural process of accumulating layers of mud carried by tides and other sources. It's their root system in particular that contributes to this resiliency and is what inspired us to research their complex hydrodynamics.""To better understand the mangrove tree's resilience and the fluid dynamics of its roots, Kazemi, Curet, and Keith Van de Riet, Ph.D., co-author and an assistant professor at the University of Kansas, modeled the complex mangrove roots as a network of circular cylinders called a patch. They performed a series of experiments varying key parameters such as length scale and porosity or flexibility. They used a water tunnel and flow visualization to determine how the diameter of the root, its flexibility and how porous the mangroves are affect the water. They studied the mangrove roots under different flow conditions to quantify how the flow structure would interact with the mangrove.They looked at the effect of porosity and spacing measures between the roots, tested force and velocity in a water tunnel, and concurrently performed 2D flow visualization.The researchers performed direct drag force measurements and high-resolution particle image velocimetry to characterize the complex unsteady wake structure posterior to the arrays of the patch, which represents a simplified mangrove root model.Results from the study show that for rigid roots, the drag force varied linearly with patch diameter and spacing between the roots. For flexible roots, the researchers discovered that a decrease in stiffness increased both the patch drag and the wake deficit behind the patch in a similar fashion as increasing the blockage of the patch. They have introduced a new length-scale (effective diameter) based on the wake signature to characterize the drag coefficient exerted on the patch for different porosities. The effective diameter incorporates the patch porosity, arrangement and individual root diameter in the patch. The results have proven that the effective diameter of the patch decreases as the porosity increases, giving rise to the Strouhal number -- used in dimensional analysis that is a dimensionless number describing oscillating flow mechanisms.""With nearly 2.4 billion people worldwide living within 60 miles of an oceanic coast, this research is extremely important for vulnerable coastlines not just in Florida but across the globe,"" said Stella Batalama, Ph.D., dean of FAU's College of Engineering and Computer Science. ""Improving our understanding of the hydrodynamics of mangrove roots will help to facilitate the incorporation of bioinspired mangrove-like structures that can be used for erosion control, coastal protection, and habitat reconstruction.""Although many low-lying areas have storm surge protection such as seawalls, these structures are expensive to build, cause their own set of environmental concerns, and obstruct the natural landscape. Information from this study has the potential to help scientists and engineers develop methods to design resilient bioinspired coastline structures. Natural shorelines are flexible, inexpensive, and adjustable, and the prototype the researchers have developed is scalable, smaller and simpler to use as well as more cost effective. Their systematic modeling provides the framework to engineer mangrove-like structures for coastal protection.""Our findings could potentially be used to build artificial mangrove banks for coastal areas. For example, our experimental work could even be applied in a uniform tidal flow where water flows constantly as the result of sea level rise,"" said Kazemi. ""We are currently working on a new model that will allow us to understand the flow in a more complex design.""Story Source:Materials provided by Florida Atlantic University. ",0.0568823,0.0634904,0.175602,0.119617,0.072066,0.102746,0.106216,0,0
608,Sudoku' X-Ray uncovers movements within opaque materials,"When strolling along the beach, our footprints tell us that the sand under the surface must have moved but not precisely where or how. Similar movements occur in many other natural and human-made substances, such as snow, construction materials, pharmaceutical powders, and even cereals.To examine these largely unknown granular movements, academics from the Sydney Centre in Geomechanics and Mining Materials (SciGEM) including Professor of Civil Engineering, Itai Einav and Postdoctoral Research Associate, Dr James Baker have developed a new X-ray method which allows scientists to see inside granular flows. Named X-ray rheography, or ""writing flow,"" their approach gathers information using 3-point high-speed radiography, and then assembles this information by solving a Sudoku-style puzzle.""Imagine coffee in grinders, rice in silos, and minerals on conveyors -- for a long time we've known that the grains hidden within the bulk move, but until now we have not understood precisely how. Our X-ray rheography is the first physical method to resolve this,"" explained Professor Einav.This breakthrough is the latest to arise from the team's ""DynamiX"" laboratory, a unique, custom-built facility that was established in 2015 to investigate flowing granular media. The laboratory consists of a large lead-lined X-ray room, with three movable source-detector pairs to allow interrogation of numerous different experiments, and an external control room.The new X-ray rheography technique has the ability to form a three dimensional image of moving grains, which has helped the researchers better understand how particles flow and behave in various circumstances. In many examples they have found that granular media tends to flow in unique patterns and waves.""Unlike fluids, we discovered that confined, three-dimensional steady granular flows arise through cycles of contraction and expansion, e la 'granular lungs'. Again, unlike fluids, we also found that grains tend to travel along parallel lines, even near curved boundaries.Experimenting with various granular materials, the team also found that shape is an important factor in determining flow, for example elongated barley grains move faster than their spherical counterparts. In other situations, different types of grains may have a tendency to group together, or ""segregate,"" much like when emptying a cereal box.The researchers' findings can be applied to a number of industries, such as helping develop better silo solutions for edible grains, preventing wastage and spoilage in both farming and large scale food manufacturing, as well as more efficient transportation and storage of mining resources.""Until now, understanding granular movements in opaque materials has long been a challenge for many industries such as engineering, science, mining and even agriculture,"" explained Dr James Baker.""The potential benefits span many areas, whether it be in understanding the mixing of pharmaceutical powders or the efficient transport of food grains or construction materials.""Professor Einav hopes that his team's creative approach in using a game of logic to answer a long misunderstood scientific issue may help rethink other research approaches, ""We have successfully applied a novel approach to an ongoing issue -- so we must ask how other games can help us make further breakthroughs?""Story Source:Materials provided by University of Sydney. ",0.0715957,0.106531,0.155763,0.177148,0.110579,0.127941,0.155368,0,0
609,Extremely strong and yet incredibly ductile multicomponent alloys developed Overcoming the critical issue of the strength-ductility trade-off dilemma for structural materials,"A research team led by City University of Hong Kong (CityU) has developed a novel strategy to develop new high-strength alloys which are extremely strong and yet also ductile and flexible. The strategy overcomes the critical issues of the strength-ductility trade-off dilemma, paving the way for developing innovative structural materials in future.Multiple-principal element alloys, generally referred as high-entropy alloys (HEAs), is a new type of materials constructed with equal or nearly equal quantities of five or more metals. They are currently the focus of attention in materials science and engineering due to their potentially desirable properties for structural applications. Yet most of the alloys share the same key detrimental feature: the higher the strength of an alloy, the less the ductility and toughness, meaning that strong alloys tend to be less deformable or stretchable without fracture.Recently, however, a research led by Professor Liu Chain Tsuan, University Distinguished Professor of the Department of Materials Science and Engineering at CityU, has found a breakthrough solution to this daunting decades-long dilemma -- by making high-entropy alloys both strong and yet also very ductile through massive precipitation of nanoscale particles. The research has just been published in the latest issue of the journal Science, titled ""Multicomponent intermetallic nanoparticles and superb mechanical behaviours of complex alloys.""Solving strength-ductility trade-off""We are able to make a new high-entropy alloy called Al7Ti7 ((FeCoNi)86-Al7Ti7) with a superior strength of 1.5 gigapascals and ductility as high as 50% in tension at ambient temperature. Strengthened by nanoparticles, this new alloy is five times stronger than that of the iron-cobalt-nickel (FeCoNi)-based alloy,"" says Professor Liu.""Most conventional alloys contain one or two major elements, such as nickel and iron to manufacture,"" he explains. ""However, by adding additional elements of aluminium and titanium to form massive precipitates in the FeCoNi-based alloy, we have found both the strength and ductility have significantly increased, solving the critical issue of the trade-off dilemma for structural materials.""Moreover, high-strength alloys usually face plastic deformation instability, known as the necking problem, meaning that when the alloy is under a high strength, its deformation would become unstable and very easily lead to necking fracture (localized deformation) with very limited uniform elongation. But the team has further found that by adding ""multicomponent intermetallic nanoparticles,"" meaning complex nanoparticles made of different element atoms, it can greatly strengthen the alloy uniformly by improving the deformation instability.Tackling ""necking problem""And they have found the ideal formula for these complex nanoparticles, which consists of nickel, cobalt, iron, titanium and aluminium atoms. Professor Liu explains that each of nanoparticle measuring 30 to 50 nanometres only. The iron and cobalt atoms which replace some of the nickel components helps to reduce the valence electron density and improve the new alloy's ductility. On the other hand, replacing some of the aluminium with titanium largely reduces the impact of moisture in air to avoid induced embrittlement in this new strong alloy.""This research opens up a new design strategy to develop superalloys, by engineering multicomponent nanoparticles to strengthen complex alloys to achieve superb mechanical properties at room and elevated temperatures,"" says Professor Liu.He believes that the new alloys developed with this novel strategy will perform well in temperatures ranging from -200eC to 1000eC. Hence they can act as a good base to further develop for structural use in cryogenic devices, aircraft and aeronautic systems and beyond.Professor Liu is the corresponding author of the paper, and Yang Tao, his PhD student and current senior research associate at CityU's Department of Materials Science and Engineering is the first-author. Other co-authors include Chair Professor Kai Jijung, Assistant professor Dr Alice Hu, post-doc fellows Zhao Yilu, Tong Yang, Wei Jie and PhD student Chen Da from CityU's Department of Mechanical Engineering and Center for Advanced Structural Materials; together with Dr Jiao Zengbao, Assistant Professor of Department of Mechanical Engineering at the Hong Kong Polytechnic University; Professor Han Xiaodong and Dr Cai Jixiang from the Institute of Microstructure and Property of Advanced Materials of Beijing University of Technology; Professor Lu Ke, Director of Shenyang National Laboratory for Materials Science, Institute of Metal Research, Chinese Academy of Sciences; and Professor Liu Yong from the State Key Laboratory of Powder Metallurgy, who is also the director of Powder Metallurgy Research Institute at Central South University.Story Source:Materials provided by City University of Hong Kong. ",0.0736043,0.155995,0.168061,0.174603,0.109338,0.143352,0.189144,0,0
610,Scientists engineer a functional optical lens out of 2D materials,"In optics, the era of glass lenses may be waning.In recent years, physicists and engineers have been designing, constructing and testing different types of ultrathin materials that could replace the thick glass lenses used today in cameras and imaging systems. Critically, these engineered lenses -- known as metalenses -- are not made of glass. Instead, they consist of materials constructed at the nanoscale into arrays of columns or fin-like structures. These formations can interact with incoming light, directing it toward a single focal point for imaging purposes.But even though metalenses are much thinner than glass lenses, they still rely on ""high aspect ratio"" structures, in which the column or fin-like structures are much taller than they are wide, making them prone to collapsing and falling over. Furthermore, these structures have always been near the wavelength of light they're interacting with in thickness -- until now.In a paper published Oct. 8 in the journal Nano Letters, a team from the University of Washington and the National Tsing Hua University in Taiwan announced that it has constructed functional metalenses that are one-tenth to one-half the thickness of the wavelengths of light that they focus. Their metalenses, which were constructed out of layered 2D materials, were as thin as 190 nanometers -- less than 1/100,000ths of an inch thick.""This is the first time that someone has shown that it is possible to create a metalens out of 2D materials,"" said senior and co-corresponding author Arka Majumdar, a UW assistant professor of physics and of electrical and computer engineering.Their design principles can be used for the creation of metalenses with more complex, tunable features, added Majumdar, who is also a faculty researcher with the UW's Molecular Engineering & Sciences Institute.Majumdar's team has been studying the design principles of metalenses for years, and previously constructed metalenses for full-color imaging. But the challenge in this project was to overcome an inherent design limitation in metalenses: in order for a metalens material to interact with light and achieve optimal imaging quality, the material had to be roughly the same thickness as the light's wavelength in that material. In mathematical terms, this restriction ensures that a full zero to two-pi phase shift range is achievable, which guarantees that any optical element can be designed. For example, a metalens for a 500-nanometer lightwave -- which in the visual spectrum is green light -- would need to be about 500 nanometers in thickness, though this thickness can decrease as the refractive index of the material increases.Majumdar and his team were able to synthesize functional metalenses that were much thinner than this theoretical limit -- one-tenth to one-half the wavelength. First, they constructed the metalens out of sheets of layered 2D materials. The team used widely studied 2D materials such as hexagonal boron nitride and molybdenum disulfide. A single atomic layer of these materials provides a very small phase shift, unsuitable for efficient lensing. So the team used multiple layers to increase the thickness, although the thickness remained too small to reach a full two-pi phase shift.""We had to start by figuring out what type of design would yield the best performance given the incomplete phase,"" said co-author Jiajiu Zheng, a doctoral student in electrical and computer engineering.To make up for the shortfall, the team employed mathematical models that were originally formulated for liquid-crystal optics. These, in conjunction with the metalens structural elements, allowed the researchers to achieve high efficiency even if the whole phase shift is not covered. They tested the metalens' efficacy by using it to capture different test images, including of the Mona Lisa and a block letter W. The team also demonstrated how stretching the metalens could tune the focal length of the lens.In addition to achieving a wholly new approach to metalens design at record-thin levels, the team believes that its experiments show the promise of making new devices for imaging and optics entirely out of 2D materials.""These results open up an entirely new platform for studying the properties of 2D materials, as well as constructing fully functional nanophotonic devices made entirely from these materials,"" said Majumdar.Additionally, these materials can be easily transferred on any substrate, including flexible materials, paving a way towards flexible photonics.Story Source:Materials provided by University of Washington. ",0.0614357,0.0896403,0.120519,0.138387,0.110902,0.0987325,0.118119,0,0
611,Creating better devices: The etch stops here,"A team of multi-disciplinary scientists and engineers at the University of Illinois at Urbana-Champaign have discovered a new, more precise, method to create nanoscale-size electromechanical devices. Their research findings are published in Nature Communications.""In the last five years, there has been a huge gold rush where researchers figured out we could make 2D materials that are naturally only one molecule thick but can have many different electronic properties, and by stacking them on top of each other, we could engineer nearly any electronic device at molecular sizes,"" said Arend van der Zande, professor of mechanical science and engineering.""The challenge was, though we could make these structures down to a few molecules thick, we couldn't pattern them,"" he said.At any scale of electronic device, layers are etched away in precise patterns to control how the current flows. ""This concept underlies many technologies, like integrated circuits. However, the smaller you go, the harder this is to do,"" said van der Zande.""For example, how do you make electrical contact on molecular layer three and five, but not on layer four at the atomic level?""A serendipitous discovery led to a method for doing just that.As a new postdoctoral researcher in van der Zande's lab, Jangyup Son was running some experiments on single layers of graphene using Xenon difluoride, XeF2, when he happened to ""throw in"" another material on hand: hexagonal Boron Nitride (hBN), an electrical insulator.""Jangyup shoved both materials into the etching chamber at the same time, and what he saw was that a single layer of graphene was still there, but a thick piece of hBN was completely etched away by the Xenon difluoride.""This accidental discovery led the team to see where they could apply graphene's ability to withstand the etching agent.""This discovery allowed us to pattern two-dimensional structures by placing layers of graphene between other materials, such as hexagonal boron nitride (hBN), transition metal dichalcogenides (TMDCs), and black phosphorus (BP), to selectively and precisely etch one layer without etching the layer underneath.""Graphene, when exposed to the etching agent XeF2, retains its molecular structure and masks, or protects, the layer below and actually stops the etch.""What we've discovered is a way to pattern complicated structures down to a molecular and atomic scale,"" he said.To explore the strengths of the new technique, the group created a simple graphene transistor to test its performance relative to traditionally made graphene transistors, which are currently patterned in a way that induces disorder in the material, degrading their performance.""Because these molecules are all surface, if you have it sitting on anything with any disorder at all, it messes up the ability for the electrons to move through the material and thus the electronic performance,"" said van der Zande. ""In order to make the best device possible, you need to encapsulate the graphene molecule in another two-dimensional material such as insulating hBN to keep it super flat and clean.""This is where the new technique is so useful. The graphene molecule can remain encapsulated and pristine, while withstanding the etching needed to make contact with the material, thereby preserving the material's properties.As proof of concept, the transistors made using the new technique out-performed all other transistors, ""making them the best graphene transistors so far demonstrated in the literature.""The next steps, said van der Zande, are to see how scalable the technique is and whether it will enable previously impossible devices. Can we take advantage of the self-arresting nature of this technique to make a million identical transistors rather than just one? Can we pattern devices down to the nanoscale in all three dimensions at the same time to make nanoribbons without any disorder?""Now that we have a way of minimizing the disorder within the material, we are exploring ways to make smaller features because we can do encapsulation and patterning at the same time,"" he said. ""Normally, when you try to make smaller features like nanoribbons of 2D materials the disorder begins to dominate, so the devices do not work properly.""""The graphene etch stop, as the technique is called, will make the entire process of building devices easier.""The research involved a multi-disciplinary collaboration of people and shared facilities equipment from the Materials Research Laboratory and the Micro & Nanotechnology Lab. Expert faculty include: Associate Professor of Physics and Director of the Illinois Materials Research Science and Engineering Center (MRSEC), Nadya Mason, for electronic transport; Associate Professor of Mechanical Science and Engineering, Elif Ertekin, for modeling interfaces; and Assistant Professor of Materials Science and Engineering, Pinshane Huang, for electron microscopy. MRSEC provided the primary funding for this research.Story Source:Materials provided by University of Illinois College of Engineering. ",0.0728942,0.138777,0.127417,0.161941,0.142323,0.115803,0.141668,0,0
612,"Fire ant colonies could inspire molecular machines, swarming robots ","Think of it as mathematics with a bite: Researchers at CU Boulder have uncovered the statistical rules that govern how gigantic colonies of fire ants (Solenopsis invicta) form bridges, ladders and floating rafts.In a new study, a team led by CU Boulder's Franck Vernerey set out to understand the engineering principles that underlie self-assembled structures of fire ants -- each one containing hundreds to thousands of insects or more. Specifically, the researchers wanted to lay out how those structures become so flexible, changing their shapes and consistencies in seconds.To do that, they used statistical mechanics to calculate the way that ant colonies respond to stresses from outside, shifting how they hang onto their neighbors based on key thresholds.The findings may help researchers understand other ""dynamic networks"" in nature, including cells in the human body, said Vernerey, an associate professor in the Department of Mechanical Engineering.Such networks ""are why human bodies can self-heal,"" Vernerey said. ""They are why we can grow. All of this is because we are made from materials that are interacting and can change their shape over time.""The study, published last week in the Journal of the Royal Society Interface, could also help engineers to craft new smart polymers and swarming robots that work together seamlessly.Fire ants are ""a bio-inspiration,"" said Shankar Lalitha Sridhar, a graduate student in mechanical engineering at CU Boulder and a coauthor of the new study. The goal is ""to mimic what they do by figuring out the rules.""The team first drew on experimental results from Georgia Tech University that demonstrated how ant colonies maintain their flexibility through a fast-paced dance. Those experiments showed that individual ants hang onto the insects next to them using the sticky pads on their feet. But they also don't stay still: In a typical colony, those ants may shift the position of their feet, grabbing onto a different neighbor every 0.7 seconds.The researchers from CU Boulder then turned to mathematic simulations to calculate how ant colonies manage that internal cha-cha.They discovered that as the forces, or shear, on ant colonies increase, the insects pick up their speed. If the force on an individual ant's leg hits more than eight times its body weight, the insect will compensate by switching between its neighbors twice as fast.""If you start increase your rate of shear, then you will start stretching their legs a little bit,"" Vernerey said. ""Their reaction will be, 'oh, we are being stretched here, so let's exchange our turnover rate.'""But if you keep increasing the forces on the ants, they can no longer keep up. When that happens, the ants will stop letting go of their neighbors, and instead hold on for dear life.""Now, they will be so stressed that they will behave like a solid,"" Vernerey said. ""Then at some point you just break them.""The researchers explained that they've only just scratched the surface of the mathematics of fire ant colonies. But their calculations are general enough that researchers can already begin using them to explore designs for new dynamic networks, including molecular machines that deliver drugs directly to cells.Story Source:Materials provided by University of Colorado at Boulder. Original written by Daniel Strain. ",0.100054,0.127333,0.154919,0.209713,0.133953,0.154931,0.177693,0,0
613,New devices to test retinal cells,"Researchers at Utah State University have developed new devices to mechanically stress human cells in the lab.In a study published in Lab on a Chip, researchers Elizabeth Vargis, a USU assistant professor of biological engineering and Farhad Farjood, a Ph.D. student in Vargis' Lab, wanted to better understand the triggers of age-related macular degeneration (AMD), a degenerative eye disease and the leading cause of adult blindness in developed countries. Physical changes within the retina are an important factor in the development of AMD. However, the effect of physical changes during the disease is not clearly understood.""Physical changes that occur prior to or during disease are difficult to model outside of the body,"" said Vargis. ""We know that these changes are important, so we decided to build devices to better replicate them.""Currently there are no devices to realistically model varying levels of physical disruption available on the market. Therefore the researchers created two new devices: one that mimics slow and continuous stress levels and one for mimicking high levels of stress.""We used these devices to replicate stress on retinal cells and found that mechanical stress results in the expression of vascular endothelial growth factor, a protein that can cause disease initiation and progression,"" said Farjood.The purpose of the study was to mimic changes in cells and find the mechanisms for the initiation and progression of diseases. The study looks at the effects of mechanical stress on elevated protein levels and abnormal development of new blood vessels.Besides AMD, mechanical stress can occur in other diseases including diabetic retinopathy and even cancer.""There are many clinical studies taking place to discover the causes of disease,"" said Farjood. ""Our work is an example of how engineering techniques can help us better understand the disease mechanisms.""Story Source:Materials provided by Utah State University. ",0.100949,0.168562,0.190456,0.236784,0.164271,0.14759,0.227062,0,0
614,Mussel-inspired defect engineering enhances the mechanical strength of graphene fibers,"Researchers demonstrated the mussel-inspired reinforcement of graphene fibers for the improvement of different material properties. A research group under Professor Sang Ouk Kim applied polydopamine as an effective infiltrate binder to achieve high mechanical and electrical properties for graphene-based liquid crystalline fibers.This bio-inspired defect engineering is clearly distinguishable from previous attempts with insulating binders and proposes great potential for versatile applications of flexible and wearable devices as well as low-cost structural materials. The two-step defect engineering addresses the intrinsic limitation of graphene fibers arising from the folding and wrinkling of graphene layers during the fiber-spinning process.Bio-inspired graphene-based fiber holds great promise for a wide range of applications, including flexible electronics, multifunctional textiles, and wearable sensors. In 2009, the research group discovered graphene oxide liquid crystals in aqueous media while introducing an effective purification process to remove ionic impurities. Graphene fibers, typically wet-spun from aqueous graphene oxide liquid crystal dispersion, are expected to demonstrate superior thermal and electrical conductivities as well as outstanding mechanical performance.Nonetheless, owing to the inherent formation of defects and voids caused by bending and wrinkling the graphene oxide layer within graphene fibers, their mechanical strength and electrical/thermal conductivities are still far below the desired ideal values. Accordingly, finding an efficient method for constructing the densely packed graphene fibers with strong interlayer interaction is a principal challenge.Professor Kim's team focused on the adhesion properties of dopamine, a polymer developed with the inspiration of the natural mussel, to solve the problem. This functional polymer, which is studied in various fields, can increase the adhesion between the graphene layers and prevent structural defects.Professor Kim's research group succeeded in fabricating high-strength graphene liquid crystalline fibers with controlled structural defects. They also fabricated fibers with improved electrical conductivity through the post-carbonization process of polydopamine.Based on the theory that dopamine with subsequent high temperature annealing has a similar structure with that of graphene, the team optimized dopamine polymerization conditions and solved the inherent defect control problems of existing graphene fibers.They also confirmed that the physical properties of dopamine are improved in terms of electrical conductivity due to the influence of nitrogen in dopamine molecules, without damaging the conductivity, which is the fundamental limit of conventional polymers.Professor Kim, who led the research, said, ""Despite its technological potential, carbon fiber using graphene liquid crystals still has limits in terms of its structural limitations."" This technology will be applied to composite fiber fabrication and various wearable textile-based application devices."" This work, in which Dr. In-Ho Kim participated as first author was selected as a front cover paper of Advanced Materials on October 4.Story Source:Materials provided by The Korea Advanced Institute of Science and Technology (KAIST). ",0.129949,0.207044,0.223856,0.217927,0.199185,0.180425,0.214166,0,0
615,High entropy alloys hold the key to studying dislocation avalanches in metals,"Mechanical structures are only as sound as the materials from which they are made. For decades researchers have studied materials from these structures to see why and how they fail. Before catastrophic failure, there are individual cracks or dislocations that form, which are signals that a structure may be weakening. While researchers have studied individual dislocations in the past, a team from the University of Illinois at Urbana-Champaign, the University of Tennessee, and Oak Ridge National Laboratory has made it possible to understand how dislocations organize and react at nanoscale.""Metals are made of polycrystals and the crystals have atoms arranged in an orderly way,"" explained lead author Jian-Mu Zuo, Ivan Racheff Professor of Materials Science and Engineering and an affiliate with the Frederick Seitz Materials Research Lab at Illinois. ""As force is applied in these metals, the crystal will slip and move against each other. A structure like a bridge might have a lot of dislocations, which can move, but the amount of movement is so small, it doesn't have a consequence. However, as thousands or tens of thousands of dislocations tangle within a metal, and they produce local stress. This organization can lead to sudden deformation, like a snow avalanche. That's very dramatic and much more difficult to control.""The team, which also includes Illinois condensed matter physicist Karin Dahmen, published its results in Communications Physics. The experimental work was done by Dr. Yang Hu, as part of his Ph. D thesis.Until this study researchers couldn't make sense of the mechanism behind dislocation avalanche within a structure. However, the Illinois team found that a series of dislocations piling up forming a dam to prohibit movement. Behind the dam are tangled dislocations. Once there is enough pressure, an avalanche forms causing the dam to give way and sudden movement of the tangled dislocations, which weakens the metal and can eventually lead to catastrophic failure. By having a better understanding of this process, this study promises to aid in developing even stronger materials in the future and to better predict when a structure may be in peril.In order to study the dislocations, which look like strings of as small as 10-9 meter in width, they followed the development of the dislocation avalanches in the compressed nanopillars of a high entropy alloy (HEA). The HEA has the same average structure as copper or gold. But the atoms are arranged in such a way to allow the researchers to do simultaneous measurements and to correlate dislocation motion with mechanical response and pinpoint exactly where the avalanche occurs. By identifying the dislocation bands, researchers are able to watch what happens before, during, and after the avalanche.""People have understood how individual dislocations move, but until this point they haven't understood how they move suddenly together,"" Zuo noted. ""Our innovation is to use a new material (the HEA) to study a very old problem and to develop this technique to do so.""Because the dislocations typically structure themselves at microns apart (think the network of cracks in a sheet of ice after walking on it), it makes it hard to pinpoint a single event by looking at them inside a microscope that only works with thin samples (inside a transmission electron microscope, the sample thickness is typically less than a micron).""In a conventional metal, the dislocations are too far apart than what we can see at one time, therefore they disappear on the surface"" Zuo explained. ""Also, a deformed metal has bunches of dislocations, but only a few that are actually active. Because of that, some scholars have commented when people look at the deformation afterward in the metal, it's like visiting a dislocation graveyard.""In order to witness a complete single avalanche, Zuo and his team needed to find a material where the dislocation interacts in a much smaller scale. The HEA is a new type of alloy comprised of five different metal elements (Al0.1CoCrFeNi). Because each metal atom has a different size and the crystal is distorted, it slows down the dislocation making it possible to store many dislocations and an avalanche within a relatively small volume.The Illinois researchers were able measure the dislocation through a technique called nanoindentation. They take a piece of HEA and use an ion beam to fabricate a nanopillar and apply the force to the nanopillar with a small flat diamond tip of a nanoindenter.""This material allows us to look at dislocations on the nanoscale (500 nanometers),"" said Zuo, explaining the process. ""We have a mechanical lab apply a force to a testing sample inside an electron microscope. As the stress is applied, the sample deforms. When stress exceeds the stress required for the dislocation to move inside the nanopillar, the dislocation will multiply. As the dislocation moves and encounters a resistance, they slow down and get tangled together and form a dislocation band. If you think of the stress like water flow, then the dislocation avalanche is like a dam breaking and water suddenly running out. The HEA makes the observation possible. ""The results of the process are two measurements -- first a mechanical measurement, which allows the researchers to study how much force it takes for the dislocations to move and by how much, and secondly, electron imaging to capture the dislocation motion in a video. No study previously has been able to couple electron imaging and mechanical force measurement together to study dislocation avalanches.""From previous accumulative studies, we knew how dislocations are produced and we have been able to study what was left behind,"" Zuo said. ""This study provides a critical answer to how dislocations interact.""Zuo adds that this type of measurement can be used to develop theory and computational models that be used to predict how materials will behave under certain stress.""That's important because catastrophic failure starts with this type of sudden deformation,"" Zuo said. ""We will be able to better predict the action before there is catastrophic failure. That in turn should lead to the development of much stronger materials.""This study coincides with strong efforts across the Illinois campus to use HEA for nuclear reactor and high temperature applications.""HEAs are stable at high temperatures and can accommodate lots of strain,"" Zuo said. ""If we understand the dislocation structure, it will help to develop materials for very challenging applications.""Story Source:Materials provided by University of Illinois College of Engineering. ",0.0420149,0.0805936,0.102322,0.118994,0.0785156,0.0853203,0.116463,0,0
616,Biomaterials with 'Frankenstein proteins' help heal tissue Biomaterial that solidifies at body temperature integrates into wounds to help cells grow,"Biomedical engineers from Duke University and Washington University in St. Louis have demonstrated that, by injecting an artificial protein made from a solution of ordered and disordered segments, a solid scaffold forms in response to body heat, and in a few weeks seamlessly integrates into tissue.The ability to combine these segments into proteins with unique properties will allow researchers to precisely control the properties of new biomaterials for applications in tissue engineering and regenerative medicine.The research appears online on October 15 in the journal Nature Materials.Proteins function by folding, origami-like, and interacting with specific biomolecular structures. Researchers previously believed that proteins needed a fixed shape to function, but over the last two decades there has been a growing interest in intrinsically disordered proteins (IDPs). Unlike their well-folded counterparts, IDPs can adopt a plethora of distinct structures. However, these structural preferences are non-random, and recent advances have shown that there are well-defined rules that connect information in the amino acid sequences of IDPs to the collections of structures they can adopt.Researchers have hypothesized that versatility in protein function is achievable by stringing together well-folded proteins with IDPs -- rather like pearl necklaces. This versatility is obvious in biological materials like muscle and silk fibers, which are made of proteins that combine ordered and disordered regions, enabling the materials to exhibit characteristics like elasticity of rubber and the mechanical strength of steel.IDPs are instrumental to cellular function, and many biomedical engineers have concentrated their efforts on an extremely useful IDP called elastin. A highly elastic protein found throughout the body, elastin allows blood vessels and organs -- like the skin, uterus and lungs -- to return to their original shape after being stretched or compressed. However, creating the elastin outside the body proved to be a challenge.So the researchers decided to take a reductionist engineering approach to the problem.""We were curious to see what types of materials we could make by adding order to an otherwise highly disordered protein,"" said Stefan Roberts, a PhD student in the Chilkoti lab and first author on the paper.Due to the challenges of using elastin itself, the research team worked with elastin-like polypeptides (ELPs), which are fully disordered proteins made to mimic pieces of elastin. ELPs are useful biomaterials because they can undergo phase changes -- go from a soluble to an insoluble state, or vice-versa -- in response to changes in temperature. While this makes these materials useful for applications like long-term drug delivery, their liquid-like behavior prevents them from being effective scaffolds for tissue engineering applications.But by adding ordered domains to the ELPs, Roberts and the team created ""Frankenstein"" proteins that combine ordered domains and disordered regions leading to so-called partially ordered proteins (POPs), which are equipped with the structural stability of ordered proteins without losing the ELPs ability to become liquid or solid via temperature changes.Designed as a fluid at room temperature that solidifies at body temperature, these new biomaterials form a stable, porous scaffold when injected that rapidly integrates into the surrounding tissue with minimal inflammation and promotes the formation of blood vessels.""This material is very stable after injection. It doesn't degrade quickly and it holds its volume really well, which is unusual for a protein-based material,"" Roberts said. ""Cells also thrive in the material, repopulating the tissue in the area where it is injected. All of these characteristics could make it a viable option for tissue engineering and wound healing.""Although the scaffold created by the POP was stable, the team also observed that the material would completely re-dissolve once it was cooled. What's more, the formation and dissolution temperatures could be independently controlled by controlling the ratios of disordered and ordered segments in the biomaterial. This independent tunability confers shape memories on the POPs via a phenomenon known as hysteresis, allowing them to return to their original shape after a temperature cue.The Duke team collaborated with the laboratory of Rohit Pappu, the Edwin H. Murty Professor of Engineering in the Department of Biomedical Engineering at Washington University in St. Louis to understand the molecular basis of sequence-encoded hysteretic behavior. Tyler S. Harmon, then a Physics PhD student in the Pappu lab, developed a computational model to show that the hysteresis arises from the differential interactions of ordered and disordered regions with solvent versus alone.""Being able to simulate the molecular basis for tunable hysteresis puts us on the path to design bespoke materials with desired structures and shape memory profiles,"" Pappu said. ""This appears to be a hitherto unrecognized feature of the synergy between ordered domains and IDPs.""Moving ahead, the team hopes to study the material in animal models to examine potential uses in tissue engineering and wound healing and to develop a better understanding of why the material promotes vascularization. If these studies are effective, Roberts is optimistic that the new material could become the basis for a biotech company. They also want to develop a deeper understanding of the interactions between the ordered and disordered portions in these versatile materials.""We've been so fascinated with the phase behavior derived from the disordered domains that we neglected the properties of the ordered domains, which turned out to be quite important,"" Chilkoti said. ""By combining ordered segments with disordered segments there's a whole new world of materials we can create with beautiful internal structure without losing the phase behavior of the disordered segment, and that's exciting.""Story Source:Materials provided by Duke University. ",0.0697292,0.116076,0.148332,0.180314,0.107717,0.134697,0.165098,0,0
617,"Breakthrough in self-healing materials Polymer that heals like skin 'very close' to industrial-scale production, professor says","The cost of making plastics, paints, coatings for cell phone screens and other materials that heal themselves like skin could be dramatically reduced thanks to a breakthrough that a Clemson University team detailed in the latest edition of the journal Science.Marek Urban and his team wrote about how they were able to give self-healing qualities to polymers that are used in relatively inexpensive commodities, such as paints, plastics and coatings. The next step is to go from making small amounts in a lab to producing large quantities.""It's not available at the industrial scale, but it's very close,"" said Urban, who is the J.E. Sirrine Foundation Chair and Professor in the Department of Materials Science and Engineering at Clemson.Researchers have been making small batches of self-healing polymers for the last two decades, but producing them on a commercial scale has so far been largely cost prohibitive.Urban said he and his team took advantage of interactions between co-polymers that he likened to spaghetti strands with little brushes on the side.The longer the spaghetti strands get, the more they become entangled, he said. The side groups interlock like two interlaced hands, making it harder to pull them apart, Urban said.""At the same time, they like each other,"" he said. ""So, when you pull them out, they come back together. It becomes self-healable at that point.""As simple as this may sound, these studies also revealed that ubiquitous and typically weak van der Waals interactions in plastics, when oriented, will result in self-healing. This discovery will impact the development of sustainable materials using weak bonding which becomes collectively very strong when oriented.""What's significant about his latest breakthrough is that if a company wanted to bring the technology to market, it would no longer have to build a new factory to produce self-healing polymers, Urban said.Urban estimated that increasing the scale to make polymers or paints by the hundreds of gallons could be done in six to 12 months.""For anybody who wants to make these types of self-healing materials, they would have to essentially design a synthetic process and scale it up,"" Urban said. ""The key is that the scale-up process would have to be precisely controlled. There is a huge difference between making something in the lab and scaling it up. We know the technology is available for them.""The article is titled ""'Key-and-lock' commodity self-healing copolymers' and marks the second time since 2009 that Urban has detailed his work on self-healing polymers in Science. Co-authors on the more recent article are Dmitriy Davydovich, Ying Yang, Tugba Demir, Yunzhi Zhang and Leah Casabianca, all of Clemson University.Rajendra Bordia, chair of the Department of Materials Science and Engineering at Clemson, congratulated Urban and his research team.""Science is a highly selective, high-impact journal,"" Bordia said. ""Dr. Urban's latest article further solidifies his place among the world's leading scholars in polymers and especially on self-healing polymers. His success is well deserved.""Anand Gramopadhye, dean of the College of Engineering, Computing and Applied Sciences, said Urban's success underscores the level of scholarship he brings to the college.""Dr. Urban is consistently published in the world's leading academic journals and is often honored with awards by his peers,"" Gramopadhye said. ""I congratulate him on his latest article in Science.""Urban said he had the idea for the research before he arrived at Clemson but didn't have the team in place to find what causes self-healing behavior.""It's very easy to design something by essentially mixing things up and say, 'Oh look, this self-heals, but we don't know why,'"" he said. ""The difference with our team is that we know exactly how to design these systems. We understand the limitations, we have analytical tools to measure, and we understand what it takes for this technology to go to the next level.""Thus, the question is: do you have the right people to do this? We are very fortunate -- Clemson is the best place to do this.""The research that led to the article was funded by the National Science Foundation.Story Source:Materials provided by Clemson University. Original written by Paul Alongi, College of Engineering, Computing and Applied Sciences. ",0.100966,0.144402,0.172669,0.215675,0.149001,0.151029,0.175334,0,0
618,Novel machine learning based framework could lead to breakthroughs in material design,"Computers used to take up entire rooms. Today, a two-pound laptop can slide effortlessly into a backpack. But that wouldn't have been possible without the creation of new, smaller processors -- which are only possible with the innovation of new materials.But how do materials scientists actually invent new materials? Through experimentation, explains Sanket Deshmukh, an assistant professor in the chemical engineering department whose team's recently published computational research might vastly improve the efficiency and costs savings of the material design process.Deshmukh's lab, the Computational Design of Hybrid Materials lab, is devoted to understanding and simulating the ways molecules move and interact -- crucial to creating a new material.In recent years, machine learning, a powerful subset of artificial intelligence, has been employed by materials scientists to accelerate the discovery of new materials through computer simulations. Deshmukh and his team have recently published research in the Journal of Physical Chemistry Letters demonstrating a novel machine learning framework that trains ""on the fly,"" meaning it instantaneously processes data and learns from it to accelerate the development of computational models.Traditionally the development of computational models are ""carried out manually via trial-and-error approach, which is very expensive and inefficient, and is a labor-intensive task,"" Deshmukh explained.""This novel framework not only uses the machine learning in a unique fashion for the first time,"" Deshmukh said, ""but it also dramatically accelerates the development of accurate computational models of materials.""""We train the machine learning model in a 'reverse' fashion by using the properties of a model obtained from molecular dynamics simulations as an input for the machine learning model, and using the input parameters used in molecular dynamics simulations as an output for the machine learning model,"" said Karteek Bejagam, a post-doctoral researcher in Deshmukh's lab and one of the lead authors of the study.This new framework allows researchers to perform optimization of computational models, at unusually faster speed, until they reach the desired properties of a new material.The best part? Regardless of how accurate the predictions of machine learning models are, as they are tested on-the-fly, these models have no negative impact on the model optimization, if it's inaccurate. ""It can't hurt, it can only help,"" said Samrendra Singh, a visiting scholar in Deshmukh's lab and another author of the study.""The beauty of this new machine learning framework is that it is very general, meaning the machine learning model can be integrated with any optimization algorithm and computational technique to accelerate the materials design,"" Singh said.The publication, led by Bejagam and Singh and with the collaboration of chemical engineering Ph.D. student Yaxin An, shows the use of this new framework by developing the models of two solvents as a proof of concept.Deshmukh's lab plan to build on the research by utilizing this novel machine learning based framework to develop models of various materials that have potential biomedicine and energy applications.Story Source:Materials provided by Virginia Tech. ",0.0721698,0.108895,0.13631,0.302326,0.121791,0.12559,0.191042,0,0
619,Catalytic active sites determined using carbon nanotubes,"Catalytic research led by University of Oklahoma researcher Steven Crossley has developed a new and more definitive way to determine the active site in a complex catalyst. His team's research was recently published in Nature Communications.Catalysts consisting of metal particles supported on reducible oxides show promising performance for a variety of current and emerging industrial reactions, such as the production of renewable fuels and chemicals. Although the beneficial results of the new materials are evident, identifying the cause of the activity of the catalyst can be challenging. Catalysts often are discovered and optimized by trial and error, making it difficult to decouple the numerous possibilities. This can lead to decisions based on speculative or indirect evidence.""When placing the metal on the active support, the catalytic activity and selectivity is much better than you would expect than if you were to combine the performance of metal with the support alone,"" explained Crossley, a chemical engineer, Teigen Presidential Professor and Sam A. Wilson Professor within the Gallogly College of Engineering. ""The challenge is that, when you put the two components together, it is difficult to understand the cause of the promising performance."" Understanding the nature of the catalytic active site is critical for controlling a catalyst's activity and selectivity.Crossley's novel method of separating active sites while maintaining the ability of the metal to create potential active sites on the support uses vertically grown carbon nanotubes that act as ""hydrogen highways."" To determine if catalytic activity was from either direct contact between the support and the metal or from metal-induced promoter effects on the oxide support, Crossley's team separated the metal palladium from the oxide catalyst titanium by a controlled distance on a conductive bridge of carbon nanotubes. The researchers introduced hydrogen to the system and verified that hydrogen was able to migrate along the nanotubes to create new potential active sites on the oxide support. They then tested the catalytic activity of these materials and contrasted it with the activity of the same materials when the metal and the support were in direct physical contact.""In three experiments, we were able to rule out different scenarios and prove that it is necessary to have physical contact between the palladium and titanium to produce methyl furan under these conditions,"" Crossley said.The carbon nanotube hydrogen highways can be used with a variety of different bifunctional catalysts.""Using this straightforward and simple method, we can better understand how these complex materials work, and use this information to make better catalysts,"" Crossley said.Crossley and his OU team are working in collaboration with Jeff Miller, a chemical engineering professor at Purdue University. This work was supported by the National Science Foundation CAREER award 165393.Story Source:Materials provided by University of Oklahoma. ",0.072531,0.204572,0.155424,0.162921,0.116613,0.132872,0.168786,0,0
620,500-year-old Leaning Tower of Pisa mystery unveiled by engineers,"Why has the Leaning Tower of Pisa survived the strong earthquakes that have hit the region since the middle ages? This is a long-standing question a research group of 16 engineers has investigated, including a leading expert in earthquake engineering and soil-structure interaction from the University of Bristol.Professor George Mylonakis, from Bristol's Department of Civil Engineering, was invited to join a 16-member research team, led by Professor Camillo Nuti at Roma Tre University, to explore this Leaning Tower of Pisa mystery that has puzzled engineers for many years.Despite leaning precariously at a five-degree angle, leading to an offset at the top of over five metres, the 58-metre tall Tower has managed to survive, undamaged, at least four strong earthquakes that have hit the region since 1280.Given the vulnerability of the structure, which barely manages to stand vertically, it was expected to sustain serious damage or even collapse because of moderate seismic activity. Surprisingly this hasn't happened and until now this has mystified engineers for a long time. After studying available seismological, geotechnical and structural information, the research team concluded that the survival of the Tower can be attributed to a phenomenon known as dynamic soil-structure interaction (DSSI).The considerable height and stiffness of the Tower combined with the softness of the foundation soil, causes the vibrational characteristics of the structure to be modified substantially, in such a way that the Tower does not resonate with earthquake ground motion. This has been the key to its survival. The unique combination of these characteristics gives the Tower of Pisa the world record in DSSI effects.Professor Mylonakis, Chair in Geotechnics and Soil-Structure Interaction, and Head of Earthquake and Geotechnical Engineering Research Group in the Department of Civil Engineering at the University of Bristol, said: ""Ironically, the very same soil that caused the leaning instability and brought the Tower to the verge of collapse, can be credited for helping it survive these seismic events.""Results from the study have been presented to international workshops and will be formally announced at the 16th European Conference in Earthquake Engineering taking place in Thessaloniki, Greece next month [18 to 21 June 2018].Story Source:Materials provided by University of Bristol. ",0.138108,0.141745,0.308936,0.197497,0.160582,0.193088,0.181284,0,0
621,"Self-healing fungi concrete could provide sustainable solution to crumbling infrastructure New concept offers low-cost, pollution-free and sustainable approach to fixing concrete","A new self-healing fungi concrete, co-developed by researchers at Binghamton University, State University of New York, could help repair cracks in aging concrete permanently, and help save America's crumbling infrastructure.Congrui Jin, assistant professor of mechanical engineering at Binghamton University, has researched concrete and found that the problem stems from the smallest of cracks.""Without proper treatment, cracks tend to progress further and eventually require costly repair,"" said Jin. ""If micro-cracks expand and reach the steel reinforcement, not only the concrete will be attacked, but also the reinforcement will be corroded, as it is exposed to water, oxygen, possibly CO2 and chlorides, leading to structural failure.""These cracks can cause huge and sometimes unseen problems for infrastructure. One potentially critical example is the case of nuclear power plants that may use concrete for radiation shielding. While remaking a structure would replace the aging concrete, this would only be a short-term fix until more cracks again spring up. Jin wanted to see if there was a way to fix the concrete permanently.""This idea was originally inspired by the miraculous ability of the human body to heal itself of cuts, bruises and broken bones,"" said Jin. ""For the damaged skins and tissues, the host will take in nutrients that can produce new substitutes to heal the damaged parts.""Jin worked with professor Guangwen Zhou and associate professor David Davies, both from Binghamton University, and associate professor Ning Zhang from Rutgers University. Together, the team set out to find a way to heal concrete and found an unusual answer: a fungus called Trichoderma reesei. When this fungus is mixed with concrete, it originally lies dormant -- until the first crack appears.""The fungal spores, together with nutrients, will be placed into the concrete matrix during the mixing process. When cracking occurs, water and oxygen will find their way in. With enough water and oxygen, the dormant fungal spores will germinate, grow and precipitate calcium carbonate to heal the cracks,"" explained Jin.""When the cracks are completely filled and ultimately no more water or oxygen can enter inside, the fungi will again form spores. As the environmental conditions become favorable in later stages, the spores could be wakened again.""The research is still in the fairly early stages, with the biggest issue being the survivability of the fungus within the harsh environment of concrete. However, Jin is hopeful that with further adjustments the Trichoderma reesei will be able to effectively fill the cracks.""There are still significant challenges to bring an efficient self-healing product to the concrete market. In my opinion, further investigation in alternative microorganisms such as fungi and yeasts for the application of self-healing concrete becomes of great potential importance,"" said Jin.Story Source:Materials provided by Binghamton University. Original written by Rachael Flores. ",0.109765,0.147797,0.299851,0.211975,0.155642,0.184679,0.201159,0,0
622,Scaling to new heights with gecko-inspired adhesive,"Some animals, such as geckos, can easily climb up walls and across ceilings. But currently, no material exists that allows everyday people to scale walls or transverse ceilings as effortlessly. Now, scientists report in ACS Applied Materials and Interfaces a dry adhesive that could someday make it easier to defy gravity.Geckos can scale walls because of their unique toe pads that help them quickly attach and detach from surfaces. Interestingly, gecko toe pads are covered with bristle-like layers of a stiff material called keratin. The bristle-like structure of the keratin in a toe pad helps it to stick -- each pad is covered with microscopic pillars, which then branch out at the tips into even smaller structures. Scientists have manufactured dry adhesives with similar properties, but they haven't been as sticky as gecko toes. And some methods involve the use of layers, but the first layer is usually damaged as successive ones are applied. Other methods are not easily scaled up. Hemant Kumar Raut, Hong Yee Low and colleagues wanted create a dry adhesive that was ultra-sticky but also simple to fabricate in large batches.The researchers made a dry adhesive with stiff polycarbonate using a nanoimprinting technique to build web-like layers. This method is cost-effective, easy to perform and is scalable. To prevent damage to the first layer, the team used a sacrificial layer, which was dissolved away after the second layer was applied. In repetitive attachment and detachment tests, only a 20 percent decline in stickiness occurred after 50 cycles. This level of adhesion lasted for up to 200 cycles. The researchers say that their film's adhesion was comparable to that of a gecko. The team also placed the adhesive film on the feet of a miniature robot, which moved with ease up a 30-degree incline.Story Source:Materials provided by American Chemical Society. ",0.129965,0.152213,0.220788,0.210029,0.175706,0.167184,0.196698,0,0
623,"System can 3-D print an entire building Tech could enable faster, cheaper, more adaptable building construction","The list of materials that can be produced by 3-D printing has grown to include not just plastics but also metal, glass, and even food. Now, MIT researchers are expanding the list further, with the design of a system that can 3-D print the basic structure of an entire building.Structures built with this system could be produced faster and less expensively than traditional construction methods allow, the researchers say. A building could also be completely customized to the needs of a particular site and the desires of its maker. Even the internal structure could be modified in new ways; different materials could be incorporated as the process goes along, and material density could be varied to provide optimum combinations of strength, insulation, or other properties.Ultimately, the researchers say, this approach could enable the design and construction of new kinds of buildings that would not be feasible with traditional building methods.The robotic system is described this week in the journal Science Robotics, in a paper by Steven Keating PhD '16, a mechanical engineering graduate and former research affiliate in the Mediated Matter group at the MIT Media Lab; Julian Leland and Levi Cai, both research assistants in the Mediated Matter group; and Neri Oxman, group director and associate professor of media arts and sciences.The system consists of a tracked vehicle that carries a large, industrial robotic arm, which has a smaller, precision-motion robotic arm at its end. This highly controllable arm can then be used to direct any conventional (or unconventional) construction nozzle, such as those used for pouring concrete or spraying insulation material, as well as additional digital fabrication end effectors, such as a milling head.Unlike typical 3-D printing systems, most of which use some kind of an enclosed, fixed structure to support their nozzles and are limited to building objects that can fit within their overall enclosure, this free-moving system can construct an object of any size. As a proof of concept, the researchers used a prototype to build the basic structure of the walls of a 50-foot-diameter, 12-foot-high dome -- a project that was completed in less than 14 hours of ""printing"" time.For these initial tests, the system fabricated the foam-insulation framework used to form a finished concrete structure. This construction method, in which polyurethane foam molds are filled with concrete, is similar to traditional commercial insulated-concrete formwork techniques. Following this approach for their initial work, the researchers showed that the system can be easily adapted to existing building sites and equipment, and that it will fit existing building codes without requiring whole new evaluations, Keating explains.Ultimately, the system is intended to be self-sufficient. It is equipped with a scoop that could be used to both prepare the building surface and acquire local materials, such as dirt for a rammed-earth building, for the construction itself. The whole system could be operated electrically, even powered by solar panels. The idea is that such systems could be deployed to remote regions, for example in the developing world, or to areas for disaster relief after a major storm or earthquake, to provide durable shelter rapidly.The ultimate vision is ""in the future, to have something totally autonomous, that you could send to the moon or Mars or Antarctica, and it would just go out and make these buildings for years,"" says Keating, who led the development of the system as his doctoral thesis work.But in the meantime, he says, ""we also wanted to show that we could build something tomorrow that could be used right away."" That's what the team did with its initial mobile platform. ""With this process, we can replace one of the key parts of making a building, right now,"" he says. ""It could be integrated into a building site tomorrow.""""The construction industry is still mostly doing things the way it has for hundreds of years,"" says Keating. ""The buildings are rectilinear, mostly built from single materials, put together with saws and nails,"" and mostly built from standardized plans.But, Keating wondered, what if every building could be individualized and designed using on-site environmental data? In the future, the supporting pillars of such a building could be placed in optimal locations based on ground-penetrating radar analysis of the site, and walls could have varying thickness depending on their orientation. For example, a building could have thicker, more insulated walls on its north side in cold climates, or walls that taper from bottom to top as their load-bearing requirements decrease, or curves that help the structure withstand winds.The creation of this system, which the researchers call a Digital Construction Platform (DCP), was motivated by the Mediated Matter group's overall vision of designing buildings without parts. Such a vision includes, for example, combining ""structure and skin,"" and beams and windows, in a single production process, and adapting multiple design and construction processes on the fly, as the structure is being built.From an architectural perspective, Oxman says, the project ""challenges traditional building typologies such as walls, floors, or windows, and proposes that a single system could be fabricated using the DCP that can vary its properties continuously to create wall-like elements that continuously fuse into windows.""To this end, the nozzles of the new 3-D printing system can be adapted to vary the density of the material being poured, and even to mix different materials as it goes along. In the version used in the initial tests, the device created an insulating foam shell that would be left in place after the concrete is poured; interior and exterior finish materials could be applied directly to that foam surface.The system can even create complex shapes and overhangs, which the team demonstrated by including a wide, built-in bench in their prototype dome. Any needed wiring and plumbing can be inserted into the mold before the concrete is poured, providing a finished wall structure all at once. It can also incorporate data about the site collected during the process, using built-in sensors for temperature, light, and other parameters to make adjustments to the structure as it is built.Keating says the team's analysis shows that such construction methods could produce a structure faster and less expensively than present methods can, and would also be much safer. (The construction industry is one of the most dangerous occupations, and this system requires less hands-on work.) In addition, because shapes and thicknesses can be optimized for what is needed structurally, rather than having to match what's available in premade lumber and other materials, the total amount of material needed could be reduced.While the platform represents an engineering advance, Oxman notes. ""Making it faster, better, and cheaper is one thing. But the ability to design and digitally fabricate multifunctional structures in a single build embodies a shift from the machine age to the biological age -- from considering the building as a machine to live in, made of standardized parts, to the building as an organism, which is computationally grown, additively manufactured, and possibly biologically augmented.""""So to me it's not merely a printer,"" she says, ""but an entirely new way of thinking about making, that facilitates a paradigm shift in the area of digital fabrication, but also for architectural design. ... Our system points to a future vision of digital construction that enables new possibilities on our planet and beyond.""Video: https://www.youtube.com/watch?v=8zt_3Gs1ksgStory Source:Materials provided by Massachusetts Institute of Technology. Original written by David L. Chandler. ",0.0300351,0.0448384,0.121818,0.106299,0.0585102,0.068666,0.0833617,0,0
624,"Spheres can make concrete leaner, greener Microscopic particles promise stronger building materials and more","Rice University scientists have developed micron-sized calcium silicate spheres that could lead to stronger and greener concrete, the world's most-used synthetic material.To Rice materials scientist Rouzbeh Shahsavari and graduate student Sung Hoon Hwang, the spheres represent building blocks that can be made at low cost and promise to mitigate the energy-intensive techniques now used to make cement, the most common binder in concrete.The researchers formed the spheres in a solution around nanoscale seeds of a common detergent-like surfactant. The spheres can be prompted to self-assemble into solids that are stronger, harder, more elastic and more durable than ubiquitous Portland cement.""Cement doesn't have the nicest structure,"" said Shahsavari, an assistant professor of materials science and nanoengineering. ""Cement particles are amorphous and disorganized, which makes it a bit vulnerable to cracks. But with this material, we know what our limits are and we can channel polymers or other materials in between the spheres to control the structure from bottom to top and predict more accurately how it could fracture.""He said the spheres are suitable for bone-tissue engineering, insulation, ceramic and composite applications as well as cement.The research appears in the American Chemical Society journal Langmuir.The work builds on a 2017 project by Shahsavari and Hwang to develop self-healing materials with porous, microscopic calcium silicate spheres. The new material is not porous, as a solid calcium silicate shell surrounds the surfactant seed.But like the earlier project, it was inspired by how nature coordinates interfaces between dissimilar materials, particularly in nacre (aka mother of pearl), the material of seashells. Nacre's strength is a result of alternating stiff inorganic and soft organic platelets. Because the spheres imitate that structure, they are considered biomimetic.The researchers discovered they could control the size of the spheres that range from 100 to 500 nanometers in diameter by manipulating surfactants, solutions, concentrations and temperatures during manufacture. That allows them to be tuned for applications, Shahsavari said.""These are very simple but universal building blocks, two key traits of many biomaterials,"" Shahsavari said. ""They enable advanced functionalities in synthetic materials. Previously, there were attempts to make platelet or fiber building blocks for composites, but this works uses spheres to create strong, tough and adaptable biomimetic materials.""Sphere shapes are important because they are far easier to synthesize, self-assemble and scale up from chemistry and large-scale manufacturing standpoints.""In tests, the researchers used two common surfactants to make spheres and compressed their products into pellets for testing. They learned that DTAB-based pellets compacted best and were tougher, with a higher elastic modulus, than either CTAB pellets or common cement. They also showed high electrical resistance.Shahsavari said the size and shape of particles in general have a significant effect on the mechanical properties and durability of bulk materials like concrete. ""It is very beneficial to have something you can control as opposed to a material that is random by nature,"" he said. ""Further, one can mix spheres with different diameters to fill the gaps between the self-assembled structures, leading to higher packing densities and thus mechanical and durability properties.""He said increasing the strength of cement allows manufacturers to use less concrete, decreasing not only weight but also the energy required to make it and the carbon emissions associated with cement's manufacture. Because spheres pack more efficiently than the ragged particles found in common cement, the resulting material will be more resistant to damaging ions from water and other contaminants and should require less maintenance and less-frequent replacement.Story Source:Materials provided by Rice University. Original written by Mike Williams. ",0.0757777,0.16357,0.206762,0.170708,0.136589,0.130088,0.17343,0,0
625,Searching for new bridge forms that can span further,"Newly identified bridge forms could enable significantly longer bridge spans to be achieved in the future, potentially making a crossing over the Strait of Gibraltar, from the Iberian Peninsula to Morocco, feasible.The new bridge forms use a new mathematical modelling technique to identify optimal forms for very long-span bridges. The research is published on 19 September 2018 in the Proceedings of the Royal Society.A bridge's span is the distance of suspended roadway between towers, with the current world record standing at just under 2km. The most popular form for long spans is the suspension bridge form, as used for the Humber Bridge, though the cable-stayed bridge form, where cables directly connect the tower to the roadway -- such as used in the recently constructed Queensferry Crossing in Scotland -- is becoming increasingly popular.As bridge spans become longer, a rapidly growing proportion of the structure is needed just to carry the bridge's own weight, rather than the traffic crossing it. This can create a vicious cycle: a relatively small increase in span requires use of significantly more material, leading to a heavier structure that requires yet more material to support it. This also sets a limit on how long a bridge span can be; beyond this limit a bridge simply cannot carry its own weight.One option is to use stronger, lighter materials. However, steel remains the preferred choice because it is tough, readily available and relatively cheap. So the only other way to increase span is change the bridge's design.Professor Matthew Gilbert from the University of Sheffield, who led the research, said: ""The suspension bridge has been around for hundreds of years and while we've been able to build longer spans through incremental improvements, we've never stopped to look to see if it's actually the best form to use. Our research has shown that more structurally efficient forms do exist, which might open the door to significantly longer bridge spans in the future.""The technique devised by the team draws on theory developed by Professor Gilbert's namesake, Davies Gilbert, who in the early 19th Century used mathematical theory to persuade Thomas Telford that the suspension cables in his original design for the Menai Strait bridge in North Wales followed too shallow a curve. He also proposed a 'catenary of equal stress' showing the optimal shape of a cable accounting for the presence of gravity loads.By incorporating this early 19th century theory into a modern mathematical optimisation model, the team have identified bridge concepts that require the minimum possible volume of material, potentially making significantly longer spans feasible.The mathematically optimal designs contain regions which resemble a bicycle wheel, with multiple 'spokes' in place of a single tower. But these would be very difficult to build in practice at large scale. The team therefore replaced these with split towers comprising just two or three 'spokes' as a compromise that retains most of the benefit of the optimal designs, while being a little easier to construct.For a 5km span, which is likely to be required to build the 14km Strait of Gibraltar crossing, a traditional suspension bridge design would require far more material, making it at least 73 per cent heavier than the optimal design. In contrast, the proposed two- and three-spoke designs would be just 12 and 6 percent heavier, making them potentially much more economical to build.The new bridge forms require less material principally because the forces from the deck are transmitted more efficiently through the bridge superstructure to the foundations. This is achieved by keeping the load paths short, and avoiding sharp corners between tensile and compressive elements.The team emphasise that their research is just the first step, and that the ideas cannot be developed immediately for construction of a mega span bridge. The current model considers only gravity loads and does not yet consider dynamic forces arising from traffic or wind loading. Further work is also required to address construction and maintenance issues.Co-author, Ian Firth, from COWI, said: ""This is an interesting development in the search for greater material efficiency in the design of super-long span bridges. There is much more work to do, notably in devising effective and economic construction methods, but maybe one day we will see these new forms taking shape across some wide estuary or sea crossing.""Story Source:Materials provided by University of Sheffield. ",0.0899088,0.100244,0.14294,0.169193,0.100951,0.163013,0.137345,0,0
626,Steps to keep buildings functioning after natural hazards,"After an earthquake, hurricane, tornado or other natural hazard, it's considered a win if no one gets hurt and buildings stay standing. But an even bigger victory is possible: keeping those structures operational. This outcome could become more likely with improved standards and codes for the construction of residential and commercial buildings, according to a new report recently delivered to the U.S. Congress by the National Institute of Standards and Technology (NIST).""Current standards and codes focus on preserving lives by reducing the likelihood of significant building damage or structural collapse from hazards,"" said Steven McCabe, director of the NIST-led, multiagency National Earthquake Hazards Reduction Program (NEHRP) and one of the authors of the new publication. ""But they generally don't address the additional need to preserve quality of life by keeping buildings habitable and functioning as normally as possible, what we call 'immediate occupancy.' The goal of our report is to put the nation on track to achieve this performance outcome.""The impact of a natural hazard on a community is usually most evident in the lives lost and physical destruction, but the accompanying economic shock, social disruptions and reduced quality of life can often be devastating as well. ""Cities and towns can be rebuilt, but lifestyles are damaged, sometimes permanently, if businesses, schools, utilities, transportation and other essential operations are out of service for an extended period,"" said Therese McAllister, manager of NIST's Community Resilience Program and another report author.The infamous 1906 San Francisco earthquake provides a dramatic example of that impact. In the half-century following the 1840s Gold Rush in California, San Francisco was the fastest growing metropolitan area in the region. That all changed on April 18, 1906, when the quake and resulting fires destroyed 80 percent of the city, killed some 3,000 residents and left nearly 300,000 people -- three-fourths of the population -- homeless, out of work and without essential services. Though San Francisco would rebuild quickly, the disaster diverted trade, industry and people south to Los Angeles, which then supplanted the ""City by the Bay"" as the largest, most important urban center in the western United States.Even with modern building codes and standards in place, there is still room for improvement, as evidenced by the massive damage from the May 2011 tornado in Joplin, Missouri, and the three major 2017 hurricanes striking Texas, Florida and Puerto Rico.""Immediate occupancy performance measures would help avoid catastrophes because they could build up a community's resiliency against natural hazards so that people still can live at home, still can go to work and still can have the supporting infrastructure providing them services such as water and electricity,"" McCabe said.In 2017, Congress tasked NIST to define what it would take to achieve immediate occupancy performance codes and standards for all buildings in all types of natural hazards, specifically in terms of fundamental research needs, possible technological applications based on that research and key strategies that could be used to implement any resulting regulations.The result of that effort is the new NIST report, Research Needs to Support Immediate Occupancy Building Performance Objective Following Natural Hazard Events. The publication identifies a large portfolio of research and implementation activities that target enhanced performance objectives for residential and commercial buildings.""The report provides valuable information about steps that could be taken to achieve immediate occupancy in the future,"" McAllister said.The potential research activities presented in the report to Congress were developed with the assistance of a steering committee of recognized experts and stakeholder input obtained during a national workshop hosted by NIST in January 2018. The workshop participants identified four key areas that they believe must be considered when developing plans to achieve immediate occupancy performance: building design, community needs, economic and social impacts, and fostering acceptance and use of new practices.For example, the report states that immediate occupancy performance measures must be developed, established and implemented with a sensitivity to how they will economically affect building owners, business operators, occupants and even whole communities. ""You have to make sure that the cost of keeping buildings functional after natural hazards remains reasonable enough that everyone will be able to afford them,"" McCabe said.The report also discusses key challenges facing the effort to make buildings functional in the wake of natural hazards, such as motivating communities to make the investment, managing how costs and benefits are balanced, and garnering public support.Finally, the report concludes by recognizing that ""increasing the performance goals for buildings would not be easily achieved, but the advantages may be substantial"" and that making such objectives a reality ""would entail a significant shift in practice for development, construction and maintenance or retrofit of buildings."" The report, its authors state, is the first step toward creating an action plan to achieve immediate occupancy across the nation with coordinated and detailed research goals and implementation activities.""Our report outlines the steps that could be taken for a big raise of the bar -- perhaps the biggest change in building standards and codes in 50 years -- but one we believe is possible,"" McCabe said.Story Source:Materials provided by National Institute of Standards and Technology (NIST). ",0.0593072,0.0765997,0.191265,0.155234,0.0834076,0.122897,0.114802,0,0
627,Mass timber: Thinking big about sustainable construction,"The construction and operation of all kinds of buildings uses vast amounts of energy and natural resources. Researchers around the world have therefore been seeking ways to make buildings more efficient and less dependent on emissions-intensive materials.Now, a project developed through an MIT class has come up with a highly energy-efficient design for a large community building that uses one of the world's oldest construction materials. For this structure, called ""the Longhouse,"" massive timbers made of conventional lumber would be laminated together like a kind of supersized plywood.The design will be presented this October at the Maine Mass Timber Conference, which is dedicated to exploring new uses of this material, which can be used to build safe, sound high-rise buildings, if building codes permit them.John Klein, a research scientist in MIT's architecture department who taught a workshop called Mass Timber Design that came up with the new design, explains that ""in North America, we have an abundance of forest resources, and a lot of it is overgrown. There's an effort to find ways to use forest products sustainably, and the forests are actively undergoing thinning processes to prevent forest fires and beetle infestations.""People tend to think of wood as a suitable material for structures just a few stories high, but not for larger structures, Klein says. But already some builders are beginning to use mass timber products (a term that basically applies to any wood products much larger than conventional lumber) for bigger structures, including medium-rise buildings of up to 20 stories. Even taller buildings should ultimately be practical with this technology, he says. One of the largest mass timber buildings in the U.S. is the new 82,000-square-foot John W. Olver Design Building at the University of Massachusetts at Amherst.One of the first questions people raise when they hear of such construction has to do with fire. Can such tall wooden structures really be safe? In fact, Klein says, tests have demonstrated that mass timber structures can resist fire as well or better than steel. That's because wood exposed to fire naturally produces a layer of char, which is highly insulating and can protect the bulk of the wood for more than two hours. Steel, in contrast, can fail suddenly when heat softens it and causes it to buckle.Klein explains that this natural fire resistance makes sense when you think about dropping a lit match onto a pile of wood shavings, versus dropping it onto a log. The shavings will burst into flames, but on the log a match will simply sputter out. The greater the bulk of the wood, the better it resists ignition.The structure designed by the class uses massive beams made from layers of wood veneers laminated together, a process known as laminated veneer lumber (LVL), made into panels 50 feet long, 10 feet wide, and more than 6 inches thick These are cut to size and used to make a series of large arches, 40 feet tall to the central peak and spanning 50 feet across, made of sections with a triangular cross-section to add structural strength. A series of these arches is assembled to create a large enclosed space with no need for internal structural supports. The pleated design of the roof is designed to accommodate solar panels and windows for natural lighting and passive solar heating.""The structural depth achieved by building up the triangular section helps us achieve the clear span desired for the communal space, all while lending a visual language on both the interior and the exterior of the structure,"" says Demi Fang, an MIT architecture graduate student who was part of the design team. ""Each arch tapers and widens along its length, because not every point along the arch will be subject to the same magnitude of forces, and this varying cross-section depth both expresses structural performance while encouraging materials savings,"" she says.The arches would be factory-built in sections, and then bolted together on site to make the complete building. Because the building would be largely prefabricated, the actual on-site construction process would be greatly streamlined, Klein says.""The Longhouse is a multifunctional building, designed to accommodate a range of event scenarios from co-working, exercise classes, social mixers, exhibitions, dinner gatherings and lectures,"" Klein says, adding that it builds on a long tradition of such communal structures in cultures around the world.Whereas the production of concrete, used in most of the world's large buildings, involves large releases of greenhouse gases from the baking of limestone, construction using mass timber has the opposite effect, Klein says. While concrete adds to the world's burden of greenhouse gases, timber actually lessens it, because the carbon removed from the air while trees grow is essentially sequestered for as long as the building lasts. ""The building is a carbon sink,"" he says.One obstacle to greater use of mass timber for large structures is in current U.S. building codes, Klein says, which limit the use of structural wood to residential buildings up to five stories, or commercial buildings up to six stories. But recent construction of much taller timber buildings in Europe, Australia, and Canada -- including an 18-story timber building in British Columbia -- should help to establish such buildings' safety and lead to the needed code changes, he says.The Longhouse design was developed by a cross-disciplinary team in 4.S13 (Mass Timber Design), a design workshop in MIT's architecture department that explores the future of sustainable buildings. The team included John Fechtel, Paul Short, Demi Fang, Andrew Brose, Hyerin Lee, and Alexandre Beaudouin-Mackay. It was supported by the Department of Architecture, BuroHappold Engineering and Nova Concepts.Story Source:Materials provided by Massachusetts Institute of Technology. Original written by David L. Chandler. ",0.0652238,0.0882793,0.209834,0.136997,0.090328,0.121611,0.117522,0,0
628,A social tool for evaluating the environmental impact of residential buildings,"The research group ARDITEC from the Higher Technical School of Building Engineering at the University of Seville has led a pioneering European project to calculate the environmental impact of residential buildings. The novelty of this initiative is that for the first time an open-source computing tool which can, simply and intuitively, calculate the CO2 emissions in each phase of a building project, in order to obtain a global picture of its carbon footprint from its conception and to help decide every variable in the construction process.""The first step in managing and reducing the CO2 emissions associated with building construction is to calculate them, to know the importance of this environmental aspect and apply measures to improve the situation. To better understand the environmental impact and work on it, it is important to measure the CO2 emissions from the design and conception of the building and, according to its measurements, know the different possibilities for reducing its carbon footprint and making a more sustainable, low-carbon building,"" explains the University of Seville teacher and head of the project, Jaime Soles.The project OERCO2 is as an Open Educational Resource financed by the European Union's Erasmus+ programme, and, as well as the University of Seville, its participants include the Centro Tecnelgico del Mermol, Piedra y Materiales (Technological Centre for Marble, Stone and Materials -- Spain) CertiMaC Soc. Cons. a r. L. (Italy), Centro Tecnologico da Ceramica e do Vidro (Portugal), Universitatea Transilvania din Brasov (Romania) and Asociatia Romania Green Building Council (Romania).The experts point out that it is vital to be aware of the CO2 emissions that are generated in the first phases of a project, so that early preventative actions can be taken by means of the choice of different materials, mean of transport, construction methods, use during the life of the building, deconstruction systems, reuse, etc., so contributing to reducing the building's emissions.""We have tried to work towards the concept of sustainable construction, taking into account, also, concepts related to the recycling and reuse of materials, and putting this tool at the disposal of all the agents involved in the construction sector, such as students, professionals and the users of the house themselves,"" adds Solis.One of the applications of this online tool is that it allows for buildings of similar characteristics to be compared from an economic and environmental point of view, so knowing which of them is more sustainable and better respects the environment.Story Source:Materials provided by University of Seville. ",0.0928077,0.131346,0.225507,0.181344,0.121919,0.158066,0.163499,0,0
629,Electrical contact to molecules in semiconductor structures established for the first time,"Electrical circuits are constantly being scaled down and extended with specific functions. A new method now allows electrical contact to be established with simple molecules on a conventional silicon chip. The technique promises to bring advances in sensor technology and medicine, as reported in the journal Nature by chemists from the University of Basel and researchers from IBM Research -- Zurich in Reschlikon.To further develop semiconductor technology, the field of molecular electronics is seeking to manufacture circuit components from individual molecules instead of silicon. Because of their unique electronic properties, molecules are suited to applications that cannot be implemented using conventional silicon technology. However, this requires reliable and inexpensive methods for creating electrical contacts at the two ends of a molecule.The ability to produce thousands of elementsResearchers from the University of Basel and IBM Research -- Zurich have now developed a technique that allows electrical contact to individual molecules to be established. Thousands of stable metal-molecule-metal components can be produced simultaneously by depositing a film of nanoparticles onto the molecules, without compromising the properties of the molecules. This approach was demonstrated using alkane-dithiol compounds, which are made up of carbon, hydrogen, and sulfur.The researchers used a type of sandwich construction in which an interlayer of molecules is brought into contact with metallic electrodes from above and below. The lower electrode consists of a layer of platinum, which is coated with a layer of non-conducting material. Tiny pores are then etched into this layer to produce arbitrary patterns of compartments of different sizes, inside which there is an electrical contact with the platinum electrode.Self-assembled monolayersThe researchers then took advantage of the ability of certain molecules to self-assemble. Onto the pattern of pores, they applied a solution containing alkane-dithiol molecules, which self-assemble into the pores, forminga densely packed monolayer film. Within this film, the individual molecules exhibit a regular arrangement and an electrical connection with the lower platinum electrode. Electrical contact with the molecular layer is established via an upper electrode made of gold nanoparticles.The new technique largely resolves the issues that previously hampered the creation of electrical contacts to molecules -- such as high contact resistance or short circuits by filaments penetrating the film. Building blocks fabricated by this method can be operated under standard conditions and provide long-term stability. Moreover, the method can be applied to a variety of other molecular systems and opens up new avenues for integrating molecular compounds into solid-state devices. Its applications could include new types of instruments in the fields of sensor technology and medicine.""Our approach will help speed up the development of chemically fabricated and controllable electronic and sensor components,"" says Professor Marcel Mayor of the Department of Chemistry at the University of Basel. The project received significant funding from the National Center of Competence in Research (NCCR) for Molecular Systems Engineering, in which the University of Basel and ETH Zurich are leading houses.Story Source:Materials provided by University of Basel. ",0.0738041,0.181812,0.111147,0.126335,0.138257,0.126459,0.125458,0,0
630,Ground-breaking discoveries could create superior alloys with many applications,"Many current and future technologies require alloys that can withstand high temperatures without corroding. Now, researchers at Chalmers University of Technology, Sweden, have hailed a major breakthrough in understanding how alloys behave at high temperatures, pointing the way to significant improvements in many technologies. The results are published in the highly ranked journal Nature Materials.Developing alloys that can withstand high temperatures without corroding is a key challenge for many fields, such as renewable and sustainable energy technologies like concentrated solar power and solid oxide fuel cells, as well as aviation, materials processing and petrochemistry.At high temperatures, alloys can react violently with their environment, quickly causing the materials to fail by corrosion. To protect against this, all high temperature alloys are designed to form a protective oxide scale, usually consisting of aluminium oxide or chromium oxide. This oxide scale plays a decisive role in preventing the metals from corroding. Therefore, research on high temperature corrosion is very focused on these oxide scales -- how they are formed, how they perform at high heat, and how they sometimes fail.The article in Nature Materials answers two classical issues in the area. One applies to the very small additives of so-called 'reactive elements' -- often yttrium and zirconium -- found in all high-temperature alloys. The second issue is about the role of water vapour.""Adding reactive elements to alloys results in a huge improvement in performance -- but no one has been able to provide robust experimental proof why,"" says Nooshin Mortazavi, materials researcher at Chalmers' Department of Physics, and first author of the study. ""Likewise, the role of water, which is always present in high-temperature environments, in the form of steam, has been little understood. Our paper will help solve these enigmas.""In this paper, the Chalmers researchers show how these two elements are linked. They demonstrate how the reactive elements in the alloy promote the growth of an aluminium oxide scale. The presence of these reactive element particles causes the oxide scale to grow inward, rather than outward, thereby facilitating the transport of water from the environment, towards the alloy substrate. Reactive elements and water combine to create a fast-growing, nanocrystalline, oxide scale.""This paper challenges several accepted 'truths' in the science of high temperature corrosion and opens up exciting new avenues of research and alloy development,"" says Lars Gunnar Johansson, Professor of Inorganic Chemistry at Chalmers, Director of the Competence Centre for High Temperature Corrosion (HTC) and co-author of the paper.""Everyone in the industry has been waiting for this discovery. This is a paradigm shift in the field of high-temperature oxidation,"" says Nooshin Mortazavi. ""We are now establishing new principles for understanding the degradation mechanisms in this class of materials at very high temperatures.""Further to their discoveries, the Chalmers researchers suggest a practical method for creating more resistant alloys. They demonstrate that there exists a critical size for the reactive element particles. Above a certain size, reactive element particles cause cracks in the oxide scale, that provide an easy route for corrosive gases to react with the alloy substrate, causing rapid corrosion. This means that a better, more protective oxide scale can be achieved by controlling the size distribution of the reactive element particles in the alloy.This ground-breaking research from Chalmers University of Technology points the way to stronger, safer, more resistant alloys in the future.More about: Potential consequences of the research breakthroughHigh temperature alloys are used in a variety of areas, and are essential to many technologies which underpin our civilisation. They are crucial for both new and traditional renewable energy technologies, such as ""green"" electricity from biomass, biomass gasification, bio-energy with carbon capture and storage (BECCS), concentrated solar energy, and solid oxide fuel cells. They are also crucial in many other important technology areas such as jet engines, petrochemistry and materials processing.All these industries and technologies are entirely dependent on materials that can withstand high temperatures -- 600 e C and beyond -- without failing due to corrosion. There is a constant demand for materials with improved heat resistance, both for developing new high temperature technologies, and for enhancing the process efficiency of existing ones.For example, if the turbine blades in an aircraft's jet engines could withstand higher temperatures, the engine could operate more efficiently, resulting in fuel-savings for the aviation industry. Or, if you can produce steam pipes with better high-temperature capability, biomass-fired power plants could generate more power per kilogram of fuel.Corrosion is one of the key obstacles to material development within these areas. The Chalmers researchers' article provides new tools for researchers and industry to develop alloys that withstand higher temperatures without quickly corroding.Story Source:Materials provided by Chalmers University of Technology. ",0.066045,0.135533,0.132586,0.122547,0.0913253,0.132905,0.129914,0,0
631,"Ceramics can deform like metals if sintered under an electric field, study shows ","Purdue researchers have observed a way that the brittle nature of ceramics can be overcome as they sustain heavy loads, leading to more resilient structures such as aircraft engine blade coatings and dental implants.While inherently strong, most ceramics tend to fracture suddenly when just slightly strained under a load unless exposed to high temperatures. Structural ceramic components also require high temperatures to form in the first place through a lengthy process called sintering, in which a powdered material coalesces into a solid mass.These issues are particularly problematic for ceramic coatings of metal engine blades intended to protect metal cores from a range of operational temperatures. A study published in Nature Communications demonstrates for the first time that applying an electric field to the formation of yttria-stabilized zirconia (YSZ), a typical thermal barrier ceramic, makes the material almost as plastic, or easily reshaped, as metal at room temperature. Engineers could also see cracks sooner since they start to slowly form at a moderate temperature as opposed to higher temperatures, giving them time to rescue a structure.""In the past, when we applied a high load at lower temperatures, a large number of ceramics would fail catastrophically without warning,"" said Xinghang Zhang, professor of materials engineering. ""Now we can see the cracks coming, but the material stays together; this is predictable failure and much safer for the usage of ceramics.""Recent studies have shown that applying an electric field, or ""flash,"" significantly accelerates the sintering process that forms YSZ and other ceramics, and at much lower furnace temperatures than conventional sintering. Flash-sintered ceramics also have very little porosity, which makes them more dense and therefore easier to deform. None have yet tested the ability of flash-sintered ceramics to change shape at room temperature or increasingly higher temperatures.""YSZ is a very typical thermal barrier coating -- it basically protects a metal core from heat,"" said Haiyan Wang, Purdue's Basil S. Turner Professor of Engineering. ""But it tends to suffer from a lot of fractures when an engine heats up and cools down due to residual stresses.""What allows metals to be fracture-resistant and easy to change shape is the presence of ""defects,"" or dislocations -- extra planes of atoms that shuffle during deformation to make a material simply deform rather than break under a load.""These dislocations will move under compression or tension, such that the material doesn't fail,"" said Jaehun Cho, a graduate research assistant in materials engineering.Ceramics normally don't form dislocations unless deformed at very high temperatures. Flash-sintering them, however, introduces these dislocations and creates a smaller grain size in the resulting material.""Smaller grains, such as nanocrystalline grains, may slide as the ceramic material deforms, helping it to deform better,"" Wang said.Pre-existing dislocations and small grain sizes enabled a flash-sintered YSZ sample thinner than human hair to grow increasingly plastic between room temperature and 600 degrees Celsius when compressed, with cracks starting to slowly spread at 400 degrees as opposed to conventionally sintered YSZ that requires 800 degrees and higher to plastically deform.Improved plasticity means more stability during operation at relatively low temperatures. The sample could also withstand almost as much compression strain as some metals do before cracks started to appear.""Metals can be compressed to 10 or 20 percent strain, no problem, but ceramics often fracture into pieces if you compress them to less than 2-3 percent strain,"" Zhang said. ""We show that flash-sintered ceramics can be compressed to 7-10 percent without catastrophic fracture.""Even when the sample did begin to crack, the cracks formed very slowly and did not result in complete collapse as would typically happen with conventional ceramics. The next steps would be using these principles to design even more resilient ceramic materials.The researchers would not have been able to perform in-situ experiments of a micron-sized ceramic sample without an in-situ nanomechanical testing tool inside a high-resolution scanning electron microscope equipped with a focused iron beam tool at Purdue's Life Science Microscopy Center and an FEI Talos 200X electron microscope facility in Purdue's Materials Engineering facility. Both microscopes were provided by Purdue's Office of the Executive Vice President for Research and Partnerships and the Colleges of Engineering and Science. Purdue is expecting an even higher-resolution aberration-corrected microscope that the researchers will soon use for future nanomaterials research.This Purdue-led research is supported by the Office of Naval Research in collaboration with the University of California, Davis and Rutgers University.Story Source:Materials provided by Purdue University. Original written by Kayla Wiles. ",0.0547657,0.130894,0.122275,0.121755,0.0946258,0.104515,0.150049,0,0
632,Wrap an electrode material for Li-ion battery into the inner spacing of carbon nanotube Electrochemical characterization of a high capacitive electrode for lithium ion batteries using phosphorus-encapsulated carbon nanotubes,"Researchers at the Toyohashi University of Technology have demonstrated the electrochemical performance of lithium ion batteries (LIBs) using phosphorus-encapsulated carbon nanotube electrodes, in which red phosphorus with considerable high capacity is introduced into the inner spacing of carbon nanotubes (CNTs) with a tubular structure. The electrodes indicated an improvement in the electrochemical reactivity of red phosphorus when accessible pathways of lithium ions, i.e., nanopores, were formed onto the sidewalls of the CNTs where the red phosphorus was encapsulated. Furthermore, the charge-discharge profiles and structural analysis revealed reversible electrochemical reactions and the relatively high structural stability of red phosphorus in the nanotubes even after the fiftieth charge-discharge cycle. The charge-discharge capacities show a value two times or higher than that of graphite used in commercial LIBs. Therefore, a new electrode material for LIBs with high capacity is proposed.Red phosphorus has attracted attention as a higher capacitive electrode material for LIBs because it can deliver a theoretical capacity approximately seven times higher than that of graphite used as a commercial electrode material for LIBs. The large difference in the capacity is thought to be due to an acceptable amount of lithium ions in the structures of graphite for LiC6 or phosphorus for Li3P. However, red phosphorus suffers enormous volumetric changes, pulverization, and peeling off during lithium ion insertion and extraction processes, resulting in rapid capacity fading due to the decrement in the amount of electrochemically reactive red phosphorus. Additionally, while electrons move onto the electrode during lithium ion insertion/extraction, red phosphorus has a disadvantage in terms of energy loss because of its low electronic conductivity.Tomohiro Tojo and his colleagues at the Department of Electrical and Electronic Information Engineering, Toyohashi University of Technology, have synthesized unique structures in which red phosphorus is encapsulated into the inner spacing of CNTs to prevent its peeling off from the electrode and improve its electronic conductivity. For improving the electrochemical reactivity of red phosphorus through accessible pathways of lithium ions, nanopores (<5 nm) were also formed onto the sidewalls of the phosphorus-encapsulated CNTs. After phosphorus encapsulation, the phosphorus atoms were distributed inside the nanotubes, confirming the structural stability of red phosphorus.Using phosphorus-encapsulated CNT electrodes, a reversible capacity showed approximately 850 mAh/g at the fiftieth charge-discharge cycle. This was a value at least two times higher than that of graphite electrodes. The estimated ratio of charge and discharge capacities (Coulombic efficiencies) of >99% after the tenth cycle and the subsequent cycles, which indicates a high reversibility of charge-discharge reactions on red phosphorus. However, the charge-discharge capacities gradually decreased with increasing cycle number because of the dissociation of some P-P bonds and other side reactions on the surface of phosphorus and the CNTs. Interestingly, the phosphorus-encapsulated CNT with nanopores facilitated the significant improvement in electrochemical performance compared with the phosphorus-encapsulated CNT without nanopores. This is suggested to be due to the high reactivity of red phosphorus with lithium ions through the nanopores on the sidewalls. After the charge-discharge cycles, red phosphorus was observed to be inside the nanotubes.We have proposed phosphorus-encapsulated CNTs as an electrode material for LIBs with high capacity, even though additional improvements in the structures are required to achieve long-term cycling without capacity fading. Further studies will be performed on the utilization of such electrodes.Story Source:Materials provided by Toyohashi University of Technology. ",0.155623,0.267442,0.2611,0.210514,0.205949,0.226185,0.222526,0,0
633,Multiple uses for empty plastic bottles during disaster relief and beyond,"Powerful hurricanes and earthquakes have wreaked havoc in the United States and around the world in recent years, often leaving people stranded for months and even years without access to water, food, and shelter. A unique collaborative project at Rensselaer Polytechnic Institute seeks to provide a sustainable solution, while also considering the environment.After a disaster strikes, first responders and other aid organizations mobilize to send needed supplies such as water, food, and medicine. Bottled water is sent by the tons, but all too often, the empty plastic bottles end up in the trash stream.The goal of the project is to re-use the empty bottles to construct and structurally test different structural components (walls, columns, and roofs) and then design and build a prototype for emergency shelters for displaced populations under conditions of distress. The structure will be approximately 15 square meters in size and will be erected on campus in front of the Greene Building May 5-12.Students and faculty in the Schools of Engineering and Architecture are conducting a multifaceted research project that includes structural and architectural design, along with strength analysis, of revolutionary transitional shelter designs built using the patented plastic interlocking bottles of Friendship Products LLC, led by inventor Timothy Carlson. The collaborative team is led by Mohammed Alnaggar, assistant professor of civil and environmental engineering, and Lydia Kallipoliti, assistant professor of architecture. They are working with Ashraf Mansour at the National Research Centre in Cairo, Egypt, and members of the Rensselaer Center for Architecture Science and Ecology in New York City, including Josh Draper. They are assisted by fabrication consultant Tom Roland from the School of Architecture and Andreas Theodoridis, Ph.D. student at CASE.According to Kallipoliti, the design studio, Second Lives | After Bottles, functions as a design-build think tank, bringing together architecture and engineering students and professors in a cross-disciplinary platform to work collaboratively and create innovative strategies for material recycling.The aim is to foster dialogue on topics relating to recycling of industrial products as building materials and on sustainable building construction. ""During the design studio, we are investigating several prototypes presenting variable solutions of construction and deconstruction of small inhabitable spaces using the bottles manufactured by Friendship LLC,"" said Kallipoliti.The idea is to use the bottles as sturdy, low-cost, easily assembled building blocks. Each modular unit slide-locks with other units to form strong wall and building structures that can be filled with dirt, sand, or other materials to form a sturdy structure without the use of mortar. The Friendship bottles are able to interlock without joints due to their embedded creases.""This project presents a unique collaboration to optimize function and shape,"" said Alnaggar. ""The structural engineering research focusing on studying all the mechanics of the interlocking between the bottles and its scaling up to the full structural scale provides the architectural engineers with the needed properties to create not only an aesthetically appealing structure, but also a structurally sound and safe one.""According to Alnaggar, the trend in dealing with these plastics is to shred them and then either recycle the material or put it in landfills. Without any additional energy costs to transform the bottles for further use, his research provides additional lives to these bottles as permanent parts in buildings. His students are investigating strength and thermal insulation properties of the bottles for their incorporation into concrete sandwich walls and hollow block slabs to save on materials and energy consumption. ""We are taking the bottles from our landfills and putting them sustainably into our structures to reduce weight and provide insulation for a better energy consumption,"" he said.After being displayed at Rensselaer, the pavilion will be brought to Industry City in Brooklyn for the WantedDesign event May 17-21. WantedDesign aims to provide a platform dedicated to promoting design and fostering the international creative community at large throughout the year. The Rensselaer Center for Architecture Science and Ecology is conducting a complementary investigation to the Second Lives | After Bottles team, focusing on the environmental performance of a transitional shelter. The project's research goals include environmental sensing and data frameworks for comfort and well-being in transitional shelters; the role of Friendship bottles in waste reduction and reuse within disaster relief logistics; and the use of Friendship bottles as a performative building system. CASE's transitional bottle shelter test bed will be installed alongside the Second Lives | After Bottles pavilion at WantedDesign Brooklyn 2018.Story Source:Materials provided by Rensselaer Polytechnic Institute. ",0.0915469,0.122125,0.254371,0.183279,0.128151,0.176563,0.173138,0,0
634,Simulation technique models material-aging process Method helps civil engineers gauge infrastructure longevity,"The nation's aging infrastructure requires massive investment. The American Society of Civil Engineers estimates the U.S. needs to spend some $4.5 trillion by 2025 to fix the country's roads, bridges, dams and other infrastructure.Imagine if engineers could build structures with materials that do not degrade over time. Researchers at the University of California, Irvine have proposed a new simulation technique that could help engineers do just that.Mohammad Javad Abdolhosseini Qomi, assistant professor of civil and environmental engineering, and engineering graduate student Ali Morshedifard have developed a numerical method to simulate the molecular aging process in amorphous materials, such as concrete and glass. This technique could help researchers not only better understand how materials weaken with age, but also develop materials that maintain their strength indefinitely. Their work appears this week in Nature Communications.According to the researchers, aging originates at the atomic and molecular levels. Because of this miniscule scale, it's nearly impossible to track microscopic changes over long periods. ""In computer simulation of materials, you would have to simulate a quadrillion time steps to capture only one second of behavior. That would not even get us close to the time scales relevant for aging phenomena, which are in the order of years and decades,"" explained Qomi.In their incremental stress-marching technique, Qomi and his graduate student subject the material's molecular structure to cyclic stress fluctuations, and then follow the material's response to such perturbations. ""Hydrated cement is composed of disk-like globules at the nanoscale. We serendipitously found that these globules gradually deform under sustained load, but the deformation comes to a stop after a certain period. We also found that the collective behavior of globules gives rise to a non-asymptotic deformation, which we believe to be at the origins of creep in cementitious materials. It was fascinating to see atomic origins of viscoelastic and logarithmic deformation under constant stress,"" said Morshedifard, the paper's lead author.Qomi and his research team plan to apply this new technique to explore the relationship between the composition and texture of structural materials and their time-dependent behavior.""The Federal Highway Administration spends more than $80 billion a year to fix bridges that degrade as a result of aging phenomena,"" Qomi continued. ""Understanding how structural materials age is the very first step toward designing reduced-aging materials that can potentially save taxpayers money.""Story Source:Materials provided by University of California - Irvine. ",0.0814768,0.131381,0.187392,0.172146,0.114075,0.140605,0.169998,0,0
635,Scientists create innovative new 'green' concrete using graphene,"A new greener, stronger and more durable concrete that is made using the wonder-material graphene could revolutionise the construction industry.Experts from the University of Exeter have developed a pioneering new technique that uses nanoengineering technology to incorporate graphene into traditional concrete production.The new composite material, which is more than twice as strong and four times more water resistant than existing concretes, can be used directly by the construction industry on building sites. All of the concrete samples tested are according to British and European standards for construction.Crucially, the new graphene-reinforced concentre material also drastically reduced the carbon footprint of conventional concrete production methods, making it more sustainable and environmentally friendly.The research team insist the new technique could pave the way for other nanomaterials to be incorporated into concrete, and so further modernise the construction industry worldwide.The research is published in the journal Advanced Functional Materials, on Monday, April 23 2018.Professor Monica Craciun, co-author of the paper and from Exeter's engineering department, said: ""Our cities face a growing pressure from global challenges on pollution, sustainable urbanization and resilience to catastrophic natural events, amongst others.""This new composite material is an absolute game-changer in terms of reinforcing traditional concrete to meets these needs. Not only is it stronger and more durable, but it is also more resistant to water, making it uniquely suitable for construction in areas which require maintenance work and are difficult to be accessed .""Yet perhaps more importantly, by including graphene we can reduce the amount of materials required to make concrete by around 50 per cent -- leading to a significant reduction of 446kg/tonne of the carbon emissions.""This unprecedented range of functionalities and properties uncovered are an important step in encouraging a more sustainable, environmentally-friendly construction industry worldwide.""Previous work on using nanotechnology has concentrated on modifying existing components of cement, one of the main elements of concrete production.In the innovative new study, the research team has created a new technique that centres on suspending atomically thin graphene in water with high yield and no defects, low cost and compatible with modern, large scale manufacturing requirements.Dimitar Dimov, the lead author and also from the University of Exeter added: ""This ground-breaking research is important as it can be applied to large-scale manufacturing and construction. The industry has to be modernised by incorporating not only off-site manufacturing, but innovative new materials as well.""Finding greener ways to build is a crucial step forward in reducing carbon emissions around the world and so help protect our environment as much as possible. It is the first step, but a crucial step in the right direction to make a more sustainable construction industry for the future.""Story Source:Materials provided by University of Exeter. ",0.06652,0.125175,0.202714,0.152013,0.102764,0.1281,0.140154,0,0
636,New tool speeds up the design of wearable tech,"In a new paper published by Nano Energy, experts from the Advanced Technology Institute (ATI) at the University of Surrey detail a new methodology that allows designers of smart-wearables to better understand and predict how their products would perform once manufactured and in use.The technology is centred on materials that become electrically charged after they come into contact with each other, known as 'triboelectric materials' -- for example, a comb through hair can create an electrical charge. 'Triboelectric Nanogenerators (TENGs)' use this static charge to harvest energy from movement through a process called electrostatic induction. Over the years, a variety of TENGs have been designed which can convert almost any type of movement into electricity. The University of Surrey's tool gives manufacturers an accurate understanding of the output power their design would create once produced.This follows the news earlier this year of the ATI announcing the creation of its e4million state-of-the-art Nano-Manufacturing Hub. The new facility will produce plastic nanoscale electronics for wearable sensors, electronic tags and other electronic devices.Ishara Dharmasena, lead scientist on this project from the University of Surrey, said: ""The future global energy mix will depend on renewable energy sources such as solar power, wind, motion, vibrations and tidal. TENGs are a leading technology to capture and convert motion energy into electricity, extremely useful in small scale energy harvesting applications. Our work will, for the first time, provide universal guidance to develop, compare and improve various TENG designs. We expect this technology in household and industrial electronic products, catering to a new generation of mobile and autonomous energy requirements.""Professor Ravi Silva, Director of the Advanced Technology Institute, said: ""This is truly an exciting area of research for our team -- an area we have been working on over a number of years. We believe that our new tool will be of great help to a lot of researchers and designers who are investigating these materials.""The world urgently needs new forms of affordable and renewable energy sources. TENGs not only present a wonderful opportunity for the consumer electronics industry, but they are an incredibly exciting material group that could be used in all countries and remote locations where the nation grid does not extend, particularly for radios, wireless communication devices and medical equipment.""Story Source:Materials provided by University of Surrey. ",0.0682367,0.0932088,0.0991638,0.135574,0.0992774,0.134617,0.118851,0,0
637,New algorithm could add life to bridges,"A new algorithm developed by the University of Surrey could help structural engineers better monitor the health of bridges and alert them to when they need repair faster.Many authorities and organisations use structural health monitoring systems to keep track of the health of bridges, along with the weight of the traffic that it withstands on a day-to-day basis. This leads to a very high sampling rate of data, with some reaching at least 10 Hz and databases that have gigabytes worth of information on a singular structure -- which is expensive to house.In a paper published by the journal Measurement, scientists detail how they created an algorithm that compresses large data from bridge monitoring systems into more manageable sizes.The Surrey scientists used a dictionary learning method called K-means Singular Value Decomposition (K-SVD) to compress data from the system that monitors the Lezeria bridge in Portugal. The team applied its algorithm to 45,000 data per channel per hour received by the Bridge Weight-in-Motion system -- one of the most widely used monitoring applications -- and managed to achieve a nearly lossless reconstruction from the information of less than 0.1 per cent. Other methods have shown that they need 50 per cent of the data to achieve similar reconstruction accuracy.Dr Ying Wang, lead author of the paper from the University of Surrey, said: ""Many authorities find it difficult to house the data they have for their bridges and other infrastructure -- with hundreds of thousands, sometimes millions of cars using some bridges every day.""We believe that this approach shows that you can dramatically reduce the large data into a much manageable size without losing information -- which is critical to structural engineers.""Story Source:Materials provided by University of Surrey. ",0.104416,0.108349,0.18464,0.242336,0.149239,0.155815,0.179231,0,0
638,Spinning the light: The world's smallest optical gyroscope Engineers create an optical gyroscope smaller than a grain of rice,"Gyroscopes are devices that help vehicles, drones, and wearable and handheld electronic devices know their orientation in three-dimensional space. They are commonplace in just about every bit of technology we rely on every day. Originally, gyroscopes were sets of nested wheels, each spinning on a different axis. But open up a cell phone today, and you will find a microelectromechanical sensor (MEMS), the modern-day equivalent, which measures changes in the forces acting on two identical masses that are oscillating and moving in opposite directions. These MEMS gyroscopes are limited in their sensitivity, so optical gyroscopes have been developed to perform the same function but with no moving parts and a greater degree of accuracy using a phenomenon called the Sagnac effect.What is the Sagnac Effect?The Sagnac effect, named after French physicist Georges Sagnac, is an optical phenomenon rooted in Einstein's theory of general relativity. To create it, a beam of light is split into two, and the twin beams travel in opposite directions along a circular pathway, then meet at the same light detector. Light travels at a constant speed, so rotating the device -- and with it the pathway that the light travels -- causes one of the two beams to arrive at the detector before the other. With a loop on each axis of orientation, this phase shift, known as the Sagnac effect, can be used to calculate orientation.The ProblemThe smallest high-performance optical gyroscopes available today are bigger than a golf ball and are not suitable for many portable applications. As optical gyroscopes are built smaller and smaller, so too is the signal that captures the Sagnac effect, which makes it more and more difficult for the gyroscope to detect movement. Up to now, this has prevented the miniaturization of optical gyroscopes.The InventionCaltech engineers led by Ali Hajimiri, Bren Professor of Electrical Engineering and Medical Engineering in the Division of Engineering and Applied Science, developed a new optical gyroscope that is 500 times smaller than the current state-of-the-art device, yet they can detect phase shifts that are 30 times smaller than those systems. The new device is described in a paper published in the November issue of Nature Photonics.How it worksThe new gyroscope from Hajimiri's lab achieves this improved performance by using a new technique called ""reciprocal sensitivity enhancement."" In this case, ""reciprocal"" means that it affects both beams of the light inside the gyroscope in the same way. Since the Sagnac effect relies on detecting a difference between the two beams as they travel in opposite directions, it is considered nonreciprocal. Inside the gyroscope, light travels through miniaturized optical waveguides (small conduits that carry light, that perform the same function as wires do for electricity). Imperfections in the optical path that might affect the beams (for example, thermal fluctuations or light scattering) and any outside interference will affect both beams similarly.Hajimiri's team found a way to weed out this reciprocal noise while leaving signals from the Sagnac effect intact. Reciprocal sensitivity enhancement thus improves the signal-to-noise ratio in the system and enables the integration of the optical gyro onto a chip smaller than a grain of rice.The detailsThe paper is titled ""Nanophotonic optical gyroscope with reciprocal sensitivity enhancement."" Graduate student Parham Khial is the lead author, and undergraduate student Alexander White is a co-author. This research was funded by the Rothenberg Innovation Initiative.Story Source:Materials provided by California Institute of Technology. Original written by Robert Perkins. ",0.129538,0.110559,0.143767,0.163531,0.178933,0.167856,0.143694,0,0
639,Light from ancient quasars helps confirm quantum entanglement Results are among the strongest evidence yet for 'spooky action at a distance',"Last year, physicists at MIT, the University of Vienna, and elsewhere provided strong support for quantum entanglement, the seemingly far-out idea that two particles, no matter how distant from each other in space and time, can be inextricably linked, in a way that defies the rules of classical physics.Take, for instance, two particles sitting on opposite edges of the universe. If they are truly entangled, then according to the theory of quantum mechanics their physical properties should be related in such a way that any measurement made on one particle should instantly convey information about any future measurement outcome of the other particle -- correlations that Einstein skeptically saw as ""spooky action at a distance.""In the 1960s, the physicist John Bell calculated a theoretical limit beyond which such correlations must have a quantum, rather than a classical, explanation.But what if such correlations were the result not of quantum entanglement, but of some other hidden, classical explanation? Such ""what-ifs"" are known to physicists as loopholes to tests of Bell's inequality, the most stubborn of which is the ""freedom-of-choice"" loophole: the possibility that some hidden, classical variable may influence the measurement that an experimenter chooses to perform on an entangled particle, making the outcome look quantumly correlated when in fact it isn't.Last February, the MIT team and their colleaguessignificantly constrainedthe freedom-of-choice loophole, by using 600-year-old starlight to decide what properties of two entangled photons to measure. Their experiment proved that, if a classical mechanism caused the correlations they observed, it would have to have been set in motion more than 600 years ago, before the stars' light was first emitted and long before the actual experiment was even conceived.Now, in a paper published today in Physical Review Letters, the same team has vastly extended the case for quantum entanglement and further restricted the options for the freedom-of-choice loophole. The researchers used distant quasars, one of which emitted its light 7.8 billion years ago and the other 12.2 billion years ago, to determine the measurements to be made on pairs of entangled photons. They found correlations among more than 30,000 pairs of photons, to a degree that far exceeded the limit that Bell originally calculated for a classically based mechanism.""If some conspiracy is happening to simulate quantum mechanics by a mechanism that is actually classical, that mechanism would have had to begin its operations -- somehow knowing exactly when, where, and how this experiment was going to be done -- at least 7.8 billion years ago. That seems incredibly implausible, so we have very strong evidence that quantum mechanics is the right explanation,"" says co-author Alan Guth, the Victor F. Weisskopf Professor of Physics at MIT.""The Earth is about 4.5 billion years old, so any alternative mechanism -- different from quantum mechanics -- that might have produced our results by exploiting this loophole would've had to be in place long before even there was a planet Earth, let alone an MIT,"" adds David Kaiser, the Germeshausen Professor of the History of Science and professor of physics at MIT. ""So we've pushed any alternative explanations back to very early in cosmic history.""Guth and Kaiser's co-authors include Anton Zeilinger and members of his group at the Austrian Academy of Sciences and the University of Vienna, as well as physicists at Harvey Mudd College and the University of California at San Diego.A decision, made billions of years agoIn 2014, Kaiser and two members of the current team, Jason Gallicchio and Andrew Friedman,proposed an experimentto produce entangled photons on Earth -- a process that is fairly standard in studies of quantum mechanics. They planned to shoot each member of the entangled pair in opposite directions, toward light detectors that would also make a measurement of each photon using a polarizer. Researchers would measure the polarization, or orientation, of each incoming photon's electric field, by setting the polarizer at various angles and observing whether the photons passed through -- an outcome for each photon that researchers could compare to determine whether the particles showed the hallmark correlations predicted by quantum mechanics.The team added a unique step to the proposed experiment, which was to use light from ancient, distant astronomical sources, such as stars and quasars, to determine the angle at which to set each respective polarizer. As each entangled photon was in flight, heading toward its detector at the speed of light, researchers would use a telescope located at each detector site to measure the wavelength of a quasar's incoming light. If that light was redder than some reference wavelength, the polarizer would tilt at a certain angle to make a specific measurement of the incoming entangled photon -- a measurement choice that was determined by the quasar. If the quasar's light was bluer than the reference wavelength, the polarizer would tilt at a different angle, performing a different measurement of the entangled photon.In their previous experiment, the team used small backyard telescopes to measure the light from stars as close as 600 light years away. In their new study, the researchers used much larger, more powerful telescopes to catch the incoming light from even more ancient, distant astrophysical sources: quasars whose light has been traveling toward the Earth for at least 7.8 billion years -- objects that are incredibly far away and yet are so luminous that their light can be observed from Earth.Tricky timingOn Jan. 11, 2018, ""the clock had just ticked past midnight local time,"" as Kaiser recalls, when about a dozen members of the team gathered on a mountaintop in the Canary Islands and began collecting data from two large, 4-meter-wide telescopes: the William Herschel Telescope and the Telescopio Nazionale Galileo, both situated on the same mountain and separated by about a kilometer.One telescope focused on a particular quasar, while the other telescope looked at another quasar in a different patch of the night sky. Meanwhile, researchers at a station located between the two telescopes created pairs of entangled photons and beamed particles from each pair in opposite directions toward each telescope.In the fraction of a second before each entangled photon reached its detector, the instrumentation determined whether a single photon arriving from the quasar was more red or blue, a measurement that then automatically adjusted the angle of a polarizer that ultimately received and detected the incoming entangled photon.""The timing is very tricky,"" Kaiser says. ""Everything has to happen within very tight windows, updating every microsecond or so.""Demystifying a mirageThe researchers ran their experiment twice, each for around 15 minutes and with two different pairs of quasars. For each run, they measured 17,663 and 12,420 pairs of entangled photons, respectively. Within hours of closing the telescope domes and looking through preliminary data, the team could tell there were strong correlations among the photon pairs, beyond the limit that Bell calculated, indicating that the photons were correlated in a quantum-mechanical manner.Guth led a more detailed analysis to calculate the chance, however slight, that a classical mechanism might have produced the correlations the team observed.He calculated that, for the best of the two runs, the probability that a mechanism based on classical physics could have achieved the observed correlation was about 10 to the minus 20 -- that is, about one part in one hundred billion billion, ""outrageously small,"" Guth says. For comparison, researchers have estimated the probability that the discovery of the Higgs boson was just a chance fluke to be about one in a billion.""We certainly made it unbelievably implausible that a local realistic theory could be underlying the physics of the universe,"" Guth says.And yet, there is still a small opening for the freedom-of-choice loophole. To limit it even further, the team is entertaining ideas of looking even further back in time, to use sources such as cosmic microwave background photons that were emitted as leftover radiation immediately following the Big Bang, though such experiments would present a host of new technical challenges.""It is fun to think about new types of experiments we can design in the future, but for now, we are very pleased that we were able to address this particular loophole so dramatically. Our experiment with quasars puts extremely tight constraints on various alternatives to quantum mechanics. As strange as quantum mechanics may seem, it continues to match every experimental test we can devise,"" Kaiser says.This research was supported in part by the Austrian Academy of Sciences, the Austrian Science Fund, the U.S. National Science Foundation, and the U.S. Department of Energy.Story Source:Materials provided by Massachusetts Institute of Technology. Original written by Jennifer Chu. ",0.0678872,0.0808739,0.112121,0.139543,0.0975598,0.115927,0.121009,0,0
640,Feeling the pressure with universal tactile imaging,"Touch, or tactile sensing, is fundamentally important for a range of real-life applications, from robotics to surgical medicine to sports science. Tactile sensors are modeled on the biological sense of touch and can help researchers to understand human perception and motion. Researchers from Osaka University have now developed a new approach to pressure distribution measurement using tactile imaging technology.Pressure is one of the primary characteristics of touch, and tactile imaging can be used to measure pressure or stress distributions across an object of interest. The most common current approach to tactile imaging involves use of an array of sensors composed of pressure-sensitive materials. However, such arrays require complex fabrication processes and place limitations on the sensor design, hence the necessity of a new method, now outlined in an article in IEEE Transactions on Industrial Electronics.""The pressure between two conductors is directly related to the electrical contact resistance between them,"" states Osaka University's Osamu Oshiro. ""We used this relationship to develop a sensor composed of a pair of electromechanically coupled conductors, where one conductor had a driving function and the other performed the probe function. This sensor has no need for pressure-sensitive materials and is simpler to manufacture.""This strategy enabled development of a universal tactile sensor for contact pressure distribution measurement using simple conductive materials such as carbon paint. The design concept combined innovation in mechatronics technology, which enabled development of a flexible sensor based on conventional conductors connected to electrodes, with a tomography-based approach to determining the pressure distribution across the coupled conductors.The proposed method improved on previous electrical impedance tomography-based tactile sensing techniques to provide sensors with high positional accuracy, adjustable sensitivity and range, and a relatively simple fabrication process. ""The sensors can be realized using various conducting materials, including conductive fabrics and paints,"" says lead author Shunsuke Yoshimoto. ""Sheet-type flexible sensors were fabricated, along with finger-shaped sensors produced by coating 3D-printed structures with conductive paint, to illustrate possible practical applications.""The ease of adjustment of the sensitivity and sensing range and the pressure estimation precision means that this tactile imaging approach is expected to enable advanced control of multipurpose robots. ""These sensors are expected to be applicable in fields including remote device operation and industrial automation,"" states co-author Yoshihiro Kuroda.Story Source:Materials provided by Osaka University. ",0.10775,0.115004,0.13276,0.178016,0.158181,0.170824,0.173296,0,0
641,A molecular sensor for in-situ analysis of complex biological fluids,"A KAIST research group presented a molecular sensor with a microbead format for the rapid in-situ detection of harmful molecules in biological fluids or foods in a collaboration with a Korea Institute of Materials Science (KIMS) research group. As the sensor is designed to selectively concentrate charged small molecules and amplify the Raman signal, no time-consuming pretreatment of samples is required.Raman spectra are commonly known as molecular fingerprints. However, their low intensity has restricted their use in molecular detection, especially for low concentrations. Raman signals can be dramatically amplified by locating the molecules on the surface of metal nanostructures where the electromagnetic field is strongly localized. However, it is still challenging to use Raman signals for the detection of small molecules dissolved in complex biological fluids. Adhesive proteins irreversibly adsorb on the metal surface, which prevents the access of small target molecules onto the metal surface. Therefore, it was a prerequisite to purify the samples before analysis. However, it takes a long time and is expensive.A joint team from Professor Shin-Hyun Kim's group in KAIST and Dr. Dong-Ho Kim's group in KIMS has addressed the issue by encapsulating agglomerates of gold nanoparticles using a hydrogel. The hydrogel has three-dimensional network structures so that molecules smaller than the mesh are selectively permeable. Therefore, the hydrogel can exclude relatively large proteins, while allowing the infusion of small molecules. Therefore, the surface of gold nanoparticles remains intact against proteins, which accommodates small molecules. In particular, the charged hydrogel enables the concentration of oppositely-charged small molecules. That is, the purification is autonomously done by the materials, removing the need for time-consuming pretreatment. As a result, the Raman signal of small molecules can be selectively amplified in the absence of adhesive proteins.Using the molecular sensors, the research team demonstrated the direct detection of fipronil sulfone dissolved in an egg without sample pretreatment. Recently, insecticide-contaminated eggs have spread in Europe, South Korea, and other countries, threatening health and causing social chaos. Fipronil is one of the most commonly used insecticides for veterinary medicine to combat ?eas. The ?pronil is absorbed through the chicken skin, from which a metabolite, ?pronil sulfone, accumulates in the eggs. As the ?pronil sulfone carries partial negative charges, it can be concentrated using positively-charged microgels while excluding adhesive proteins in eggs, such as ovalbumin, ovoglobulin, and ovomucoid. Therefore, the Raman spectrum of fipronil sulfone can be directly measured. The limit of direct detection of fipronil sulfone dissolved in an egg was measured at 0.05 ppm.Professor Kim said, ""The molecular sensors can be used not only for the direct detection of harmful molecules in foods but also for residual drugs or biomarkers in blood or urine."" Dr. Dong-Ho Kim said, ""It will be possible to save time and cost as no sample treatment is required.""Story Source:Materials provided by The Korea Advanced Institute of Science and Technology (KAIST). ",0.11769,0.223794,0.19253,0.185493,0.192173,0.153149,0.195946,0,0
642,Monitoring electromagnetic signals in the brain with MRI Technique could be used to detect light or electrical fields in living tissue,"Researchers commonly study brain function by monitoring two types of electromagnetism -- electric fields and light. However, most methods for measuring these phenomena in the brain are very invasive.MIT engineers have now devised a new technique to detect either electrical activity or optical signals in the brain using a minimally invasive sensor for magnetic resonance imaging (MRI).MRI is often used to measure changes in blood flow that indirectly represent brain activity, but the MIT team has devised a new type of MRI sensor that can detect tiny electrical currents, as well as light produced by luminescent proteins. (Electrical impulses arise from the brain's internal communications, and optical signals can be produced by a variety of molecules developed by chemists and bioengineers.)""MRI offers a way to sense things from the outside of the body in a minimally invasive fashion,"" says Aviad Hai, an MIT postdoc and the lead author of the study. ""It does not require a wired connection into the brain. We can implant the sensor and just leave it there.""This kind of sensor could give neuroscientists a spatially accurate way to pinpoint electrical activity in the brain. It can also be used to measure light, and could be adapted to measure chemicals such as glucose, the researchers say.Alan Jasanoff, an MIT professor of biological engineering, brain and cognitive sciences, and nuclear science and engineering, and an associate member of MIT's McGovern Institute for Brain Research, is the senior author of the paper, which appears in the Oct. 22 issue of Nature Biomedical Engineering. Postdocs Virginia Spanoudaki and Benjamin Bartelle are also authors of the paper.Detecting electric fieldsJasanoff's lab has previously developed MRI sensors that can detect calcium and neurotransmitters such as serotonin and dopamine. In this paper, they wanted to expand their approach to detecting biophysical phenomena such as electricity and light. Currently, the most accurate way to monitor electrical activity in the brain is by inserting an electrode, which is very invasive and can cause tissue damage. Electroencephalography (EEG) is a noninvasive way to measure electrical activity in the brain, but this method cannot pinpoint the origin of the activity.To create a sensor that could detect electromagnetic fields with spatial precision, the researchers realized they could use an electronic device -- specifically, a tiny radio antenna.MRI works by detecting radio waves emitted by the nuclei of hydrogen atoms in water. These signals are usually detected by a large radio antenna within an MRI scanner. For this study, the MIT team shrank the radio antenna down to just a few millimeters in size so that it could be implanted directly into the brain to receive the radio waves generated by water in the brain tissue.The sensor is initially tuned to the same frequency as the radio waves emitted by the hydrogen atoms. When the sensor picks up an electromagnetic signal from the tissue, its tuning changes and the sensor no longer matches the frequency of the hydrogen atoms. When this happens, a weaker image arises when the sensor is scanned by an external MRI machine.The researchers demonstrated that the sensors can pick up electrical signals similar to those produced by action potentials (the electrical impulses fired by single neurons), or local field potentials (the sum of electrical currents produced by a group of neurons).""We showed that these devices are sensitive to biological-scale potentials, on the order of millivolts, which are comparable to what biological tissue generates, especially in the brain,"" Jasanoff says.The researchers performed additional tests in rats to study whether the sensors could pick up signals in living brain tissue. For those experiments, they designed the sensors to detect light emitted by cells engineered to express the protein luciferase.Normally, luciferase's exact location cannot be determined when it is deep within the brain or other tissues, so the new sensor offers a way to expand the usefulness of luciferase and more precisely pinpoint the cells that are emitting light, the researchers say. Luciferase is commonly engineered into cells along with another gene of interest, allowing researchers to determine whether the genes have been successfully incorporated by measuring the light produced.Smaller sensorsOne major advantage of this sensor is that it does not need to carry any kind of power supply, because the radio signals that the external MRI scanner emits are enough to power the sensor.Hai, who will be joining the faculty at the University of Wisconsin at Madison in January, plans to further miniaturize the sensors so that more of them can be injected, enabling the imaging of light or electrical fields over a larger brain area. In this paper, the researchers performed modeling that showed that a 250-micron sensor (a few tenths of a millimeter) should be able to detect electrical activity on the order of 100 millivolts, similar to the amount of current in a neural action potential.Jasanoff's lab is interested in using this type of sensor to detect neural signals in the brain, and they envision that it could also be used to monitor electromagnetic phenomena elsewhere in the body, including muscle contractions or cardiac activity.""If the sensors were on the order of hundreds of microns, which is what the modeling suggests is in the future for this technology, then you could imagine taking a syringe and distributing a whole bunch of them and just leaving them there,"" Jasanoff says. ""What this would do is provide many local readouts by having sensors distributed all over the tissue.""The research was funded by the National Institutes of Health.Story Source:Materials provided by Massachusetts Institute of Technology. Original written by Anne Trafton. ",0.0806312,0.113654,0.126629,0.198947,0.160212,0.134865,0.159416,0,0
643,High-performance flexible transparent force touch sensor for wearable devices,"Researchers reported a high-performance and transparent nanoforce touch sensor by developing a thin, flexible, and transparent hierarchical nanocomposite (HNC) film. The research team says their sensor simultaneously features all the necessary characters for industrial-grade application: high sensitivity, transparency, bending insensitivity, and manufacturability.Force touch sensors that recognize the location and pressure of external stimuli have received considerable attention for various applications, such as wearable devices, flexible displays, and humanoid robots. For decades, huge amounts of research and development have been devoted to improving pressure sensitivity to realize industrial-grade sensing devices. However, it remains a challenge to apply force touch sensors in flexible applications because sensing performance is subject to change and degraded by induced mechanical stress and deformation when the device is bent.To overcome these issues, the research team focused on the development of non-air gap sensors to break away from the conventional technology where force touch sensors need to have air-gaps between electrodes for high sensitivity and flexibility.The proposed non air-gap force touch sensor is based on a transparent nanocomposite insulator containing metal nanoparticles which can maximize the capacitance change in dielectrics according to the pressure, and a nanograting substrate which can increase transparency as well as sensitivity by concentrating pressure. As a result, the team succeeded in fabricating a highly sensitive, transparent, flexible force touch sensor that is mechanically stable against repetitive pressure.Furthermore, by placing the sensing electrodes on the same plane as the neutral plane, the force touch sensor can operate, even when bending to the radius of the ballpoint pen, without changes in performance levels.The proposed force touch has also satisfied commercial considerations in mass production such as large-area uniformity, production reproducibility, and reliability according to temperature and long-term use.Finally, the research team applied the developed sensor to a pulse-monitoring capable healthcare wearable device and detected a real-time human pulse. In addition, the research team confirmed with HiDeep, Inc. that a seven-inch large-area sensor can be integrated into a commercial smartphone.Story Source:Materials provided by The Korea Advanced Institute of Science and Technology (KAIST). ",0.131772,0.13453,0.157365,0.187616,0.177161,0.182263,0.1782,0,0
644,Innovative sensing technique could improve greenhouse gas analysis Ghost imaging technique achieves sensitive detection of gases at fine resolution,"An international team of researchers has used an unconventional imaging technique known as ghost imaging to make spectroscopic measurements of a gas molecule. The new approach by scientists at Tampere University of Technology in Finland, the University of Eastern Finland and the University of Burgundy Franche-Comte in France, works over a wide range of wavelengths and could improve measurements of atmospheric greenhouse gases such as methane.In The Optical Society (OSA) journal Optics Letters, the researchers report their approach to expand ghost imaging techniques to produce very efficient spectral measurements that reveal information about the chemical makeup of a gas molecule. They achieve this by using ghost imaging with a supercontinuum light source, to capture the wavelength-dependent light transmitted through samples and demonstrate that the technique can measure the spectral signature of the greenhouse gas methane with subnanometer resolution.""Monitoring atmospheric greenhouse gases such as methane, carbon dioxide, nitrous oxide and ozone is important for assessing how changing levels of these gases relates to climate change,"" said Caroline Amiot, a research team member from Tampere University of Technology. ""In some specific circumstances, our method could enable more sensitive detection of greenhouse gases, providing more accurate information about these important chemical compounds.""Ghost imaging produces images by correlating the intensity of two light beams that, taken individually, do not carry any meaningful information on the shape of the object, but instead allow indirect inferences about its properties. This approach can eliminate some of the distortions associated with typical imaging systems in harsh environments and has been used to create high-resolution images of physical objects and, more recently, to restore scrambled ultrafast signals on picosecond timescales.Gas molecules are often sparse and thus only change the total light transmittance by a small amount. This means that powerful light sources or extremely sensitive detectors are generally needed to detect them.""Because our technique works by detecting an integrated signal containing many wavelengths -- as opposed to one wavelength like traditional spectroscopy methods -- it enables measurements using less powerful light sources and at wavelengths where highly sensitive detectors aren't available,"" said Amiot.Spectral ghost imaging Ghost imaging creates a spectral image, which can contain the transmission or reflection spectrum of an object, by correlating two arms of a beam of light: one that encodes a random pattern that acts as a probing reference and the other that illuminates the sample. The new ghost imaging approach uses a supercontinuum light source, which emits pulses that each contain many wavelengths of light. The researchers used the random fluctuations that occur between the spectra associated with consecutive pulses to create the reference necessary for performing spectral ghost imaging.The light transmitted through a sample is then sensed with a fast detector without spectral resolution that provides an integrated signal for all the wavelengths of the spectral bandwidth under consideration. The image starts out looking like a noisy blob, but once it is correlated with the reference spectral fluctuations, the spectral image begins to appear.""It's possible to reconstruct the spectral image without sending large amounts of light through the sample,"" said Amiot. ""This can be very beneficial for light-sensitive samples, for example.""Generating a stronger signalMaking measurements of gases in the atmosphere has traditionally required sending high-power laser light into the atmosphere, where it interacts with the gas. ""In order to measure which gas is present and at what quantity, the very faint light signal that comes back must be further split into various wavelengths for detection,"" said Amiot. ""This may be problematic when the signal is very weak. Our method detects all the wavelengths mixed together, creating a much stronger signal that allows more sensitive measurements.""The researchers tested their technique by using it to produce a spectral image of methane. The ghost imaging measurements perfectly reproduced the series of discrete absorption lines that are the fingerprints of methane and matched well with more conventional direct spectroscopy measurements that the researchers performed for comparison.The researchers are now working on controlling the spectral fluctuations using pre-programmable light sources that would remove the need to measure the reference spectral patterns. They are also working towards using spectral-domain ghost imaging with an optical coherence tomography setup, which could allow sensitive information to be gained from tissue or other biological samples without using damaging amounts of light.Story Source:Materials provided by The Optical Society. ",0.103368,0.131858,0.156043,0.188819,0.180084,0.145939,0.161277,0,0
645,First experiments at new X-ray laser reveal unknown structure of antibiotics killer,"An international collaboration led by DESY and consisting of over 120 researchers has announced the results of the first scientific experiments at Europe's new X-ray laser European XFEL. The pioneering work not only demonstrates that the new research facility can speed up experiments by more than an order of magnitude, it also reveals a previously unknown structure of an enzyme responsible for antibiotics resistance. ""The groundbreaking work of the first team to use the European XFEL has paved the way for all users of the facility who greatly benefit from these pioneering experiments,"" emphasises European XFEL managing director Robert Feidenhans'l. ""We are very pleased -- these results show that the facility works even better than we had expected and is ready to deliver new scientific breakthroughs."" The scientists present their results, including the first new protein structure solved at the European XFEL, in the journal Nature Communications.""Being at a totally new class of facility we had to master many challenges that nobody had tackled before,"" says DESY scientist Anton Barty from the Center for Free-Electron Laser Science (CFEL), who led the team of about 125 researchers involved in the first experiments that were open to the whole scientific community. ""I compare it to the maiden flight of a novel aircraft: All calculations and assembly completed, everything says it will work, but not until you try it do you know whether it actually flies.""The 3.4 kilometres long European XFEL is designed to deliver X-ray flashes every 0.000000 220 seconds (220 nanoseconds). To unravel the three-dimensional structure of a biomolecule, such as an enzyme, the pulses are used to obtain flash X-ray exposures of tiny crystals grown from that biomolecule. Each exposure gives rise to a characteristic diffraction pattern on the detector. If enough such patterns are recorded from all sides of a crystal, the spatial structure of the biomolecule can be calculated. The structure of a biomolecule can reveal much about how it works.However, every crystal can only be X-rayed once since it is vaporised by the intense flash (after it has produced a diffraction pattern). So, to build up the full three-dimensional structure of the biomolecule, a new crystal has to be delivered into the beam in time for the next flash, by spraying it across the path of the laser in a water jet. Nobody has tried to X-ray samples to atomic resolution at this fast rate before. The fastest pulse rate so far of any such X-ray laser has been 120 flashes per second, that is one flash every 0.008 seconds (or 8,000,000 nanoseconds). To probe biomolecules at full speed, not only the crystals must be replenished fast enough -- the water jet is also vaporised by the X-rays and has to recover in time.""We revved up the speed of the water jet carrying the samples to 100 metres per second, that's about as fast as the speed record in formula 1,"" explains Max Wiedorn, who took care of the sample delivery together with his colleague Dominik Oberther, both from CFEL. A specially designed nozzle made sure the high-speed jet would be stable and meet the requirements.To record X-ray diffraction patterns at this fast rate, an international consortium led by DESY scientist Heinz Graafsma designed and built one of the world's fastest X-ray cameras, tailor-made for the European XFEL. The 'Adaptive Gain Integrating Pixel Detector' (AGIPD) can not only record images as fast as the X-ray pulses arrive, it can also tune the sensitivity of every pixel individually, making the most of the delicate diffraction patterns in which the information on the structure of the sample is encoded. ""The requirements of the European XFEL are so unique that the detector had to be designed completely from scratch and tailored to this task,"" reports Graafsma, who heads the detector group at DESY's photon science division and is also a professor at the Mid Sweden University. ""This could only be achieved thanks to the comprehensive expertise and fruitful collaboration of the large team involved.""The scientists first determined the structure of a very well-known sample, the enzyme lysozyme from egg-white, as a touchstone to verify the system worked as expected. Indeed, the structure derived at the European XFEL perfectly matches the known lysozyme structure, showing details as fine as 0.18 nanometres (millionths of a millimetre).""This is an excellent proof of the X-ray laser's performance,"" stresses XFEL pioneer Henry Chapman, a leading scientist at DESY and a professor at the University of Hamburg. ""We are very excited about the speed of the analysis: Experiments that used to take hours can now be done in a few minutes, as we have shown. And the set-up that we used can even be further optimised, speeding up data acquisition even more. The European XFEL offers bright prospects for the exploration of the nanocosm."" The striking performance of the X-ray laser is also a particular success of the DESY accelerator division that led the construction of the world's longest and most advanced superconducting linear accelerator driving the European XFEL.As their second target, the team chose a bacterial enzyme that plays an important role in antibiotics resistance. The molecule designated CTX-M-14 ?-lactamase was isolated from the bacterium Klebsiella pneumoniae whose multidrug-resistant strains are a grave concern in hospitals worldwide. Two years ago, even a 'pandrug-resistant' strain of Klebsiella pneumoniae was identified in the U.S. according to the Centers for Disease Control and Prevention (CDC), unaffected by all 26 commonly available antibiotics.The bacterium's enzyme CTX-M-14 ?-lactamase is present in all strains. It works like a molecular pair of scissors cutting lactam rings of penicillin derived antibiotics open, thereby rendering them useless. To avoid this, antibiotics are often administered together with a compound called avibactam that blocks the molecular scissors of the enzyme. Unfortunately, mutations change the form of the scissors. ""Some hospital strains of Klebsiella pneumoniae are already able to cleave even specifically developed third generation antibiotics,"" explains Christian Betzel, co-author of the paper and also a professor at the University of Hamburg. ""If we understand how this happens, it might help to design antibiotics that avoid this problem.""The scientists investigated a complex of CTX-M-14 ?-lactamase from the non-resistant, 'wild type' of the bacterium with avibactam bound to the enzyme's active centre, a structure that has not been analysed before. ""The results show with 0.17 nanometres precision how avibactam fits snug into a sort of canyon on the enzyme's surface that marks its active centre,"" says Markus Perbandt from the University of Hamburg, also a co-author of the paper. ""This specific complex has never been seen before, although the structure of the two separate components were already known.""The measurements show that it is possible to record high quality structural information, which is the first step towards recording snapshots of the biochemical reaction between enzymes and their substrates at different stages with the European XFEL. Together with the research groups of co-authors Martin Aepfelbacher and Holger Rohde, professors at the University Hospital UKE in Hamburg, the team plans to use the X-ray laser as a film camera to assemble those snapshots into movies of the molecular dynamics of avibactam and this ?-lactamase. ""Such movies would give us crucial insights into the biochemical process that could one day help us to design better inhibitors, reducing antibiotics resistance,"" says Betzel.Movies of chemical and biochemical reactions are just one example of a whole new spectrum of scientific experiments enabled by the European XFEL. A key factor is the speed at which data can be collected. ""This opens up new avenues of structural discovery,"" stresses European XFEL scientist Adrian Mancuso who heads the SPB/SFX instrument (Single Particles, Clusters and Biomolecules & Serial Femtosecond Crystallography) where the pioneering experiments were done. ""The difference in rate of discovery possible using European XFEL, demonstrated by this experiment, is as dramatic as the difference in travel time between being able to catch a plane across the Atlantic rather than taking a ship. The impact is potentially enormous.""This first 'beamtime' for experiments at the European XFEL took place two weeks after the opening of the facility in September 2017, and was open to all scientists from the community to participate, contribute, learn, and gain experience in how to carry out such measurements at this facility. ""The success of this 'open science' policy is illustrated by -- among other things -- the rapid dissemination of results from later campaigns at the SPB/SFX instrument by participating groups,"" explains Chapman. ""Additionally, the large concentration of effort by the community addressed previously unsolved challenges of managing and visualising data -- crucial to conducting all serial crystallography experiments at the European XFEL.""Story Source:Materials provided by Deutsches Elektronen-Synchrotron DESY. ",0.0463124,0.0709233,0.0857905,0.11021,0.0662624,0.0840679,0.0888921,0,0
646,Attosecond pulse leads to highest molecular level probe resolution,"Attosecond pulses enable physicists to probe dynamic processes in matter with unprecedented time resolution. This means such technology can provide better insights into the dynamics of electrons in molecules. Devising a source of ultra-fast X-ray pulsating in the attosecond range is no mean feat.Comparing an attosecond is to a second is the equivalent of comparing a second to about 31.71 billion years. Now, a team of physicists from China has exploited an optical phenomenon, opening the door to creating high-order oscillations in existing light sources. This makes it possible to shift the frequency of the original source into X-rays with a laser beam source pulsating in an ultra-fast manner, to reach the attosecond range. The trouble is that yield of such higher order oscillations decreases as the source laser wavelength increases. In a new study published in EPJ D, Liqiang Feng and Yi Li from Liaoning University of Technology, Jinzhou, China, have developed a method to select, enhance and extend the higher order emission peak from a laser beam changing from ultraviolet to a mid-infrared.Ensuring that the oscillation created is of suitable intensity and duration in the attosecond scale is tricky. In this study, the authors examine various ways of enhancing the efficiency of producing such higher order oscillations by coaxing the oscillations into a single peak instead of multiple peaks.To achieve this objective, they eliminate the sensitivity of the detector to the laser pulse duration and the delay time between pulses by opting for a technology based on a polarisation gate, which involves comparing the arrival time difference of the two polarised pulses from two mid-infrared polarisation fields once they have crossed the polarisation gate.The authors then show that by adding an additional pulse, the higher order oscillation can be extended to the X-ray region.Story Source:Materials provided by Springer. ",0.131979,0.145137,0.147778,0.182185,0.188256,0.18188,0.165053,0,0
647,"Instrument boosts analysis of small, extremely dark materials Spectrometer can probe materials that reflect almost no light such as miniscule meteorites","Researchers have developed a new instrument that can analyze light reflected from very small or extremely dark materials such as some meteorite samples and VANTABlack, the darkest human-made substance created. The instrument is already revealing new information about these and other difficult-to-analyze surfaces.Using spectroscopy to measure how light reflects off a surface can provide information on the physical and chemical properties of a sample material. However, extremely dark or small samples such as those from meteorites make this type of analysis difficult because they reflect very little light.In The Optical Society (OSA) journal Applied Optics, first author Sandra Potin and colleagues from Institut de Planetologie et Astrophysique de Grenoble (IPAG), University Grenoble Alpes in France, report that their new instrument, called SHADOWS, can be used for reflectance spectroscopy of samples measuring less than one millimeter cubed and that reflect less than 0.03 percent of light illuminating the sample.The new instrument can be used to better understand the composition of meteorites and to identify the asteroid or comet from which they originated. It can also be used to analyze surfaces on spacecraft, where very dark coatings are used to reduce stray light or remove heat from instrumentation used in space.Improving sensitivityThe spectroscopy instrument created by the researchers is known as a spectro-gonio radiometer, which works by shining light on a sample from a precise direction and then detecting light reflected back from a different direction. This approach is known as bi-directional reflectance spectroscopy because it calculates the reflectance of a material based on the direction of the illumination light and the direction from which that reflected light is detected. The spectra produced by this measurement are like a fingerprint that can be used to determine a sample's composition.The researchers began with a radiometer they previously developed for large, bright samples and worked to improve its sensitivity by reducing the illumination to a diameter of around 5.2 mm. The illumination spot can be made even smaller to map heterogeneous surfaces such as a surface that contains multiple types of materials.""The instrument has a very good signal-to-noise ratio because we used what we call synchronous detection,"" said Potin. ""This means that rather than using continuous light, we illuminate the samples with pulses of light at a very precise frequency. By linking the radiometer's two detectors to this frequency, everything that is not light reflected from the sample can be removed from the measurement.""The new instrument uses multiple colors of light and moves the light source and the detector around the sample to measure light coming from and reflected to multiple directions. The information obtained from the various directions and colors of light is used to build a 3D angular map of the sample's light reflection that can provide even more information about the sample.Wide range of temperaturesAnother aspect of the new instrument is that it can be used to analyze samples at -20 degrees C up to 250 degrees C, with plans to operate at even lower temperatures, down to -210 degrees C. This is important because the temperature of the sample can change the spectra obtained with the instrument.""Some asteroids are very far from the sun and thus rather cold, but when a comet goes near the sun, it gets very hot,"" said Potin. ""If we're trying to compare meteorite samples found on Earth with spectra obtained from asteroids in space, we need to take the measurements at a very wide range of temperatures.""The researchers used SHADOWS to take measurements of a VANTABlack sample, which is a chemical substance made of carbon nanotubes grown on aluminum foil. The spectra of this material obtained with the new instrument looked completely different from those acquired by other spectroscopy techniques because it included information taken from different directions.""By customizing bi-directional spectral analysis for very dark surfaces, our new approach can reveal structural information that has not been observed with other types of measurements,"" said Potin.The researchers say they are now working on making improvements to the instrument, including incorporating polarization measurements to provide even more information about samples analyzed. The instrument is available for use by researchers in Europe through the Trans National Access activity of the Europlanet 2020-RI program.Story Source:Materials provided by The Optical Society. ",0.104771,0.137631,0.167311,0.153434,0.144741,0.143333,0.150508,0,0
648,Fiber optic sensor measures tiny magnetic fields New technology is sensitive enough to detect magnetic fields from the brain and heart,"Researchers have developed a light-based technique for measuring very weak magnetic fields, such as those produced when neurons fire in the brain. The inexpensive and compact sensors could offer an alternative to the magnetic resonance imaging (MRI) systems currently used to map brain activity without the expensive cooling or electromagnetic shielding required by MRI machines.""A portable, low-cost brain imaging system that can operate at room temperature in unshielded environments would allow real-time brain activity mapping after potential concussions on the sports field and in conflict zones where the effect of explosives on the brain can be catastrophic,"" said researcher member Babak Amirsolaimani of the University of Arizona, Tucson.As detailed in The Optical Society (OSA) journal Optics Letters, the researchers fabricated the magnetic sensors using optical fibers and a newly developed polymer-nanoparticle composite that is sensitive to magnetic fields. The sensors can detect the brain's magnetic field, which is 100 million times weaker than the magnetic field of earth.The researchers also showed that the new sensor can detect the weak magnetic pattern of a human heartbeat and has the capability to detect magnetic fluctuations that change every microsecond from an area as small as 100 square microns.""The all-optical design of the sensor means it could be fabricated inexpensively on a silicon photonics chip, making it possible to produce a system that is almost as small as the sensor's 10-micron-diameter optical fiber,"" said Amirsolaimani. ""Multiple sensors could then be used together to provide high spatial resolution brain mapping.""The new sensors could help scientists better understand the activity of the brain and diseases of the brain such as dementia and Alzheimer's. They might also be useful for measuring the magnetic fields used to predict volcanic eruptions and earthquakes, identify oil and minerals for excavation and detect military submarines.Optical detection of magnetic fieldsThe optical method for detecting weak magnetic fields takes advantage of the fact that a magnetic field causes the polarization of light to rotate, with the degree of rotation dependent on the material through which the light passes. The researchers developed a new composite material made of nanoparticles dispersed in a polymer that imparts a detectible polarization rotation in light when very weak magnetic fields are present.They selected nanoparticles based on magnetite and cobalt because these materials exhibit very high magnetic sensitivity. They then optimized the size, spacing and coating of the nanoparticles to create a composite material that is extremely sensitive to magnetic fields.The researchers detected the polarization rotation using an optical interferometer. This works by splitting laser light into two paths, one of which passes through the highly-sensitive material while the other does not. The polarization of each light path is detected and compared to measure fluctuations in very small magnetic fields.When detecting weak magnetic fields, noise can easily cover up the signal being detected. For this reason, the researchers used an interferometer setup that eliminates ambient environmental effects such as vibration and temperature fluctuations. This setup kept noise levels very close to the theoretical limit of the optical design, which was key for detecting very weak magnetic fields.The researchers used the sensors to measure the magnetic field created by electrical impulses produced during the human heartbeat. They were able to detect a clear magnetic signal exhibiting high contrast, demonstrating the technology's potential as a simple replacement for electrocardiography, or ECG, tests commonly performed to detect heart problems.Next, the researchers plan to study the long-term stability of the sensors and how well they withstand environmental changes. They also want to fabricate several hundred sensors to make a system for evaluating and imaging the entire magnetic field of a human brain.Story Source:Materials provided by The Optical Society. ",0.100112,0.109279,0.148078,0.182671,0.152381,0.171699,0.153823,0,0
649,"Improving structural health monitoring with magnetostrictive transducer New sensor design provides more precise inspections, reduces human error","A new, more powerful generation of a patented Southwest Research Institute magnetostrictive sensor withstands extreme temperatures, automatically adjusts frequencies and incorporates a stronger magnet. The compact magnetostrictive transducer (MsTe) more accurately detects potential problems in oil, gas and chemical industry metal and nonmetal structures such as pipelines, storage tanks and anchor rods.""The MsT system offers the next level of signal strength. This user-friendly technology more precisely locates structural issues,"" said Dr. Sergey Vinogradov, an SwRI staff engineer who led the initiative to improve the sensor. ""It is an extremely reliable, state-of-the-art, durable sensor that you can install just about anywhere from subsea to open space.""The circular, hard-shell MsT sensor clamps around pipes and other structures and is available in circumferences ranging from one-half inch to 70 inches. It detects material flaws, corrosion and areas at risk of developing cracks and leaks. At just 1.1 inches wide, MsT uses less shear-wave couplant, a gel that aids energy transmission, and requires less clamping force when dry coupled, meaning when no couplant is used.The MsT sensor can be permanently installed on a structure to provide ongoing monitoring. A battery-operated hub collects and wirelessly transmits data from the sensor and can connect to multiple sensors at once. The hub can be programmed to change the sensors' frequencies automatically to meet industry safety standards, which require a structure be tested using at least two frequencies.""The MsT is compact and easy to install, but it's also more convenient. With this improved sensor, the operator does not change hardware to change frequencies. And this function is automated, which reduces human error,"" Vinogradov said. ""This feature is especially useful when the sensors need to be installed at multiple, hard-to-access locations daily.""High temperatures accelerate corrosion in metal structures. For that reason, MsT is designed to withstand up to 500 degrees Celsius (932 F). It can be installed on extremely hot structures to detect weaknesses and can also withstand temperatures significantly below freezing.Additionally, the sensor can now scan a structure in segments, providing more thorough inspections. MsT has also made the time-consuming task of shifting from transversal to longitudinal guided waves easier with the flip of a built-in switch. Longitudinal waves are ideal for buried pipes, while transversal waves are better for inspecting structures filled with fluids.This past spring, the American Society for Nondestructive Testing (ASNT) presented Vinogradov with the 2018 Research Recognition for Innovation Award for the MsT sensor. The award recognizes highly distinguished breakthroughs in nondestructive evaluation and testing research.Vinogradov and his team are showcasing the MsT sensor at the ASNT Annual Meeting, October 28 -- 30, 2018, at booth 932 at the George R. Brown Convention Center in Houston.Story Source:Materials provided by Southwest Research Institute. ",0.0834534,0.104446,0.152171,0.163346,0.132209,0.139646,0.146103,0,0
650,First particle tracks seen in prototype for international neutrino experiment Scientists announce big step in Deep Underground Neutrino Experiment,"The largest liquid-argon neutrino detector in the world has just recorded its first particle tracks, signaling the start of a new chapter in the story of the international Deep Underground Neutrino Experiment (DUNE).DUNE's scientific mission is dedicated to unlocking the mysteries of neutrinos, the most abundant (and most mysterious) matter particles in the universe. Neutrinos are all around us, but we know very little about them. Scientists on the DUNE collaboration think that neutrinos may help answer one of the most pressing questions in physics: why we live in a universe dominated by matter. In other words, why we are here at all.The enormous ProtoDUNE detector -- the size of a three-story house and the shape of a gigantic cube -- was built at CERN, the European Laboratory for Particle Physics, as the first of two prototypes for what will be a much, much larger detector for the DUNE project, hosted by the U.S. Department of Energy's Fermi National Accelerator Laboratory in the United States. When the first DUNE detector modules record data in 2026, they will each be 20 times larger than these prototypes.It is the first time CERN is investing in infrastructure and detector development for a particle physics project in the United States.The first ProtoDUNE detector took two years to build and eight weeks to fill with 800 tons of liquid argon, which needs to be kept at temperatures below -184 degrees Celsius (-300 degrees Fahrenheit). The detector records traces of particles in that argon, from both cosmic rays and a beam created at CERN's accelerator complex. Now that the first tracks have been seen, scientists will operate the detector over the next several months to test the technology in depth.""Only two years ago we completed the new building at CERN to house two large-scale prototype detectors that form the building blocks for DUNE,"" said Marzio Nessi, head of the Neutrino Platform at CERN. ""Now we have the first detector taking beautiful data, and the second detector, which uses a different approach to liquid-argon technology, will be online in a few months.""The technology of the first ProtoDUNE detector will be the same to be used for the first of the DUNE detector modules in the United States, which will be built a mile underground at the Sanford Underground Research Facility in South Dakota. More than 1,000 scientists and engineers from 32 countries spanning five continents -- Africa, Asia, Europe, North America and South America -- are working on the development, design and construction of the DUNE detectors. The groundbreaking ceremony for the caverns that will house the experiment was held in July of 2017.""Seeing the first particle tracks is a major success for the entire DUNE collaboration,"" said DUNE co-spokesperson Stefan Soldner-Rembold of the University of Manchester, UK. ""DUNE is the largest collaboration of scientists working on neutrino research in the world, with the intention of creating a cutting-edge experiment that could change the way we see the universe.""When neutrinos enter the detectors and smash into the argon nuclei, they produce charged particles. Those particles leave ionization traces in the liquid, which can be seen by sophisticated tracking systems able to create three-dimensional pictures of otherwise invisible subatomic processes. (An animation of how the DUNE and ProtoDUNE detectors work, along with other videos about DUNE, is available here.)""CERN is proud of the success of the Neutrino Platform and enthusiastic about being a partner in DUNE, together with Institutions and Universities from its Member States and beyond"" said Fabiola Gianotti, Director-General of CERN. ""These first results from ProtoDUNE are a nice example of what can be achieved when laboratories across the world collaborate. Research with DUNE is complementary to research carried out by the LHC and other experiments at CERN; together they hold great potential to answer some of the outstanding questions in particle physics today.""DUNE will not only study neutrinos, but their antimatter counterparts as well. Scientists will look for differences in behavior between neutrinos and antineutrinos, which could give us clues as to why the visible universe is dominated by matter. DUNE will also watch for neutrinos produced when a star explodes, which could reveal the formation of neutron stars and black holes, and will investigate whether protons live forever or eventually decay. Observing proton decay would bring us closer to fulfilling Einstein's dream of a grand unified theory.""DUNE is the future of neutrino research,"" said Fermilab Director Nigel Lockyer. ""Fermilab is excited to host an international experiment with such vast potential for new discoveries, and to continue our long partnership with CERN, both on the DUNE project and on the Large Hadron Collider.""To learn more about the Deep Underground Neutrino Experiment, the Long-Baseline Neutrino Facility that will house the experiment, and the PIP-II particle accelerator project at Fermilab that will power the neutrino beam for the experiment, visit http://www.fnal.gov/dune.DUNE comprises 175 institutions from 32 countries: Armenia, Brazil, Bulgaria, Canada, Chile, China, Colombia, Czech Republic, Finland, France, Greece, India, Iran, Italy, Japan, Madagascar, Mexico, Netherlands, Paraguay, Peru, Poland, Portugal, Romania, Russia, South Korea, Spain, Sweden, Switzerland, Turkey, Ukraine, United Kingdom, and United States. The DUNE interim design report provides a detailed description of the technologies that will be used for the DUNE detectors. More information is at dunescience.org.Story Source:Materials provided by DOE/Brookhaven National Laboratory. ",0.142803,0.162326,0.186364,0.229627,0.17982,0.206555,0.191352,0,0
651,Compact fiber laser may enable wearable tech and better endoscopes,"By creating a new twist on fiber optic sensors, researchers in China have developed a smart, flexible photoacoustic imaging technique that may have potential applications in wearable devices, instrumentation and medical diagnostics.Lead researcher Long Jin from the Institute of Photonics Technology at Jinan University in Guangzhou will present the new fiber laser-based ultrasound sensor at the OSA Frontiers in Optics + Laser Science APS/DLS conference, being held 16-20 Sept., 2018 in Washington, D.C. Jin will also present the results of a study using an in vivo photoacoustic microscope.The presentation will be part of the ""Advanced Microscopy"" session, to be held at 2:30 p.m. on Monday, 17 September in the Jefferson West ballroom of the Washington Hilton hotel.Their new technique relies on optical fiber technology to provide new sensors for photoacoustic imaging. It uses fiber-optic ultrasound detection, exploiting the acoustic effects on laser pulses via the thermoelastic effect -- temperature changes that occur as a result of the elastic strain.""Conventional fiber optic sensors detect extremely weak signals by taking advantage of their high sensitivity via phase measurement,"" said Jin. These same sorts of sensors are used in military applications to detect low-frequency (kilohertz) acoustic waves. But it turns out that they don't work so well for ultrasound waves at the megahertz frequencies used for medical purposes because ultrasound waves typically propagate as spherical waves and have a very limited interaction length with optical fibers. The new sensors were specifically developed for medical imaging, Jin said, and can provide better sensitivity than the piezoelectric transducers in use today.The group designed a special ultrasound sensor that's essentially a compact laser built within the 8-micron-diameter core of a single-mode optical fiber. ""It has a typical length of only 8 millimeters,"" Jin said. ""To build up the laser, two highly reflective grating mirrors are UV-written into the fiber core to provide optical feedback.""This fiber then gets doped with ytterbium and erbium to provide sufficient optical gain at 1,530 nanometers. They use a 980-nanometer semiconductor laser as the pump laser.""Such fiber lasers with a kilohertz-order linewidth -- the width of the optical spectrum -- can be exploited as sensors because they offer a high signal-to-noise ratio,"" said research team member Yizhi Liang, an assistant professor at the Institute of Photonics Technology.The ultrasound detection benefits from the combined technique because side-incident ultrasound waves deform the fiber, modulating the lasing frequency.""By detecting the frequency shift, we can reconstruct the acoustic waveform,"" Liang said.The team does not demodulate the ultrasound signal, extracting the original information, using conventional interferometry-based methods or any additive frequency locking. Rather, they use another method, called ""self-heterodyning,"" where the result of mixing two frequencies is detected. Here, they measure the radio frequency-domain beat note given by two orthogonal polarization modes of the fiber cavity. This demodulation also intrinsically guarantees a stable signal output.The fiber laser-based ultrasound sensors offer opportunities for use in photoacoustic microscopy. The researchers used a focused 532-nanometer nanosecond pulse laser to illuminate a sample and excite ultrasound signals. They place a sensor in a stationary position near the biological sample to detect optically induced ultrasound waves.""By raster scanning the laser spot, we can obtain a photoacoustic image of the vessels and capillaries of a mouse's ear,"" Jin said. ""This method can also be used to structurally image other tissues and functionally image oxygen distribution by using other excitation wavelengths -- which takes advantage of the characteristic absorption spectra of different target tissues.""Optical fibers are useful because they are tiny, lightweight, and intrinsically flexible, Jin added.""The development of our laser sensor is very encouraging because of its potential for endoscopes and wearable applications,"" Jin said. ""But current commercial endoscopic products are typically millimeters in dimension, which can cause pain, and they don't work well within hollow organs with limited space.""Story Source:Materials provided by The Optical Society. ",0.123518,0.150908,0.168574,0.185817,0.208225,0.156726,0.17251,0,0
652,New understanding of light allows researchers to see around corners,"Covert sensing of objects around a corner may soon become a reality.Aristide Dogariu, a University of Central Florida Pegasus Professor of Optics and Photonics, and his colleagues published a paper in Nature Communications this month demonstrating how to passively sense an object even when direct vision is impeded.It's a bit complicated, but the new sensing method the UCF team developed could lends itself to a number of practical applications, including use in defense, surveillance, search and rescue and medicine.Imagine trying to see something around a corner. This is easily done with mirrors, but imagine that light from the object can only reach the detector after it bounces off a diffusing wall that acts like a shattered mirror. Even though light looks totally dispersed, some of its initial properties do not completely vanish. Dogariu and his colleagues were able to measure subtle similarities in the scattered light, undo the effects of this broken mirror, and get an idea of what lies around the corner.""The fact that fundamental properties are not completely destroyed when light bounces off a diffuse medium like a wall can be used in so many different ways,"" says Dogariu. ""The question is, how much information you can still recover through this broken mirror-like surface.""When a digital picture is taken, the spatial distribution of light across an object is mapped point by point, pixel by pixel, onto the plane of the camera. However, in its propagation from object to the camera, properties of light can be affected by what the light reflects off of. When it reflects off of a mirror, a clear image can be produced. But when it is reflected off of a shattered mirror, for example, the direction of light is altered and only a distorted version of the image can be seen.Dogariu and his colleagues have found a way to describe how this measure of similarity between two points, called spatial coherence of light, transfers in a reflection from a diffuse wall. By learning how the light transforms, the researchers can determine where the light came from. Dogariu describes their findings as an aspect of light propagation that has simply been overlooked before. By undoing the effects of the diffusing wall, Dogariu and his colleagues have eliminated the need to control the light that illuminates the target object.This is the first time that there has been a practical demonstration of passively detecting an object around a corner in this way.The technique does not recover a complete image but collects more than enough information needed for task oriented surveillance.""The potential of this technique goes beyond sensing,"" says George Atia, a professor in EECS and member of a larger UCF team funded under Defense Advanced Research Projects Agency's Revolutionary Enhancement of Visibility by Exploiting Active Light-fields project that initiated this research. ""Based on results of our recent simulations, we envision that non-line-of-sight, passive imaging of complex scenes could be achieved by data fusing that combines spatial coherence with additional intensity information.""Until now, detection of objects around a corner has only been possible by emitting light toward the object and modifying some of its properties, say by sending a pulse of light, the light will bounce off the diffuse wall, onto the object, to the diffuse wall again, and back to the detector. The amount of time the light takes to return to the detector is then used to triangulate the position of the object. The problem with such methods is that the emission of light discloses the intent to see, which can be problematic in covert situations.The new sensing method is not specific to light. It could be applied for example to infrared or microwaves radiation.The other co-authors on the Nature Communications paper are Mahed Batarseh, Sergey Sukhov, Zhean Chen, Heath Gemar, Roxana Rezvani from UCF.Story Source:Materials provided by University of Central Florida. Original written by Rachel Wimmer. ",0.110914,0.109382,0.15249,0.205677,0.160992,0.150669,0.164589,0,0
653,Enhanced 3-D imaging poised to advance treatments for brain diseases Microscope add-on tackles trade-offs in high-resolution imaging of large tissue volumes,"Researchers have developed a combination of commercially available hardware and open-source software, named PySight, which improves rapid 2D and 3D imaging of the brain and other tissues. By seamlessly enabling integration of the fastest 3D imaging solution available today, the advance in microscopy could help scientists to better understand brain dynamics and discover new treatments for health problems such as stroke, epilepsy and dementia.In Optica, The Optical Society's journal for high impact research, the researchers describe PySight, which serves as an add-on for laser scanning microscopes. Geared with this novel combination of software and hardware, they improved the quality of 2D and 3D imaging of neuronal activity in the living brain.Because it can image deep into tissue, a laser-based imaging technique known as multiphoton microscopy is often used to study the rapid activity patterns of neurons, blood vessels and other cells at high resolution over time. This microscopy method uses laser pulses to excite fluorescent probes, eliciting the emission of photons, some of which are detected and used to form 2D and 3D images.Trying to capture the full breadth of neuronal activity with multiphoton microscopy forces scientists to image faster. As a result, fewer and fewer photons become available to form images, much like taking a photo with shorter and shorter exposure times. The challenge then becomes how to get meaningful images under these dim conditions.""To tackle this challenge, microscopists have used a detector-readout method called photon counting,"" said research team leader Pablo Blinder from Tel Aviv University in Israel. ""However, because its implementation required extensive electronics knowledge and custom components, photon counting has never been widely adopted. In addition, commercially available photon counting solutions were ill-suited to perform very fast imaging such as required for 3D imaging. PySight's easy installation procedure and its integration with state-of-the-art hardware eliminate such concerns.""In addition to advancing neural imaging research, PySight's improved sensitivity could facilitate rapid intraoperative identification of malignant cells in human patients using multiphoton microscopy. PySight's novel approach for reconstructing 3D scenes could also improve performance of light detection and ranging, or LIDAR. This could help lower the costs of self-driving cars that use LIDAR to map their surroundings.Detecting single photons in 3DPySight provides high spatiotemporal resolution while producing a data stream that scales with the number of detected photons, not the volume or area being imaged. ""Conventional data acquisition hardware stores the brightness of every pixel or 3D voxel even when it is zero as no photons were detected in that particular location,"" Blinder explained. ""PySight, however, only stores the precise detection time of each photon. If no photons were detected, nothing gets written to disk. This allows researchers to conduct rapid imaging of large volumes over long sessions, without compromising spatial or temporal resolution.""To reconstruct a multidimensional image, knowing when each photon hits the detector isn't enough. It's necessary to also know where it originated in the brain. ""This is even trickier if you want to simplify the system and avoid synchronizing the different scanning elements,"" said Blinder. ""To accomplish this, our software reads a list of photon arrival times along with timing signals from the scanning elements, determines the origin of each photon within the sample and generates the corresponding 3D movies.""The photon arrival times are generated by a device known as a multiple-event time digitizer, or multiscaler, which records the times with a precision of 100 picoseconds. Another key component was an off-the-shelf resonant axial scanning lens that changes the focal plane hundreds of thousands of times per second. This lens was used to rapidly scan the laser beam across different depths within the brain and allowed the team to reconstruct continuous 3D images.Easier, cost-effective, continuous 3D imaging""The multiscaler we used hasn't been applied to neuroimaging because the output isn't easy to interpret, and using a resonant axial scanning lens for bioimaging has required custom-made scanning synchronization hardware or proprietary code to obtain the 3D data,"" said Blinder. ""PySight turns the output from both components into a 3D movie effortlessly. As Pysight is free and open-source software, it should greatly aid labs previously deterred by the high technical barrier that accompanied continuous 3D imaging."" Further, having a generic application interface, PySight could also be used to interpret similar photon detection times from other suitable hardware devices.To test whether PySight was truly plug and play, the researchers walked with their multiscaler to another imaging lab on the Tel Aviv University campus. They were able to simply plug in the device into the existing multiphoton microscope, download the PySight software and start recording single-trail odor responses in fruit flies genetically modified to express voltage indicators. This fast probe for neuronal activity detects the finest aspects of neuronal activity yet it is considered too dim to be used without photon counting with this type of microscopy. PySight capabilities pave the road for and easy implementation of multiphoton voltage imaging in almost any laboratory.In addition to continuing to improve the PySight software, the researchers would like to add support for other microscopy imaging methods such as fluorescence lifetime imaging, which relies on the timing of each photon relative to its originating laser pulse. Because the software is open source and provides direct access to photon arrival times, it enables other scientists to add new features and meet their specific needs.Story Source:Materials provided by The Optical Society. ",0.0674305,0.0976742,0.110316,0.229484,0.135874,0.118008,0.14615,0,0
654,A trick of the light,"Particle physicists are on the hunt for light. Not just any light, but a characteristic signal produced by the interaction of certain particles -- like ghostly neutrinos, which are neutral fundamental particles with very low mass -- with a detector that contains an atomic sea of liquefied noble gases.Even if it were brighter, this light signal would be undetectable by our eyes because it falls in the ultraviolet (UV) range of the electromagnetic spectrum. And just as our eyes are not equipped to see UV light, most conventional photodetector systems for particle physics experiments work much better in the visible range than they do in UV.However, new work at the U.S. Department of Energy's (DOE) Argonne National Laboratory is bringing the power of nanotechnology to particle physics in an effort to make photosensors work better in experimental environments where UV light is produced, like massive liquid argon-filled detector modules.""You can go online and buy photosensors from companies, but most of them are in the visible range, and they sense photons that we can see, visible light,"" said Argonne high-energy physicist Stephen Magill.To make their photosensors more sensitive to UV radiation, Magill and his colleagues at Argonne and the University of Texas at Arlington applied coatings of different nanoparticles to conventional photodetectors. Across a wide range of varying compositions, the results were dramatic. The enhanced photosensors demonstrated significantly greater sensitivity to UV light than the coating-free photodetectors.The reason that the nanoparticles work, according to Magill, has to do with their size. Smaller nanoparticles can absorb photons of shorter wavelengths, which are later re-emitted as photons of longer wavelengths with lower energy, he said. This transition, known to scientists as the ""Stokes shift,"" converts UV photons to visible ones.""We're always looking to find better materials that will allow us to detect our particles,"" Magill said. ""We'd like to find a single material that will allow us to identify a specific particle and not see other particles. These nanoparticles help get us closer.""The types of experiments for which scientists use these enhanced photodetectors are considered part of the ""intensity frontier"" of high-energy physics. By being more sensitive to whatever small ultraviolet signal is produced, these nanoparticle coatings increase the chances of detecting rare events and may allow scientists a better view of phenomena like neutrino oscillations, in which a neutrino changes type.The advantages of this kind of new material could also reach beyond the purview of particle physics. Magill suggested that the particles could be incorporated into a transparent glass that could enhance the amount of visible light available in some dim environments.""There's a lot of light out there between 300 nanometers and 400 nanometers that we don't see and don't use,"" Magill said. ""By shifting the wavelength, we could create a way for that light to become more useful.""A paper based on the study, ""Wavelength-shifting properties of luminescence nanoparticles for high-energy particle detection and specific physics process observation,"" appeared in the July 12 edition of Scientific Reports. Argonne physicist Junqi Xie collaborated on the research, as well as University of Texas at Arlington scientists Wei Chen, Benjamin Jones, Lun Ma, David Nguyen and Sunil Sahi.The work was funded by DOE's Office of Science (Office of High Energy Physics) with additional support from the National Science Foundation and the U.S. Department of Homeland Security.Story Source:Materials provided by DOE/Argonne National Laboratory. Original written by Jared Sagoff. ",0.0913763,0.144783,0.12028,0.148841,0.149182,0.132379,0.140132,0,0
655,"Diamond dust enables low-cost, high-efficiency magnetic field detection ","UC Berkeley engineers have created a device that dramatically reduces the energy needed to power magnetic field detectors, which could revolutionize how we measure the magnetic fields that flow through our electronics, our planet, and even our bodies.""The best magnetic sensors out there today are bulky, only operate at extreme temperatures, and can cost tens of thousands of dollars,"" said Dominic Labanowski, who helped create the device, which is made from nitrogen-infused diamonds, as a postdoctoral researcher in the department of electrical engineering and computer science. ""Our sensors could replace those more difficult-to-use sensors in a lot of applications from navigation to medical imaging to natural resource exploration.""Each time a diamond-based sensor measures a magnetic field, it must first be blasted with 1 to 10 Watts of microwave radiation to prime them to be sensitive to magnetic fields, which is enough power to melt electronic components. The researchers found a new way to excite tiny diamonds with microwaves using 1000 times less power, making it feasible to create magnetic-sensing devices that can fit into electronics like cell phones.This work was led by Sayeef Salahuddin's lab at UC Berkeley in collaboration with researchers from the Ohio State University. The team reports their device online Sep. 7 in the journal Science Advances.Defective DiamondsBombarding a diamond with a jet of nitrogen gas can knock out some of its highly ordered carbon atoms, replacing them with nitrogen atoms. These nitrogen interlopers -- called nitrogen vacancy (NV) centers -- have unique properties that are well-understood by scientists.""You can use these NV centers as very powerful sensors, but traditionally their applications have been limited because it takes a lot of power to read them,"" said Labanowski.To detect magnetic fields, scientists first have to hit the NV centers with high-powered microwave radiation, equal to about one-hundredth the power of your standard microwave or ten times the power consumed by an average cell phone. They then illuminate the NV centers with a laser, which is absorbed and emitted by the nitrogen atoms.The strength of the magnetic field is related to the strength of the emitted laser light: the intensity of the emitted light can be used to measure the field strengthTo create the device, the researchers placed diamond nanocrystals -- containing thousands of NV centers apiece -- onto a film called a multiferroic. This new type of material is capable of transferring microwave energy to the crystals much more efficiently.""This technique dramatically lowers the power consumption of the sensors and makes them usable for realistic applications,"" Labanowski said.Imaging Inside the Body and Under the EarthMedical applications of magnetic sensors include magnetoencephalography, which uses magnetic fields to measure brain waves, or magnetocardiography, which uses magnetic fields to image heart function. Currently these machines are the size of a small room and can cost upwards of $3 million.""With low-power NV sensors, you could imagine taking a room-sized magnetoencephalography machine and turning it into something like a helmet, dramatically reducing the size and the costs,"" Labanowski said.The sensors could also be placed in planes or drones to aid in spotting rare earth metals underground, or used in cell phones to improve navigation.Magnetic field detection is just one application of NV centers, Salahuddin says. The team is planning to refine their technology to use NV centers and other types of quantum systems in a wide variety of applications.""While we emphasized magnetic field sensing, our work could lead to electrical manipulation of quantum systems in general with much broader areas of application including quantum computing,"" Salahuddin said.Story Source:Materials provided by University of California - Berkeley. Original written by Kara Manke. ",0.0968964,0.117409,0.123974,0.159208,0.156817,0.147249,0.138998,0,0
656,"Taking a closer look at unevenly charged biomolecules Using Debye lengths, researchers improve sensor response to better analyze biomolecules","In clinical diagnostics, it is critical to monitor biomolecules in a simple, rapid and sensitive way. Clinicians most often monitor antibodies because these small proteins attach to antigens, or foreign substances, we face every day. Most biomolecules, however, have complicated charge characteristics, and the sensor response from conventional carbon nanotube systems can be erratic. A team in Japan recently revealed how these systems work and proposed changes to dramatically improve biomolecule detection. They report their findings in the Journal of Applied Physics, from AIP Publishing.These researchers demonstrated a new technique to detect, measure and analyze biomolecules with inhomogeneous charge distributions by adjusting the solution in which they monitor the biomolecule. They used carbon nanotube thin film transistors (CNT-TFTs) to zero in on the precise amount of a specific biomolecule is in a specimen.CNT-TFT biosensors use immune antibody receptors called aptamers to detect the net electric charge of the part of the target molecule. After scientists identify a molecule, an antibody is made to attach to it in solution. That antibody then connects to an aptamer on a thin film of carbon nanotubes that converts the connection into an electrical signal for sensor detection. With this enhanced sensor response, researchers can determine the Debye length, or the distance between a point charge and the molecule, to map out a molecule's uneven charge distributions.The group discovered that they had to look at how the charges were distributed close to a molecule's surface to understand the complicated behavior in the sensor signal. ""Despite being the same target molecule, the polarities of the sensor response are completely different from positive or negative,"" said Ryota Negishi, an author on the paper.""We achieved the improvement of dynamic range by using low concentration of buffer solution,"" Negishi said. ""As a result, we clarified the mechanism of complicated sensor response which has not been clarified in previous reports.""Many different features of an experiment can affect a molecule's Debye length, so these results show promise for further controlling sensors and modifying their dynamic range.Next, Negishi and his colleagues hope to find a way to use their findings in more real-life scenarios. ""For practical application, it is essential to develop a sensing technology that can be detected under high concentration conditions close to blood.""Story Source:Materials provided by American Institute of Physics. ",0.117137,0.16003,0.146374,0.176924,0.16143,0.172415,0.165329,0,0
657,Can radar replace stethoscopes? Procedure for touch-free monitoring of heart sounds,"In conjunction with researchers at Brandenburg University of Technology (BTU) in Cottbus and the Department of Palliative Medicine at Universitetsklinikum Erlangen, electronic engineers at Friedrich-Alexander-Universitet Erlangen-Nernberg (FAU) have developed a procedure for reliably detecting and diagnosing heart sounds using radar. In future, mobile radar devices could replace conventional stethoscopes and permanent touch-free monitoring of patients' vital functions could be possible using stationary radar devices.Along with a white coat, a stethoscope is the hallmark of doctors everywhere. Stethoscopes are used to diagnose the noises produced by the heart and lungs. Used in the conventional way, vibrations from the surface of the body are transmitted to a membrane in the chest-piece and then to the user's eardrum where they are perceived as sounds. Acoustic stethoscopes are comparatively inexpensive and have been used reliably for several decades, but they have one drawback. The diagnosis of heart murmurs, such as the assessment of heart valve function, is carried out subjectively and is directly dependent on the experience of the doctor conducting the examination.Radar can measure heart soundsIn a joint project funded by the Federal Ministry of Education and Research, FAU researchers at the Institute of Electronics Engineering (LTE) have now developed a procedure that could eventually replace conventional phonocardiology. Using a six-port continuous wave radar system, they measured the vibrations on the skin caused by the heartbeat. 'In principle, we're using a similar method to detecting speed in road traffic,' explains Christoph Will, a doctoral candidate at LTE. 'During this process, a radar wave is aimed at the surface of an object and reflected. If the object moves, the phase of the reflecting wave changes. This is used to calculate the strength and frequency of the movement -- of the chest in our case.' In contrast to radar systems for traffic monitoring, the biomedical radar system can detect changes in movement that measure a few micrometres, which is an important prerequisite to diagnosing even the smallest anomalies such as insufficiency, stenoses or heart valves that do not close properly.As reliable as established measuring methodsInitial tests were very successful. The test patients were examined in various states of activity such as while resting and after sports and their heart sounds were detected. A direct comparison between the radar system and conventional standard instruments with a digital stethoscope and an electrocardiograph (ECG) showed a very high correlation. 'While diagnosing S1, which is the first heart sound, for example, we achieved a correlation of 92 percent with the ECG,' says Kilin Shi, who is also a doctoral candidate at LTE. 'The correlation was 83 percent in a direct comparison of the signal shapes with the digital stethoscope. That's absolutely reliable.' The researchers say that the slight deviations are caused by the fact that measurements using the radar system and the reference systems cannot be carried out simultaneously on exactly the same place on the body. In addition, the radar system measures a surface area and not a single spot like the stethoscope, which is also a reason for the varying measurement values.Touch-free and objectiveThe FAU researchers are optimistic that mobile radar systems could replace conventional stethoscopes in diagnosing heart function in the near future. A significant advantage offered by radar is the fact that the values are recorded digitally and are thus not subjective allowing human error to be increasingly ruled out during the diagnosis of anomalies or diseases. Using biomedical radar systems for automated prophylactic examinations for example in doctors' waiting rooms, at work, or at home, is also feasible.The researchers are already working on another project for monitoring the vital functions of patients who are seriously ill using stationary radar systems around the clock and without disruptive cables. 'Touch-free and therefore stress-free measurement of vital parameters such as heart sounds has the potential to revolutionise clinical care and research, for example, in palliative medicine,' explains Prof. Dr. Christoph Ostgathe, Head of Palliative Medicine at Universitetsklinikum Erlangen at FAU and co-author of the study. 'For example, we could inform relatives of terminally ill patients more quickly at the beginning of the dying phase, as the radar system immediately detects any changes in patients' health. It would also be possible to detect any painful symptoms in patients who cannot communicate'.Story Source:Materials provided by University of Erlangen-Nuremberg. ",0.123632,0.11435,0.134674,0.190055,0.138491,0.180855,0.163273,0,0
658,Terahertz technology creates new insight into how semiconductor lasers work,"Pioneering engineers working with terahertz frequency technology have been researching how individual frequencies are selected when a laser is turned on, and how quickly the selection is made.The development of specific terahertz equipment has allowed them to investigate this process for the first time. Their results, published in Nature Communications, will underpin the future development of semiconductor lasers, including those used in public and private sector-owned telecommunications systems.For many years, it has been predicted that operating frequencies within semiconductor lasers stabilise on a timescale of a few nanoseconds (ie a few billionths of a second) and can be changed within a few hundreds of picoseconds (ie thousandths of a nanosecond).Until now, though, no detector has been capable of measuring and proving this precisely, and the best results have only been achieved on nanosecond timescales, which are too slow to allow really efficient analysis or to be used to develop the most effective new systems.The University of Leeds researchers, working with international colleagues at ecole Normal Superieure in Paris, France and the University of Queensland in Brisbane, Australia have now used terahertz frequency quantum cascade lasers and a technique called terahertz time-domain spectroscopy to understand this laser stabilisation process.The terahertz-powered technology can measure the wavelength of light in periods of femtoseconds (ie millionths of a nanosecond) giving unprecedented levels of detail. By knowing the speed at which wavelengths change within lasers, and what happens during that process within miniscule time frames, more efficient devices and systems can be built.The Leeds elements of the study were carried out in the University's Terahertz Photonics Laboratory, part of the University's Bragg Centre for Materials Research.Dr Iman Kundu, principal author of the research paper explaining the group's findings, said: ""We've exploited the ultrafast detection capabilities of terahertz technology to watch laser emissions evolve from multiple colours to a single wavelength over less than a billionth of a second.""Now that we can see the detailed emission of the lasers over such incredibly small time frames, we can see how the wavelength of light changes as one moves from one steady state to a new steady state.""The benefits for commercial systems designers are potentially significant. Terahertz technology isn't available to many sectors, but we believe its value lies in being able to highlight trends and explain the detailed operation of integrated photonic devices, which are used in complex imaging systems which might be found in the pharmaceutical or electronics sectors.""Designers can then apply these findings to lasers operating at different parts of the electromagnetic spectrum, as the underlying physics will be very similar.""Professor Edmund Linfield, Chair of Terahertz Electronics at the University of Leeds, who was also involved in the study said: ""We're using the highly advanced capabilities of terahertz technology to shine a light on the operation of lasers.""Our research is aimed at showing engineers and developers where to look to drive increased performance in their own systems. By doing this, we will increase the global competitiveness of the UK's science and engineering base.""Story Source:Materials provided by University of Leeds. ",0.080402,0.1055,0.115319,0.165533,0.144194,0.114352,0.132765,0,0
659,Next-generation photodetector camera to deploy during demo mission,"Testing tools and technologies for refueling and repairing satellites in orbit won't be the only demonstration taking place aboard the International Space Station during NASA's next Robotic Refueling Mission 3, or RRM3.An advanced, highly compact thermal camera that traces its heritage to one now flying on NASA's Landsat 8 has been mounted in a corner of the RRM3 payload and from that position will image and videotape Earth's surface below once the SpaceX Dragon resupply vehicle delivers the payload to the orbiting outpost in November.While RRM3 demonstrates its specially developed satellite-servicing tools developed by NASA's Satellite Servicing Projects Division, its hitchhiker companion, the Compact Thermal Imager, or CTI, will image and measure fires, ice sheets, glaciers, and snow surface temperatures.CTI will also measure the transfer of water from soil and plants into the atmosphere -- important measurements for understanding plant growth. Many of the conditions that Earth scientists study, including these, are easily detected in the infrared or thermal wavelength bands.Strained-Layer Superlattice Technology Enables CTICTI's enabling technology is a relatively new photodetector technology known as Strained-Layer Superlattice, or SLS.In addition to being very small, measuring nearly 16 inches long and six inches tall, SLS consumes little power, operates at liquid-nitrogen temperatures, is easily fabricated in a high-technology environment, and is inexpensive ""almost to the point of being disposable,"" said Murzy Jhabvala, a detector engineer at NASA's Goddard Space Flight Center in Greenbelt, Maryland. Jhabvala collaborated with his industry partner, the New Hampshire-based QmagiQ, to develop the SLS detector assembly.The detector technology is also quickly and easily customized for different applications, he added. The Goddard Detector Development Laboratory, for example, recently fabricated a 1,024 x 1,024-pixel SLS array and plans to increase its size to 2,048 x 2,048 pixels in the very near future.Another enabling technology that CTI and its SLS detectors will employ is the Goddard-developed SpaceCube 2.0, a powerful hybrid computing system that will control the instrument and process the images and video that it takes while in orbit.The demonstration's goal, Jhabvala said, is to raise SLS's technology-readiness level to nine -- or TRL-9 -- meaning that it has flown in space and has demonstrated that it operates well under the extreme environmental conditions found in space. ""This is a very important technology milestone,"" Jhabvala said. ""We needed this mission. When we demonstrate our detector array, multiple copies can be made, assembled, and aligned into focal plane arrays that would allow us to image large swaths of Earth's surface from space in the future.""QWIP BasedSLS is based on the Quantum Well Infrared Photodetector, or QWIP, technology that Jhabvala and his government and industry collaborators spent more than two decades refining. The QWIP detectors are now operating on Landsat 8 and will be flying on the upcoming Landsat 9 Thermal Infrared Sensor Instrument, which Goddard scientists built to monitor the ebb and flow of land-surface levels and the health of vegetation -- data that western states use to monitor water consumption.Like its QWIP predecessor, SLS is a large-format detector. The arrays are fabricated on a semiconductor wafer. The wafer's surface consists of hundreds of alternating, very thin layers of differing materials that are epitaxially grown and tuned to absorb infrared photons and convert them into electrons -- the fundamental particles that carry an electric current. Only light with a specific energy, or wavelength, can release the electrons. A read-out chip directly mated to the array then converts the electrons to a voltage that a computer uses to recreate an image of the infrared source. The CTI can also capture video from its orbit nearly 249 miles above Earth's surface.Ten Times More SensitiveCompared with its QWIP predecessor, SLS detectors are 10 times more sensitive and operate over a broader infrared spectral range and at substantially warmer temperatures -- 70K (about -334 degrees Fahrenheit) for the SLS array compared with 42K (about -384 degrees Fahrenheit) for the QWIP array.The increase in operating temperature will have multiple positive effects on future missions, Jhabvala said.Infrared radiation is sensed as heat. Therefore, detectors designed to measure infrared wavelengths must be cooled to prevent heat generated inside an instrument or spacecraft from contaminating the measurements of the object being observed. That's why engineers use cryocoolers and other devices to keep the detector arrays and other critical instrument components as cold as necessary.Because Jhabvala and his team have created an array that can operate at warmer temperatures, its cooling system is smaller and consumes less power. In the future, these attributes will lead to smaller satellites, increased longevity, shorter build cycles, and a lower cost, Jhabvala said.Just a few months before the RRM3 launch, Jhabvala reflected on the evolution of his photodetector technology and collaboration with QmagiQ, which has received NASA Small Business Innovation Research grants to create the technology that the CTI team then ruggedized for use in space. ""Together, with this company, we have made some outstanding achievements over the years,"" Jhabvala said. ""Our on-going collaboration has yielded some truly extraordinary returns for NASA and the U.S. government. I give QmagiQ and NASA a lot of credit.""Story Source:Materials provided by NASA/Goddard Space Flight Center. ",0.099487,0.0732614,0.105464,0.119095,0.128793,0.110864,0.0975805,0,0
660,Study of high-energy neutrinos again proves Einstein right Most thorough test to date finds no Lorentz violation in high-energy neutrinos,"The universe should be a predictably symmetrical place, according to a cornerstone of Einstein's theory of special relativity, known as Lorentz symmetry. This principle states that any scientist should observe the same laws of physics, in any direction, and regardless of one's frame of reference, as long as that object is moving at a constant speed.For instance, as a consequence of Lorentz symmetry, you should observe the same speed of light -- 300 million meters per second -- whether you are an astronaut traveling through space or a molecule moving through the bloodstream.But for infinitesimally small objects that operate at incredibly high energies, and over vast, universe-spanning distances, the same rules of physics may not apply. At these extreme scales, there may exist a violation to Lorentz symmetry, or Lorentz violation, in which a mysterious, unknown field warps the behavior of these objects in a way that Einstein would not predict.The hunt has been on to find evidence of Lorentz violation in various phenomena, from photons to gravity, with no definitive results. Physicists believe that if Lorentz violation exists, it might also be seen in neutrinos, the lightest known particles in the universe, which can travel over vast distances and are produced by cataclysmic high-energy astrophysical phenomena. Any confirmation that Lorentz violation exists would point to completely new physics that cannot be explained by Einstein's theory.Now MIT scientists and their colleagues on the IceCube Experiment have led the most thorough search yet of Lorentz violation in neutrinos. They analyzed two years of data collected by the IceCube Neutrino Observatory, a massive neutrino detector buried in the Antarctice ice. The team searched for variations in the normal oscillation of neutrinos that could be caused by a Lorentz-violating field. According to their analysis, no such abnormalities were observed in the data, which comprises the highest-energy atmospheric neutrinos that any experiment has collected.The team's results, published today in Nature Physics, rule out the possibility of Lorentz violation in neutrinos within the high energy range that the researchers analyzed. The results establish the most stringent limits to date on the existence of Lorentz violation in neutrinos. They also provide evidence that neutrinos behave just as Einstein's theory predicts.""People love tests of Einstein's theory,"" says Janet Conrad, professor of physics at MIT and a lead author on the paper. ""I can't tell if people are cheering for him to be right or wrong, but he wins in this one, and that's kind of great. To be able to come up with as versatile a theory as he has done is an incredible thing.""Conrad's co-authors at MIT, who also led the search for Lorentz violation, are postdoc Carlos Argeelles and graduate student Gabriel Collin, who collaborated closely with Teppei Katori, a former postdoc in Conrad's group who is now a lecturer in particle physics at Queen Mary University of London. Their co-authors on the paper include the entire IceCube Collaboration, comprising more than 300 researchers from 49 institutions in 12 countries.Flavor changeNeutrinos exist in three main varieties, or as particle physicists like to call them, ""flavors"": electron, muon, and tau. As a neutrino travels through space, its flavor can oscillate, or morph into any other flavor. The way neutrinos oscillate typically depends on a neutrino's mass or the distance that it has traveled. But if a Lorentz-violating field exists somewhere in the universe, it could interact with neutrinos passing through that field, and affect their oscillations.To test whether Lorentz violation can be found in neutrinos, the researchers looked to data gathered by the IceCube Observatory. IceCube is a 1-gigaton particle-detector designed to observe high-energy neutrinos produced from the most violent astrophysical sources in the universe. The detector is composed of 5,160 digital optical modules, or light sensors, each of which are attached to vertical strings that are frozen into 86 boreholes arrayed over a cubic kilometer of Antarctic ice.Neutrinos streaming through space and the Earth can interact with the ice that comprises the detector or the bedrock below it. This interaction produces muons -- charged particles that are heavier than electrons. Muons emit light as they go through the ice, producing long tracks that can go through the entire detector. Based on the recorded light, scientists can track the trajectory and estimate the energy of a muon, which they can use to back-calculate the energy -- and expected oscillation -- of the original neutrino.The team, led by Argeelles and Katori, decided to look for Lorentz violation in the highest-energy neutrinos that are produced in the Earth's atmosphere.""Neutrino oscillations are a natural interferometer,"" explains Katori. ""Neutrino oscillations observed with IceCube act as the biggest interferometer in the world to look for the tiniest effects such as a space-time deficit.""The team looked through two years of data gathered by IceCube, which comprised more than 35,000 interactions between a muon neutrino and the detector. If a Lorentz-violating field exists, the researchers theorized that it should produce an abnormal pattern of oscillations from neutrinos arriving at the detector from a particular direction, which should become more relevant as the energy increases. Such an abnormal oscillation pattern should correspond to a similarly abnormal energy spectrum for the muons.The researchers calculated the deviation in the energy spectrum that they would expect to see if Lorentz violation existed, and compared this spectrum to the actual energy spectrum IceCube observed, for the highest-energy neutrinos from the atmosphere.""We are looking for a deficit of muon neutrinos along the direction that traverses large fractions of the Earth,"" Argeelles says. ""This Lorentz violation-induced disappearance should increase with increasing energy.""If Lorentz violation exists, physicists believe it should have a more obvious effect on objects at extremely high energies. The atmospheric neutrino dataset analyzed by the team is the highest-energy neutrino data collected by any experiment.""We were looking to see if a Lorentz violation caused a deviation, and we didn't see it,"" Conrad says. ""This closes the book on the possibility of Lorentz violation for a range of high-energy neutrinos, for a very long time.""A violating limitThe team's results set the most stringent limit yet on how strongly neutrinos may be affected by a Lorentz-violating field. The researchers calculated, based on IceCube data, that a violating field with an associated energy greater than 10-36 GeV-2 should not affect a neutrino's oscillations. That's .01 with 35 more zeros preceding the 1, of one-billionth an electronvolt squared -- an extremely small force that is far weaker than neutrinos' normally weak interactions with the rest of matter, which is at the level of 10-5 GeV-2.""We were able to set limits on this hypothetical field that are much, much better than any that have been produced before,"" Conrad says. ""This was an attempt to go out and look at new territory we hadn't looked at before and see if there are any problems in that space, and there aren't. But that doesn't stop us from looking further.""To that point, the group plans to look for Lorentz violation in even higher-energy neutrinos that are produced from astrophysical sources. IceCube does record astrophysical neutrinos, along with atmospheric ones, but scientists don't have a complete understanding of their behavior, such as their normal oscillations. Once they can better model these interactions, Conrad says the team will have a better chance of looking for patterns that deviate from the norm.""Every paper that comes out of particle physics assumes that Einstein is right, and all the rest of our work builds on that,"" Conrad says. ""And to a very good approximation, he's correct. It is a fundamental fabric of our theory. So trying to understand whether there are any deviations to it is a really important thing to do.""This research was supported, in part, by National Science Foundation.Story Source:Materials provided by Massachusetts Institute of Technology. Original written by Jennifer Chu. ",0.0947419,0.106473,0.12474,0.192842,0.131357,0.153068,0.15622,0,0
661,Realization of color filter-free image sensors,"A South Korean research team has developed a new-concept image sensor that captures vivid colors without color filters. The Korea Research Foundation announced that Professor Dae Sung Chung (Daegu Gyeongbuk Institute of Science and Technology, DGIST)'s research team has developed an organic image sensor with high color selection using a bonding technique between organic semiconductors and transparent electrodes.The image sensor is a key recording element in cameras, CCTV, and self-driving cars. Most image sensors that have been commercialized thus far are silicon based, and color filters are essential to accurately identify the color of light. However, color filters are expensive and have the fatal drawback if increasing image sensor thickness.The research team developed thin image sensors based on organic semiconductors that can compensate for the shortcomings of silicon image sensors. In particular, their research results have been garnering much attention because they enable images to be implemented clearly without using color filters by increasing the color selection of organic semiconductors.The research team developed a method to fill chemical defects in the transparent electrode surface made of zinc oxide with sulfur atoms. Through this, Schottky junction property between organic semiconductor and transparent electrode have been maximized, and thus increased each R/G/B color selection options.In addition, as the surface defects of transparent electrodes are dramatically reduced and the quality of thin films is excellent, it can greatly improve reproduction, which has been a chronic problem of organic semiconductors.Professor Chung explained the significance of the study by saying, ""We have developed high-performance color filter-free organic image sensors using an ideal Schottky junction between organic semiconductors and transparent electrodes. As well as color filter-free image sensors, it is expected to be applicable to many industrial applications that require various forms of bonding, such as solar cells, thin film transistors, and gas sensors.""This research outcome was published on May 30, 2018 in the online edition of Advanced Functional Materials, an international journal in the field of material engineering and will be published as a cover paper; the research was conducted with support from the Ministry of Science, Technology, and Information and Communication and the Basic Research Project (Mainstay Researcher) of the Korea Research Foundation.Story Source:Materials provided by DGIST (Daegu Gyeongbuk Institute of Science and Technology). ",0.112171,0.15463,0.197513,0.217939,0.202962,0.147067,0.188482,0,0
662,"Versatile ultrasound system could transform how doctors use medical imaging Optical ultrasound system compatible with MRI, would allow doctors to rapidly switch modes for best view","While ultrasound is one of the most common medical imaging tools, conventional electronic ultrasound devices tend to be bulky and cannot be used at the same time as some other imaging technologies. A new ultrasound system that uses optical, instead of electronic components, could improve performance while giving doctors significantly more flexibility in how they use ultrasound to diagnose and treat medical problems.In The Optical Society (OSA) journal Biomedical Optics Express, researchers demonstrate for the first time the use of an all-optical ultrasound imager for video-rate, real-time 2D imaging of biological tissue. The achievement is an important step toward making all-optical ultrasound practical for routine clinical use.Because they require no electronic components in the imaging probe, all-optical ultrasound systems could be safely used at the same time as magnetic resonance imaging (MRI) scanners. This would give doctors a more comprehensive picture of the tissues around an area of interest, such as a tumor or blood vessel.""All-optical ultrasound imaging probes have the potential to revolutionize image-guided interventions,"" said Erwin J. Alles, University College London, United Kingdom. ""A lack of electronics and the resulting MRI compatibility will allow for true multimodality image guidance, with probes that are potentially just a fraction of the cost of conventional electronic counterparts.""Lightbeam scanning mirrors built into the device increase image quality and make it possible to acquire images in different modes. In a clinical setting, this would allow doctors to rapidly toggle between modes on a single instrument to suit the task at hand. Acquiring different types of images using conventional ultrasound systems typically requires separate specialized probes.""The flexibility offered by the scanning mirrors will allow for seamless switching between 2D and 3D imaging, as well as a dynamically adjustable trade-off between image resolution and penetration depth, without the need to swap imaging probe,"" said Alles. ""Especially in a minimally invasive interventional setting, swapping imaging probes is highly disruptive, extends procedure times and introduces risks to the patient.""Eliminating electronicsConventional ultrasound imagers use arrays of electronic transducers to transmit high-frequency sound waves into tissue and receive the reflections. A computer then constructs images of the tissue.By contrast, all-optical ultrasound imagers use light to both transmit and receive ultrasound waves. Pulsed laser light is used to generate ultrasound waves, and scanning mirrors control where the waves are transmitted into the tissue. A fiber optic sensor receives the reflected waves.The electronic components of conventional ultrasound devices make them difficult to miniaturize for internal use, so most existing ultrasound devices are large, handheld probes that are placed against the skin. While some high-resolution minimally invasive ultrasound probes have been developed, they are too expensive for routine clinical use. Optical components are easily miniaturized and tiny all-optical ultrasound probes would likely be significantly less expensive to manufacture than compact electronic ultrasound systems, researchers say.Speeding up image processingTo generate images, an all-optical ultrasound system must acquire data from multiple optical source locations, combine them together and then create a visualization that reconstructs the area being imaged.Researchers have previously demonstrated using all-optical ultrasound to generate high-quality 2D and 3D images, but acquiring the images took hours, making these devices too slow to be used in a clinical setting. The new demonstration is the first to acquire and display images with all-optical ultrasound at video rates.""Through the combination of a new imaging paradigm, new optical ultrasound generating materials, optimized ultrasound source geometries and a highly sensitive fiber-optic ultrasound detector, we achieved image frame rates that were up to three orders of magnitude faster than the current state-of-the-art,"" said Alles.A medical multitoolOptical ultrasound systems are inherently more versatile than their electronic counterparts because they can produce sound at a much larger bandwidth. Alles and colleagues demonstrated how the light source can be manipulated to generate either low frequency ultrasound, which results in greater penetration into the tissue, or high frequency ultrasound, which offers higher resolution images at a shallower depth.The team tested their prototype system by imaging a deceased zebrafish, as well as a pig artery that they manipulated to emulate the dynamics of pulsing blood. The demonstration showed imaging capabilities comparable to an electronic high-frequency ultrasound system, with a sustained frame rate of 15 Hertz, a dynamic range of 30 decibels, a penetration depth of 6 millimeters and a resolution of 75 by 100 micrometers.To adapt the technology for clinical use, the researchers are working to develop a long, flexible imaging probe for free-hand operation, as well as miniaturized versions for endoscopic applications.Story Source:Materials provided by The Optical Society. ",0.158388,0.185395,0.178506,0.249254,0.233002,0.19609,0.208264,0,0
663,"Scientists fine-tune carbon nanotubes for flexible, fingertip-wearable terahertz imagers ","Researchers at Tokyo Institute of Technology have developed flexible terahertz imagers based on chemically ""tunable"" carbon nanotube materials. The findings expand the scope of terahertz applications to include wrap-around, wearable technologies as well as large-area photonic devices.Carbon nanotubes (CNTs) are beginning to take the electronics world by storm, and now their use in terahertz (THz) technologies has taken a big step forward.Due to their excellent conductivity and unique physical properties, CNTs are an attractive option for next-generation electronic devices. One of the most promising developments is their application in THz devices. Increasingly, THz imagers are emerging as a safe and viable alternative to conventional imaging systems across a wide range of applications, from airport security, food inspection and art authentication to medical and environmental sensing technologies.The demand for THz detectors that can deliver real-time imaging for a broad range of industrial applications has spurred research into low-cost, flexible THz imaging systems. Yukio Kawano is of the Laboratory for Future Interdisciplinary Research of Science and Technology, Tokyo Institute of Technology (Tokyo Tech). In 2016 he announced the development of wearable terahertz technologies based on multiarrayed carbon nanotubes.Kawano and his team have since been investigating THz detection performance for various types of CNT materials, in recognition of the fact that there is plenty of room for improvement to meet the needs of industrial-scale applications.Now, they report the development of flexible THz imagers for CNT films that can be fine-tuned to maximize THz detector performance.Publishing their findings in ACS Applied Nano Materials, the new THz imagers are based on chemically adjustable semiconducting CNT films.By making use of a technology known as ionic liquid gating[1], the researchers demonstrated that they could obtain a high degree of control over key factors related to THz detector performance for a CNT film with a thickness of 30 micrometers. This level of thickness was important to ensure that the imagers would maintain their free-standing shape and flexibility.""Additionally,"" the team says, ""we developed gate-free Fermi-level[2] tuning based on variable-concentration dopant solutions and fabricated a Fermi-level-tuned p?n junction[3] CNT THz imager."" In experiments using this new type of imager, the researchers achieved successful visualization of a metal paper clip inside a standard envelope.The bendability of the new THz imager and the possibility of even further fine-tuning will expand the range of CNT-based devices that could be developed in the near future.Moreover, low-cost fabrication methods such as inkjet coating could make large-area THz imaging devices more readily available.Technical terms[1] Ionic liquid gating: A technique used to modulate a material's charge carrier properties.[2] Fermi level: A measure of the electrochemical potential for electrons, which is important for determining the electrical and thermal properties of solids. The term is named after the Italian-American physicist Enrico Fermi.[3] p-n junction: Refers to the interface between positive (p-type) and negative (n-type) semiconducting materials. These junctions form the basis of semiconductor electronic devices.Story Source:Materials provided by Tokyo Institute of Technology. ",0.109738,0.159654,0.161562,0.173214,0.21118,0.138755,0.157818,0,0
664,Materials scientist creates fabric alternative to batteries for wearable devices,"A major factor holding back development of wearable biosensors for health monitoring is the lack of a lightweight, long-lasting power supply. Now scientists at the University of Massachusetts Amherst led by materials chemist Trisha L. Andrew report that they have developed a method for making a charge-storing system that is easily integrated into clothing for ""embroidering a charge-storing pattern onto any garment.""As Andrew explains, ""Batteries or other kinds of charge storage are still the limiting components for most portable, wearable, ingestible or flexible technologies. The devices tend to be some combination of too large, too heavy and not flexible.""Their new method uses a micro-supercapacitor and combines vapor-coated conductive threads with a polymer film, plus a special sewing technique to create a flexible mesh of aligned electrodes on a textile backing. The resulting solid-state device has a high ability to store charge for its size, and other characteristics that allow it to power wearable biosensors.Andrew adds that while researchers have remarkably miniaturized many different electronic circuit components, until now the same could not be said for charge-storing devices. ""With this paper, we show that we can literally embroider a charge-storing pattern onto any garment using the vapor-coated threads that our lab makes. This opens the door for simply sewing circuits on self-powered smart garments."" Details appear online in ACS Applied Materials & Interfaces.Andrew and postdoctoral researcher and first author Lushuai Zhang, plus chemical engineering graduate student Wesley Viola, point out that supercapacitors are ideal candidates for wearable charge storage circuits because they have inherently higher power densities compared to batteries.But ""incorporating electrochemically active materials with high electrical conductivities and rapid ion transport into textiles is challenging,"" they add. Andrew and colleagues show that their vapor coating process creates porous conducting polymer films on densely-twisted yarns, which can be easily swelled with electrolyte ions and maintain high charge storage capacity per unit length as compared to prior work with dyed or extruded fibers.Andrew, who directs the Wearable Electronics Lab at UMass Amherst, notes that textile scientists have tended not to use vapor deposition because of technical difficulties and high costs, but more recently, research has shown that the technology can be scaled up and remain cost-effective.She and her team are currently working with others at the UMass Amherst Institute for Applied Life Sciences' Personalized Health Monitoring Center on incorporating the new embroidered charge-storage arrays with e-textile sensors and low-power microprocessors to build smart garments that can monitor a person's gait and joint movements throughout a normal day.Story Source:Materials provided by University of Massachusetts at Amherst. ",0.0940626,0.135401,0.128408,0.172445,0.154259,0.156079,0.15311,0,0
665,Artificial magnetic field produces exotic behavior in graphene sheets,"A simple sheet of graphene has noteworthy properties due to a quantum phenomenon in its electron structure named Dirac cones in honor of British theoretical physicist Paul Dirac (1902-1984), who was awarded the Nobel Prize for Physics in 1933.The system becomes even more interesting if it comprises two superimposed graphene sheets, and one is very slightly turned in its own plane so that the holes in the two carbon lattices no longer completely coincide.For specific angles of twist, the bilayer graphene system displays exotic properties such as superconductivity (zero resistance to electrical current flow).A new study conducted by Brazilian physicist Aline Ramires with Jose Lado, a Spanish-born researcher at the Swiss Federal Institute of Technology (ETH Zurich), shows that the application of an electrical field to such a system produces an effect identical to that of an extremely intense magnetic field applied to two aligned graphene sheets.An article on the study has recently been published in Physical Review Letters and was selected to feature on the issue's cover. It can also be downloaded from the arXiv platform.Ramires is a researcher at Seo Paulo State University's Institute of Theoretical Physics (IFT-UNESP) and the South American Institute for Fundamental Research (ICTP-SAIFR). She is supported by Seo Paulo Research Foundation -- FAPESP through a Young Investigator grant.""I performed the analysis, and it was computationally verified by Lado,"" Ramires told. ""It enables graphene's electronic properties to be controlled by means of electrical fields, generating artificial but effective magnetic fields with far greater magnitudes than those of the real magnetic fields that can be applied.""The two graphene sheets must be close enough together for the electronic orbitals of one to interact with the electronic orbitals of the other, she explained.This means a separation as close as approximately one angstrom (10-10 meter or 0.1 nanometer), which is the distance between two carbon atoms in graphene.Another requirement is a small angle of twist for each sheet compared to the other -- less than one degree (?<1e).Although entirely theoretical (analytical and numerical), the study has clear technological potential, as it shows that a versatile material such as graphene can be manipulated in hitherto unexplored regimes.""The artificial magnetic fields proposed previously were based on the application of forces to deform the material. Our proposal enables the generation of these fields to be controlled with much greater precision. This could have practical applications,"" Ramires said.The exotic states of matter induced by artificial magnetic fields are associated with the appearance of ""pseudo-Landau levels"" in graphene sheets.Landau levels -- named after the Soviet physicist and mathematician Lev Landau (1908-1968), Nobel Laureate in Physics in 1962 -- are a quantum phenomenon whereby in the presence of a magnetic field, electrically charged particles can only occupy orbits with discrete energy values. The number of electrons in each Landau level is directly proportional to the magnitude of the applied magnetic field.""These states are well-located in space; when particles interact at these levels, the interactions are much more intense than usual. The formation of pseudo-Landau levels explains why artificial magnetic fields make exotic properties such as superconductivity or spin liquids appear in the material,"" Ramires said.Story Source:Materials provided by Fundaeeo de Amparo e Pesquisa do Estado de Seo Paulo. ",0.122373,0.134545,0.140561,0.159901,0.140823,0.193555,0.148997,0,0
666,Detecting light in a 'different dimension',"Scientists from the Center for Functional Nanomaterials (CFN) -- a U.S. Department of Energy (DOE) Office of Science User Facility at Brookhaven National Laboratory -- have dramatically improved the response of graphene to light through self-assembling wire-like nanostructures that conduct electricity. The improvement could pave the way for the development of graphene-based detectors that can quickly sense light at very low levels, such as those found in medical imaging, radiation detection, and surveillance applications.Graphene is a two-dimensional (2-D) nanomaterial with unusual and useful mechanical, optical, and electronic properties. It is both extremely thin and incredibly strong, detects light of almost any color, and conducts heat and electricity well. However, because graphene is made of sheets of carbon only one atom thick, it can only absorb a very small amount of incoming light (about two percent).One approach to overcoming this problem is to combine graphene with strong light-absorbing materials, such as organic compounds that conduct electricity. Scientists recently demonstrated an improved photoresponse by placing thin films (a few tens of nanometers) of one such conductive polymer, poly(3-hexylthiophene), or P3HT, on top of a single layer of graphene.Now, the CFN scientists have improved the photoresponse by an additional 600 percent by changing the morphology (structure) of the polymer. Instead of thin films, they used a mesh of nanowires -- nanostructures that are many times longer than they are wide -- made of the same polymer and similar thickness. The research is described in an article published online on Oct. 12 in ACS Photonics, a journal of the American Chemical Society (ACS).""We used self-assembly, a very simple and reproducible method, to create the nanowire mesh,"" said first author Mingxing Li, a research associate in the CFN Soft and Bio Nanomaterials Group. ""Placed in an appropriate solution and stirred overnight, the polymer will form into wire-like nanostructures on its own. We then spin-casted the resulting nanowires onto electrical devices called graphene field-effect transistors (FETs).""The scientists fabricated FETs made of graphene only, graphene and P3HT thin films, and graphene and P3HT nanowires. After checking the thickness and crystal structure of the FET devices through atomic force microscopy, Raman spectroscopy, and x-ray scattering techniques, they measured their light-induced electrical properties (photoresponsivity). Their measurements of the electric current flowing through the FETs under various light illumination powers revealed that the nanowire FETs improve photoresponse by 600 percent compared to the thin film FETs and 3000 percent compared to graphene-only FETs.""We did not expect to see such a dramatic improvement just by changing the morphology of the polymer,"" said co-corresponding author Mircea Cotlet, a materials scientist in the CFN Soft and Bio Nanomaterials Group.The scientists believe that there are two explanations behind their observations.""At a certain polymer concentration, the nanowires have dimensions comparable to the wavelength of light,"" said Li. ""This size similarity has the effect of increasing light scattering and absorption. In addition, crystallization of P3HT molecules within the nanowires provides more charge carriers to transfer electricity to the graphene layer.""""In contrast to conventional thin films where polymer chains and crystals are mostly randomly oriented, the nanoscale dimension of the wires forces the polymer chains and crystals into a specific orientation, enhancing both light absorption and charge transfer,"" said co-author Dmytro Nykyphanchuck, a materials scientist in the CFN Soft and Bio Nanomaterials Group.The scientists have filed a U.S. patent for their fabrication process, and they are excited to explore light-matter interactions in other 2-D -- as well as 0-D and 1-D -- materials.""Plasmonics and nanophotonics -- the study of light at the nanometer scale -- are emerging research areas,"" said Cotlet, who earlier this year co-organized a workshop for user communities of the CFN and the National Synchrotron Light Source II (NSLS-II) -- another DOE Office of Science User Facility at Brookhaven -- to explore frontiers in these areas. ""Nanostructures can manipulate and control light at the nanoscale in very interesting ways. The advanced nanofabrication and nanocharacterization tools at the CFN and NSLS-II are perfectly suited for creating and studying materials with enhanced optoeletronic properties.""Story Source:Materials provided by DOE/Brookhaven National Laboratory. ",0.0878853,0.188182,0.146377,0.165195,0.160711,0.136669,0.167554,0,0
667,Discovery of new superconducting materials using materials informatics,"A NIMS-Ehime University joint research team succeeded in discovering new materials that exhibit superconductivity under high pressures using materials informatics (MI) approaches (data science-based material search techniques). This study experimentally demonstrated that MI enables efficient exploration of new superconducting materials. MI approaches may be applicable to the development of various functional materials, including superconductors.Superconducting materials which enable long-distance electricity transmission without energy loss in the absence of electrical resistance?are considered to be a key technology in solving environmental and energy issues. The conventional approach by researchers searching for new superconducting materials or other materials has been to rely on published information on material properties, such as crystalline structures and valence numbers, and their own experience and intuition. However, this approach is time-consuming, costly and very difficult because it requires extensive and exhaustive synthesis of related materials. As such, demand has been high for the development of new methods enabling more efficient exploration of new materials with desirable properties.This joint research team took advantage of the AtomWork database, which contains more than 100,000 pieces of data on inorganic crystal structures. The team first selected approximately 1,500 candidate material groups whose electronic states could be determined through calculation. The team then narrowed this list to 27 materials with desirable superconducting properties by actually performing electronic state calculations. From these 27, two materials?SnBi2Se4 and PbBi2Te4?were ultimately chosen because they were relatively easy to synthesize.The team synthesized these two materials and confirmed that they exhibit superconductivity under high pressures using an electrical resistivity measuring device. The team also found that the superconducting transition temperatures of these materials increase with increasing pressure. This data science-based approach, which is completely different from the conventional approaches, enabled identification and efficient and precise development of superconducting materials.Experiments revealed that these newly discovered materials may have superb thermoelectric properties in addition to superconductivity. The method we developed may be applicable to the development of various functional materials, including superconductors. In future studies, we hope to discover innovative functional materials, such as room-temperature superconducting materials, by including a wider range of materials in our studies and increasing the accuracy of the parameters relevant to desirable properties.Story Source:Materials provided by National Institute for Materials Science, Japan. ",0.100451,0.157723,0.154356,0.214902,0.142045,0.158748,0.190687,0,0
668,Scientists make new 'green' electronic polymer-based films with protein nanowires New class of electronic materials,"An interdisciplinary team of scientists at the University of Massachusetts Amherst has produced a new class of electronic materials that may lead to a ""green,"" more sustainable future in biomedical and environmental sensing, say research leaders microbiologist Derek Lovley and polymer scientist Todd Emrick.They say their new work shows it is possible to combine protein nanowires with a polymer to produce a flexible electronic composite material that retains the electrical conductivity and unique sensing capabilities of protein nanowires. Results appear in the journal Small.Protein nanowires have many advantages over the silicon nanowires and carbon nanotubes in terms of their biocompatibility, stability, and potential to be modified to sense a wide range of biomolecules and chemicals of medical or environmental interest, says Lovley. However, these sensor applications require that the protein nanowires be incorporated into a flexible matrix suitable for manufacturing wearable sensing devices or other types of electronic devices.As Lovley explains, ""We have been studying the biological function of protein nanowires for over a decade, but it is only now that we can see a path forward for their use in practical fabrication of electronic devices."" Postdoctoral research Yun-Lu Sun, now at the University of Texas at Austin, discovered the proper conditions for mixing protein nanowires with a non-conductive polymer to yield the electrically conductive composite material. He demonstrated that although the wires are made of protein, they are very durable and easy to process into new materials.""An additional advantage is that protein nanowires are a truly 'green,' sustainable material,"" Lovley adds. ""We can mass-produce protein nanowires with microbes grown with renewable feedstocks. The manufacture of more traditional nanowire materials requires high energy inputs and some really nasty chemicals."" By contrast, he says, ""Protein nanowires are thinner than silicon wires, and unlike silicon are stable in water, which is very important for biomedical applications, such as detecting metabolites in sweat.""Emrick adds, ""These electronic protein nanowires bear surprising resemblance to polymer fibers and we're trying to figure out how to combine the two most effectively.""In their proof-of-concept study, the protein nanowires formed an electrically conductive network when introduced into the polymer polyvinyl alcohol. The material can be treated with harsh conditions, such as heat, or extreme pH such as high acidity, that might be expected to ruin a protein-based composite, but it continued to work well.The conductivity of the protein nanowires embedded in the polymer changed dramatically in response to pH. ""This is an important biomedical parameter diagnostic of some serious medical conditions,"" Lovley explains. ""We can also genetically modify the structure of the protein nanowires in ways that we expect will enable detection of a wide range of other molecules of biomedical significance.""The electrically conductive protein nanowires are a natural product of the microorganism Geobacter discovered in Potomac River mud by Lovley more than 30 years ago. Geobacter uses the protein nanowires to make electrical connections with other microbes or minerals. He notes, ""Material science experts like Todd Emrick and Thomas Russell on our team deserve the credit for bringing protein nanowires into the materials field. It's not just about mud anymore.""In this work supported by UMass Amherst campus funds for exploratory research, next steps for the collaborative materials-microbiology team include scaling up production of nanowire-polymer matrices, Lovley says.He points out, ""Materials scientists need a lot more nanowires than we're used to making. We're were making thimblefuls for our biological studies. They need buckets full, so we are now concentrating on producing larger amounts and on tailoring the nanowires so they'll respond to other molecules."" The researchers have also applied for a patent on the idea of a conductive polymer made with protein nanowires.Story Source:Materials provided by University of Massachusetts at Amherst. ",0.142653,0.238976,0.227072,0.235684,0.213116,0.199533,0.227915,0,0
669,Understanding the building blocks for an electronic brain,"Computer bits are binary, with a value of 0 or 1. By contrast, neurons in the brain can have all kinds of different internal states, depending on the input that they received. This allows the brain to process information in a more energy-efficient manner than a computer. University of Groningen (UG) physicists are working on memristors, resistors with a memory, made from niobium-doped strontium titanate, which mimic how neurons work. Their results were published in the Journal of Applied Physics on 21 October.The brain is superior to traditional computers in many ways. Brain cells use less energy, process information faster and are more adaptable. The way that brain cells respond to a stimulus depends on the information that they have received, which potentiates or inhibits the neurons. Scientists are working on new types of devices which can mimic this behavior, called memristors.MemoryUG researcher Anouk Goossens, the first author of the paper, tested memristors made from niobium-doped strontium titanate. The conductivity of the memristors is controlled by an electric field in an analog fashion: 'We use the system's ability to switch resistance: by applying voltage pulses, we can control the resistance, and using a low voltage we read out the current in different states. The strength of the pulse determines the resistance in the device. We have shown a resistance ratio of at least 1000 to be realizable. We then measured what happened over time.' Goossens was especially interested in the time dynamics of the resistance states.She observed that the duration of the pulse with which the resistance was set determined how long the 'memory' lasted. This could be between one to four hours for pulses lasting between a second and two minutes. Furthermore, she found that after 100 switching cycles, the material showed no signs of fatigue.Forgetting'There are different things you could do with this', says Goossens. 'By ""teaching"" the device in different ways, using different pulses, we can change its behavior.' The fact that the resistance changes over time can also be useful: 'These systems can forget, just like the brain. It allows me to use time as a variable parameter.' In addition, the devices that Goossens made combine both memory and processing in one device, which is more efficient than traditional computer architecture in which storage (on magnetic hard discs) and processing (in the CPU) are separated.Goossens conducted the experiments described in the paper during a research project as part of the Master in Nanoscience degree programme at the University of Groningen. Goossens' research project took place within the group of students supervised by Dr. Tamalika Banerjee of Spintronics of Functional Materials. She is now a Ph.D. student in the same group.QuestionsBefore building brain-like circuits with her device, Goossens plans to conduct experiments to really understand what happens within the material. 'If we don't know exactly how it works, we can't solve any problems that might occur in these circuits. So, we have to understand the physical properties of the material: what does it do, and why?'Questions that Goossens want to answer include what parameters influence the states that are achieved. 'And if we manufacture 100 of these devices, do they all work the same? If they don't, and there is device-to-device variation, that doesn't have to be a problem. After all, not all elements in the brain are the same.'Story Source:Materials provided by University of Groningen. ",0.0563383,0.085609,0.10591,0.210983,0.127035,0.10817,0.133146,0,0
670,Molecular memory can be used to increase the memory capacity of hard disks,"Single-molecule magnets are molecules capable of remembering the direction of a magnetic field that has been applied to them over relatively long periods of time once the magnetic field is switched off. Thus, one can ""write"" information into molecules. Single-molecule magnets have potential applications, for example, as high-density digital storage media and as parts of microprocessors in quantum computers. Practical applications have, however, been greatly hindered by the fact that single-molecule magnets are operational only at extremely low temperatures. Their intrinsic memory properties often vanish if they are heated more than a few degrees above absolute zero (-273eC); therefore, single-molecule magnets can be only studied under laboratory conditions by cooling them with liquid helium.More favorable conditions for technological applicationsResearchers have now, for the first time, managed to synthesize and characterize a single-molecule magnet which retains its memory properties above the temperature of liquid helium (-196eC). The magnet can be called the first high-temperature single-molecule magnet.- When considering our everyday life, liquid nitrogen is extremely cold. However, compared to liquid helium, which has so far been required to study single-molecule magnets, the liquid nitrogen temperature is a huge leap upwards. Liquid nitrogen is more than 300 times cheaper than liquid helium and much more readily available, enabling technological applications. Therefore, the research constitutes an important scientific milestone, describes postdoctoral researcher Akseli Mansikkameki from the Department of Chemistry of the University of Jyveskyle.New insights from computationsThe new dysprosium metallocene compound is the culmination of several years of research. The project has required the development of new approaches in organometallic lanthanide chemistry and deep insights of the relationship between the microscopic electronic structure and magnetic properties of the studied systems.The research also provides new insights and guidelines how to further improve the magnetic properties of single-molecule magnets and how to bring technological applications closer to reality.Story Source:Materials provided by University of Jyveskyle - Jyveskylen yliopisto. ",0.126921,0.211424,0.153696,0.211097,0.171811,0.200802,0.195314,0,0
671,Scientists find unusual behavior in topological material,"Argonne scientists have identified a new class of topological materials made by inserting transition metal atoms into the atomic lattice of a well-known two-dimensional material.In recent years, scientists have become intrigued by a new type of material that shows a kind of unusual and split behavior. These structures, called topological materials, can demonstrate different properties at their surface than in their bulk. This behavior has attracted the attention of scientists interested in new states of matter and technologists interested in potential electronic and spintronic applications.In a new study from the U.S. Department of Energy's (DOE) Argonne National Laboratory, scientists have identified a new class of topological materials made by inserting transition metal atoms into the atomic lattice of niobium disulfide (NbS2), a well-known two-dimensional material. They found that CoNb3S6, an antiferromagnetic material, exhibits an extremely large anomalous Hall effect, a sign of the topological character of materials.The ordinary Hall effect occurs in all electrical conductors. The effect is essentially a force that an electron experiences as it moves through a magnetic field. ""In every metal, electrons will get pushed perpendicular to their direction of travel and perpendicular to an applied external magnetic field, creating a voltage,"" said Nirmal Ghimire, an assistant professor at George Mason University and a recent Argonne director's postdoctoral fellow who was the first author of the study. ""If the material itself is a ferromagnet, an additional contribution superimposes on the ordinary Hall voltage; this is known as the anomalous Hall effect (AHE).""In the study, Ghimire and his colleagues looked at CoNb3S6 and found something unexpected: a large AHE in modest magnetic fields. ""An AHE can also be found in materials where the electronic structure has special characteristics known as topological features,"" said Ghimire. ""The configuration of atoms in the lattice creates symmetries in the material that lead to the creation of topological bands -- energy regions that electrons inhabit. It is these bands, in certain configurations, that can lead to an exceptionally large AHE.""Based on calculations and measurements, Ghimire and his colleagues suggest that CoNb3S6 contains these topological bands.""The topological features arise from a combination of the symmetry of the material, as well as the right electron concentration to put these topological features at the Fermi level, which is the highest available electronic energy state at zero temperature,"" noted John Mitchell, interim director of Argonne's Materials Science division and a co-author of the study.""Only a handful of materials so far have been shown to have the necessary characteristic topological points near the Fermi level,"" Mitchell said. ""To find more requires an understanding both of the materials physics and chemistry at play.""The discovery could pave the way for future advances in a broad class of materials, according to Mitchell. ""We now have a design rule for making materials that demonstrate these properties,"" he said. ""CoNb3S6 is a member of a big class of layered two-dimensional materials and so this might open the door to a big space of new topological matter.""Story Source:Materials provided by DOE/Argonne National Laboratory. ",0.102917,0.166688,0.153653,0.213109,0.155196,0.168877,0.182348,0,0
672,"Flexy, flat and functional magnets An overview on the surprising features and enormous potentials of 2D magnetic van der Waals materials","In the nanoworld, magnetism has proven to be truly surprising. Just a few atoms thick, magnetic 2D materials could help to satisfy scientists' curiosities and fulfil dreams for ever-smaller post-silicon electronics. An international research team led by PARK Je-Geun at the Center for Correlated Electron Systems, within the Institute for Basic Science (IBS), has just published a Perspective Review paper in Nature. It presents the latest achievements and future potentials of 2D magnetic van der Waals (vdW) materials, which were unknown until 6 years ago and have recently attracted worldwide attention.VdW materials are made of piles of ultra-thin layers held together by weak van der Waals bonds. The success of graphene -- vdW's star material -- stimulated scientists to look for other 2D crystals, where layers can be changed, added or removed in order to introduce new physical properties, like magnetism.How do materials become magnetic?You can imagine that each electron in a material acts like a tiny compass with its own north and south poles. The orientation of these ""compass needles"" determines the magnetization. More specifically, magnetization arises from electrons' spin (magnetic moment) and depends on temperature. A ferromagnet, like a standard fridge magnet, acquires its magnetic properties below the magnetic transition temperature (Tc, Curie temperature), when all the magnetic moments are aligned, all ""compass needles"" point in the same direction. Other materials, instead, are antiferromagnetic, meaning that below the transition temperature (in this case called Neel temperature, TN), the ""compass needles"" point in the opposite direction. For temperatures aboveTc or TN,the individual atomic moments are not aligned, and the materials lose their magnetic properties.However, the situation can dramatically change upon reducing materials to the 2D nanometer scale. An ultra-thin slice of a fridge magnet will probably show different features from the whole object. This is because 2D materials are more sensitive to temperature fluctuations, which can destroy the pattern of well-aligned ""compass needles."" For example, conventional bulk magnets, such as iron and nickel, have a much lower Tc in 2D than in 3D. In other cases, the magnetism in 2D really depends on the thickness: chromium triiodide (CrI3) is ferromagnetic as monolayer, anti-ferromagnetic as bilayer, and back to ferromagnetic as trilayer. However, there are other examples, like iron trithiohypophosphate (FePS3), which remarkably keeps its antiferromagnetic ordering intact all the way down to monolayer.The key for producing 2D magnetic materials is to tame their spin fluctuations. 2D materials with a preferred spin direction (magnetic anisotropy) are more likely to be magnetic. Anisotropy can also be introduced artificially by adding defects, magnetic dopants or by playing with the interaction between the electron's spin and the magnetic field generated by the electron's movement around the nucleus.However, these are all technically challenging methods.Park explains it with an analogy: ""It is like supervising a group of restless and misbehaving kids, where each kid represents an atomic compass. You want to line them up, but they would rather play. It is a hard task, as any kindergarten teacher would tell you. You would need to precisely know the movements of each of them in time and space. And to control them, you need to respond right there and then, which is technically very difficult.""Why are physicists so interested in 2D magnetic vdW materials?Several fundamental questions can be answered thanks to 2D magnetic vdW materials. In particular, vdW materials are the testbed to find experimental evidence for some mathematical-physical models that still remains unsolved. These models explain the magnetic transition behaviour in relation to the spin. In particular, the Ising model describes spins (""compass needles"") constrained to point either up or down, perpendicular to the plane. The XY model allows spins to point at any direction on the plane, and finally, in the Heisenberg model, spins are free to point in any x, y, z direction.In 2016, IBS scientists of Prof. Park's group found the first experimental proof of the Onsager solution for the Ising model. They found that FePS3's Tc is 118 Kelvin, or minus 155 degrees Celsius, in both 3D and 2D. However, the XY and Heisenberg models in 2D have encountered more experimental barriers, and are still lacking a proof after 50 years.""My interest in 2D magnetic materials began with the simple idea of: What if...? The discovery of graphene led me to wonder if I could introduce magnetism to 2D materials similar to graphene,"" explains Park. ""Physicists have inherited the challenge of studying and explaining the physical properties of the two-dimensional world. In spite of its academic importance and applicability, this field is very much underexplored,"" he adds.Scientists are also keen on exploring ways to control and manipulate the magnetic properties of these materials electrically, optically, and mechanically. Their thinness makes them more susceptible to external stimuli. It is a limitation, but can also be a potential. For example, magnetism can also be induced or tuned by strain, or by arranging the overlapping layers in a specific pattern, known as the moire pattern.Which are the expected applications of magnetic vdW materials?Although several fundamental questions are still waiting for an answer. Controlling and modifying electrons' spins and magnetic structures is expected to lead to several desirable outputs. This Nature Perspective Review lists possible hot research directions for the future.One of the most sought-after applications is the use of spins to store and encode information. Controlled spins could replace the current hard drive platters, and even become the key to quantum computing. In particular, spintronics is the subject that aims to control electrons' spins. 2D materials are good candidates as they would require less power consumption in comparison with their 3D counterparts. One interesting hypothesis is to store long-term memory in stable whorls-oriented magnetic poles patterns, called skyrmions, in magnetic materials.Potentially, vdW materials could unveil some exotic state of matter, like quantum spin liquids: a hypothetical state of matter characterized by disordered ""compass needles"" even at extremely low temperatures, and expected to harbor the elusive Majorana fermions, particles that have been theorized, but have never been seen before.In addition, although superconductivity and magnetism cannot be easily accommodated in the same material, tinkering with spins' orders could produce new, unconventional superconductors.Lastly, although the list of vdW materials has grown very quickly over the last few years, less than ten magnetic vdW materials have been discovered so far, so engineering more materials, especially materials that can be used at room temperature, is also an important goal of condensed matter physicists.Story Source:Materials provided by Institute for Basic Science. ",0.0697019,0.0904772,0.0992781,0.131163,0.0916757,0.169164,0.117242,0,0
673,Solution in fight against fake graphene,"Ever since the isolation of graphene was first achieved in 2004, there has been an explosion in graphene-related research and development, with hundreds of business opportunists producing graphene to capitalise on this rapidly expanding industry. However, a new study by researchers from the National University of Singapore (NUS) has uncovered a major problem -- a lack of production standards has led to many cases of poor quality graphene from suppliers. Such practices can impede the progress of research that depend fundamentally on the use of high-quality graphene.""It is alarming to uncover that producers are labelling black powders as graphene and selling them for top dollar, while in reality, they contain mostly cheap graphite. There is a strong need to set up stringent standards for graphene characterisation and production to create a healthy and reliable graphene market worldwide,"" said Professor Antonio Castro Neto, Director of the NUS Centre for Advanced 2D Materials, who led the study.The results of the study were published in the journal Advanced Materials on 13 September 2018.How to tell a graphene flake from a graphene fakeGraphene has been tipped as the miracle material of the future due to its remarkable properties. Despite being the thinnest material on Earth, it is 200 times stronger than steel. At just one atom thick, it is also an incredible electrical conductor, but remains light, flexible, and is transparent. Therefore, graphene is finding potential applications in everything from transistors to biomedical devices, and has even been proposed as a material for building an elevator to space.Graphene is typically produced by exfoliating graphite, which can be found in common pencil leads, into a powder, submerging this powder into a liquid, and then separating the tiniest graphene flakes by using sound energy to vibrate the mixture. The aim of this synthesis is to produce the thinnest graphene possible. Pure graphene would be just one atomic layer thick, however the International Organization for Standardisation (ISO) states that stacks of graphene flakes up to ten layers thick can still behave like graphene.With this in mind, Prof Castro Neto and his team set out to develop a systematic and reliable method for establishing the quality of graphene samples from around the world. They were able to achieve this by using a wide range of analytical techniques and tested samples from many suppliers.Upon analysing samples from over 60 different providers from the Americas, Asia and Europe, the NUS team discovered that the majority contained less than 10 per cent of what can be considered graphene flakes. The bulk of the samples was graphite powder that was not exfoliated properly.""Whether producers of the counterfeit graphene are aware of the poor quality is unclear. Regardless, the lack of standards for graphene production gives rise to bad quality of the material sold in the open market. This has been stalling the development of the future applications,"" elaborated Prof Castro Neto.Graphite powder and graphene have wildly different properties, so any research conducted under the pretext that the sample was pure graphene would give inaccurate results. In addition, just one of the samples tested in the study contained more than 40 per cent of high-quality graphene. Moreover, some samples were even contaminated with other chemicals used in the production process. These findings mean that researchers could be wasting valuable time and money performing experiments on a product that is falsely advertised.""This is the first ever study to analyse statistically the world production of graphene flakes. Considering the important challenges related to health, climate, and sustainability that graphene may be able to solve, it is crucial that research is not hindered in this way,"" explained Prof Castro Neto.Overcoming the barrier to graphene innovationWith this discovery, and the development of a reliable testing procedure, graphene samples may now be held to a higher standard.""We hope that our results will speed up the process of standardisation of graphene within ISO as there is a huge market need for that. This will urge graphene producers worldwide to improve their methods to produce a better, properly characterised product that could help to develop real-world applications,"" said Prof Castro Neto.In addition, testing graphene using a universal and standardised way could ensure easy quantitative comparisons between data produced from different laboratories and users around the world.Story Source:Materials provided by National University of Singapore. ",0.159202,0.230241,0.212003,0.258173,0.20404,0.219575,0.233784,0,0
674,Graphene on the way to superconductivity,"Carbon atoms have diverse possibilities to form bonds. Pure carbon can therefore occur in many forms, as diamond, graphite, as nanotubes, football molecules or as a honeycomb-net with hexagonal meshes, graphene. This exotic, strictly two-dimensional material conducts electricity excellently, but is not a superconductor. But perhaps this can be changed.A complicated option for superconductivityIn April 2018, a group at MIT, USA, showed that it is possible to generate a form of superconductivity in a system of two layers of graphene under very specific conditions: To do this, the two hexagonal nets must be twisted against each other by exactly the magic angle of 1.1e. Under this condition a flat band forms in the electronic structure. The preparation of samples from two layers of graphene with such an exactly adjusted twist is complex, and not suitable for mass production. Nevertheless, the study has attracted a lot of attention among experts.The simple way to flat bandsBut there is one more, much simpler way of flat band formation. This was shown by a group at the HZB around Prof. Oliver Rader and Dr. Andrei Varykhalov with investigations at BESSY II.The samples were provided by Prof. Thomas Seyller, TU Chemnitz. There they are produced using a process that is also suitable for the production of larger areas and in large quantities: A silicon carbide crystal is heated until silicon atoms evaporate from the surface, leaving first a single-layer of graphene on the surface, and then a second layer of graphene. The two graphene layers are not twisted against each other, but lie exactly on top of each other.Scanning the band structure with ARPESAt BESSY II, the physicists are able to scan the so-called band structure of the sample. This band structure provides information on how the charge carriers are distributed among the quantum-mechanically permitted states and which charge carriers are available for transport at all. The angle-resolved photoemission spectroscopy (ARPES) at BESSY II enables such measurements with extremely high resolution.An interesting region under scrutinyVia an exact analysis of the band structure, they identified an area that had previously been overlooked. ""The double layer of graphene has been studied before because it is a semiconductor with a band gap,"" explains Varykhalov. ""But on the ARPES instrument at BESSY II, the resolution is high enough to recognize the flat area next to this band gap.""""It is an overseen property of a well-studied system,"" first author Dr. Dmitry Marchenko points out: ""It was previously unknown that there is a flat area in the band structure in such a simple well-known system.""Superconductivity? Needs still a little helpThis flat area is a prerequisite for superconductivity but only if it is situated exactly at the so-called Fermi energy. In the case of the two-layer graphene, its energy level is only 200 milli-electron volts below the Fermi energy, but it is possible to raise the energy level of the flat area to the Fermi energy either by doping with foreign atoms or by applying an external voltage, the so-called gate voltage.The physicists have found that the interactions between the two graphene layers and between graphene and the silicon carbide lattice are jointly responsible for the formation of the flat band area. ""We can predict this behavior with very few parameters and could use this mechanism to control the band structure,"" adds Oliver Rader.Story Source:Materials provided by Helmholtz-Zentrum Berlin fer Materialien und Energie. ",0.11891,0.179168,0.16536,0.177445,0.203092,0.154462,0.172841,0,0
675,Scientists shuffle the deck to create materials with new quantum behaviors,"Layered transition metal dichalcogenides or TMDCs -- materials composed of metal nanolayers sandwiched between two other layers of chalcogens -- have become extremely attractive to the research community due to their ability to exfoliate into 2D single layers. Similar to graphene, they not only retain some of the unique properties of the bulk material, but also demonstrate direct-gap semiconducting behavior, excellent electrocatalytic activity and unique quantum phenomena such as charge density waves (CDW).Generating complex multi-principle element TMDCs essential for the future development of new generations of quantum, electronic, and energy conversion materials is difficult.""It is relatively simple to make a binary material from one type of metal and one type of chalcogen,"" said Ames Laboratory Senior Scientist Viktor Balema. ""Once you try to add more metals or chalcogens to the reactants, combining them into a uniform structure becomes challenging. It was even believed that alloying of two or more different binary TMDCs in one single-phase material is absolutely impossible.""To overcome this obstacle, postdoctoral research associate Ihor Hlova used ball-milling and subsequent reactive fusion to combine such TMDCs as MoS2, WSe2, WS2, TaS2 and NbSe2. Ball-milling is a mechanochemical process capable of exfoliating layered materials into single- or few-layer-nanosheets that can further restore their multi-layered arrangements by restacking.""Mechanical processing treats binary TMDCs like shuffling together two separate decks of cards, said Balema. ""They are reordered to form 3D-heterostructured architectures -- an unprecedented phenomenon first observed in our work.""Heating of the resulting 3D-heterostructures brings them to the edge of their stability, reorders atoms within and between their layers, resulting in single-phase solids that can in turn be exfoliated, or peeled into 2D single layers similar to graphene, but with their own, unique tunable properties.""Preliminary examination of properties of only a few, earlier unavailable compounds, proves as exciting as synthetic results are,"" adds Ames Laboratory Senior Scientist and Distinguished Professor of Materials Science and Engineering Vitalij Pecharsky. ""Very likely, we have just opened doors to the entirely new class of finely tunable, quantum matter.""This research was supported by the Ames Laboratory's Directed Research and Development program under contract with the U.S. Department of Energy.The research is further discussed in the paper, ""Multi-Principal Element Transition Metal Dichalcogenides via Reactive Fusion of 3D-Heterostructures,"" authored by Ihor Z. Hlova, Oleksandr Dolotko, Brett W. Boote, Arjun K. Pathak, Emily A. Smith, Vitalij K. Pecharsky, and Viktor P. Balema; and published on the cover of Chemical Communications.Story Source:Materials provided by DOE/Ames Laboratory. ",0.077174,0.149554,0.125719,0.187294,0.129376,0.134387,0.164068,0,0
676,Physicists name and codify new field in nanotechnology: 'electron quantum metamaterials',"When two atomically thin two-dimensional layers are stacked on top of each other and one layer is made to rotate against the second layer, they begin to produce patterns -- the familiar moire patterns -- that neither layer can generate on its own and that facilitate the passage of light and electrons, allowing for materials that exhibit unusual phenomena. For example, when two graphene layers are overlaid and the angle between them is 1.1 degrees, the material becomes a superconductor.""It's a bit like driving past a vineyard and looking out the window at the vineyard rows. Every now and then, you see no rows because you're looking directly along a row,"" said Nathaniel Gabor, an associate professor in the Department of Physics and Astronomy at the University of California, Riverside. ""This is akin to what happens when two atomic layers are stacked on top of each other. At certain angles of twist, everything is energetically allowed. It adds up just right to allow for interesting possibilities of energy transfer.""This is the future of new materials being synthesized by twisting and stacking atomically thin layers, and is still in the ""alchemy"" stage, Gabor added. To bring it all under one roof, he and physicist Justin C. W. Song of Nanyang Technological University, Singapore, have proposed this field of research be called ""electron quantum metamaterials"" and have just published a perspective article in Nature Nanotechnology.""We highlight the potential of engineering synthetic periodic arrays with feature sizes below the wavelength of an electron. Such engineering allows the electrons to be manipulated in unusual ways, resulting in a new range of synthetic quantum metamaterials with unconventional responses,"" Gabor said.Metamaterials are a class of material engineered to produce properties that do not occur naturally. Examples include optical cloaking devices and super-lenses akin to the Fresnel lens that lighthouses use. Nature, too, has adopted such techniques -- for example, in the unique coloring of butterfly wings -- to manipulate photons as they move through nanoscale structures.""Unlike photons that scarcely interact with each other, however, electrons in subwavelength structured metamaterials are charged, and they strongly interact,"" Gabor said. ""The result is an enormous variety of emergent phenomena and radically new classes of interacting quantum metamaterials.""Gabor and Song were invited by Nature Nanotechnology to write a review paper. But the pair chose to delve deeper and lay out the fundamental physics that may explain much of the research in electron quantum metamaterials. They wrote a perspective paper instead that envisions the current status of the field and discusses its future.""Researchers, including in our own labs, were exploring a variety of metamaterials but no one had given the field even a name,"" said Gabor, who directs the Quantum Materials Optoelectronics lab at UCR. ""That was our intent in writing the perspective. We are the first to codify the underlying physics. In a way, we are expressing the periodic table of this new and exciting field. It has been a herculean task to codify all the work that has been done so far and to present a unifying picture. The ideas and experiments have matured, and the literature shows there has been rapid progress in creating quantum materials for electrons. It was time to rein it all in under one umbrella and offer a roadmap to researchers for categorizing future work.""In the perspective, Gabor and Song collect early examples in electron metamaterials and distil emerging design strategies for electronic control from them. They write that one of the most promising aspects of the new field occurs when electrons in subwavelength-structure samples interact to exhibit unexpected emergent behavior.""The behavior of superconductivity in twisted bilayer graphene that emerged was a surprise,"" Gabor said. ""It shows, remarkably, how electron interactions and subwavelength features could be made to work together in quantum metamaterials to produce radically new phenomena. It is examples like this that paint an exciting future for electronic metamaterials. Thus far, we have only set the stage for a lot of new work to come.""Story Source:Materials provided by University of California - Riverside. ",0.0887376,0.152335,0.126465,0.203379,0.145282,0.139277,0.168173,0,0
677,"New tech delivers high-tech film that blocks electromagnetic interference Cost-effective process used to make strong, flexible films that allow light in but keep electromagnetic interference out","Electromagnetic interference (EMI), which can harm smartphones, tablets, chips, drones, wearables, and even aircraft and human health, is increasing with the explosive proliferation of devices that generate it. The market for EM-blocking solutions, which employ conductive or magnetic materials, is expected to surpass $7 billion by 2022.Andre Taylor, associate professor of chemical and biomolecular engineering at the NYU Tandon School of Engineering, along with a team that included Yury Gogotsi, Distinguished University and Charles T. and Ruth M. Bach Professor Materials Science and Engineering at Drexel University, and Menachem Elimelech, Roberto C. Goizueta Professor of Chemical & Environmental Engineering at Yale University used an innovative technique to produce relatively low-cost EMI-blocking composite films.The study, ""Layer-by-Layer Assembly of Cross-Functional Semi-transparent MXene-Carbon Nanotubes Composite Films for Next-Generation Electromagnetic Interference Shielding,"" appears in the October 31, 2018 issue of Advanced Functional Materials. Lead authors include Guo-Ming Weng, a post-doctoral fellow at NYU Tandon, and Jinyang Li, associate professor of materials science and engineering at Southwest Jiaotong University, Chengdu, China.To fashion the films, the team employed spin-spray layer-by-layer processing (SSLbL), a method Taylor pioneered in 2012. The system employs mounted spray heads above a spin coater that deposit sequential nanometer-thick monolayers of oppositely charged compounds on a component, producing high quality films in much less time than by traditional methods, such as dip coating.The process allowed them to fashion flexible, semi-transparent EMI-shielding film comprising hundreds of alternating layers of carbon nanotube (CNT), an oppositely charged titanium carbide called MXene -- a family of carbide flakes first engineered by Gogotsi -- and polyelectrolytes. Taylor explained that those charge characteristics confer benefits beyond EMI shielding.""As we worked to discern the roles different components play,"" he said, ""We found that the strong electrostatic and hydrogen bonding between oppositely charged CNT and MXene layers conferred high strength and flexibility."" He added that MXene has the dual benefits of being both adsorbing (it easily adheres to a surface) and conductive, which is important for blocking EMI. ""And since the film itself is semi-transparent, it has the benefit of being applicable as EMI shielding for devices with display screens, such as smartphones. Other kinds of shields -- metal for example -- are opaque. Shielding is good, but shielding that allows visible light through is even better.""The SSLbL method also confers nanometer-level control over the architecture of the film, allowing manufacturers to change specific qualifies such as conductivity or transparency, because it allows for discrete changes in the composition of each layer. By contrast, films comprising a monolayer melange of nanoparticles, polyelectrolytes and graphene in a matrix cannot be so modified. Besides high stability, flexibility and semi-transparency, the MXene-CNT composite films also demonstrated high conductivity, a property critical to electromagnetic shielding because it dissipates EM pulses across the film's surface, weakening and dispersing it.While manufacturers have shown interest in EMI shielding made of carbon nanotubes and graphene combined with conductive polymer composites, until now a relatively fast, inexpensive, means of creating an optimal mix of these qualities on a thin flexible film was elusive, explained Taylor.""The primary interest in adding carbon materials to shielding was to add conductive pathways through the film,"" said Taylor. ""But the SSLbL system is also much faster than traditional dip coating, in which a component to be shielded is repeatedly dipped in a material, rinsed, then dipped again in another layer, and on and on. That takes days. Our system can create hundreds of bi-layers of alternating MXene and CNT in minutes.""While spin-spraying limits component size, Taylor said that, in theory, the system could create EMI shielding for devices and components equivalent in diameter to the 12-inch wafers, for which spin-coating is frequently employed as a coating mechanism in the semiconductor industry.""It is less expensive to produce it this way and faster because of the tighter connection between materials, and the LbL process facilitates the controlled arrangement and assembly of disparate nanostructured materials much better than just depositing repeated layers of a mix on several components. One can envision tuning the desired properties of a cross-functional thin film using a wide range of parameters, nanostructured materials and polyelectrolytes using this system.""Story Source:Materials provided by NYU Tandon School of Engineering. ",0.0866832,0.151773,0.124913,0.152857,0.150873,0.139128,0.148863,0,0
678,"2D magnetism: Atom-thick platforms for energy, information and computing research Scientists say the tiny 'spins' of electrons show potential to one day support next-generation innovations in many fields","Two-dimensional magnetism has long intrigued and motivated researchers for its potential to unleash new states of matter and utility in nano-devices.In part the excitement is driven by predictions that the magnetic moments of electrons -- known as ""spins"" -- would no longer be able to align in perfectly clean systems. This enhancement in the strengths of the excitations could unleash numerous new states of mater, and enable novel forms of quantum computing.A key challenge has been the successful fabrication of perfectly clean systems and their incorporation with other materials. However, for more than a decade, materials known as ""van der Waals"" crystals, held together by friction, have been used to isolate single-atom-thick layers leading to numerous new physical effects and applications.Recently this class has been expanded to include magnetic materials, and it may offer one of the most ambitious platforms yet in scientific efforts to investigate and manipulate phases of matter at the nanoscale, researchers from Boston College, the University of Tennessee, and Seoul National University, write in the latest edition of the journal Nature.Two-dimensional magnetism, the subject of theoretical explorations and experimentation for the past 80 years, is enjoying a resurgence thanks to a group of materials and compounds that are relatively plentiful and easy to manipulate, according to Boston College Associate Professor of Physics Kenneth Burch, a first author of the article ""'Magnetism in two-dimensional van der Waals materials.""The most oft-cited example of these materials is graphene, a crystal constructed in uniform, atom-thick layers. A procedure as simple as applying a piece of scotch tape to the crystal can remove a single layer, providing a thin, uniform section to serve as a platform to create novel materials with a range of physical properties open to manipulation.""What's amazing about these 2-D materials is they're so flexible,"" said Burch. ""Because they are so flexible, they give you this huge array of possibilities. You can make combinations you could not dream of before. You can just try them. You don't have to spend this huge amount of time and money and machinery trying to grow them. A student working with tape puts them together. That adds up to this exciting opportunity people dreamed of for a long time, to be able to engineer these new phases of matter.""At that single layer, researchers have focused on spin, what Burch refers to as the ""magnetic moment"" of an electron. While the charge of an electron can be used to send two signals -- either ""off"" or ""on,"" results represented as either zero or one -- spin excitations offer multiple points of control and measurement, an exponential expansion of the potential to signal, store or transmit information in the tiniest of spaces.""One of the big efforts now is to try to switch the way we do computations,"" said Burch. ""Now we record whether the charge of the electron is there or it isn't. Since every electron has a magnetic moment, you can potentially store information using the relative directions of those moments, which is more like a compass with multiple points. You don't just get a one and a zero, you get all the values in between.""Potential applications lie in the areas of new ""quantum"" computers, sensing technologies, semiconductors, or high-temperature superconductors.""The point of our perspective is that there has been a huge emphasis on devices and trying to pursue these 2-D materials to make these new devices, which is extremely promising,"" said Burch. ""But what we point out is magnetic 2D atomic crystals can also realize the dream of engineering these new phases -- superconducting, or magnetic or topological phases of matter, that is really the most exciting part. It is not just fundamentally interesting to realize these theorems that have been around for 40 years. These new phases would have applications in various forms of computing, whether in spintronics, producing high temperature superconductors, magnetic and optical sensors and in topological quantum computing.""Burch and his colleagues -- the University of Tennessee's David Mandrus and Seoul National University's Je-Geun Park -- outline four major directions for research into magnetic van der Waals materials:Germano Iannacchione, a National Science Foundation (NSF) program officer who oversees grants to Burch and other materials scientists, said the co-authors offer the broader community of scientists ideas that can serve to guide a dynamic field pushing beyond boundaries in materials research.""Magnetism in 2D van Der Waals materials has grown into a vibrant field of study,"" said Iannacchione. ""Its investigators have matured from highly focused researchers to statesmen shepherding a field, broadening applications into as many channels as possible. The review captures the multiplicative aspect of steady, focused, and sometimes risky research that opens vast new frontiers, with tremendous potential for applications in quantum computing and spintronics.""Story Source:Materials provided by Boston College. ",0.0471551,0.0849808,0.0930444,0.146249,0.0912343,0.106913,0.114634,0,0
679,More goals in quantum soccer Study may improve the transmission of quantum information over long distances,"Let's suppose you were allowed to blindfold German soccer star Timo Werner and turn him on his own axis several times. Then you'd ask him to take a shot blind. It would be extremely unlikely that this would hit the goal.With a trick, Bonn physicists nevertheless managed to achieve a 90-percent score rate in a similar situation. However, their player was almost 10 billion times smaller than the German star striker -- and much less predictable.It was a rubidium atom that the researchers had irradiated with laser light. The atom had absorbed radiation energy and had entered an excited state. This has a defined lifespan. The atom subsequently releases the absorbed energy by emitting a particle of light: a photon.The direction in which this photon flies is purely coincidental. However, this changes when the rubidium is placed between two parallel mirrors, because then the atom prefers to shoot at one of the mirrors. In the example with Timo Werner, it would be as if the goal magically attracted the ball.This phenomenon is called the Purcell effect. The existence of it was already proven several decades ago. ""We have now used the Purcell effect for the targeted emission of photons by a neutral atom,"" explains Dr. Wolfgang Alt from the Institute of Applied Physics at the University of Bonn.There is great interest in the Purcell effect, partly because it makes the construction of so-called quantum repeaters possible. These are needed to transmit quantum information over long distances. Because, whilst it is possible to put a photon into a certain quantum state and send it through a light guide, this can only be done over limited distances; for greater distances, the signal has to be buffered.Repeaters pass on quantum informationIn the quantum repeater, the photon is for instance guided to an atom which swallows it and thereby changes into another state. In response to a reading pulse with a laser beam, the atom spits out the light particle again. The stored quantum information is retained.The emitted photon must now be collected and fed back into a light guide. But that is difficult when the photon is released in a random direction. ""We have succeeded in forcing the photons onto the path between the two mirrors using the Purcell effect,"" explains Alt. ""We have now made one of the mirrors partially transmissive and connected a glass fiber to it. This allowed us to introduce the photon relatively efficiently into this fiber.""The Purcell effect also has another advantage: It shortens the time it takes the rubidium atom to store and release the quantum information. This gain in speed is extremely important: Only if the repeater works fast enough can it communicate with the transmitter of the information, a so-called quantum dot. Today, quantum dots are regarded as the best source for single photons for the transmission of quantum information, which is completely safe from being intercepted. ""Our experiments are taking this important future technology one step further,"" says Alt.Story Source:Materials provided by University of Bonn. ",0.106604,0.150138,0.137132,0.189446,0.168988,0.163043,0.170281,0,0
680,Producing defectless metal crystals of unprecedented size,"A research group at the Center for Multidimensional Carbon Materials, within the Institute for Basic Science (IBS), have published in Science about a new method to convert inexpensive polycrystalline metal foils to single crystals with superior properties. It is expected that these materials will find many uses in science and technology.The structure of most metal materials can be thought of as a patchwork of different tiny crystals, bearing some defects on the borders between each patch. These defects, known as grain boundaries (GBs), worsen the electrical and sometimes mechanical properties of the metal. Single crystal metals, instead, have no GBs and show higher electrical conductivity and other enhanced qualities that can play a major role in multiple fields, such as electronics, plasmonics, and catalysis, among others. Single crystal metal foils have attracted great attention also because certain single crystal metals, such as copper, nickel, and cobalt, are suitable for the growth of defectless graphene, boron nitride, and diamond on top of them.Single crystals are normally fabricated beginning with a 'crystal seed'. Conventional approaches, such as the Czochralski or Bridgman methods, or others based on the deposition of thin metal films on single crystal inorganic substrates, achieve small single crystals at high processing costs.To unlock the full potential of such metal structures, the IBS team led by Rodney Ruoff at Ulsan National Institute of Science and Technology (UNIST), along with JIN Sunghwan and SHIN Hyung-Joon, invented the ""contact-free annealing"" (CFA) technique. CFA involves heating the polycrystalline metal foils to a temperature slightly below the melting point of each metal. This new method does not need single crystal seeds or templates, which limit the maximum crystal size, and was tested with five different types of metal foils: copper, nickel, cobalt, platinum, and palladium. It resulted in a 'colossal grain growth', reaching up to 32 square centimeters for copper.The details of the experiment varied according to the metal used. In the case of copper, quartz holders and a rod were used to hang the metal foil, like clothes suspended on clothes lines. Then, the foil was heated in a tube-shaped furnace to approximately 1050 degrees Celsius (1323 degrees Kelvin), a temperature close to copper's melting point (1358 K) for several hours in an atmosphere with hydrogen and argon, and then cooled down.The scientists also achieved single crystals from nickel and cobalt foils, each about 11 cm2. The achieved sizes are limited by the size of the furnace, so that one could expect production of larger foils with 'industrial' processing methods.For platinum, resistive heating was used because of its higher melting temperature (2041 K). Current was passed through a platinum foil attached to two opposing electrodes, then one electrode was moved and adjusted to keep the foil flat during expansion and contraction. The research team expects this trick to work for other foils, because it also worked for palladium.These large single crystal metal foils are useful in several applications. For example, they can serve to grow graphene on top of them: the group obtained very high quality single crystal monolayer graphene on single crystal copper foil, and multilayer graphene on a single crystal copper-nickel alloy foil.The new single crystal copper foil showed improved electrical properties. Collaborators, YOO Won Jong and MOON Inyong at Sungkyunkwan University, measured a 7% increase in the room temperature electrical conductivity of the single crystal copper foil, compared to the commercially-available polycrystalline foil.""Now that we have explored these five metals and invented a straightforward scalable method to make such large single crystals, there's the exciting question of whether other types of polycrystalline metal films, such as iron, can also be converted to single crystals,"" note first author of the study, JIN Sunghwan, and his supervisor Ruoff. Ruoff enthusiastically concludes ""Now that these cheap single crystal metal foils are available, it will be tremendously exciting to see how they are used by the scientific and engineering communities!""Story Source:Materials provided by Institute for Basic Science. ",0.106562,0.199667,0.150606,0.164753,0.189466,0.159672,0.163462,0,0
681,3D-printed supercapacitor electrode breaks records in lab tests Advances in supercapacitor technology could lead to wider use of fast-charging energy storage devices and novel designs for electronic gadgets,"Scientists at UC Santa Cruz and Lawrence Livermore National Laboratory (LLNL) have reported unprecedented performance results for a supercapacitor electrode. The researchers fabricated electrodes using a printable graphene aerogel to build a porous three-dimensional scaffold loaded with pseudocapacitive material.In laboratory tests, the novel electrodes achieved the highest areal capacitance (electric charge stored per unit of electrode surface area) ever reported for a supercapacitor, said Yat Li, professor of chemistry and biochemistry at UC Santa Cruz. Li and his collaborators reported their findings in a paper published October 18 in Joule.As energy storage devices, supercapacitors have the advantages of charging very rapidly (in seconds to minutes) and retaining their storage capacity through tens of thousands of charge cycles. They are used for regenerative braking systems in electric vehicles and other applications. Compared to batteries, they hold less energy in the same amount of space, and they don't hold a charge for as long. But advances in supercapacitor technology could make them competitive with batteries in a much wider range of applications.In earlier work, the UCSC and LLNL researchers demonstrated ultrafast supercapacitor electrodes fabricated using a 3D-printed graphene aerogel. In the new study, they used an improved graphene aerogel to build a porous scaffold which was then loaded with manganese oxide, a commonly used pseudocapacitive material.A pseudocapacitor is a type of supercapacitor that stores energy through a reaction at the electrode surface, giving it more battery-like performance than supercapacitors that store energy primarily through an electrostatic mechanism (called electric double-layer capacitance, or EDLC).""The problem for pseudocapacitors is that when you increase the thickness of the electrode, the capacitance decreases rapidly because of sluggish ion diffusion in bulk structure. So the challenge is to increase the mass loading of pseudocapacitor material without sacrificing its energy storage capacity per unit mass or volume,"" Li explained.The new study demonstrates a breakthrough in balancing mass loading and capacitance in a pseudocapacitor. The researchers were able to increase mass loading to record levels of more than 100 milligrams of manganese oxide per square centimeter without compromising performance, compared to typical levels of around 10 milligrams per square centimeter for commercial devices.Most importantly, the areal capacitance increased linearly with mass loading of manganese oxide and electrode thickness, while the capacitance per gram (gravimetric capacitance) remained almost unchanged. This indicates that the electrode's performance is not limited by ion diffusion even at such a high mass loading.First author Bin Yao, a graduate student in Li's lab at UC Santa Cruz, explained that in traditional commercial fabrication of supercapacitors, a thin coating of electrode material is applied to a thin metal sheet that serves as a current collector. Because increasing the thickness of the coating causes performance to decline, multiple sheets are stacked to build capacitance, adding weight and material cost because of the metallic current collector in each layer.""With our approach, we don't need stacking because we can increase capacitance by making the electrode thicker without sacrificing performance,"" Yao said.The researchers were able to increase the thickness of their electrodes to 4 millimeters without any loss of performance. They designed the electrodes with a periodic pore structure that enables both uniform deposition of the material and efficient ion diffusion for charging and discharging. The printed structure is a lattice composed of cylindrical rods of the graphene aerogel. The rods themselves are porous, in addition to the pores in the lattice structure. Manganese oxide is then electrodeposited onto the graphene aerogel lattice.""The key innovation in this study is the use of 3D printing to fabricate a rationally designed structure providing a carbon scaffold to support the pseudocapacitive material,"" Li said. ""These findings validate a new approach to fabricating energy storage devices using 3D printing.""Supercapacitor devices made with the graphene aerogel/manganese oxide electrodes showed good cycling stability, retaining more than 90 percent of initial capacitance after 20,000 cycles of charging and discharging. The 3D-printed graphene aerogel electrodes allow tremendous design flexibility because they can be made in any shape needed to fit into a device. The printable graphene-based inks developed at LLNL provide ultrahigh surface area, lightweight properties, elasticity, and superior electrical conductivity.Story Source:Materials provided by University of California - Santa Cruz. ",0.0981784,0.178085,0.156752,0.166532,0.149541,0.158207,0.164586,0,0
682,Exploring new spintronics device functionalities in graphene heterostructures,"Graphene Flagship researchers have shown in a paper published in Science Advances how heterostructures built from graphene and topological insulators have strong, proximity induced spin-orbit coupling which can form the basis of novel information processing technologies.Spin-orbit coupling is at the heart of spintronics. Graphene's spin-orbit coupling and high electron mobility make it appealing for long spin coherence length at room temperature. Graphene Flagship researchers from Chalmers University of Technology (Sweden), Catalan Institute of Nanoscience and Nanotechnology -- ICN2 (Spain), Universitat Autenoma de Barcelona (Spain) and ICREA Institucie Catalana de Recerca i Estudis Avaneats (Spain) showed a strong tunability and suppression of the spin signal and spin lifetime in heterostructures formed by graphene and topological insulators. This can lead to new graphene spintronic applications, ranging from novel circuits to new non-volatile memories and information processing technologies.""The advantage of using heterostructures built from two Dirac materials is that, graphene in proximity with topological insulators still supports spin transport, and concurrently acquires a strong spin-orbit coupling,"" said Associate Professor Saroj Prasad Dash, from Chalmers University of Technology.""We do not just want to transport spin we want to manipulate it,"" said Professor Stephan Roche from ICN2 and deputy leader of the Graphene Flagship's spintronics Work-Package, ""the use of topological insulators is a new dimension for spintronics, they have a surface state similar to graphene and can combine to create new hybrid states and new spin features. By combining graphene in this way we can use the tuneable density of states to switch on/off -- to conduct or not conduct spin. This opens an active spin device playground.""The Graphene Flagship, from its very beginning, saw the potential of spintronics devices made from graphene and related materials. This paper shows how combining graphene with other materials to make heterostructures opens new possibilities and potential applications.""This paper combines experiment and theory and this collaboration is one of the strengths of the Spintronics Work-Package within the Graphene Flagship,"" said Roche.""Topological insulators belong to a class of material that generate strong spin currents, of direct relevance for spintronic applications such as spin-orbit torque memories. As reported by this article, the further combination of topological insulators with two-dimensional materials like graphene is ideal for enabling the propagation of spin information with extremely low power over long distances, as well as for exploiting complementary functionalities, key to further design and fabricate spin-logic architectures,"" said Kevin Garello from IMEC, Belgium who is leader of the Graphene Flagships Spintronics Work-Package.Professor Andrea C. Ferrari, Science and Technology Officer of the Graphene Flagship, and Chair of its Management Panel added ""This paper brings us closer to building useful spintronic devices. The innovation and technology roadmap of the graphene Flagship recognises the potential of graphene and related materials in this area. This work yet again places the Flagship at the forefront of this field, initiated with pioneering contributions of European researchers.""Story Source:Materials provided by Graphene Flagship. ",0.204713,0.238115,0.23185,0.276587,0.252373,0.259567,0.255115,0,0
683,Perovskites: Materials of the future in optical communication,"Researchers at the universities in Linkeping and Shenzhen have shown how an inorganic perovskite can be made into a cheap and efficient photodetector that transfers both text and music. ""It's a promising material for future rapid optical communication,"" says Feng Gao, researcher at Linkeping University.""Perovskites of inorganic materials have a huge potential to influence the development of optical communication. These materials have rapid response times, are simple to manufacture, and are extremely stable."" So says Feng Gao, senior lecturer at LiU who, together with colleagues who include Chunxiong Bao, postdoc at LiU, and scientists at Shenzhen University, has published the results in the journal Advanced Materials.All optical communication requires rapid and reliable photodetectors -- materials that capture a light signal and convert it into an electrical signal. Current optical communication systems use photodetectors made from materials such as silicon and indium gallium arsenide. But these are expensive, partly because they are complicated to manufacture. Moreover, these materials cannot to be used in some new devices, such as mechanically flexible, light-weight or large-area devices.Researcher have been seeking cheap replacement, or at least supplementary, materials for many years, and have looked at, for example, organic semi-conductors. However, the charge transport of these has proved to be too slow. A photodetector must be rapid.The new perovskite materials have been extremely interesting in research since 2009, but the focus has been on their use in solar cells and efficient light-emitting diodes. Feng Gao, researcher in Biomolecular and Organic Electronics at LiU, was awarded a Starting Grant of EUR 1.5 million from the European Research Council (ERC) in the autumn of 2016, intended for research into using perovskites in light-emitting diodes.Perovskites form a completely new family of semi-conducting materials that are defined by their crystal structures. They can consist of both organic and inorganic substances. They have good light-emitting properties and are easy to manufacture. For applications such as light-emitting diodes and efficient solar cells, most interest has been placed on perovskites that consist of an organic substance (containing carbon and hydrogen), metal, and halogen (fluorine, chlorine, bromine or iodine) ions. However, when this composition was used in photodetectors, it proved to be too unstable.The results changed, however, when Chunxiong Bao used the right materials, and managed to optimise the manufacturing process and the structure of the film. The film in the new perovskite, which contains only inorganic elements (caesium, lead, iodine and bromine), has been tested in a system for optical communication, which confirmed its ability to transfer both text and images, rapidly and reliably. The quality didn't deteriorate, even after 2,000 hours at room temperature.""It's very gratifying that we have already achieved results that are very close to application,"" says Feng Gao, who leads the research, together with Professor Wenjing Zhang at Shenzhen University.Story Source:Materials provided by Linkeping University. ",0.086902,0.161965,0.142165,0.169712,0.177216,0.130249,0.161771,0,0
684,Blue phosphorus mapped and measured for the first time,"The element phosphorus can exist in various allotropes and changes its properties with each new form. So far, red, violet, white and black phosphorus have been known. While some phosphorus compounds are essential for life, white phosphorus is poisonous and inflammable and black phosphorus -- on the contrary -- particularly robust. Now, another allotrope has been identified: In 2014, a team from Michigan State University, USA, performed model calculations to predict that ""blue phosphorus"" should be also stable. In this form, the phosphorus atoms arrange in a honeycomb structure similar to graphene, however, not completely flat but regularly ""buckled."" Model calculations showed that blue phosphorus is not a narrow gap semiconductor like black phosphorus in the bulk but possesses the properties of a semiconductor with a rather large band gap of 2 electron volts. This large gap, which is seven times larger than in bulk black phosphorus, is important for optoelectronic applications.Blue P examined at BESSY IIIn 2016, blue phosphorus was successfully stabilized on a gold substrate by evaporation. Nevertheless, only now we know for certain that the resulting material is indeed blue phosphorus. To this end, a team from HZB around Evangelos Golias has probed the electronic band structure of the material at BESSY II. They were able to measure by angle-resolved photoelectron spectroscopy the distribution of electrons in its valence band, setting the lower limit for the band gap of blue phosphorus.Band structure influenced by the substrateThey found that the P atoms do not arrange independently of the gold substrate but try to adjust to the spacings of the Au atoms. This distorts the corrugated honeycomb lattice in a regular manner which in turn affects the behavior of electrons in blue phosphorus. As a result, the top of the valence band that defines the one end of the semiconducting band gap agrees with the theoretical predictions about its energy position but is somewhat shifted.Outlook: optoelectronic applications""So far, researchers have mainly used bulk black phosphorus to exfoliate atomically thin layers,"" Prof. Oliver Rader, head of HZB-Department Materials for green spintronics explains. ""These also show a large semiconducting band gap but do not possess the honeycomb structure of blue phosphorus and, above all, cannot be grown directly on a substrate. Our work not only reveals all the material properties of this novel two-dimensional phosphorus allotrope but highlights the impact of the supporting substrate on the behavior of electrons in blue phosphorus, an essential parameter for any optoelectronic application.""Story Source:Materials provided by Helmholtz-Zentrum Berlin fer Materialien und Energie. ",0.154136,0.220965,0.262576,0.233623,0.224732,0.213877,0.220065,0,0
685,Arsenic for electronics Covalently modified two-dimensional arsenic,"The discovery of graphene, a material made of one or very few atomic layers of carbon, started a boom. Today, such two-dimensional materials are no longer limited to carbon and are hot prospects for many applications, especially in microelectronics. In the journal Angewandte Chemie, scientists have now introduced a new 2D material: they successfully modified arsenene (arsenic in a graphene-like structure) with chloromethylene groups.Two-dimensional materials are crystalline materials made of just a single or very few layers of atoms that often display unusual properties. However, the use of graphene for applications such as transistors is limited because it behaves more like a conductor than a semiconductor. Modified graphene and 2D materials based on other chemical elements with semiconducting properties have now been developed. One such material is ?-arsenene, a two-dimensional arsenic in a buckled honeycomb structure derived from gray arsenic. Researchers hope that modification of this previously seldom-studied material could improve its semiconducting properties and lead the way to new applications in fields such as sensing, catalysis, optoelectronics, and other semiconductor technologies.A team at the University of Chemistry and Technology Prague (Czech Republic) and Nanyang Technical University (Singapore), led by Zdenek Sofer and Martin Pumera has now successfully produced a highly promising covalent modification of ?-arsenene.The arsenene was produced by milling gray arsenic in tetrahydrofuran. The shear forces cause two-dimensional layers to split off and disperse into the solvent. The researchers then introduce dichloromethane and add an organic lithium compound (butyllithium). These two reagents form an intermediate called chlorocarbene, a molecule made of one carbon atom, one hydrogen atom, and one chlorine atom. The carbon atom is short two bonding partners, a state that makes the whole class of carbene molecules highly reactive. Arsenene contains free electron pairs that ""stick out"" from the surface and can easily enter into bonds to chlorocarbene.This approach leads to high coverage of the arsenene surface with chloromethylene groups, as confirmed by a variety of analysis methods (X-ray photoelectron spectroscopy, FT-IR spectroscopy, elemental analysis by transmission electron microscopy). The modified arsenene is more stable than pure arsenene and exhibits strong luminescence and electronic properties that make it attractive for optoelectronic applications. In addition, the chloromethylene units could serve as a starting point for further interesting modifications.Story Source:Materials provided by Wiley. ",0.0772025,0.185944,0.124391,0.148511,0.132296,0.132693,0.160719,0,0
686,More sensitive MRI diagnostics thanks to innovative 'elastic' contrast media,"Researchers from the Leibniz-Forschungsinstitut fer Molekulare Pharmakologie (FMP) have found a new method for obtaining high-quality images in magnetic resonance imaging (MRI), that requires less contrast medium compared to current methods. It is made possible by using an ""elastic"" protein structure that can absorb dissolved xenon in a self-regulating way: The greater the amount of this noble gas, the higher the quality of the image, without the need to adjust the amount of contrast medium applied.Nowadays, magnetic resonance imaging (MRI) is an indispensable method for diagnosing diseases and monitoring the course of treatment. It creates sectional images of the human body without the use of any harmful radiation. Typically, the water molecules in the tissue are exposed to a strong magnetic field. However, MRI is very insensitive and needs a high concentration of molecules in order to absorb a usable signal. Contrast media are often used to improve diagnostics in order to detect specific changes such as tumors more clearly.However, even with these contrast media, the sensitivity of MRI cannot be significantly increased, and many markers that are known from cell biology cannot be detected during imaging. Besides this, the safety of certain contrast media containing the element gadolinium is currently the subject of increasing discussion. ""We need new, improved methods in which as little contrast medium as possible influences as much of the signal-transmitting substance as possible, which is typically water,"" says FMP researcher Dr. Leif Schroeder. He and his team have now achieved an important breakthrough.The researchers have been working for some time in developing contrast media based on xenon, a harmless noble gas. The group employs a process with powerful lasers in which the xenon is artificially magnetized and then -- even in small quantities -- generates measurable signals. To detect specific cellular disease markers, the xenon has to be bound to them for a short time. In a cooperation with scientists from the California Institute of Technology (Caltech) funded by the Human Frontiers Science Program (HFSP), Dr. Leif Schroeder and his team have now looked into a new class of contrast media that binds the xenon reversibly. These are hollow protein structures produced by certain bacteria in order to regulate the depth at which they float in water, similar to a miniaturized swim bladder in fish but on a nanometer scale. The research group led by cooperation partner Mikhail Shapiro at Caltech introduced these so-called ""gas vesicles"" some time ago as MR contrast media. However, it was not yet known how well they could be ""charged"" with xenon.In the study, which has been published in the ACS Nano journal, both groups now describe how these vesicles form an ideal contrast medium: They can ""elastically"" adjust their influence on the measured xenon. ""The protein structures have a porous wall structure through which the xenon can flow in and out. Unlike conventional contrast media, the gas vesicles always absorb a fixed portion of the xenon that is provided by the environment, in other words also larger amounts if more Xe is provided,"" Dr. Leif Schreder reports. This characteristic can be employed in MRI diagnostics, because more xenon must be used in order to obtain better images. The concentration of a conventional contrast medium would also need to be adjusted in order to achieve a change in signal for all the xenon atoms. The gas vesicles, on the other hand, automatically fill up with more xenon when this is offered by the environment.""They act like a kind of balloon, to which an external pump is attached. If the balloon is 'inflated' by xenon atoms flowing into the gas vesicle, its size does not change, but the pressure does increase -- similar to a bicycle tire tube,"" explains Dr. Leif Schroeder. Because much more xenon passes into the vesicles than with conventional contrast media, the xenon atoms can then be read out much better after they have left the vesicle again and show a changed signal. This way, the image contrast is many times higher than the background noise while the quality of the image is significantly improved. These contrast media can thus also be used to identify disease markers that occur in relatively low concentrations.During the further course of the cooperation, the two groups intend to test these contrast media in initial animal studies. The newly discovered behavior will be a decisive advantage in order to use these very sensitive contrast media in living tissue as well. Dr. Leif Schroeder and his team were able to make the first MRI images with particle concentrations one million times lower than those of the contrast media currently employed.Story Source:Materials provided by Forschungsverbund Berlin. ",0.0665437,0.106923,0.11632,0.162901,0.114718,0.11277,0.136616,0,0
687,New technology for the first experiment with the greatest source of x-rays in the world,"The Flow Focusing technology (also known as GDVN, Gas Dynamic Virtual Nozzle), has been one of the key elements in the success of the first experiments carried out by the European XFEL project, the greatest source of x-rays in the world today. It is a tool that was created and developed by the Fluid Mechanics teacher Alfonso Gaeen Calvo, of the Higher Technical School of Engineering at the University of Seville. This technology, which has been used for the study of microscopic biological samples, places the University of Seville among the institutions at the global vanguard in the used of XFEL (X-Ray Free-Electron Laser).The fundamental advance obtained in the European XFEL project is the increase in the amount of data that can be obtained per second in the analysis of a sample. This achievement is possible thanks to the use of a frequency of pulses that had not been used before, more than a megahertz. For that, a high rate of renewal is necessary -- that is to say, that each pulse has clean samples, not affected by the previous pulse. Therefore, they have to be given sufficient velocity.""This means photographing or ""hunting"" the molecules using an ultra-rapid, and ultra-powerful, flash, before the samples disintegrate due to the intensity of the ionising radiation that they receive,"" Gaeen explains.The biological samples (normally protein microcrystals) have to be in an aqueous environment. The challenge has been to present them in the correct way for them to be intercepted by x-ray pulses that are scarcely a few microns in diameter and last for less than ten femtoseconds (a hundredth of one thousand billionth of a second), and to generate the clearest and most coherent diffraction pattern possible.To this end, the GDVN technology has been capable of generating jets of liquid of less than 2.5 microns in diameter (1 micron = one thousandth of a millimetre) with speeds that reach 100 metres per second (260 km per hour), enough to transmit protein microcrystals and continually renew them at the point of impact without them being affected by the previous pulses.This has been achieved thanks to the use of helium as the a focusing gas for the micro-jet, as helium, due to its physical properties, acquires expansion speeds three times greater than those of air. In addition, an extraordinarily precise method of nano-fabrication has been used (3D nano-printing) for the device that emits the jet.The combination of XFEL technology (trains of ultra-short and ultra-powerful x-ray pulses) with the Flow Focusing vehicle (GDVN) has given rise to what is now known as 'Serial Femtosecond Crystallography' (SFX), a revolution in molecular biology.GDVN technology has been adopted as the most efficient, robust and reproducible (standard) method for the introduction of samples for Serial Femtosecond Crystallography (SFX) y time-resolved SFX at the European XFEL (Hamburg, Germany), SACLA (Japan), LCLS (Stanford, USA), SwissFEL (Zerich, Switzerland), and the newly constructed Chinese and Korean XFELs, among others.The European XFEL has been a pioneer in this regard, followed by LINAC LCLS III, which is still not operative. With this frequency, the so-called ""impact index"" is maximised. This measures the percentage of samples that are effectively intercepted by the x-ray. The scientific review Nature Communications has published an article which has presented the principal advances.Story Source:Materials provided by University of Seville. ",0.095029,0.116921,0.113555,0.133878,0.107235,0.140726,0.126814,0,0
688,Fullerene compounds made simulation-ready New model helps understand compound nanomolecules made of football-shaped fullerenes,"What in the smart nanomaterials world is widely available, highly symmetrical and inexpensive? Hollow carbon structures, shaped like a football, called fullerenes. Their applications range from artificial photosynthesis and nonlinear optics to the production of photoactive films and nanostructures. To make them even more flexible, fullerenes can be combined with added nanostructures. In a new study published in EPJ D, Kirill B. Agapev from ITMO University, St. Petersburg, Russia, and colleagues have developed a method that can be used for future simulations of fullerene complexes and thus help understand their characteristics.Because of the high affinity to the electron and low rearrangement energy, fullerenes, and C60 in particular, tend to play the role of electron acceptors. Specific polymers can therefore transfer electrons to the core of fullerene C60. For example, the best known donor-acceptor compound involving C60 has been used in photoelectric solar cells. In this study, the authors therefore propose a new model showing variations of the C60 fullerene (in its negative ion form (C60-), neutral form (C60), and positively charged ion form (C60+)) that can be used in molecular dynamics simulations. Particularly, understanding its energy -- referred to as electrostatic potential energy, or pseudopotential, which depends on the level of correlation of the molecule with its electrons -- can facilitate subsequent studies of these complex compounds.Agapev and colleagues have developed a model which relies on electronic charge densities that are calculated from scratch. By averaging the total electrostatic potential energy over the entire sphere of the fullerene molecule and their dependency over the distance from the centre of the molecule, the authors provide a model of the energy spread of electrons in the various forms of the fullerene molecules. They demonstrate that the electron correlations, combined with the decrease in electronic density, make the potential energy well for electrons deeper.Story Source:Materials provided by Springer. ",0.099877,0.210403,0.124003,0.157787,0.138226,0.154954,0.163589,0,0
689,Optimization of alloy materials: Diffusion processes in nano particles decoded,"Aluminium alloys have unique material properties and are indispensable materials in aircraft manufacturing and space technology. With the help of high-resolution electron tomography, researchers at TU Graz have for the first time been able to decode mechanisms crucial for understanding these properties. The research results have recently been published in Nature Materials.Nano structures responsible for material qualityAlloy elements such as scandium and zircon are added to the aluminium matrix to improve the strength, corrosion resistance and weldability of aluminium alloys. After further treatment, tiny roundish particles only a few nanometres in size, so-called nano-precipitates, are formed. Their form, atomic structure and the 'struggle' of the scandium and zircon atoms for the 'best place' in the crystal lattice are decisive for the properties and usability of the material.Researchers at TU Graz analysed these structures with the help of the Austrian Scanning Transmission Electron Microscope (ASTEM) at the Graz Centre for Electron Microscopy (ZFE). The device can produce high-resolution element mappings of three-dimensional structures. 'The thus arrived at tomographic analysis provided an image which, surprisingly, could not be interpreted according to the previous level of knowledge,' said Gerald Kothleitner, head of the working group for analytic transmission electron microscopy at the TU Graz's Institute of Electron Microscopy and Nanoanalysis. 'We detected anomalies in the generated core-shell structures. On the one hand, we found higher quantities of aluminium in the nano-precipitates then we had presumed. On the other hand, we discovered a zircon-enriched core as well as border zones between the core and shell with an almost perfect composition and crystal structure.'Quantum mechanics and Monte Carlo methods provide answersTo track down this phenomenon of self-organisation, researchers from the Institute of Electron Microscopy and Nanoanalysis (FELMI) and the Institute of Materials Science, Joining and Forming (IMAT) fell back on quantum mechanical calculations and simulations. It was found that the system separates itself and forms atomically narrow channels in which the foreign atoms can diffuse. Atoms encountering each other block these channels and stabilise the system. Doctoral student Angelina Orthacker, whose thesis was funded by the Austrian Cooperative Research (ACR), gives a graphic explanation of the movement of the atoms: 'The diffusion process can be compared with the formation of an emergency corridor in an urban area with heavy traffic. The traffic manages to organise itself in a split second to enable the free flow of emergency vehicles. But it only takes a few individual vehicles to block the emergency corridor thus stopping it from working.' And this is exactly the same behaviour in the interior of aluminium alloys. 'Emergency corridors' promote the material transport of scandium and zircon atoms and even slight disturbances stop this transport reaction. The research team presumes that the new findings about these diffusion processes also play a role in other multi-component alloys. Their properties can now be adjusted even more.Story Source:Materials provided by Graz University of Technology. ",0.0798488,0.159941,0.143754,0.148868,0.116085,0.131818,0.154027,0,0
690,Two quantum dots are better than one: Using one dot to sense changes in another,"Quantum dots are nanometer-sized boxes that have attracted huge scientific interest for use in nanotechnology because their properties obey quantum mechanics and are requisites to develop advanced electronic and photonic devices. Quantum dots that self-assemble during their formation are particularly attractive as tunable light emitters in nanoelectronic devices and to study quantum physics because of their quantized transport behavior. It is important to develop a way to measure the charge in a single self-assembled quantum dot to achieve quantum information processing; however, this is difficult because the metal electrodes needed for the measurement can screen out the very small charge of the quantum dot. Researchers at Osaka University have recently developed the first device based on two self-assembled quantum dots that can measure the single-electron charge of one quantum dot using a second as a sensor.The device was fabricated using two indium arsenide (InAs) quantum dots connected to electrodes that were deliberately narrowed to minimize the undesirable screening effect.""The two quantum dots in the device showed significant capacitive coupling,"" says Haruki Kiyama. ""As a result, the single-electron charging of one dot was detected as a change in the current of the other dot.""The current response of the sensor quantum dot depended on the number of electrons in the target dot. Hence the device can be used for real-time detection of single-electron tunneling in a quantum dot. The tunneling events of single electrons in and out of the target quantum dot were detected as switching between high and low current states in the sensor quantum dot. Detection of such tunneling events is important for the measurement of single spins towards electron spin qubits.""Sensing single charges in self-assembled quantum dots is exciting for a number of reasons,"" explains Akira Oiwa. ""The ability to achieve electrical readout of single electron states can be combined with photonics and used in quantum communications. In addition, our device concept can be extended to different materials and systems to study the physics of self-assembled quantum dots.""An electronic device using self-assembled quantum dots to detect single-electron events is a novel strategy for increasing our understanding of the physics of quantum dots and to aid the development of advanced nanoelectronics and quantum computing.Story Source:Materials provided by Osaka University. ",0.184562,0.229656,0.180027,0.256677,0.244522,0.248818,0.228085,0,0
691,Evidence of matter-matter coupling Physicists show first proof of Dicke cooperativity in a matter-matter system,"After their recent pioneering experiments to couple light and matter to an extreme degree, Rice University scientists decided to look for a similar effect in matter alone. They didn't expect to find it so soon.Rice physicist Junichiro Kono, graduate student Xinwei Li and their international colleagues have discovered the first example of Dicke cooperativity in a matter-matter system, a result reported in Science this week.The discovery could help advance the understanding of spintronics and quantum magnetism, Kono said. On the spintronics side, he said the work will lead to faster information processing with lower power consumption and will contribute to the development of spin-based quantum computing. The team's findings on quantum magnetism will lead to a deeper understanding of the phases of matter induced by many-body interactions at the atomic scale.Instead of using light to trigger interactions in a quantum well, a system that produced new evidence of ultrastrong light-matter coupling earlier this year, the Kono lab at Rice used a magnetic field to prompt cooperativity among the spins within a crystalline compound made primarily of iron and erbium.""This is an emerging subject in condensed matter physics,"" Kono said. ""There's a long history in atomic and molecular physics of looking for the phenomenon of ultrastrong cooperative coupling. In our case, we'd already found a way to make light and condensed matter interact and hybridize, but what we're reporting here is more exotic.""Dicke cooperativity, named for physicist Robert Dicke, happens when incoming radiation causes a collection of atomic dipoles to couple, like gears in a motor that don't actually touch. Dicke's early work set the stage for the invention of lasers, the discovery of cosmic background radiation in the universe and the development of lock-in amplifiers used by scientists and engineers.""Dicke was an unusually productive physicist,"" Kono said. ""He had many high-impact papers and accomplishments in almost all areas of physics. The particular Dicke phenomenon that's relevant to our work is related to superradiance, which he introduced in 1954. The idea is that if you have a collection of atoms, or spins, they can work together in light-matter interaction to make spontaneous emission coherent. This was a very strange idea.""When you stimulate many atoms within a small volume, one atom produces a photon that immediately interacts with another atom in the excited state,"" Kono said. ""That atom produces another photon. Now you have coherent superposition of two photons.""This happens between every pair of atoms within the volume and produces macroscopic polarization that eventually leads to a burst of coherent light called superradiance,"" he said. Taking light out of the equation meant the Kono lab had to find another way to excite the material's dipoles, the compass-like magnetic force inherent in every atom, and prompt them to align. Because the lab is uniquely equipped for such experiments, when the test material showed up, Kono and Li were ready.""The sample was provided by my colleague (and co-author) Shixun Cao at Shanghai University,"" Kono said. Characterization tests with a small or no magnetic field performed by another co-author, Dmitry Turchinovich of the University of Duisburg-Essen, drew little response.""But Dmitry is a good friend, and he knows we have a special experimental setup that combines terahertz spectroscopy, low temperatures and high magnetic field,"" Kono said. ""He was curious to know what would happen if we did the measurements.""""Because we have some experience in this field, we got our initial data, identified some interesting details in it and thought there was something more we could explore in depth,"" Li added. ""But we certainly didn't predict this,"" Kono said.Li said that to show cooperativity, the magnetic components of the compound had to mimic the two essential ingredients in a standard light-atom coupling system where Dicke cooperativity was originally proposed: one a species of spins that can be excited into a wave-like object that simulates the light wave, and another with quantum energy levels that would shift with the applied magnetic field and simulate the atoms.""Within a single orthoferrite compound, on one side the iron ions can be triggered to form a spin wave at a particular frequency,"" Li said. ""On the other side, we used the electron paramagnetic resonance of the erbium ions, which forms a two-level quantum structure that interacts with the spin wave.""While the lab's powerful magnet tuned the energy levels of the erbium ions, as detected by the terahertz spectroscope, it did not initially show strong interactions with the iron spin wave at room temperature. But the interactions started to appear at lower temperatures, seen in a spectroscopic measurement of coupling strength known as vacuum Rabi splitting.Chemically doping the erbium with yttrium brought it in line with the observation and showed Dicke cooperativity in the magnetic interactions. ""The way the coupling strength increased matches in an excellent manner with Dicke's early predictions,"" Li said. ""But here, light is out of the picture and the coupling is matter-matter in nature.""""The interaction we're talking about is really atomistic,"" Kono said. ""We show two types of spin interacting in a single material. That's a quantum mechanical interaction, rather than the classical mechanics we see in light-matter coupling. This opens new possibilities for not only understanding but also controlling and predicting novel phases of condensed matter.""Story Source:Materials provided by Rice University. Original written by Mike Williams. ",0.0608073,0.116184,0.0946635,0.127585,0.104056,0.125614,0.122631,0,0
692,Explaining a fastball's unexpected twist,"An unexpected twist from a four-seam or a two-seam fastball can make the difference in a baseball team winning or losing the World Series. However, ""some explanations regarding the different pitches are flat-out wrong,"" said Barton Smith, a professor of mechanical and aerospace engineering at Utah State University who considers himself a big fan of the game.He and his doctoral student, Nazmus Sakib, are conducting experiments to explain how baseballs move. Sakib and Smith will present their findings at the American Physical Society's Division of Fluid Dynamics 71st Annual Meeting, which will take place Nov. 18-20 at the Georgia World Congress Center in Atlanta, Georgia.A baseball is asymmetric owing to the figure-eight stitching pattern, and the way a baseball moves through the air depends on the degree and direction of its spin and its orientation when the hand releases it. The Magnus effect, or the force on a spinning object moving through a fluid like air, pushes in the direction that the front of the ball is spinning. So it causes a ball with topspin to drop and a ball with backspin to gain some lift -- enough to slow its fall, but not enough to overcome gravity.This well-studied phenomenon affects most pitches except for the virtually spin-free knuckle ball, which is gripped with the thumb and fingertips. The two-seam fastball, which is gripped by the middle and index fingers along the seams, seemed to also behave in a way not explained by the Magnus effect.Sakib and Smith focus on these two pitches, which are influenced by forces other than the Magnus effect. In their study, the researchers set up a pitching machine that hurls fastballs and knuckleballs through a smoky path. Automatic photographs, triggered by laser sensors, captured two images of the ball and smoke after release. Then, using a technique called particle image velocimetry, Sakib and Smith tracked the movements of the smoke particles to compute the velocity field around the ball and the direction of the rotating air at a given spot.Then, they computed the ""boundary layer separation"" by identifying the portions of the ball's surface where the layer of air surrounding the ball had separated to form the wake. While the boundary layer separation varies differently for the two fastball pitches as the ball rotates, the net effect is the same.Sakib and Smith found that the two-seam pitch has a tilted spin axis due to the fact that one finger leaves the seam before the other, which can cause the ball to move sideways, unlike a four-seam fastball. In the case of the knuckleball, the point of separation can change midflight, causing the ball to randomly shift directions.Smith is now ""hoping to meet a major league pitcher who wants to use what we've learned through fluid dynamics to throw a better pitch.""Story Source:Materials provided by American Physical Society. ",0.169255,0.140124,0.145392,0.184331,0.157987,0.211785,0.18368,0,0
693,A two-atom quantum duet,"Researchers at the Center for Quantum Nanoscience (QNS) within the Institute for Basic Science (IBS) achieved a major breakthrough in shielding the quantum properties of single atoms on a surface. The scientists used the magnetism of single atoms, known as spin, as a basic building block for quantum information processing. The researchers could show that by packing two atoms closely together they could protect their fragile quantum properties much better than for just one atom.The spin is a fundamental quantum mechanical object and governs magnetic properties of materials. In a classical picture, the spin often can be considered like the needle of a compass. North or south pole of the needle, for example, can represent spin up or down. However, according to the laws of quantum mechanics, the spin can also point in both directions at the same time. This superposition state is very fragile since the interaction of the spin with the local environment causes dephasing of the superposition. Understanding the dephasing mechanism and enhancing the quantum coherence are one of the key ingredients toward spin-based quantum information processing.In this study, published in the journal Science Advances in November 9, 2018, QNS scientists tried to suppress the decoherence of single atoms by assembling them closely together. The spins, for which they used single titanium atoms, were studied by using a sharp metal tip of a scanning tunneling microscope and the atoms' spin states were detected using electron spin resonance. The researchers found that by bringing the atoms very close together (one million times closer than a millimeter), they could protect the superposition states of these two magnetically-coupled atoms 20 times longer compared to an individual atom. ""Like a phalanx, the two atoms were able to protect each other from external influences, better than on their own."" said Dr. Yujeong Bae, researcher at QNS and first author of the study. ""In that way the entangled quantum states we created were not affected by environmental disruptions such as magnetic field noise.""""This is a significant development that shows how we can engineer and sense the states of atoms. This allows us to explore their possibility to be used as quantum bits for future quantum information processing.,"" added Prof. Andreas Heinrich, director of QNS. In future experiments, the researchers plan to build even more sophisticated structures in order to explore and improve the quantum properties of single atoms and nanostructures.Story Source:Materials provided by Institute for Basic Science. ",0.116264,0.18009,0.147792,0.198055,0.161871,0.183477,0.18195,0,0
694,Pushing the (extra cold) frontiers of superconducting science,"Measuring the properties of superconducting materials in magnetic fields at close to absolute zero temperatures is difficult, but necessary to understand their quantum properties.How cold? Lower than 0.05 Kelvin (-272eC).""For many modern (quantum) materials, to properly study the fine details of their quantum mechanical behavior you need to be cool. Cooler than was formerly thought possible,"" said Ruslan Prozorov, a physicist at the U.S. Department of Energy's Ames Laboratory, who specializes in developing instrumentation which measures just such things.Prozorov and his research team have developed a method to measure magnetic properties of superconducting and magnetic materials that exhibit unusual quantum behavior at very low temperatures in high magnetic fields. The method is being used to study quantum critical behavior, mechanisms of superconductivity, magnetic frustration and phase transitions in materials, many of which were first fabricated at Ames Laboratory.They did so by placing a tunnel diode resonator, an instrument that makes precise radio-frequency measurements of magnetic properties, in a dilution refrigerator, a cryogenic device that is able to cool samples down to milli-Kelvin temperature range. While this was already achieved before, previous works did not have the ability to apply large static magnetic fields, which is crucial for studying quantum materials.Prozorov's group worked to overcome the technical difficulties of maintaining high-resolution magnetic measurements, while at the same time achieving ultra-cold temperatures down to 0.05 K and in magnetic fields up to 14 tesla. A similar circuit has already been used in a very high magnetic field (60 T) when the team performed the experiments at Los Alamos National Lab.""When we first installed the dilution refrigerator, the joke was that my lab had the coldest temperatures in Iowa,"" said Prozorov, who conducts his research where Midwestern winters are no laughing matter. ""But we were not doing this just for fun, to see how cold we could go. Many unusual quantum properties of materials can only be uncovered at these extremely low temperatures.""The group studied pairing symmetry in several unconventional superconductors, mapped a very complex phase diagram in a system with field-induced quantum critical behavior, and recently uncovered very unusual properties of a spin-ice system, ""none of which would be possible without this setup,"" said Prozorov.Story Source:Materials provided by DOE/Ames Laboratory. ",0.107286,0.142604,0.153898,0.179331,0.138601,0.183948,0.179132,0,0
695,Research on light-matter interaction could improve electronic and optoelectronic devices Fundamental research sheds light on new many-particle quantum physics in atomically thin semiconductors,"A paper published in Nature Communications by Sufei Shi, assistant professor of chemical and biological engineering at Rensselaer, increases our understanding of how light interacts with atomically thin semiconductors and creates unique excitonic complex particles, multiple electrons, and holes strongly bound together. These particles possess a new quantum degree of freedom, called ""valley spin."" The ""valley spin"" is similar to the spin of electrons, which has been extensively used in information storage such as hard drives and is also a promising candidate for quantum computing.The paper, titled ""Revealing the biexciton and trion-exciton complexes in BN encapsulated WSe2,"" was published in the Sept. 13, 2018, edition of Nature Communications. Results of this research could lead to novel applications in electronic and optoelectronic devices, such as solar energy harvesting, new types of lasers, and quantum sensing.Shi's research focuses on low dimensional quantum materials and their quantum effects, with a particular interest in materials with strong light-matter interactions. These materials include graphene, transitional metal dichacogenides (TMDs), such as tungsten diselenide (WSe2), and topological insulators.TMDs represent a new class of atomically thin semiconductors with superior optical and optoelectronic properties. Optical excitation on the two-dimensional single-layer TMDs will generate a strongly bound electron-hole pair called an exciton, instead of freely moving electrons and holes as in traditional bulk semiconductors. This is due to the giant binding energy in monolayer TMDs, which is orders of magnitude larger than that of conventional semiconductors. As a result, the exciton can survive at room temperature and can thus be used for application of excitonic devices.As the density of the exciton increases, more electrons and holes pair together, forming four-particle and even five-particle excitonic complexes. An understanding of the many-particle excitonic complexes not only gives rise to a fundamental understanding of the light-matter interaction in two dimensions, it also leads to novel applications, since the many-particle excitonic complexes maintain the ""valley spin"" properties better than the exciton. However, despite recent developments in the understanding of excitons and trions in TMDs, said Shi, an unambiguous measure of the biexciton-binding energy has remained elusive.""Now, for the first time, we have revealed the true biexciton state, a unique four-particle complex responding to light,"" said Shi. ""We also revealed the nature of the charged biexciton, a five-particle complex.""At Rensselaer, Shi's team has developed a way to build an extremely clean sample to reveal this unique light-matter interaction. The device was built by stacking multiple atomically thin materials together, including graphene, boron nitride (BN), and WSe2, through van der Waals (vdW) interaction, representing the state-of-the-art fabrication technique of two-dimensional materials.This work was performed in collaboration with the National High Magnetic Field Laboratory in Tallahasee, Florida, and researchers at the National Institute for Materials Science in Japan, as well as with Shengbai Zhang, the Kodosky Constellation Professor in the Department of Physics, Applied Physics, and Astronomy at Rensselaer, whose work played a critical role in developing a theoretical understanding of the biexciton.The results of this research could potentially lead to robust many-particle optical physics, and illustrate possible novel applications based on 2D semiconductors, Shi said. Shi has received funding from the Air Force Office of Scientific Research. Zhang was supported by the Department of Energy, Office of Science.Story Source:Materials provided by Rensselaer Polytechnic Institute. ",0.0758446,0.133707,0.122203,0.14134,0.120721,0.137082,0.140406,0,0
696,"Topology, symmetry and magnetism: Heusler, Weyl and Berry ","Fritz Heusler (1866-1947), Hermann Weyl (1885-1955) and Michael Berry (1941-) are three renowned scientists whose work has led to new and important insights into materials science, topology and condensed-matter physics. These three fields of science have come together recently with the discovery of new and exciting quantum properties in new classes of material which could enable new science including computing technologies and catalysis.Heusler is the name of the discoverer of mostly magnetic compounds that were of interest quite some time ago. But these compounds were recently found to host non-trivial topological properties that open a large field of novel physics. Hidden in the energy band structure of these materials are singular points that can be described with mathematical tools that originate from Weyl; these points are associated with the discovery of quasi-particles that are now called Weyl fermions. They are not found among the elementary particles of high energy physics, but we believe they exist in solid materials and determine their topology. The third name Berry stands for the measurable effects that reveal the physics at hand. Under certain well defined conditions there exists a vector field, similar to the magnetic field, called the Berry curvature. It determines the magnitude of a number of important effects, such as the anomalous Hall Effect and the Spin Hall Effect. It is the art of the experimentalist to suitably modify the materials in order to tune the Berry curvature and thus render the topology visible. In this review a great number of examples are given for various symmetry properties of Heusler compounds, a large class of materials that can easily be tuned to display ferromagnetic, antiferromagnetic, non-collinear or compensated magnetic order. These magnetic orderings give rise to pronounced electric and thermoelectric effects whose fingerprints are uncovered and explained including particle-like vortex spin structures, the antiskyrmions that are typical for a certain subset of Heusler compounds.Considering the large number of existing inorganic compounds and the recently proposed large number of nonmagnetic topological materials, Heusler compounds serve as a model system for the understanding and impact of magnetism on topology. Breaking time reversal symmetry via magnetism or an external magnetic field can lead to even larger effects than in non-magnetic materials based on the large separation between Weyl points of different chiralities. Based on a systematic study of Heusler materials we predict that there are a huge number of magnetic topological materials awaiting to be discovered.With regard to applications, the large Nernst effect and classical and quantum Hall effects around room temperature based on the high Curie temperatures of Heusler compounds and their relatives have the potential to have great impact in energy conversion and quantum electronic devices for spintronics or quantum computing.Story Source:Materials provided by Max Planck Institute for Chemical Physics of Solids. ",0.0813008,0.141863,0.120878,0.168908,0.111893,0.168758,0.150433,0,0
697,Nanoscale pillars as a building block for future information technology,"Researchers from Linkeping University and the Royal Institute of Technology in Sweden propose a new device concept that can efficiently transfer the information carried by electron spin to light at room temperature -- a stepping stone towards future information technology. They present their approach in an article in Nature Communications.In today's information technology, light and electron charge are the main media for information processing and transfer. In the search for information technology that is even faster, smaller and more energy-efficient, scientists around the globe are exploring another property of electrons -- their spin. Electronics that exploit both the spin and the charge of the electron are called ""spintronics.""Just as the Earth spins around its own axis, an electron spins around its own axis, either clockwise or counterclockwise. The handedness of the rotation is referred to as spin-up and spin-down states. In spintronics, the two states represent the binary bits of 0 and 1 and thus carry information. The information encoded by these spin states can in principle be converted by a light-emitting device into light, which then carries the information over a long distance through optic fibres. Such transfer of quantum information opens the possibility of future information technology that exploits both electron spin and light, and the interaction between them, a technology known as ""opto-spintronics.""The information transfer in opto-spintronics is based on the principle that the spin state of the electron determines the properties of the emitted light. More specifically, it is chiral light, in which the electric field rotates either clockwise or counter-clockwise when seen in the direction of travel of the light. The rotation of the electric field is determined by the direction of spin of the electron. But there is a catch.""The main problem is that electrons easily lose their spin orientations when the temperature rises. A key element for future spin-light applications is efficient quantum information transfer at room temperature, but at room temperature the electron spin orientation is nearly randomized. This means that the information encoded in the electron spin is lost or too vague to be reliably converted to its distinct chiral light,"" says Weimin Chen at the Department of Physics, Chemistry and Biology, IFM, at Linkeping University.Now, researchers from Linkeping University and the Royal Institute of Technology have devised an efficient spin-light interface.""This interface can not only maintain and even enhance the electron spin signals at room temperature. It can also convert these spin signals to corresponding chiral light signals travelling in a desired direction,"" says Weimin Chen.The key element of the device is extremely small disks of gallium nitrogen arsenide, GaNAs. The disks are only a couple of nanometres high and stacked on top of each other with a thin layer of gallium arsenide (GaAs) between to form chimney-shaped nanopillars. For comparison, the diameter of a human hair is about a thousand times larger than the diameter of the nanopillars.The unique ability of the proposed device to enhance spin signals is due to minimal defects introduced into the material by the researchers. Fewer than one out of a million gallium atoms are displaced from their designated lattice sites in the material. The resulting defects in the material act as efficient spin filters that can drain electrons with an unwanted spin orientation and preserve those with the desired spin orientation.""An important advantage of the nanopillar design is that light can be guided easily and more efficiently coupled in and out,"" says Shula Chen, first author of the article.The researchers hope that their proposed device will inspire new designs of spin-light interfaces, which hold great promise for future opto-spintronics applications.Story Source:Materials provided by Linkeping University. ",0.145673,0.162541,0.145822,0.187981,0.191514,0.204203,0.176862,0,0
698,Neutrons scan magnetic fields inside samples,"Measuring magnetic fields inside samples has only been possible indirectly up to now. Magnetic orientations can be scanned with light, X-rays, or electrons -- but only on the surfaces of materials. Neutrons, on the other hand, penetrate deeply into the sample and thanks to their own magnetic orientation can provide precise information about the magnetic fields inside. So far, however, it has only been possible to roughly map the variously aligned magnetic domains using neutrons, but not the vector fields (directions and strengths) of the magnetic fields inside samples.Spin polarisation is the keyA team led by Dr. Nikolay Kardjilov and Dr. Ingo Manke at the HZB has now developed a new method for measuring the magnetic field lines inside massive, thick samples: For tensorial neutron tomography, they employ spin filters, spin flippers, and spin polarisers that allow only neutrons with mutually aligned spins to penetrate the sample. When these spin-polarised neutrons encounter a magnetic field inside, the field excites the neutron spins to precess, so that the direction of the spin polarisation changes, allowing conclusions to be drawn about the field lines encountered.3D image calculated with new TMART algorithmThe newly developed experimental method enables a three-dimensional image of the magnetic field inside the sample to be calculated using nine individual tomographic scans, each with a different neutron spin setting. A highly complex mathematical tensor algorithm was newly developed for this purpose by Dr. Andre Hilger at the HZB, christened TMART.The experts tested and evaluated the new method on well-understood samples. Subsequently, they were able to map the complex magnetic field inside superconducting lead for the first time.Flux lines inside superdonductorsThe sample of solid, polycrystalline lead was cooled to 4 Kelvin (lead becomes superconducting below 7 Kelvin) and exposed to a magnetic field of 0.5 millitesla. Although the magnetic field is displaced from the interior of the sample due to the Meissner effect, magnetic flux lines nevertheless remain attached to the (non-superconducting) grain boundaries of the polycrystalline sample. These flux lines do not disappear even after the external field has been switched off, because they have previously induced currents inside the superconducting crystal grains, which now maintain these fields.""For the first time, we can make the magnetic vector field visible in three dimensions in all its complexity within a massive material,"" says HZB physicist Manke. ""Neutrons can simultaneously penetrate massive materials and detect magnetic fields. There is currently no other method that can accomplish this.""Applications from basic research to industryMagnetic tensor tomography is non-destructive and can achieve resolutions down to the micrometer range. The areas of application are extremely diverse. They range from the mapping of magnetic fields in superconductors and the observation of magnetic phase transitions, to material analysis, which is also of great interest for industry: Field distributions in electric motors and metallic components can be mapped and current flows in batteries, fuel cells, or other propulsion systems can be visualized with this method.Story Source:Materials provided by Helmholtz-Zentrum Berlin fer Materialien und Energie. ",0.133385,0.143744,0.168018,0.191596,0.150183,0.234316,0.182806,0,0
699,Breaking supersymmetry,"Supersymmetry predicts a relationship between the fundamental particles fermions and bosons. An extended version of a pioneering model of non-relativistic supersymmetry-the Nicolai supersymmetric fermion lattice model- is studied.Previously, it was verified that supersymmetry of the model breaks down only when the adjustable constant g > g0 ? 4/?. A researcher at Kanazawa University removed the restriction on g and showed that the extended version of the Nicolai supersymmetric fermion lattice model breaks supersymmetry dynamically for any nonzero g.The remarkable discoveries and theories of physicists since the 1930s have shown that all matter in the universe is made from a small number of basic building blocks called fundamental particles. However, this isn't the complete story. Supersymmetry is a hypothesis in high-energy physics that aims to fill some of the gaps.Hajime Moriya from the Institute of Science and Engineering at Kanazawa University has shown that for an extended version of a pioneering model in non-relativistic supersymmetry-the Nicolai supersymmetric fermion lattice model-supersymmetry is broken for any nonzero value of a particular adjustable constant.Supersymmetry predicts that two basic classes of fundamental particles, fermions and bosons, accompany each other in the same representation. Fermions, such as quarks, have a half a unit of spin, which is an intrinsic form of angular momentum, and bosons, such as photons, have zero, one, or two units of spin. In 1976, Hermann Nicolai proposed the fermion lattice model, which is made by fermions with no bosons, but supersymmetry is still satisfied.Nicolai's original model was extended by Noriaki Sannomiya et al., who showed that for any nonzero adjustable constant g ? ? on finite systems, supersymmetry breaks down. However, in the infinite-volume limit, they verified that supersymmetry breaks down only when g > g0 ? 4/?. ""This restriction on parameter g seems to be technical,"" says Moriya, ""and its meaning in terms of physics is unclear.""So, Moriya considered spinless fermions over an infinitely extended lattice and removed the restriction on g in the case of the infinite-volume limit. Moriya showed that for any nonzero g, the extended Nicolai model breaks supersymmetry dynamically. In addition, the original Nicolai model has been shown to have highly degenerate vacua, also known as supersymmetric ground states. Moriya also proved that for any nonzero g, the energy density of any homogeneous ground state for the extended Nicolai model is strictly positive.""Even if supersymmetry is broken for any finite subsystem, it may be restored in the infinite-volume limit,"" explains Moriya, ""as exemplified by some supersymmetry quantum mechanical model."" So, Moriya showed that such a restoration does not occur for the extended Nicolai model. ""The breaking of supersymmetry is verified in a rather model-independent manner by applying C*-algebraic techniques, which seem not well known in physics community,"" adds Moriya.Story Source:Materials provided by Kanazawa University. ",0.135384,0.156699,0.195982,0.26042,0.179677,0.214699,0.222136,0,0
700,Graphene bilayer provides efficient transport and control of spins,"University of Groningen physicists in collaboration with a theoretical physics group from Universitet Regensburg have built an optimized bilayer graphene device which displays both long spin lifetimes and electrically controllable spin-lifetime anisotropy. It has the potential for practical applications such as spin-based logic devices. The results were published in Physical Review Letters on 20 September.The tremendous development of computer systems over the last 60 years has increased their capability enabling them to spread into nearly all aspects of our daily life. The development approach of the last decades has been to miniaturize the elements on a computer chip. This has now reached scales below 100 atoms and is approaching its fundamental limit. Since the increasing range of applications makes higher demands of performance and energy efficiency, new concepts are required which can provide enhanced functionalities.SpintronicsIn this context, researchers are studying the use of spins for the transport and storage of information. Spin is a quantum mechanical property of electrons, which gives them a magnetic moment that could be used to transfer or store information. The field of spin-based electronics (spintronics) has already made its way into the hard drives of computers, and also promises to revolutionize the processing units.A focus of spintronics research is on optimizing materials for the transport and control of spins. Graphene is an excellent conductor of electron spins, but it is hard to control spins in this material because of their weak interaction with the carbon atoms (the spin-orbit coupling). Previous work by the University of Groningen Physics of Nanodevices group led by Professor Bart van Wees placed graphene in close proximity to a transition metal dichalcogenide, a layered material with a high intrinsic spin-orbit coupling strength. The high spin-orbit coupling strength was transferred to graphene via a short-range interaction at the interface. This made it possible to control the spin currents, but was at the cost of reduced spin life.Control of spin currentsIn the new study, the researchers managed to control spin currents in a graphene bilayer. 'This was actually predicted in a theoretical paper in 2012, but the technology to measure the effect accurately only became available recently', explains Christian Leutenantsmeyer, a Ph.D. student in the Van Wees group and first author of the PRL paper. The paper is a collaboration between the Van Wees group and a theoretical physics group from Universitet Regensburg in Germany.The 2012 paper predicted anisotropic spin transport in graphene bilayers as a consequence of spin-orbit coupling in bilayer graphene. Anisotropic spin transport describes the situation in which spins pointing either in or out of the graphene plane are conducted with different efficiencies. This was indeed observed in the devices Leutenantsmeyer and his colleagues produced.InsightThe spin current could also be controlled using spin-lifetime anisotropy since in-plane spins live much shorter than out-of-plane ones, and could be used in devices to polarize spin currents. Leutenantsmeyer: 'We found that the strength anisotropy is comparable to graphene/transition metal dichalcogenide devices, but we observed a 100 times larger spin lifetime. We therefore achieved both efficient spin transport and efficient control of spins.'The work provides insight into the fundamental properties of spin-orbit coupling in bilayer graphene. 'And furthermore, our findings open up new avenues for the efficient electrical control of spins in high-quality graphene, a milestone for graphene.'Story Source:Materials provided by University of Groningen. ",0.171322,0.182443,0.172169,0.22521,0.179574,0.263207,0.200822,0,0
701,Light provides spin Dynamics of electrons in perovskite crystals deciphered,"Physicists at Friedrich-Alexander-Universitet Erlangen-Nernberg (FAU) have proven that incoming light causes the electrons in warm perovskites to rotate thus influencing the direction of the flow of electrical current. They have thus found the key to an important characteristic of these crystals, which could play an important role in the development of new solar cells.Efficiency from spinning electronsThe sun plays an important role in the use of renewable energy sources. Its radiation energy provides heat and the light it provides can be converted into electricity thanks to photovoltaics. Perovskites, which are crystalline compounds that can be simply manufactured using chemical processes, have been considered a promising means of using the power of sunlight cost effectively for several years now. Under laboratory conditions, prototypes have achieved surprising levels of efficiency.There is little knowledge about precisely why perovskites are so powerful. 'Two factors are decisive for generating electrical energy cost-efficiently from sunlight', says Dr. Daniel Niesner from the Chair of Solid State Physics at FAU. 'One the one hand, the light must excite as many electrons as possible in a layer that's as thin as possible. On the other, the electrons must be able to flow as freely as possible to the electrodes that pick up the current.' Researchers suspect that perovskites make particularly good use of the rotation of electrons for efficient current flow. 'Each electron has 'spin', similar to the intrinsic rotation of a billiard ball', explains Niesner. 'As is the case with billiard balls, where left-hand or right-hand spin when they are hit with the cue leads to a curved path on the table, scientists have suspected that rotation and forward movement in electrons in perovskites could also be linked.'Orderly atomic structurePhysicists at FAU in Erlangen have now confirmed this suspicion for the first time. In their experiments, they used a laser whose light also has spin or a direction of rotation. The result: If a crystal is exposed to light with a left-hand spin, the electrons move to the left. If the direction of the light is reversed, the direction of the flow of electrons also reverses. 'The experiments clearly demonstrate that the direction of rotation of the electrons and the direction of flow of current are linked.'Up to now, scientists presumed that the atomic structure of perovskites was too 'orderly' for such behaviour. In actual fact, experiments with cooled perovskite crystals show only a very weak link between the direction of rotation of the electrons and the direction of current flow. 'This changes, however, when the crystals are heated to room temperature because the movement of the atoms leads to fluctuating deviations of the highly-ordered structure', says Nieser. 'The heat enables the crystals of perovskite to link the direction of rotation and flow of the electrons. A 'normal' crystal couldn't do that.'The discovery of the connection between heat and spin in electrons means that the FAU researchers have uncovered a vital aspect of the unusual flow of current in perovskites. Their work could contribute to improving the understanding of the high energy efficiency of these crystals and to developing new materials for photovoltaics in the future.Story Source:Materials provided by University of Erlangen-Nuremberg. ",0.112567,0.133371,0.117942,0.135306,0.135335,0.167164,0.141528,0,0
702,Organic ferromagnetism: Trapping spins in glassy state,"An international team of researchers, affiliated UNIST has introduced an exiting new organic network structure that shows pure organic ferromagnetic property at room temperature. As described in the CHEM journal this pure organic material exhibits ferromagnetism from pure p-TCNQ without any metal contamination.This breakthrough has been led by by Professor Jong-Beom Baek and his research team in the School of the Energy and Chemical Engineering at UNIST. In the study, the research team has synthesized a network structure from the self polymerization of tetracyanoquinodimethane (TCNQ) monomer. The designed organic network structure generates stable neutral radicals.For over two decades, there has been widespread scepticism around claims of organic plastic ferromagnetism, mostly due to contamination by transition metals. Extensive effort has been devoted to developing magnets in purely organic compounds based on free radicals, driven by both scientific curiosity and the potential applications of a 'plastic magnet'. Excluding the contamination issues and realizing magnetic properties from pure organic plastics must occur to revive the quest for plastic magnetism.This research work reports the design, synthesis and magnetic properties of a triazine network structure demonstrating room-temperature ferromagnetism derived from pure organic material. The polymer network was realized through the self-polymerization of TCNQ in trifluoromethanesulfonic acid (TFMSA) at 155 eC . Highly stable free radicals are generated by twisting ? bonds around the triazine rings and by trapping in the glassy state of a polymerized TCNQ (p-TCNQ) network structure.The presence of unpaired electrons (radicals) in the p-TCNQ was confirmed by solid-state electron spin resonance (ESR) spectroscopy and magnetic characterization revealed the presence of spin e moments, which leads to ferromagnetic ordering with a critical temperature significantly higher than room temperature. The experimental results were supported by rigorous theoretical calculation to verify the origin of organic ferromagnetism.This study has been jointly conducted by researchers, Javeed Mahmood of Energy and Chemical Engineering, Jungmin Park of Material Science and Engineering, and Dongbin Shin of Department of Physics at UNIST. Professor Jong-Beom Baek, Professor Jung-Woo Yoo of Material Science and Engineering supervised the project as a corresponding authors of this study.""Our study not only suggests new directions in the organic magnetic materials, but also presents a wide range of possibilities for designing new structures with neutral stable radicals, which show ferromagnetic ordering,"" says Professor Baek. ""This material is expected to attract attention in many areas because of the scientific curiosity and the potential applications of plastic magnets.""Story Source:Materials provided by Ulsan National Institute of Science and Technology(UNIST). ",0.1026,0.16391,0.174054,0.173867,0.128865,0.218954,0.166267,0,0
703,New devices could reduce excess heat in computers Physicists explore long-distance information transmission in antiferromagnetic iron oxide,"Scientists have succeeded in observing the first long-distance transfer of information in a magnetic group of materials known as antiferromagnets. These materials make it possible to achieve computing speeds much faster than existing devices. Conventional devices using current technologies have the unwelcome side effect of getting hot and being limited in speed. This is slowing down the progress of information technology.The emerging field of magnon spintronics aims to use insulating magnets capable of carrying magnetic waves, known as magnons, to help solve these problems. Magnon waves are able to carry information without the disadvantage of the production of excess heat. Physicists at Johannes Gutenberg University Mainz (JGU) in Germany, in cooperation with theorists from Utrecht University in the Netherlands and the Center for Quantum Spintronics (QuSpin) at the Norwegian University of Science and Technology (NTNU) in Norway, demonstrated that antiferromagnetic iron oxide, which is the main component of rust, is a cheap and promising material to transport information with low excess heating at increased speeds. Their study has been published recently in the scientific journal Nature.By reducing the amount of heat produced, components can continue to become smaller alongside an increased information density. Antiferromagnets, the largest group of magnetic materials, have several crucial advantages over other commonly used magnetic components based on iron or nickel. For example, they are stable and unaffected by external magnetic fields, which is a key requirement for future data storage. Additionally, antiferromagnet-based devices can be potentially operated thousands of times faster than current technologies as their intrinsic dynamics are in the terahertz range, potentially exceeding a trillion operations per second.Fast computers with antiferromagnetic insulators are within reachIn their study, the researchers used platinum wires on top of the insulating iron oxide to allow an electric current to pass close by. This electric current leads to a transfer of energy from the platinum into the iron oxide, thereby creating magnons. The iron oxide was found to carry information over the large distances needed for computing devices. ""This result demonstrates the suitability of antiferromagnets to replace currently used components,"" said Dr. Romain Lebrun from the JGU Institute of Physics. ""Devices based on fast antiferromagnet insulators are now conceivable,"" he continued.Andrew Ross, one of the lead authors of the study, added: ""If you are able to control insulating antiferromagnets, they can operate without excessive heat production and are robust against external perturbations."" Professor Mathias Kleui, senior author of the paper, commented on the joint effort: ""I am very happy that this work was achieved as an international collaboration. Internationalization is a key aim of our research group and in particular of our Graduate School of Excellence Materials Science in Mainz and the spintronics research center Spin+X. Collaborations with leading institutions globally, like the Center for Quantum Spintronics and Utrecht University enable such exciting research.""Story Source:Materials provided by Johannes Gutenberg Universitaet Mainz. ",0.0745565,0.109888,0.0946818,0.137071,0.105396,0.145981,0.124334,0,0
704,Novel materials: Probing individual edge states with unprecedented precision,"A new technique makes it possible to obtain an individual fingerprint of the current-carrying edge states occurring in novel materials such as topological insulators or 2D materials. Physicists of the University of Basel present the new method together with American scientists in Nature Communications.While insulators do not conduct electrical currents, some special materials exhibit peculiar electrical properties: though not conducting through their bulk, their surfaces and edges may support electrical currents due to quantum mechanical effects, and do so even without causing losses.Such so-called topological insulators have attracted great interest in recent years due to their remarkable properties. In particular, their robust edge states are very promising since they could lead to great technological advances.Currents flowing only along the edgesSimilar effects as the edge states of such topological insulators also appear when a two-dimensional metal is exposed to a strong magnetic field at low temperatures. When the so-called quantum Hall effect is realized, current is thought to flow only at the edges, where several conducting channels are formed.Probing individual edge statesUntil now, it was not possible to address the numerous current carrying states individually or to determine their positions separately. The new technique now makes it possible to obtain an exact fingerprint of the current carrying edge states with nanometer resolution.This is reported by researchers of the Department of Physics and the Swiss Nanoscience Institute of the University of Basel in collaboration with colleagues of the University of California, Los Angeles, as well as of Harvard and Princeton University, USA.In order to measure the fingerprint of the conducting edge states, the physicists lead by Prof. Dominik Zumbehl have further developed a technique based on tunneling spectroscopy.They have used a gallium arsenide nanowire located at the sample edge which runs in parallel to the edge states under investigation. In this configuration, electrons may jump (tunnel) back and forth between a specific edge state and the nanowire as long as the energies in both systems coincide. Using an additional magnetic field, the scientists control the momentum of tunneling electrons and can address individual edge states. From the measured tunneling currents, the position and evolution of each edge state may be obtained with nanometer precision.Tracking the evolutionThis new technique is very versatile and can also be used to study dynamically evolving systems. Upon increasing the magnetic field, the number of edge states is reduced, and their distribution is modified. For the first time, the scientists were able to watch the full edge state evolution starting from their formation at very low magnetic fields.With increasing magnetic field, the edge states are first compressed towards the sample boundary until eventually, they move towards the inside of the sample and then disappear completely. Analytical and numerical models developed by the research team agree very well with the experimental data.""This new technique is not only very useful to study the quantum Hall edge states,"" Dominik Zumbehl comments the results of the international collaboration. ""It might also be employed to investigate new exotic materials such as topological insulators, graphene or other 2D materials.""Story Source:Materials provided by University of Basel. ",0.109621,0.12617,0.141099,0.167476,0.141846,0.167447,0.156083,0,0
705,Magnetization in small components can now be filmed in the laboratory Technique for imaging magnetization dynamics developed in a joint project,"In the future, today's electronic storage technology may be superseded by devices based on tiny magnetic structures. These individual magnetic regions correspond to bits and need to be as small as possible and capable of rapid switching. In order to better understand the underlying physics and to optimize the components, various techniques can be used to visualize the magnetization behavior. Scientists at Johannes Gutenberg University Mainz (JGU) in Germany have now refined an electron microscope-based technique that makes it possible not only to capture static images of these components but also to film the high-speed switching processes. They have also employed a specialized signal processing technology that suppresses image noise.""This provides us with an excellent opportunity to investigate magnetization in small devices,"" Daniel Schenke of the JGU Institute of Physics explained. The research was carried out in cooperation with Surface Concept GmbH and the results have been published in the journal Review of Scientific Instruments.Scanning electron microscopy with polarization analysis is a lab-based technique for imaging magnetic structures. Compared with optical methods, it has the advantage of high spatial resolution. The main disadvantage is the time it takes to acquire an image in order to achieve a good signal-to-noise ratio. However, the time required to measure the periodically excited and therefore periodically changing magnetic signal can be shortened by using a digital phase-sensitive rectifier that only detects signals of the same frequency as the excitation.Such signal processing requires measurements to be time-resolved. The instrumentation developed by the scientists at JGU provides a time resolution of better than 2 nanoseconds. As a result, the technique can be employed to investigate high-speed magnetic switching processes. It also makes it possible to both capture images and select individual images at a defined point in time within the entire excitation phase.New technique compares favorably with more complex imaging techniquesThis development means the technique is now comparable with the much more complex imaging techniques used at large accelerator facilities and opens up the possibility of investigating the magnetization dynamics of small magnetic components in the laboratory.The research was carried out within the framework of the Collaborative Research Center CRC/Transregio 173 ""Spin+X: Spin in its collective environment,"" which is based at Johannes Gutenberg University Mainz and TU Kaiserslautern and financed by the German Research Foundation (DFG). The CRC/TRR involves interdisciplinary teams of researchers from the fields of chemistry, physics, mechanical engineering, and process engineering, who undertake research into magnetic effects with a view to converting these into applications. The primary focus is on the phenomenon of spin. Physicists use this term to refer to the intrinsic angular momentum of a quantum particle, such as an electron or proton. This underlies many magnetic effects.The development of the novel technique results from the successful and close collaboration of the researchers with the company Surface Concept GmbH, a spin-off of Johannes Gutenberg University Mainz.Story Source:Materials provided by Johannes Gutenberg Universitaet Mainz. ",0.0796954,0.0913936,0.100946,0.151375,0.116222,0.141445,0.123854,0,0
706,Novel nano material for quantum electronics,"An international team led by Assistant Professor Kasper Steen Pedersen, DTU Chemistry, has synthesized a novel nano material with electrical and magnetic properties making it suitable for future quantum computers and other applications in electronics.Chromium-Chloride-Pyrazine (chemical formula CrCl2(pyrazine)2) is a layered material, which is a precursor for a so-called 2D material. In principle, a 2D material has a thickness of just a single molecule and this often leads to properties very different from those of the same material in a normal 3D version. Not least will the electrical properties differ. While in a 3D material, electrons are able to take any direction, in a 2D material they will be restricted to moving horizontally -- as long as the wavelength of the electron is longer than the thickness of the 2D layer.Organic/inorganic hybridGraphene is the most well-known 2D material. Graphene consists of carbon atoms in a lattice structure, which yields it remarkable strength. Since the first synthesis of graphene in 2004, hundreds of other 2D materials have been synthesized, some of which may be candidates for quantum electronics applications. However, the novel material is based on a very different concept. While the other candidates are all inorganic -- just like graphene -- Chromium-Chloride-Pyrazine is an organic/inorganic hybrid material.""The material marks a new type of chemistry, in which we are able to replace various building blocks in the material and thereby modify its physical and chemical properties. This can not be done in graphene. For example, one can't choose to replace half the carbon atoms in graphene with another kind of atoms. Our approach allows designing properties much more accurately than known in other 2D materials,"" Kasper Steen Pedersen explains.Besides the electrical properties, also the magnetic properties in Chromium-Chloride-Pyrazine can be accurately designed. This is especially relevant in relation to ""spintronics.""""While in normal electronics, only the charge of the electrons is utilized, also their spin -- which is a quantum mechanical property -- is used in spintronics. This is highly interesting for quantum computing applications. Therefore, development of nano-scale materials which are both conducting and magnetic is most relevant,"" Kasper Steen Pedersen notes.A new world of 2D materialsBesides for quantum computing, Chromium-Chloride-Pyrazine may be of interest in future superconductors, catalysts, batteries, fuel cells, and electronics in general.Still, companies are not keen to begin producing the material right away, the researcher stresses: ""Not yet, at least! This is still fundamental research. Since we are suggesting a material synthesized from an entirely novel approach, a number of questions remain unanswered. For instance, we are not yet able to determine the degree of stability of the material in various applications. However, even if Chromium-Chloride-Pyrazine should for some reason prove unfit for the various possible applications, the new principles behind its synthesis will still be relevant. This is the door to a new world of more advanced 2D materials opening up.""Story Source:Materials provided by Technical University of Denmark. ",0.0786684,0.174649,0.143937,0.170553,0.127652,0.142123,0.167189,0,0
707,Ultracold atoms used to verify 1963 prediction about 1D electrons Theory is increasingly relevant to chipmakers,"Rice University atomic physicists have verified a key prediction from a 55-year-old theory about one-dimensional electronics that is increasingly relevant thanks to Silicon Valley's inexorable quest for miniaturization.""Chipmakers have been shrinking feature sizes on microchips for decades, and device physicists are now exploring the use of nanowires and nanotubes where the channels that electrons pass through are almost one-dimensional,"" said Rice experimental physicist Randy Hulet. ""That's important because 1D is a different ballgame in terms of electron conductance. You need a new model, a new way of representing reality, to make sense of it.""With IBM and others committed to incorporating one-dimensional carbon nanotubes into integrated circuits, chip designs will increasingly need to account for 1D effects that arise from electrons being fermions, antisocial particles that are unwilling to share space.The 1D implications of this standoffishness caught the attention of physicists Sin-Itiro Tomonaga and J.M. Luttinger, whose model of 1D electron behavior was published in 1963. A key prediction of Tomonaga-Luttinger liquid (TLL) theory is that exciting one electron in a 1D wire leads to a collective, organized response from every electron in the wire.Stranger still, because of this collective behavior, TLL theory predicts that a moving electron in 1D will seemingly split in two and travel at different speeds, despite the fact that electrons are fundamental particles that have no constituent parts. This strange breakup, known as spin-charge separation, instead involves two inherent properties of the electron -- negative charge and angular momentum, or ""spin.""In a study online this week in Physical Review Letters, Hulet, University of Geneva theoretical physicist Thierry Giamarchi and their colleagues used another type of fermion -- ultracold lithium atoms cooled to within 100 billionths of a degree of absolute zero -- to both verify the predicted speed that charge waves move in 1D and offer confirmation that 1D charge waves increase their speed in proportion to the strength of the interaction between them.""In a one-dimensional wire, electrons can move to the left or to the right, but they cannot go around other electrons,"" said Hulet, Rice's Fayez Sarofim Professor of Physics. ""If you add energy to the system, they move, but because they're fermions and can't share space, that movement, or excitation, causes a kind of chain reaction.""One electron moves, and it nudges the next one to move and the one next to that one and so on, causing the energy you've added to move down the wire like a wave,"" Hulet said. ""That single excitation has created a ripple everywhere in the wire.""In their experiments, Hulet's team used lithium atoms as stand-ins for electrons. The atoms are trapped and slowed with lasers that oppose their motion. The slower they go, the colder the lithium atoms become, and at temperatures far colder than any in nature, the atoms behave like electrons. More lasers are used to form optical waveguides, one-dimensional tubes wide enough for just one atom. Despite the effort needed to create these conditions, Hulet said the experiments offer a big advantage.""We can use a magnetic field in our experiment to tune the strength of the repulsive interaction between the lithium atoms,"" Hulet said. ""In studying these collective, or correlated electron behaviors, interaction strength is an important factor. Stronger or weaker electron interactions can produce wholly different effects, but it's extraordinarily difficult to study this with electrons because of the inability to directly control interactions. With ultracold atoms, we can essentially dial the interaction strength to any level we want and watch what happens.""While previous groups have measured the speed of collective waves in nanowires and in gases of ultracold atoms, none had measured it as a function of interaction strength, Hulet said.""Charge excitations are predicted to move faster with increasing interaction strength, and we showed that,"" he said. ""Thierry Giamarchi, who literally wrote the book on this topic, used TLL theory to predict how the charge wave would behave in our ultracold atoms, and his predictions were borne out in our experiments.""Having that ability to control interactions also sets the stage for testing the next TLL prediction: The speed of charge waves and spin waves diverge with increasing interaction strength, meaning that as electrons are made to repel one another with greater force, charge waves will travel faster and spin waves will travel slower.Now that the team has verified the predicted behavior of charge waves, Hulet said they next plan to measure spin waves to see if they behave as predicted.""The 1D system is a paradigm for strongly correlated electron physics, which plays a key role in many things we'd like to better understand, like high-temperature superconductivity, heavy fermion materials and more,"" Hulet said.Story Source:Materials provided by Rice University. Original written by Jade Boyd. ",0.0742766,0.115602,0.10068,0.127493,0.114083,0.127676,0.125422,0,0
708,New material could improve efficiency of computer processing and memory Discovery could have major impact on semiconductor industry,"A team of researchers led by the University of Minnesota has developed a new material that could potentially improve the efficiency of computer processing and memory. The researchers have filed a patent on the material with support from the Semiconductor Research Corporation, and people in the semiconductor industry have already requested samples of the material.The findings are published in Nature Materials, a peer-reviewed scientific journal published by Nature Publishing Group.""We used a quantum material that has attracted a lot of attention by the semiconductor industry in the past few years, but created it in unique way that resulted in a material with new physical and spin-electronic properties that could greatly improve computing and memory efficiency,"" said lead researcher Jian-Ping Wang, a University of Minnesota Distinguished McKnight Professor and Robert F. Hartmann Chair in electrical engineering.The new material is in a class of materials called ""topological insulators,"" which have been studied recently by physics and materials research communities and the semiconductor industry because of their unique spin-electronic transport and magnetic properties. Topological insulators are usually created using a single crystal growth process. Another common fabrication technique uses a process called Molecular Beam Epitaxy in which crystals are grown in a thin film. Both of these techniques cannot be easily scaled up for use in the semiconductor industry.In this study, researchers started with bismuth selenide (Bi2Se3), a compound of bismuth and selenium. They then used a thin film deposition technique called ""sputtering,"" which is driven by the momentum exchange between the ions and atoms in the target materials due to collisions. While the sputtering technique is common in the semiconductor industry, this is the first time it has been used to create a topological insulator material that could be scaled up for semiconductor and magnetic industry applications.However, the fact that the sputtering technique worked was not the most surprising part of the experiment. The nano-sized grains of less than 6 nanometers in the sputtered topological insulator layer created new physical properties for the material that changed the behavior of the electrons in the material. After testing the new material, the researchers found it to be 18 times more efficient in computing processing and memory compared to current materials.""As the size of the grains decreased, we experienced what we call 'quantum confinement' in which the electrons in the material act differently giving us more control over the electron behavior,"" said study co-author Tony Low, a University of Minnesota assistant professor of electrical and computer engineering.Researchers studied the material using the University of Minnesota's unique high-resolution transmission electron microscopy (TEM), a microscopy technique in which a beam of electrons is transmitted through a specimen to form an image.""Using our advanced aberration-corrected scanning TEM we managed to identify those nano-sized grains and their interfaces in the film,"" said Andre Mkhoyan, a University of Minnesota associate professor of chemical engineering and materials science and electron microscopy expert.Researchers say this is only the beginning and that this discovery could open the door to more advances in the semiconductor industry as well as related industries, such as magnetic random access memory (MRAM) technology.""With the new physics of these materials could come many new applications,"" said Mahendra DC (Dangi Chhetri), first author of the paper and a physics Ph.D. student in Professor Wang's lab.Wang agrees that this cutting-edge research could make a big impact.""Using the sputtering process to fabricate a quantum material like a bismuth-selenide-based topological insulator is against the intuitive instincts of all researchers in the field and actually is not supported by any existing theory,"" Wang said. ""Four years ago, with a strong support from Semiconductor Research Corporation and the Defense Advanced Research Projects Agency, we started with a big idea to search for a practical pathway to grow and apply the topological insulator material for future computing and memory devices. Our surprising experimental discovery led to a new theory for topological insulator materials.""Research is all about being patient and collaborating with team members. This time there was a big pay off,"" Wang said.Story Source:Materials provided by University of Minnesota. ",0.0652478,0.129552,0.115863,0.174786,0.129259,0.116667,0.145006,0,0
709,Link between magnetic field strength and temperature,"Researchers recently discovered that the strength of the magnetic field required to elicit a particular quantum mechanical process, such as photoluminescence and the ability to control spin states with electromagnetic (EM) fields, corresponds to the temperature of the material. Based on this finding, scientists can determine a sample's temperature to a resolution of one cubic micron by measuring the field strength at which this effect occurs. Temperature sensing is integral in most industrial, electronic and chemical processes, so greater spatial resolution could benefit commercial and scientific pursuits. The team reports their findings in AIP Advances, from AIP Publishing.In diamonds, nitrogen atoms can replace carbon atoms; when this occurs next to vacancies in the crystal lattice, it produces useful quantum properties. These vacancies can have a negative or neutral charge. Negatively charged vacancy centers are also photoluminescent and produce a detectable glow when exposed to certain wavelengths of light. Researchers can use a magnetic field to manipulate the spins of the electrons in the vacancies, which alters the intensity of the photoluminescence.A team of Russian and German researchers created a system that can measure temperatures and magnetic fields at very small resolutions. The scientists produced crystals of silicon carbide with vacancies similar to the nitrogen-vacancy centers in diamonds. Then, they exposed the silicon carbide to infrared laser light in the presence of a constant magnetic field and recorded the resulting photoluminescence.Stronger magnetic fields make it easier for electrons in these vacancies to transfer between energy spin states. At a specific field strength, the proportion of electrons with spin 3/2 quickly changes, in a process called anticrossing. The brightness of the photoluminescence depends on the proportion of electrons in various spin states, so the researchers could gauge the strength of the magnetic field by monitoring the change in brightness.Additionally, the luminescence abruptly changes when electrons in these vacancies undergo cross-relaxation, a process where one excited quantum system shares energy with another system in its ground state, bringing both to an intermediate state. The strength of the field needed to induce cross-relaxation is directly tied to the temperature of the material. By varying the strength of the field, and recording when photoluminescence suddenly changed, the scientists could calculate the temperature of the region of the crystal under investigation. The team was surprised to discover that the quantum effects remained even at room temperature.""This study allows us to create temperature and magnetic field sensors in one device,"" said Andrey Anisimov, of the Ioffe Physical-Technical Institute of the Russian Academy of Sciences and one of the authors of the paper. Moreover, sensors can be miniaturized to 100 nanometers, which would enable their use in the space industry, geophysical observations and even biological systems. ""In contrast to diamond, silicon carbide is already an available semiconductor material, and diodes and transistors are already made from it,"" Anisimov said.Story Source:Materials provided by American Institute of Physics. ",0.0965065,0.138594,0.135396,0.132507,0.13529,0.161559,0.134712,0,0
710,Energy-efficient spin current can be controlled by magnetic field and temperature SCMR effect simplifies the design of fundamental spintronic components,"The transition from light bulbs to LEDs has drastically cut the amount of electricity we use for lighting. Most of the electricity consumed by incandescent bulbs was, after all, dissipated as heat. We may now be on the verge of a comparable breakthrough in electronic computer components. Up to now, these have been run on electricity, generating unwanted heat. If spin current were employed instead, computers and similar devices could be operated in a much more energy-efficient manner. Dr. Olena Gomonay from Johannes Gutenberg University Mainz (JGU) in Germany and her team together with Professor Eiji Saitoh from the Advanced Institute for Materials Research (AIMR) at Tohoku University in Japan and his work group have now discovered an effect that could make such a transition to spin current a reality. This effect significantly simplifies the design of fundamental spintronic components.Touching a computer that has been running for some time, you will feel heat. This heat is an -- undesirable -- side effect of the electric current. Undesirable because the heat generated, naturally, also consumes energy. We are all familiar with this effect from light bulbs, which became so hot after being on for hours that they could burn your fingers. This is because light bulbs converted only a fraction of the energy required to do their job of creating light. The energy used by LEDs, on the other hand, is almost completely used for lighting, which is why they don't become hot. This makes LEDs significantly more energy-efficient than traditional incandescent bulbs.Instead of using an electric current composed of charged particles, a computer using a stream of particles with a spin other than zero could manipulate the material of its components in the same way to perform calculations. The primary difference is that no heat is generated, the processes are much more energy-efficient. Dr. Olena Gomonay from Mainz University and Professor Eiji Saitoh from Tohoku University have now laid the foundations for using these spin currents. More precisely, they have used the concept of spin currents and applied it to a specific material. Gomonay compares the spin currents involved with how our brains work: ""Our brains process immeasurable amounts of information, but they don't heat up in the process. Nature is, therefore, way ahead of us."" The team from Mainz is hoping to emulate this model.Drastic change in current flowHow well spin currents flow depends on the material -- just like in the case of electric current. While spin currents can always flow in ferromagnetic materials, in antiferromagnetic materials states with low resistance alternate with those with high resistance. ""We have now found a way to control spin currents by means of a magnetic field and temperature, in other words, to control the resistance of an antiferromagnetic system based on spin,"" explained Gomonay, summarizing her results.At a temperature close to the phase transition temperature, Gomonay and her team applied a small magnetic field to the material. While the applied magnetic field alters the orientation of the spin currents to allow them to be easily transported through the material, the temperature has precisely two effects. On the one hand, a higher temperature causes more particles of the material to be in excited states, meaning there are more spin carriers that can be transported, which makes spin transport easier. On the other hand, the high temperature makes it possible to operate at a low magnetic field.Thus the resistance and the current flow change drastically by several orders of magnitude. ""This effect, which we call spin colossal magnetoresistance or SCMR for short, has the potential to simplify the design of fundamental spintronic components significantly,"" explained the scientist from Mainz. This is particularly interesting for storage devices such as hard disks. This effect might be employed, for example, to create spin current switches as well as spin current based storage media.Story Source:Materials provided by Johannes Gutenberg Universitaet Mainz. ",0.110701,0.11463,0.114474,0.161776,0.128988,0.226897,0.151511,0,0
711,How plastic waste moves in the environment,"A Washington State University researcher for the first time has modeled how microplastic fibers move through the environment.The work, published in the November print issue of the journal Advances in Water Resources, could someday help communities better understand and reduce plastics pollution, which is a growing problem around the world.Millions of tons of plastic waste in tiny microscopic pieces are bobbing around the world's oceans and are finding their way into soil, sediments and freshwater. Plastic debris comes from many sources including synthetic clothing fibers, cosmetics, packaging and industrial processes. These plastic bits often end up in the oceans, harming the marine life that eats them.Researchers have studied and measured microplastics in a variety of environments, but Nick Engdahl, an assistant professor in the Department of Civil and Environmental Engineering, is the first to model how the synthetic fibers move.""I wanted to know whether they keep moving and spreading or if they just accumulate in one place,"" said Engdahl, who has studied the movement of a variety of contaminants in the environment.He used a novel physics-based approach to simulate the movement of microplastic fibers, specifically. These synthetic fibers in clothing are created during their manufacturing process.""Every time you walk or rub against something your clothes are shedding fibers,"" said Engdahl.The microfibers, which are mainly released when clothes are washed, end up in wastewater plants, where a significant proportion pass through water filtration systems. Even the ones that are filtered end up in the sewage sludge that can be applied to farm soils as fertilizer or dumped in landfills.Engdahl found that the length of the fibers and the speed of water that they're floating in determined whether they settle in soil or continue moving in the environment. He also found that the movement of shorter microplastic fibers was complex, and that they moved faster than dissolved substances in the water.Engdahl is working to verify and refine his model against direct observations of microplastic fiber movement in a lab. He also plans to measure the fibers in a wastewater treatment facility.""The more data I can get from the real world, the more accurately I will be able to see if these things move around or stay put and pile up,"" he said. ""This will help us more accurately measure their environmental impact, which is largely unknown right now.""Story Source:Materials provided by Washington State University. ",0.0857228,0.125976,0.220849,0.174228,0.135807,0.138718,0.166106,0,0
712,Tool for speedy diagnosis of bacterial infections Inexpensive biosensor provides instant and accurate results,"Using a small and inexpensive biosensor, researchers at UBC Okanagan, in collaboration with the University of Calgary, have built a diagnostic tool that provides health care practitioners almost instant diagnosis of a bacterial infection.The tool is able to provide accurate and reliable results in real-time rather than the two-to-five days required for existing processes that test infections and antibiotic susceptibility.""Advances in lab-on-a-chip microfluidic technology are allowing us to build smaller and more intricate devices that, in the medical research space, can provide more information for health care practitioners while requiring less invasive sampling from patients,"" explains Mohammad Zarifi, an assistant professor at UBC Okanagan.According to health care statistics from 2017, every hour of delay in antibiotic treatment increases mortality rates by nearly eight per cent due to infection complications in the bloodstream.Zarifi, and his research group in the School of Engineering's Microelectronics and Advanced Sensors Laboratory, tested their device by tracking the amount of bacteria present in a variety of samples under various scenarios. The scenarios resembled those encountered in clinical microbiological laboratories.By sending a microwave signal through the sample, the device quickly and accurately analyzes and then generates a profile of existing bacteria.The diagnostic tool not only provides a rapid, label-free and contactless diagnostic tool for clinical analysis but it also goes further, says Zarifi.""The device is able to rapidly detect bacteria and in addition, it screens the interaction of that bacteria with antibiotics,"" he adds. ""The combined results give health care practitioners more information than they currently have available, helping them move forward to determine accurate treatments.""This biosensor, explains Zarifi is a significant step forward in improving the complex antibiotic susceptibility testing workflow and provides a rapid and automated detection of bacteria as well as screening the bacteria proliferation in response to antibiotics.Story Source:Materials provided by University of British Columbia Okanagan campus. ",0.108216,0.132146,0.190012,0.217315,0.161914,0.172655,0.180265,0,0
713,"Cellphone technology developed to detect HIV New, affordable mobile device could aid people in developing countries","The management of human immunodeficiency virus 1 (HIV), which cripples the immune system by attacking healthy cells, remains a major global health challenge in developing countries that lack infrastructure and trained medical professionals. Investigators from Brigham and Women's Hospital have designed a portable and affordable mobile diagnostic tool, utilizing a cellphone and nanotechnology, with the ability to detect HIV viruses and monitor its management in resource-limited regions. The novel platform is described in a paper published recently in Nature Communications.""Early detection of HIV is critical to prevent disease progression and transmission, and it requires long-term monitoring, which can be a burden for families that have to travel to reach a clinic or hospital,"" said senior author Hadi Shafiee, PhD, a principal investigator in the Division of Engineering in Medicine and Renal Division of Medicine at the Brigham. ""This rapid and low-cost cellphone system represents a new method for detecting acute infection, which would reduce the risk of virus transmission and could also be used to detect early treatment failure.""Traditional virus monitoring methods for HIV are expensive, requiring the use of polymerase chain reaction (PCR). Shafiee and his colleagues sought to design an affordable, simple tool that makes HIV testing and monitoring possible for individuals in developing countries with less access to medical care.Utilizing nanotechnology, a microchip, a cellphone and a 3D-printed phone attachment, the researchers created a platform that can detect the RNA nucleic acids of the virus from a single drop of blood. The device detects the amplified HIV nucleic acids through on-phone monitoring of the motion of DNA-engineered beads without using bulky or expensive equipment. The detection precision was evaluated for specificity and sensitivity.Researchers found that the platform allowed the detection of HIV with 99.1 percent specificity and 94.6 percent sensitivity at a clinically relevant threshold value of 1,000 virus particles/ml, with results within one hour. Notably, the total material cost of the microchip, phone attachment and reagents was less than $5 per test.""Health workers in developing countries could easily use these devices when they travel to perform HIV testing and monitoring. Because the test is so quick, critical decisions about the next medical step could be made right there,"" said Shafiee. ""This would eliminate the burden of trips to the medical clinic and provide individuals with a more efficient means for managing their HIV.""""We could use this same technology as a rapid and low-cost diagnostic tool for other viruses and bacteria as well,"" said lead author Mohamed Shehata Draz, PhD, an instructor in the Division of Engineering in Medicine and Renal Division of Medicine at the Brigham.. ""This platform could help a lot of people worldwide.""Funding for this work was provided by the National Institute of Health under award numbers R01AI118502, R21HD092828, and P30ES000002; Harvard T.H. Chan School of Public Health, Harvard Center for Environmental Health through Harvard NIEHS Grant; and American Board of Obstetrics and Gynecology, American College of Obstetricians and Gynecologists, American Society for Reproductive Medicine, Society for Reproductive Endocrinology and Infertility through ASRMAward, and Harvard University Center for AIDS Research (CFAR) under award number 5P30AI060354-14.Story Source:Materials provided by Brigham and Women's Hospital. ",0.0704501,0.118824,0.145538,0.201061,0.12915,0.121855,0.151794,0,0
714,"Robot-bat, 'Robat,' uses sound to navigate and map a novel environment Robat uses a bat-like approach, emitting sound and analyzing the returning echoes","A fully autonomous bat-like terrestrial robot, named Robat, can use echolocation to move through a novel environment while mapping it solely based on sound, according to a study published in PLOS Computational Biology by Itamar Eliakim of Tel Aviv University, and colleagues.Bats use echolocation to map novel environments while simultaneously navigating through them by emitting sound and extracting information from the echoes reflected from objects in their surroundings. Many theoretical frameworks have been proposed to explain how bats routinely solve one of the most challenging problems in robotics, but few attempts have been made to build an actual robot that mimics their abilities. Unlike most previous efforts to apply sonar in robotics, Eliakim and colleagues developed a robot that uses a biological bat-like approach, emitting sound and analyzing the returning echoes to generate a map of space.Robat has an ultrasonic speaker that mimics the mouth, producing frequency modulated chirps at a rate typically used by bats, as well as two ultrasonic microphones that mimic ears. It moved autonomously through a novel outdoor environment and mapped it in real time using only sound. Robat delineates the borders of objects it encounters, and classifies them using an artificial neural network, thus creating a rich, accurate map of its environment while avoiding obstacles. For example, when reaching a dead end, the robot used its classification abilities to determine whether it was blocked by a wall or by a plant through which it could pass.""To our best knowledge, our Robat is the first fully autonomous bat-like biologically plausible robot that moves through a novel environment while mapping it solely based on echo information -- delineating the borders of objects and the free paths between them and recognizing their type,"" Eliakim said. ""We show the great potential of using sound for future robotic applications.""Story Source:Materials provided by PLOS. ",0.121273,0.122341,0.167375,0.267035,0.151283,0.170375,0.222083,0,0
715,Picture this: Camera with no lens Engineers develop computerized camera without optics that instead uses an ordinary window as the lens,"In the future, your car windshield could become a giant camera sensing objects on the road. Or each window in a home could be turned into a security camera.University of Utah electrical and computer engineers have discovered a way to create an optics-less camera in which a regular pane of glass or any see-through window can become the lens.Their innovation was detailed in a research paper, ""Computational Imaging Enables a 'See-Through' Lensless Camera,"" published in the newest issue of Optics Express. A copy of the paper, which was co-authored by University of Utah electrical and computer engineering graduate Ganghun Kim, can be downloaded here.University of Utah electrical and computer engineering associate professor Rajesh Menon argues that all cameras were developed with the idea that humans look at and decipher the pictures. But what if, he asked, you could develop a camera that can be interpreted by a computer running an algorithm?""Why don't we think from the ground up to design cameras that are optimized for machines and not humans. That's my philosophical point,"" he says.If a normal digital camera sensor such as one for a mobile phone or an SLR camera is pointed at an object without a lens, it results in an image that looks like a pixelated blob. But within that blob is still enough digital information to detect the object if a computer program is properly trained to identify it. You simply create an algorithm to decode the image.Through a series of experiments, Menon and his team of researchers took a picture of the University of Utah's ""U"" logo as well as video of an animated stick figure, both displayed on an LED light board. An inexpensive, off-the-shelf camera sensor was connected to the side of a plexiglass window, but pointed into the window while the light board was positioned in front of the pane at a 90-degree angle from the front of the sensor. The resulting image from the camera sensor, with help from a computer processor running the algorithm, is a low-resolution picture but definitely recognizable. The method also can produce full-motion video as well as color images, Menon says.The process involves wrapping reflective tape around the edge of the window. Most of the light coming from the object in the picture passes through the glass, but just enough -- about 1 percent -- scatters through the window and into the camera sensor for the computer algorithm to decode the image.While the resulting photo is not enough to win a Pulitzer Prize, it would be good enough for applications such as obstacle-avoidance sensors for autonomous cars. But Menon says more powerful camera sensors can produce higher-resolution images.Applications for a lensless camera can be almost unlimited. Security cameras could be built into a home during construction by using the windows as lenses. It could be used in augmented-reality goggles to reduce their bulk. With current AR glasses, cameras have to be pointed at the user's eyes in order to track their positions, but with this technology they could be positioned on the sides of the lens to reduce size. A car windshield could have multiple cameras along the edges to capture more information. And the technology also could be used in retina or other biometric scanners, which typically have cameras pointed at the eye.""It's not a one-size-fits-all solution, but it opens up an interesting way to think about imaging systems,"" Menon says.From here, Menon and his team will further develop the system, including 3-D images, higher color resolution and photographing objects in regular household light. His current experiments involved taking pictures of self-illuminated images from the light board.Story Source:Materials provided by University of Utah. ",0.106572,0.105336,0.144653,0.262224,0.170506,0.158739,0.174412,0,0
716,Lift off for world-first ultrasound levitation that bends around barriers,"Researchers at the University of Sussex have become the first in the world to develop technology which can bend sound waves around an obstacle and levitate an object above it.SoundBender, developed by Professor Sriram Subramanian, Dr Gianluca Memoli and Dr Diego Martinez Plasencia at the University of Sussex, is an interface capable of producing dynamic self-bending beams that enable both levitation of small objects and tactile feedback around an obstacle.The technology, to be presented at the 31st ACM User Interface Software and Technology Symposium in Berlin this Monday [October 15], overcomes two key limitations of previous ultrasound levitation set-ups, which were unable to create sound fields of similar complexity and could not bypass obstacles that lay between the transducers and the levitating object.Dr Memoli, Lecturer in Novel Interfaces and Interactions at the University of Sussex, said: ""This is a significant step forward for ultrasound levitation and overcomes a significant drawback that has been hampering development in this field. We have achieved incredibly dynamic and responsive control, so that real-time adjustments are just one step away.""University of Sussex researchers overcame these challenges by developing a hybrid system that combines the versatility of phased arrays of transducers (PATs) with the precision of acoustic metamaterials while helping to eliminate the restrictions on sound field resolution and variability each of the previous approaches applied.The technology allows users to experience haptic feedback beyond an obstacle; to levitate around an obstacle and to manipulate non-solid objects such as changing the direction of a candle's flame.With SoundBender, the metamaterial provides a low modulator pitch to help create sound fields with high spatial resolution while the PAT adds dynamic amplitude and phase control of the field.Dr Martinez-Plasencia, Lecturer in Interactive Graphics at the University of Sussex, said: ""We were attracted to this project due to its similarities between optical holography and acoustics. However, the project has been a great trip of discovery, helping us understand how crucial it is to have high spatial resolution (i.e. the metamaterial), or the techniques required to combine PATs and metamaterials. I am really happy that we can now share all this insight with the rest of the community.""The development opens up new potential in ultrasound levitation, which has a distinct advantage over other levitation techniques because it requires no specific physical properties, such as magnetic or electric, in the object to be levitated and can therefore be applied to a far wider range of materials including liquids and food.The concept of self-bending beams was initially used in engineering applications, to obscure buildings from noise or protect areas from earthquakes, but this is the first time it has been adopted for use in acoustic levitation.The hybrid system allows for a number of fun applications including new educational experiences with museum displays, enhanced board games with new levels of interactivity, the potential to direct desired smells from a diffuser to where they are needed, the ability to control motion in non-solid items (such as dry ice or fire) and the potential to synchronize these movements to music.Prof Sriram Subramanian, Professor of Informatics at the University of Sussex and Royal Academy of Engineering (RAEng) Emerging Technologies Chair specialising in developing novel acoustic interfaces, said: ""Following our breakthrough, the potential now is for a device that can bend around larger objects, potentially even as the obstacle is moving. We are also pursuing how to make the device broadband so it can work for all frequencies of sound. This would allow, for instance, sending the music of a radio behind a corner or creating zones of silence in the middle of a dance floor.""Story Source:Materials provided by University of Sussex. ",0.0881907,0.0892638,0.120756,0.18826,0.130605,0.134278,0.147159,0,0
717,Transparent loudspeakers and MICs that let your skin play music,"An international team of researchers, affiliated with UNIST has presented an innovative wearable technology that will turn your skin into a loudspeaker.This breakthrough has been led by Professor Hyunhyub Ko in the School of Energy and Chemical Engineering at UNIST. Created in part to help the hearing and speech impaired, the new technology can be further explored for various potential applications, such as wearable IoT sensors and conformal health care devices.In the study, the research team has developed ultrathin, transparent, and conductive hybrid nanomembranes with nanoscale thickness, consisting of an orthogonal silver nanowire array embedded in a polymer matrix. They, then, demonstrated their nanomembrane by making it into a loudspeaker that can be attached to almost anything to produce sounds. The researchers also introduced a similar device, acting as a microphone, which can be connected to smartphones and computers to unlock voice-activated security systems.Nanomembranes (NMs) are molcularly thin seperation layers with nanoscale thickness. Polymer NMs have attracted considerable attention owing to their outstanding advantages, such as extreme flexibility, ultralight weight, and excellent adhesibility in that they can be attached directly to almost any surface. However, they tear easily and exhibit no electrical conductivity.The research team has solved such issues by embedding a silver nanowire network within a polymer-based nanomembrane. This has enabled the demonstration of skin-attachable and imperceptible loudspeaker and microphone.""Our ultrathin, transparent, and conductive hybrid NMs facilitate conformal contact with curvilinear and dynamic surfaces without any cracking or rupture,"" says Saewon Kang in the doctroral program of Energy and Chemical Engineering at UNIST, the first author of the study.He adds, ""These layers are capable of detecting sounds and vocal vibrations produced by the triboelectric voltage signals corresponding to sounds, which could be further explored for various potential applications, such as sound input/output devices.""Using the hybrid NMs, the research team fabricated skin-attachable NM loudspeakers and microphones, which would be unobtrusive in appearance because of their excellent transparency and conformal contact capability. These wearable speakers and microphones are paper-thin, yet still capable of conducting sound signals.""The biggest breakthrough of our research is the development of ultrathin, transparent, and conductive hybrid nanomembranes with nanoscale thickness, less than 100 nanometers,"" says Professor Ko. ""These outstanding optical, electrical, and mechanical properties of nanomembranes enable the demonstration of skin-attachable and imperceptible loudspeaker and microphone.""The skin-attachable NM loudspeakers work by emitting thermoacoustic sound by the temperature-induced oscillation of the surrounding air. The periodic Joule heating that occurs when an electric current passes through a conductor and produces heat leads to these temperature oscillations. It has attracted considerable attention for being a stretchable, transparent, and skin-attachable loudspeaker.Wearable microphones are sensors, attached to a speaker's neck to even sense the vibration of the vocal folds. This sensor operates by converting the frictional force generated by the oscillation of the transparent conductive nanofiber into electric energy. For the operation of the microphone, the hybrid nanomembrane is inserted between elastic films with tiny patterns to precisely detect the sound and the vibration of the vocal cords based on a triboelectric voltage that results from the contact with the elastic films.""For the commercial applications, the mechanical durability of nanomebranes and the performance of loudspeaker and microphone should be improved further,"" says Professor Ko.Story Source:Materials provided by Ulsan National Institute of Science and Technology(UNIST). ",0.111408,0.144638,0.141815,0.190704,0.190271,0.166869,0.175965,0,0
718,Creating a (synthetic) song from a zebra finch's muscle,"Birds create songs by moving muscles in their vocal organs to vibrate air passing through their tissues. While previous research reported that each of the different muscles controls one acoustic feature, new research shows that these muscles act in concert to create sound.An international team of researchers describes how zebra finches produce songs in the journal Chaos, from AIP Publishing. Using electromyographic (EMG) signals, researchers tracked the activity of one of the main muscles involved in creating sound, the syringealis ventralis (vS) muscle. They then used the data from this muscle to create a synthetic zebra finch song.""The activity of this muscle provided us information on gating, when the sound starts or ends,"" said Juan Deppler, one of the researchers and a doctoral student at the University of Buenos Aires. ""What's interesting is that when the bird is asleep, despite lacking the airflow necessary to produce sound, this muscle also activates and shows electrical activity, similar to the activity it shows while singing.""The researchers inserted pairs of bipolar electrodes into the vS muscle of five adult zebra finches. It was previously assumed that the vS muscle was primarily involved in frequency modulation, but this team found that the vS muscle was involved in making sounds or phonation too.Using EMG data, they identified phonating intervals with a success rate above 70 percent. The team developed a set of criteria to accurately predict sound production intervals in a set of data from five different birds. The criteria performed particularly well in the short, simple syllables, when the vS is mostly silent during the creation of sound. For more complex syllables, it was clear that incorporating the activity of other syringeal muscles that impact the start and stop of sound in a zebra finch's song would improve their gating prediction.Still, the researchers were able to create a full reconstruction of the song using the activity of one muscle. The vS muscle is particularly active during sleep, so the researchers collected the electric activity of the vS muscle during sleep, and translated these patterns into song.""So in the big picture we expect to soon be able to use this tools to determine when birds are dreaming about singing and to actually listen what songs they are 'singing' when asleep,"" Deppler said.Story Source:Materials provided by American Institute of Physics. ",0.151215,0.164276,0.183261,0.241064,0.200335,0.192932,0.220465,0,0
719,Vibrations at an exceptional point New laser uses light to create sound,"A team of international researchers led by engineers at Washington University in St. Louis has seen the light and now has a lasing system that produces ""good vibrations.""They developed a lasing system already adept at producing tiny light packets called photons into a tunable system that also makes little bits of mechanical energy called phonons -- the energy products of oscillation, or vibration.In doing so, they are the first research group to broaden what is called a laser linewidth in the phonon laser and steer it through a physical system known as the ""exceptional point.""Linewidth is a key component of lasing, showing the physical integrity of the lasing signal as well as the measure of usually unwanted noise in the laser.The study, which involved collaborators from China, Austria, Japan and Michigan, was published in the July 9, 2018, issue of Nature Photonics.""We've shown that you can use a light field to trigger the mechanical movement that will generate an acoustic (sound) wave,"" said Lan Yang, the Edwin H. & Florence G. Skinner Professor of Electrical & Systems Engineering in the School of Engineering & Applied Science.""Think of phonon lasing as a counterpart to traditional optic, or photon lasing, with exciting applications in medical surgery, materials science and communications. We have demonstrated a controllable phonon laser that can be tuned for threshold and linewidth, among other potential parameters.""Our study for the first time provides direct evidence that exceptional point-enhanced optical noises can be transferred directly to mechanical noises,"" Yang said.Yang's laser, an acronym for light amplification by stimulated emission of radiation, belongs to a category called whispering gallery mode resonators (WGM), which work like the famous whispering gallery in St. Paul's Cathedral in London, where someone on the one side of the dome can hear a message spoken to the wall by someone on the other side. Unlike the dome, which has resonances or sweet spots in the audible range, the sensor resonates at light frequencies and now at vibrational or mechanical frequencies.Think of the exceptional point as a complex, super-energy mode, where often unpredictable and counterintuitive phenomena occur. Already, the exceptional point has contributed to a number of counterintuitive activities and results in recent physics studies -- with more expected to be discovered. In this international research project, mathematical tools were used to describe the physical system: An exceptional point arose in a physical field when two complex eigenvalues and their eigenvectors coalesced, or became one and the same.""We use the phonon laser system rather than the photon laser to demonstrate our main results because it is easier to check the linewidth of the phonon laser compared with that of the photon laser,"" she said.Picture the two WGM microresonators set closely to each other in a field with two photon detectors connected by a wave guide which brings light in and out of the system. Call these two ""super modes"" Resonator 1 and Resonator 2.""In the first resonator, which supports photons and produces phonons, we know when the light field is strong enough the radiation will trigger the mechanical oscillation related to the acoustic wave vibration,"" Yang said. ""We calibrated the acoustic wave frequency at 10 megahertz. Then we adjusted the gap between the two resonators to look at the transmission spectrum of the coupled resonators. When we changed the gap, we found that we could tune the spectral distance. We tuned it from 100 megahertz, to 80 to 50 depending on the physical gap between the resonators. If you tune the gap nicely to match the spectral distance between the two to the frequency of the mechanical vibration, then you have resonance.""Resonance is the phenomenon whereby an external force from a system causes another system to oscillate at certain frequencies.The two light fields have two different frequencies and energy levels. The mechanical frequency of 10 megahertz matches the energy difference between the two super modes. Both resonators support photons, but only one produces phonons.In future work, Yang intends to study more deeply the energy exchange between the two super modes in phonon lasing and continue to seek surprises at the exceptional point.She calls phonon lasing's future, ironically, ""very bright.""Story Source:Materials provided by Washington University in St. Louis. Original written by Tony Fitzpatrick. ",0.085466,0.111201,0.12511,0.166162,0.155486,0.136597,0.14467,0,0
720,Deconstructing crowd noise at college basketball games,"With thousands of fans clapping, chanting, shouting and jeering, college basketball games can be almost deafeningly loud. Some arenas have decibel meters, which, accurately or not, provide some indication of the noise volume generated by the spectators and the sound systems. However, crowd noise is rarely the focus of scientific inquiry.""Whenever it comes up in the literature, it's mainly been something investigators are trying to get around,"" noted Brooks Butler, an undergraduate physics student at Brigham Young University and member of the BYU team that will present research at the Acoustical Society of America's 176th Meeting, held in conjunction with the Canadian Acoustical Association's 2018 Acoustics Week in Canada, Nov. 5-9, at the Victoria Conference Centre in Victoria, Canada.""Crowd noise is typically treated as background interference -- something to screen out."" But the BYU researchers felt that crowd noise was worthy of its own investigation. In particular, they wanted to see whether machine learning algorithms could pick out patterns within the raw acoustical data that indicated what the crowd was doing at a given time, thereby providing clues as to what was happening in the game itself. One possible application of this could be the early detection of unruly or violent crowd behavior -- though that idea has not been tested.The BYU team made high-fidelity acoustic measurements during men's and women's basketball games at the university, later doing the same for football and volleyball games. They broke up the games into half-second intervals, measuring the frequency content (as displayed on spectrograms), sound levels, the ratio of the maximum to minimum sound levels within a set time block, and other variables. Then they applied signal processing tools that identified 512 distinct acoustical features comprised of different frequency bands, amplitudes and so forth.The group used these variables to construct a 512-dimensional space, utilizing machine learning techniques to perform a computerized, clustering analysis of this complicated, multidimensional realm.BYU physics professor Kent Gee was a principal investigator on the project along with professors Mark Transtrum and Sean Warnick. Together they led a team of several students focusing on different aspects of the problem, including data collection, analysis and machine learning.Gee explained the process with a simple analogy. ""Suppose you have a plot of points on a two-dimensional, x-y graph and measure the distance between those points,"" he said. ""You might see that the points are bunched together in three clumps or clusters. We did something similar with our 512-dimensional space, though you obviously need a computer to keep track of all that.""The so-called ""K-means clustering"" analysis they ran revealed six separate clusters that corresponded to what was happening in the arena, depending on whether people were cheering, singing, booing, being quiet, or letting the loudspeakers dominate the soundscape.In this way, Gee and his colleagues were able to gauge the emotional state of the audience, simply from a machine-run analysis of the sound data. ""One important eventual application of our research,"" he said, ""may be the early detection of unruly or violent crowd behavior.""Story Source:Materials provided by Acoustical Society of America. ",0.0817157,0.0802498,0.119743,0.224341,0.119035,0.124788,0.150658,0,0
721,New imaging tool captures how sound moves through the chinchilla ear OCT-based instrument provides new insight into hearing that could inform diagnosis and treatment of hearing problems,"Researchers have developed a new device that can be used to visualize how sound-induced vibrations travel through the ear. The technology is providing new insight into how the ear receives and processes sound waves and, with additional development, might one day be used by physicians to diagnose diseases that affect hearing.The new imaging tool is based on optical coherence tomography (OCT), a biomedical imaging technique, which can provide high resolution images of the microstructures of tissue. It can be used to image the middle ear through the intact eardrum and to measure the tiny vibrations within the ear that contribute to sound perception. The middle-ear ossicles, the smallest bones in the human body, form a chain that converts sound waves received by the eardrum into mechanical vibrations that can be detected by the inner ear.In The Optical Society (OSA) journal Biomedical Optics Express, the researchers describe how they used the tool, which they call OCT vibrography, to make new discoveries about how ossicles in chinchilla cadavers respond to high-frequency sounds. The chinchilla is commonly used in hearing research because its ears are similar to those of humans in terms of size and sensitivity to different sound frequencies.Watching the ear hear""There are multiple theories of how high-frequency sounds are conducted to the inner ear, and the ability to view the sound-driven motion of large portions of the ossicular chain will help us understand what actually happens,"" said research team leader Seok-Hyun Yun of the Wellman Center for Photomedicine at Mass General Hospital.With more development, the new technique could also find its way to the clinic. Although methods exist for identifying the presence of a problem with middle-ear sound conduction in people, diagnosing the exact problem and location requires surgically moving the eardrum to observe the middle ear.""If our approach is accepted clinically, it would allow clinicians to differentiate between different middle-ear problems and plan a treatment strategy before, or instead, of performing surgery to view the area,"" said John Rosowski of Massachusetts Eye and Ear, co-leader of the project.Measuring sound vibrationsTo measure vibrations caused by sound in the middle ear, the researchers synchronized an OCT measurement system with sound from a high-fidelity speaker. As the sound from the speakers pushes on the eardrum, the bones begin to move and are imaged with OCT. The researchers developed algorithms to extract accurate measurements of the vibration from the OCT images.Previous studies of the sound vibrations traveling through the middle-ear typically measured the motion of individual locations on the bones or from a collection of no more than 30 individual locations. The unique combination of high-resolution imaging and high-sensitivity vibration measurements available with OCT allowed the researchers to simultaneously measure structure and motion at over 10,000 points on the ossicular surface and eardrum.The researchers demonstrated the capabilities of the new OCT vibrography system by measuring sound-driven eardrum and ossicular motion in chinchilla cadavers. They observed a previously unknown mode of ossicular motion at high frequencies that is consistent with some of the theories describing how high-frequency sounds travel to the inner ear.""Our demonstration of a unique mode of ossicular motion at high frequencies has already led to new ideas of how the bones conduct sound to the inner ear,"" said first author of the paper Antoine Ramier of the Wellman Center. ""That information may help define new ways for surgeons to reconstruct diseased ears.""Toward diagnosing diseaseOne reason the researchers used cadavers was that it took almost 60 seconds to acquire the measurements, during which the breathing and heartbeat of a live animal would likely cause artifacts in the motion measurements. The research team's colleagues at Dalhousie University in Canada are exploring whether measurements of the motion taken from 3 to 5 points, combined with an anatomical OCT scan of the whole eardrum and middle ear, may provide enough information to diagnose ear disease in living organisms.The researchers plan to use their instrument to study ears from human cadavers to find out if the new mode of ossicular motion they found in chinchillas also occurs in humans. Future research will also further examine how this new tool could be applied in specific clinical applications such as diagnosing a particular disease or hearing problem.Story Source:Materials provided by The Optical Society. ",0.085855,0.0942377,0.157948,0.191275,0.131739,0.130412,0.159309,0,0
722,Big discoveries about tiny particles Doctoral student leads international effort to uncover  of polymer nanoparticles,"From photonics to pharmaceuticals, materials made with polymer nanoparticles hold promise for products of the future. However, there are still gaps in understanding the properties of these tiny plastic-like particles.Now, Hojin Kim, a graduate student in chemical and biomolecular engineering at the University of Delaware, together with a team of collaborating scientists at the Max Planck Institute for Polymer Research in Germany, Princeton University and the University of Trento, has uncovered new insights about polymer nanoparticles. The team's findings, including properties such as surface mobility, glass transition temperature and elastic modulus, were published in Nature Communications.Under the direction of MPI Prof. George Fytas, the team used Brillouin light spectroscopy, a technique that spelunks the molecular properties of microscopic nanoparticles by examining how they vibrate.""We analyzed the vibration between each nanoparticle to understand how their mechanical properties change at different temperatures,"" Kim said. ""We asked, 'What does a vibration at different temperatures indicate? What does it physically mean?' ""The characteristics of polymer nanoparticles differ from those of larger particles of the same material. ""Their nanostructure and small size provide different mechanical properties,"" Kim said. ""It's really important to understand the thermal behavior of nanoparticles in order to improve the performance of a material.""Take polystyrene, a material commonly used in nanotechnology. Larger particles of this material are used in plastic bottles, cups and packaging materials.""Polymer nanoparticles can be more flexible or weaker at the glass transition temperature at which they soften from a stiff texture to a soft one, and it decreases as particle size decreases,"" Kim said. That's partly because polymer mobility at small particle surface can be activated easily. It's important to know when and why this transition occurs, since some products, such as filter membranes, need to stay strong when exposed to a variety of conditions.For example, a disposable plastic cup made with the polymer polystyrene might hold up in boiling water -- but that cup doesn't have nanoparticles. The research team found that polystyrene nanoparticles start to experience the thermal transition at 343 Kelvin (158 degrees F), known as the softening temperature, below a glass transition temperature of 372 K (210 F) of the nanoparticles, just short of the temperature of boiling water. When heated to this point, the nanoparticles don't vibrate -- they stand completely still.This hadn't been seen before, and the team found evidence to suggest that this temperature may activate a highly mobile surface layer in the nanoparticle, Kim said. As particles heated up between their softening temperature and glass transition temperature, the particles interacted with each other more and more. Other research groups have previously suspected that glass transition temperature drops with decreases in particle size decreases because of differences in particle mobility, but they could not observe it directly.""Using different method and instruments, we analyzed our data at different temperatures and actually verified there is something on the polymer nanoparticle surface that is more mobile compared to its core,"" he said.By studying interactions between the nanoparticles, the team also uncovered their elastic modulus, or stiffness.Next up, Kim plans to use this information to build a nanoparticle film that can govern the propagation of sound waves.Eric Furst, professor and chair of the Department of Chemical and Biomolecular Engineering at UD, is also a corresponding author on the paper.""Hojin took the lead on this project and achieved results beyond what I could have predicted,"" said Furst. ""He exemplifies excellence in doctoral engineering research at Delaware, and I can't wait to see what he does next.""Story Source:Materials provided by University of Delaware. ",0.0975502,0.189803,0.168932,0.170047,0.149448,0.14043,0.184443,0,0
723,Method to cancel noise without ear-blocking headphones,"Disruptive noise is almost everywhere, from people talking in the office corridor to road construction down the street to the neighbor's lawn mower. Research being conducted at the University of Illinois' Coordinated Science Laboratory is looking to improve this noisy frustration.Current noise cancelling technology comes in the form of headphones and earbuds. To cancel noise, these headphones emit an anti-noise signal to contrast the external sounds. The time available for the headphones to produce this anti-noise signal is extremely short. This results in some noise getting through, which is why all these devices must cover the entire ear with noise-canceling material. However, wearing such ear-blocking devices for long periods of time is not comfortable, and can even be harmful.""Our goal is to not block the ear canal,"" said Sheng Shen, lead author and a Ph.D. candidate in the Coordinated Science Laboratory and Dept. of Electrical and Computer Engineering (ECE). ""We envision a behind-the-ear device that still achieves noise cancellation as good as the best headphones or earbuds available today.""The main idea behind this research involves combining wireless IoT networks with noise cancellation. A microphone is placed in the environment that senses sounds and sends them over wireless signals to an earpiece. Since wireless signals travel a million times faster than sound, the earphone can receive the sound information much faster than the actual sound itself.""This is similar to lightning and thunder -- the lightning arrives much before the thunder, allowing people to prepare for the loud rumble,"" said Shen's advisor, Romit Roy Choudhury, an ECE Professor. ""Similarly, our ear device gets the sound information in advance, and has much more time to produce a better anti-noise signal.""Here's an example of how this technology would work: The person who wants to cancel noise (Alice) would place the IoT microphone away from her, say on her office door. The noise from her coworkers' conversation in the hallway is picked up by this IoT device and transmitted to Alice's earpiece over a wireless connection. The actual sound arrives at the earpiece later, and because of this lead time, the noise can be fully canceled. As a result, it is no longer necessary to block the ear canal.There are however, a few limitations. The IoT microphone needs to be between the noise source and Alice. If noise is coming at her from all directions, a couple more IoT devices would need to be placed around her. In tests conducted during the research, the device designed by Shen and Choudhury's group outperformed a leading headphone in overall noise cancellation and was rated better by the human participants.When asked about the potential privacy concerns involved with the device, Shen nullified some of the most common fears.""The most common privacy concern is that the device will secretly record someone's voice,"" Shen said. ""This device is analog, so it has no capacity to record the sound. The moment the device hears the sound it is sent out wirelessly.""This differentiation is what separates Shen's device from many IoT devices commonly used today, such as Amazon Echo and Google Home, which must record voice samples in order to operate.Shen and his colleagues are already receiving attention for their work. The research paper about the project was recently selected for presentation at the upcoming Association for Computing Machinery Special Interest Group on Data Communication (ACM SIGCOMM) Conference which takes place in Budapest, Hungary, in late August. The conference brings together global researchers across the field of computer networking and communications.""This is bound to change the way we think of noise cancellation, where networks of IoT sensors coordinate to enable quieter and more comfortable environments,"" said ECE Assistant Professor Haitham Hassanieh, who is a co-author on this paper. Other co-authors include Nirupam Roy and Junfeng Guan, both ECE Ph.D. students at the University of Illinois Urbana-Champaign.Looking forward, the team would like to expand the use of their device to provide noise cancellation for more than one individual at a time and make the wearable device even lighter for continuous use.Story Source:Materials provided by University of Illinois College of Engineering. ",0.114304,0.116631,0.14787,0.231454,0.18398,0.167926,0.169392,0,0
724,A system to synthesize realistic sounds for computer animation,"Advances in computer-generated imagery have brought vivid, realistic animations to life, but the sounds associated with what we see simulated on screen, such as two objects colliding, are often recordings. Now researchers at Stanford University have developed a system that automatically renders accurate sounds for a wide variety of animated phenomena.""There's been a Holy Grail in computing of being able to simulate reality for humans. We can animate scenes and render them visually with physics and computer graphics, but, as for sounds, they are usually made up,"" said Doug James, professor of computer science at Stanford University. ""Currently there exists no way to generate realistic synchronized sounds for complex animated content, such as splashing water or colliding objects, automatically. This fills that void.""The researchers will present their work on this sound synthesis system as part of ACM SIGGRAPH 2018, the leading conference on computer graphics and interactive techniques. In addition to enlivening movies and virtual reality worlds, this system could also help engineering companies prototype how products would sound before being physically produced, and hopefully encourage designs that are quieter and less irritating, the researchers said.""I've spent years trying to solve partial differential equations -- which govern how sound propagates -- by hand,"" said Jui-Hsien Wang, a graduate student in James' lab and in the Institute for Computational and Mathematical Engineering (ICME), and lead author of the paper. ""This is actually a place where you don't just solve the equation but you can actually hear it once you've done it. That's really exciting to me and it's fun.""Predicting soundInformed by geometry and physical motion, the system figures out the vibrations of each object and how, like a loudspeaker, those vibrations excite sound waves. It computes the pressure waves cast off by rapidly moving and vibrating surfaces but does not replicate room acoustics. So, although it does not recreate the echoes in a grand cathedral, it can resolve detailed sounds from scenarios like a crashing cymbal, an upside-down bowl spinning to a stop, a glass filling up with water or a virtual character talking into a megaphone.Most sounds associated with animations rely on pre-recorded clips, which require vast manual effort to synchronize with the action on-screen. These clips are also restricted to noises that exist -- they can't predict anything new. Other systems that produce and predict sounds as accurate as those of James and his team work only in special cases, or assume the geometry doesn't deform very much. They also require a long pre-computation phase for each separate object.""Ours is essentially just a render button with minimal pre-processing that treats all objects together in one acoustic wave simulation,"" said Ante Qu, a graduate student in James' lab and co-author of the paper.The simulated sound that results from this method is highly detailed. It takes into account the sound waves produced by each object in an animation but also predicts how those waves bend, bounce or deaden based on their interactions with other objects and sound waves in the scene.Challenges aheadIn its current form, the group's process takes a while to create the finished product. But, now that they have proven this technique's potential, they can focus on performance optimizations, such as implementing their method on parallel GPU hardware, that should make it drastically faster.And, even in its current state, the results are worth the wait.""The first water sounds we generated with the system were among the best ones we had simulated -- and water is a huge challenge in computer-generated sound,"" said James. ""We thought we might get a little improvement, but it is dramatically better than previous approaches even right out of the box. It was really striking.""Although the group's work has faithfully rendered sounds of various objects spinning, falling and banging into each other, more complex objects and interactions -- like the reverberating tones of a Stradivarius violin -- remain difficult to model realistically. That, the group said, will have to wait for a future solution.Story Source:Materials provided by Stanford University. Original written by Taylor Kubota. ",0.0703573,0.0780177,0.108902,0.218916,0.105158,0.116106,0.149863,0,0
725,Sound-waves: Making opaque materials totally transparent,"Most naturally occurring materials have a disordered atomic structure that interferes with the propagation of both sound and electromagnetic waves. When the waves come into contact with these materials, they bounce around and disperse -- and their energy dissipates according to a highly complex interference pattern, diminishing in intensity. That means it's virtually impossible to transmit data or energy intact across wave-scattering media and fully leverage the potential of wave technology.For an example, you need look no further than your smartphone -- the geolocation function works less well inside buildings where radiofrequency waves scatter in all directions. Other potential applications include biomedical imaging and geological surveying, where it's important to be able to send waves across highly disordered media.A team of researchers from two labs at EPFL's School of Engineering, working in association with TU Wien and the University of Crete, has developed a system that allows sound waves to travel across such media with no distortion. It uses tiny speakers as acoustic relays to offset the wave scattering, and has been successfully tested on a real acoustic system. Their work has just been published in Nature Physics.Using speakers to eliminate obstaclesIn the researchers' system, the tiny speakers can be controlled to amplify, attenuate or shift the phase of the sound waves. That lets them offset the diffusion that results when the waves hit obstacles, and thereby reproduce the original sound exactly on the other side of the disordered medium.How does it work? ""We realized that our acoustic relays had to be able to change the waves' amplitudes and phases at strategic locations, to either magnify or attenuate them,"" says Romain Fleury, head of EPFL's Laboratory of Wave Engineering (LWE) and a co-author of the study.The researchers tested their system by building an air-filled tube and placing various kinds of obstacles such as walls, porous materials and chicanes into it, in order to create a highly disordered medium through which no sound waves could pass. They then placed their tiny speakers between the obstacles and set up electronic controls to adjust the speakers' acoustic properties. ""We've been working on using controlled speakers as active sound absorbers for years, so it made sense to use them for this new application too,"" says Herve Lissek, head of the acoustics research group at EPFL's Signal Processing Laboratory 2 (LTS2) and a co-author of the study. ""Until now, we only needed to attenuate sound waves. But here we had to develop a new control mechanism so we could also amplify them, like how we can already amplify optical waves with lasers,"" adds Etienne Rivet, another co-author at EPFL who wrote a thesis on the subject. Their new method -- the only one of its kind in acoustics -- uses programmable circuits to control several speakers simultaneously and in real time.Making objects invisibleThe researchers' method for active acoustic control is similar to that used in noise cancelling headphones and could potentially be used for sounds containing common ambient frequencies. It could also be used to eliminate the waves that bounce off objects like submarines, making them undetectable by sonar. Moreover, the theory underlying their work is universal and could have parallel applications in optics or radiofrequencies, to make objects invisible or to take images through opaque materials.Story Source:Materials provided by Ecole Polytechnique Federale de Lausanne. ",0.125885,0.11804,0.1366,0.178366,0.155626,0.175038,0.159438,0,0
726,Scientists use hydrophone to listen in on methane seeps in ocean,"A research team has successfully recorded the sound of methane bubbles from the seafloor off the Oregon coast using a hydrophone, opening the door to using acoustics to identify -- and perhaps quantify -- this important greenhouse gas in the ocean.The next step, researchers say, is to fine-tune their ability to detect the acoustic signature of the bubbles so they can use the sounds to estimate the volume of methane in the offshore reservoirs.Results of the study have just been published in the journal Deep-Sea Research II.""The bubbles in the streams make sound, and the frequency of the sound is related to the size of the bubble,"" said Robert Dziak, an acoustics scientist with the National Oceanic and Atmospheric Administration and lead author on the study. ""The smaller the bubble, the higher the pitch. And the larger the bubble, the lower the sound pitch, but the more methane it contains.""Our ultimate goal is to use sound to estimate the volume and rate of methane gas exiting these seafloor fields,"" added Dziak, who has a courtesy appointment in OSU's College of Earth, Ocean, and Atmospheric Sciences.In recent years, scientists have found hundreds of bubble streams emanating from methane deposits off the Pacific Northwest coast, but they have no way to determine how much methane is stored there. Methane is found both as an icy hydrate deposit and in a gas phase within the sediments of the continental margins.It potentially could be a new energy source, or it could pose a serious environmental threat as a greenhouse gas.The research team used the remotely operated vehicle (ROV) Hercules from the Exploration Vessel (E/V) Nautilus, owned and operated by the Ocean Exploration Trust, to deploy a hydrophone about 10 kilometers off Heceta Bank on the Oregon continental margin in 1,228 meters of water (about three-fourths of a mile deep). The acoustic signatures of the bubbles from the seep site are depicted in the hydrophone record as a series of short, high-frequency bursts, lasting 2-3 seconds.The researchers then compared the sound record with still images from the ROV and found their estimates of bubble size from the hydrophone record matched the visual evidence.This June, a project led by OSU researchers Tamara Baumberger and Susan Merle and utilizing the E/V Nautilus will map additional methane seep sites off the Pacific Northwest coast. Dziak and OSU researcher Haru Matsumoto will work with the team to deploy a hydrophone into the depths of Astoria Canyon at the site of a high-rate methane seep and leave it there for 2-3 days.""The frequencies are so high on some of these recordings that the data drive fills up quickly on battery-operated hydrophones,"" Dziak said. ""However, this new experiment will record for a longer time period, allowing us to see how seafloor methane emissions vary over time, and how they may be influenced by the ocean tides.""Story Source:Materials provided by Oregon State University. ",0.136199,0.147625,0.189415,0.179928,0.136977,0.190006,0.172511,0,0
727,New tech improves ability to reflect sound back to its source,"Researchers have developed a device that reflects sound in the direction it came from, rather than deflecting it at an angle. The ""retroreflector"" can reflect sound across an operating range of 70 degrees in either direction -- more than doubling the effective range of previous technologies.""The technology makes use of two engineered materials,"" says Yun Jing, an associate professor of mechanical and aerospace engineering at North Carolina State University and co-corresponding author of a paper on the work.""The first layer focuses the incoming sound waves onto a second layer, which then sends the sound waves back to their source. We were inspired by a similar approach used in optics research, but we think we are the first to use this technique in the acoustics field.""Previous techniques for creating retroreflective surfaces relied on rectangular pits arrayed across a material. Sound waves would ricochet from the side of the rectangle to the bottom, before bouncing back in the direction they came from.""However, designs using that approach can be bulky, and have a fairly narrow range of angles that they can reflect properly,"" Jing says. ""Our technology is both slimmer and effective across a wider range of angles.""Experiments using a prototype of the new technology find that it is also fairly efficient. At 0 degrees -- when the sound source is perpendicular to the surface -- 60 percent of the sound is sent back to the source. At 70 degrees -- the extreme end of the effective range -- 40 percent of the sound is directed back to the source.""We have a fully functional prototype now, and our next steps include fine-tuning the technology for use in specific applications, such as medical ultrasound,"" Jing says. ""Frankly, we think there are likely applications that we haven't thought of yet.""Story Source:Materials provided by North Carolina State University. ",0.128564,0.1135,0.14578,0.20159,0.150624,0.172407,0.165225,0,0
728,What causes the sound of a dripping tap -- and how do you stop it?,"Scientists have solved the riddle behind one of the most recognisable, and annoying, household sounds: the dripping tap. And crucially, they have also identified a simple solution to stop it, which most of us already have in our kitchens.Using ultra-high-speed cameras and modern audio capture techniques, the researchers, from the University of Cambridge, found that the 'plink, plink' sound produced by a water droplet hitting a liquid surface is caused not by the droplet itself, but by the oscillation of a small bubble of air trapped beneath the water's surface. The bubble forces the water surface itself to vibrate, acting like a piston to drive the airborne sound.In addition, the researchers found that changing the surface tension of the surface, for example by adding dish soap, can stop the sound. The results are published in the journal Scientific Reports.Despite the fact that humans have been kept awake by the sound of dripping water from a leaky tap or roof for generations, the exact source of the sound has not been known until now.""A lot of work has been done on the physical mechanics of a dripping tap, but not very much has been done on the sound,"" said Dr Anurag Agarwal of Cambridge's Department of Engineering, who led the research. ""But thanks to modern video and audio technology, we can finally find out exactly where the sound is coming from, which may help us to stop it.""Agarwal, who leads the Acoustics Lab and is a Fellow of Emmanuel College, first decided to investigate this problem while visiting a friend who had a small leak in the roof of his house. Agarwal's research investigates acoustics and aerodynamics of aerospace, domestic appliances and biomedical applications. ""While I was being kept awake by the sound of water falling into a bucket placed underneath the leak, I started thinking about this problem,"" he said. ""The next day I discussed it with my friend and another visiting academic, and we were all surprised that no one had actually answered the question of what causes the sound.""Working with Dr Peter Jordan from the University of Poitiers, who spent a term in Cambridge through a Fellowship from Emmanuel College, and final-year undergraduate Sam Phillips, Agarwal set up an experiment to investigate the problem. Their setup used an ultra-high-speed camera, a microphone and a hydrophone to record droplets falling into a tank of water.Water droplets have been a source of scientific curiosity for more than a century: the earliest photographs of drop impacts were published in 1908, and scientists have been trying to figure out the source of the sound ever since.The fluid mechanics of a water droplet hitting a liquid surface are well-known: when the droplet hits the surface, it causes the formation of a cavity, which quickly recoils due to the surface tension of the liquid, resulting in a rising column of liquid. Since the cavity recoils so fast after the droplet's impact, it causes a small air bubble to get trapped underwater.Previous studies have posited that the 'plink' sound is caused by the impact itself, the resonance of the cavity, or the underwater sound field propagating through the water surface, but have not been able to confirm this experimentally.In their experiment, the Cambridge researchers found that somewhat counter-intuitively, the initial splash, the formation of the cavity, and the jet of liquid are all effectively silent. The source of the sound is the trapped air bubble.""Using high-speed cameras and high-sensitivity microphones, we were able to directly observe the oscillation of the air bubble for the first time, showing that the air bubble is the key driver for both the underwater sound, and the distinctive airborne 'plink' sound,"" said Phillips, who is now a PhD student in the Department of Engineering. ""However, the airborne sound is not simply the underwater sound field spreading to the surface, as had been previously thought.""In order for the 'plink' to be significant, the trapped air bubble needs to be close to the bottom of the cavity caused by the drop impact. The bubble then drives oscillations of the water surface at the bottom of the cavity, acting like a piston driving sound waves into the air. This is a more efficient mechanism by which the underwater bubble drives the airborne sound field than had previously been suggested.According to the researchers, while the study was purely curiosity-driven, the results could be used to develop more efficient ways to measure rainfall or to develop a convincing synthesised sound for water droplets in gaming or movies, which has not yet been achieved.Video: https://www.youtube.com/watch?v=-iP3Dwy0RSQStory Source:Materials provided by University of Cambridge. The original story is licensed under a Creative Commons License. ",0.121417,0.124571,0.151754,0.175731,0.153407,0.15079,0.182313,0,0
729,Musical training improves visual timing. Counterintuitive finding demonstrates superior sensory learning and memory from cross-training the brain's audio and visual systems,"Drummers and brass players are better able to judge the timing of visual stimuli than members of the color guard, according to a naturalistic study of the world-class drum corps Bluecoats published in eNeuro. This counterintuitive finding extends previous research demonstrating superior sensory learning and memory from cross-training the brain's audio and visual systems.During an intensive, five-week spring training program, Nestor Matthews and colleagues compared the ability of young adult Bluecoat percussionists, brass players, and color guard to detect the order of moving stimuli mimicking the color guards' visual displays. This study design enabled the researchers to investigate the effect of musical and visual training on visual timing, while controlling for experience and skill level.The results reveal percussionists perform the task more precisely and quickly than brass players, who perform better than the color guard. Taken together with findings from neuroimaging and brain stimulation research, this pattern suggests musical training shapes cortical areas responsible for synchronizing rhythm and behavior.Story Source:Materials provided by Society for Neuroscience. ",0.151524,0.169307,0.220189,0.33868,0.22022,0.206365,0.245266,0,0
730,How beatboxers produce sound: Using real-time MRI to understand Researchers are using real-time MRI to study the production of beatboxing sounds that mimic the sound of percussion instruments,"Beatboxing is a musical art form in which performers use their vocal tract to create percussive sounds. Sometimes individual beatboxers perform as a part of an ensemble, using their vocal tracts to provide beats for other musicians; other times, beatboxers perform alone, where they might sing and beatbox simultaneously or simply make sounds without singing.A team of researchers is using real-time MRI to study the production of beatboxing sounds. Timothy Greer, a doctoral candidate at the University of Southern California, will describe their work showing how real-time MRI can characterize different beatboxing styles and how video signal processing can demystify the mechanics of artistic style. Greer will present the study at the Acoustical Society of America's 176th Meeting, held in conjunction with the Canadian Acoustical Association's 2018 Acoustics Week in Canada, Nov. 5-9 at the Victoria Conference Centre in Victoria, Canada.The team is interested in using real-time MRI data to observe the vocal tracts of beatboxers just before they make a sound to see if those movements are different from speech. The real-time MRI data provides a dynamic view of the entire midsagittal vocal tract and at a frame rate high enough to observe the movement and coordination of critical articulators.""Beatboxers may learn something different in preparing to make a sound than they do when they're talking,"" said Greer. ""Using real-time MRI allows us to investigate the difference in the production of music and language and to see how the mind parses these different modalities.""Previous research in this field usually consists of a case study of one beatboxer and suggests that beatboxers can only produce sounds that exist within the world's known languages. The new study looks at several beatboxers of different ages and genders, and with different native languages.""We found that beatboxers can create sounds that are not seen in any language. They have an acrobatic ability to put together all these different sounds,"" said Greer. ""They can hear a sound like a snare drum and they can figure out what they need to do with their mouth to re-create it.""One of the main challenges for the researchers has been developing the algorithms they use to quantify the movement of the beatboxer's vocal tract. A linguist labels all the various parts of the body involved in sound production such as the tongue and the roof of the mouth, and the algorithm tracks the images of these various parts as they move during the production of sound.""The vocal tract is amazing but it's also incredibly complex. We need to keep creating better computer algorithms to understand how it all works together,"" said Greer.This work is supported by NIH grant R01DC007124 and NSF grant IIS 1514544.Presentation #3aMU5, ""How beatboxers produce percussion sounds: A real-time magnetic resonance imaging investigation,"" by Timothy Greer, Shri Narayanan, Reed Blaylock and Nimisha Patil will take place Wednesday, Nov. 7, 11:25 a.m. in the Crystal Ballroom (FE) of the Victoria Conference Center in Victoria, British Columbia, Canada.Story Source:Materials provided by Acoustical Society of America. ",0.10972,0.123919,0.17805,0.239065,0.14753,0.167977,0.18686,0,0
731,New method enables high quality speech separation,"People have a natural knack for focusing on what a single person is saying, even when there are competing conversations in the background or other distracting sounds. For instance, people can often make out what is being said by someone at a crowded restaurant, during a noisy party, or while viewing televised debates where multiple pundits are talking over one another. To date, being able to computationally -- and accurately -- mimic this natural human ability to isolate speech has been a difficult task.""Computers are becoming better and better at understanding speech, but still have significant difficulty understanding speech when several people are speaking together or when there is a lot of noise,"" says Ariel Ephrat, a PhD candidate at Hebrew University of Jerusalem-Israel and lead author of the research. (Ephrat developed the new model while interning at Google the summer of 2017.) ""We humans know how to understand speech in such conditions naturally, but we want computers to be able to do it as well as us, maybe even better.""To this end, Ephrat and his colleagues at Google have developed a novel audio-visual model for isolating and enhancing the speech of desired speakers in a video. The team's deep network-based model incorporates both visual and auditory signals in order to isolate and enhance any speaker in any video, even in challenging real-world scenarios, such as video conferencing, where multiple participants oftentimes talk at once, and noisy bars, which could contain a variety of background noise, music, and competing conversations.The team, which includes Google's Inbar Mosseri, Oran Lang, Tali Dekel, Kevin Wilson, Avinatan Hassidim, William T. Freeman, and Michael Rubinstein, will present their work at SIGGRAPH 2018, held 12-16 August in Vancouver, British Columbia. The annual conference and exhibition showcases the world's leading professionals, academics, and creative minds at the forefront of computer graphics and interactive techniques.In this work, the researchers did not just focus on auditory cues to separate speech but also visual cues in the video -- i.e., the subject's lip movements and potentially other facial movements that may lend to what he or she is saying. The visual features garnered are used to ""focus"" the audio on a single subject who is speaking and to improve the quality of speech separation.To train their joint audio-visual model, Ephrat and collaborators curated a new dataset, ""AVSpeech,"" comprised of thousands of YouTube videos and other online video segments, such as TED Talks, how-to videos, and high-quality lectures. From AVSpeech, the researchers generated a training set of so-called ""synthetic cocktail parties"" -- mixtures of face videos with clean speech and other speech audio tracks with background noise. To isolate speech from these videos, the user is only required to specify the face of the person in the video whose audio is to be singled out.In multiple examples detailed in the paper, titled ""Looking to Listen at the Cocktail Party: A Speaker-Independent Audio-Visual Model for Speech Separation,"" the new method turned out superior results as compared to existing audio-only methods on pure speech mixtures, and significant improvements in delivering clear audio from mixtures containing overlapping speech and background noise in real-world scenarios. While the focus of the work is speech separation and enhancement, the team's novel method could also be applied to automatic speech recognition (ASR) and video transcription -- i.e., closed captioning capabilities on streaming videos and TV. In a demonstration, the new joint audio-visual model produced more accurate captions in scenarios where two or more speakers were involved.Surprised at first by how well their method worked, the researchers are excited about its future potential.""We haven't seen speech separation done 'in-the-wild' at such quality before. This is why we see an exciting future for this technology,"" notes Ephrat. ""There is more work needed before this technology lands in consumer hands, but with the promising preliminary results that we've shown, we can certainly see it supporting a range of applications in the future, like video captioning, video conferencing, and even improved hearing aids if such devices could be combined with cameras.""The researchers are currently exploring opportunities for incorporating it into various Google products.Story Source:Materials provided by Association for Computing Machinery. ",0.0806386,0.105854,0.134035,0.264682,0.150919,0.133233,0.170395,0,0
732,Detecting the birth and death of a phonon,"Phonons are discrete units of vibrational energy predicted by quantum mechanics that correspond to collective oscillations of atoms inside a molecule or a crystal. When such vibrations are produced by light interacting with a material, the vibrational energy can be transferred back and forth between individual phonons and individual packets of light energy, the photons. This process is called the Raman effect.In a new study, the lab of Christophe Galland at EPFL's Institute of Physics has developed a technique for measuring, in real time and at room-temperature, the creation and destruction of individual phonons, opening up exciting possibilities in various fields such as spectroscopy and quantum technologies.The technique uses ultra-short laser pulses, which are bursts of light that last less than 10-13 seconds (a fraction of a trillionth of a second). First, one such pulse is shot onto a diamond crystal to excite a single phonon inside it. When this happens, a partner photon is created at a new wavelength through the Raman effect and is observed with a specialized detector, heralding the success of the preparation step.Second, to interrogate the crystal and probe the newly created phonon, the scientists fire another laser pulse into the diamond. Thanks to another detector, they now record photons that have reabsorbed the energy of the vibration. These photons are witnesses that the phonon was still alive, meaning that the crystal was still vibrating with exactly the same energy.This is in strong contradiction with our intuition: we are used to seeing vibrating objects progressively lose their energy over time, like a guitar string whose sound fades away. But in quantum mechanics this is ""all or nothing"": the crystal either vibrates with a specific energy or it is in its resting state; there is no state allowed in between. The decay of the phonon over time is therefore observed as a decrease of the probability of finding it in the excited state instead of having jumped down to the rest state.Through this approach, the scientists could reconstruct the birth and death of a single phonon by analyzing the output of the two photon detectors. ""In the language of quantum mechanics, the act of measuring the system after the first pulse creates a well-defined quantum state of the phonon, which is probed by the second pulse,"" says Christophe Galland. ""We can therefore map the phonon decay with very fine time resolution by changing the time delay between the pulses from zero to a few trillionths of a second (10-12 seconds or picoseconds).""The new technique can be applied to many different types of materials, from bulk crystals down to single molecules. It can also be refined to create more exotic vibrational quantum states, such as entangled states where energy is ""delocalized"" over two vibrational modes. And all this can be performed in ambient conditions, highlighting that exotic quantum phenomena may occur in our daily life -- we just need to watch very fast.Story Source:Materials provided by Ecole Polytechnique Federale de Lausanne. ",0.0980836,0.146439,0.11979,0.157578,0.160797,0.152298,0.158839,0,0
733,Tunable diamond string may hold key to quantum memory. A process similar to guitar tuning improves storage time of quantum memory,"A quantum internet promises completely secure communication. But using quantum bits or qubits to carry information requires a radically new piece of hardware -- a quantum memory. This atomic-scale device needs to store quantum information and convert it into light to transmit across the network.A major challenge to this vision is that qubits are extremely sensitive to their environment, even the vibrations of nearby atoms can disrupt their ability to remember information. So far, researchers have relied on extremely low temperatures to quiet vibrations but, achieving those temperatures for large-scale quantum networks is prohibitively expensive.Now, researchers at the Harvard John A. Paulson School of Engineering and Applied Sciences (SEAS) and the University of Cambridge have developed a quantum memory solution that is as simple as tuning a guitar.The researchers engineered diamond strings that can be tuned to quiet a qubit's environment and improve memory from tens to several hundred nanoseconds, enough time to do many operations on a quantum chip.""Impurities in diamond have emerged as promising nodes for quantum networks,"" said Marko Loncar, the Tiantsai Lin Professor of Electrical Engineering at SEAS and senior author of the research. ""However, they are not perfect. Some kinds of impurities are really good at retaining information but have a hard time communicating, while others are really good communicators but suffer from memory loss. In this work, we took the latter kind and improved the memory by ten times.""The research is published in Nature Communications.Impurities in diamond, known as silicon-vacancy color centers, are powerful qubits. An electron trapped in the center acts as a memory bit and can emit single photons of red light, which would in turn act as long-distance information carriers of a quantum internet. But with the nearby atoms in the diamond crystal vibrating randomly, the electron in the center quickly forgets any quantum information it is asked to remember.""Being an electron in a color center is like trying to study at a loud marketplace,"" said Srujan Meesala, a graduate student at SEAS and co-first author of the paper. ""There is all this noise around you. If you want to remember anything, you need to either ask the crowds to stay quiet or find a way to focus over the noise. We did the latter.""To improve memory in a noisy environment, the researchers carved the diamond crystal housing the color center into a thin string, about one micron wide -- a hundred times thinner than a strand of hair -- and attached electrodes to either side. By applying a voltage, the diamond string stretches and increases the frequency of vibrations the electron is sensitive to, just like tightening a guitar string increases the frequency or pitch of the string.""By creating tension in the string, we increase the energy scale of vibrations that the electron is sensitive to, meaning it can now only feel very high energy vibrations,"" said Meesala. ""This process effectively turns the surrounding vibrations in the crystal to an irrelevant background hum, allowing the electron inside the vacancy to comfortably hold information for hundreds of nanoseconds, which can be a really long time on the quantum scale. A symphony of these tunable diamond strings could serve as the backbone of a future quantum internet.""Next, the researchers hope to extend the memory of the qubits to the millisecond, which would enable hundreds of thousands of operations and long-distance quantum communication.The Harvard Office of Technology Development has protected the intellectual property relating to this project and is exploring commercialization opportunities.Story Source:Materials provided by Harvard John A. Paulson School of Engineering and Applied Sciences. Original written by Leah Burrows. ",0.0783927,0.121489,0.126637,0.227666,0.161116,0.127483,0.161831,0,0
734,Can a quantum drum vibrate and stand still at the same time?,"Researchers have studied how a 'drumstick' made of light could make a microscopic 'drum' vibrate and stand still at the same time.A team of researchers from the UK and Australia have made a key step towards understanding the boundary between the quantum world and our everyday classical world.Quantum mechanics is truly weird. Objects can behave like both particles and waves, and can be both here and there at the same time, defying our common sense. Such counterintuitive behaviour is typically confined to the microscopic realm and the question ""why don't we see such behaviour in everyday objects?"" challenges many scientists today.Now, a team of researchers have developed a new technique to generate this type of quantum behaviour in the motion of a tiny drum just visible to the naked eye. The details of their research are published today in New Journal of Physics.Project principal investigator, Dr Michael Vanner from the Quantum Measurement Lab at Imperial College London, said: ""Such systems offer significant potential for the development of powerful new quantum-enhanced technologies, such as ultra-precise sensors, and new types of transducers.""Excitingly, this research direction will also enable us to test the fundamental limits of quantum mechanics by observing how quantum superpositions behave at a large scale.""Mechanical vibrations, such as those that create the sound from a drum, are an important part of our everyday experience. Hitting a drum with a drumstick causes it to rapidly move up and down, producing the sound we hear.In the quantum world, a drum can vibrate and stand still at the same time. However, generating such quantum motion is very challenging. lead author of the project Dr Martin Ringbauer from the University of Queensland node of the Australian Research Council Centre for Engineered Quantum Systems, said: ""You need a special kind of drumstick to make such a quantum vibration with our tiny drum.""In recent years, the emerging field of quantum optomechanics has made great progress towards the goal of a quantum drum using laser light as a type of drumstick. However, many challenges remain, so the authors' present study takes an unconventional approach.Dr Ringbauer continues: ""We adapted a trick from optical quantum computing to help us play the quantum drum. We used a measurement with single particles of light -- photons -- to tailor the properties of the drumstick.""This provides a promising route to making a mechanical version of Schrodinger's cat, where the drum vibrates and stands still at the same time.""These experiments have made the first observation of mechanical interferences fringes, which is a crucial step forward for the field.In the experiment, the fringes were at a classical level due to thermal noise, but motivated by this success, the team are now working hard to improve their technique and operate the experiments at temperatures close to absolute zero where quantum mechanics is expected to dominate.These future experiments may reveal new intricacies of quantum mechanics and may even help light the path to a theory that links the quantum world and the physics of gravity.Story Source:Materials provided by Imperial College London. ",0.1086,0.133013,0.136143,0.183909,0.129662,0.191125,0.177259,0,0
735,"Underwater acoustic ground cloak designed Researchers engineer material with properties not typically found in nature, concealing object from sound waves","Cloaking devices play a pivotal role in many sci-fi television programs. Scientists are now working to take this technology from the dramatic realm of science fiction and make it real. Amanda D. Hanford, at Pennsylvania State University, is taking the introductory steps to make acoustic ground cloaks. These materials redirect approaching waves around an object without scattering the wave energy, concealing the object from the sound waves.During the 175th Meeting of the Acoustical Society of America, being held May 7-11, 2018, in Minneapolis, Minnesota, Hanford will describe the physics behind an underwater acoustic shield designed in her lab.Hanford and her team set out to engineer a metamaterial that can allow the sound waves to bend around the object as if it were not there. Metamaterials commonly exhibit extraordinary properties not found in nature, like negative density. To work, the unit cell -- the smallest component of the metamaterial -- must be smaller than the acoustic wavelength in the study.""These materials sound like a totally abstract concept, but the math is showing us that these properties are possible,"" Hanford said. ""So, we are working to open the floodgates to see what we can create with these materials.""To date, most acoustic metamaterials have been designed to deflect sound waves in air. Hanford decided to take this work one step further and accept the scientific challenge of trying the same feat underwater. Acoustic cloaking underwater is more complicated because water is denser and less compressible than air. These factors limit engineering options.After multiple attempts, the team designed a 3-foot-tall pyramid out of perforated steel plates. They then placed the structure on the floor of a large underwater research tank. Inside the tank, a source hydrophone produced acoustic waves between 7,000 Hz and 12,000 Hz, and several receiver hydrophones around the tank monitored reflected acoustic waves.The wave reflected from the metamaterial matched the phase of the reflected wave from the surface. Additionally, the amplitude of the reflected wave from the cloaked object decreased slightly. These results demonstrate that this material could make an object appear invisible to underwater instruments like sonar.Using linear coordinate transformation, the researchers were able to map the flat surface of the bottom of the tank and determined that space was compressed into two triangular cloaking regions consisting of the engineered metamaterial.These results show potential to contribute to real-world applications, such as acoustic materials to dampen sound and appear invisible underwater.Penn State University researchers Dean E. Capone, Ph.D., and Benjamen  S. Beck, Ph.D., along with Hanford's former graduate student Peter  Kerrian, Ph.D., also contributed to this project.Story Source:Materials provided by Acoustical Society of America. ",0.147434,0.128566,0.151698,0.165744,0.147324,0.181061,0.155378,0,0
736,"Ultrasonic attack is unlikely, but incidental exposure presents plenty of problems While ultrasonic death rays are still a thing of fiction, there is growing evidence that exposure to ultrasonic waves may cause adverse health effects","New technologies for mobile devices may use ultrasonic sound waves for a variety of purposes, from charging your phone when you enter your room to collecting data on which advertisements you watch. Pest deterrents, dog controllers, some automatic sliding doors, public address voice alarms -- and even a device marketed in the U.K. as a teenager repellant to keep kids from loitering outside storefronts -- also emit ultrasound at different frequencies.These devices have varying effects on different subsets of the population, and many of these are covert, or inaudible, so the public usually doesn't know when they are exposed.Regulation of these emerging technologies is in many ways ""the wild west,"" according to Timothy Leighton at the University of Southampton, who wrote a guide for moving forward in today's new world of ultrasonic exposure. His work outlines a logical approach for advancing research on ultrasonics.""I looked at all this stuff, and I said, 'This is an amazing detective story,'"" Leighton said. ""I began to pull together a picture that, in fact, the public was being exposed.""Leighton is an expert in ultrasonics and underwater acoustics, who analyzed the landscape of ultrasonic regulations. He discovered a string of regulations that were all based on insufficient data, but had been shared around the globe.""It was extraordinary when you opened up this topic,"" Leighton said. He found that many national safety guideline limits could be traced back to a handful of studies performed in the 1970s on ""a small number of adult men, many of whom had experienced gunfire, some of whom had worked in the rock-n-roll hi-fi industry... So it was inadequate data on which to base standards.""Emerging technologies are often marketed as harmless in terms of emitted ultrasound. Ultimately, Leighton found that these claims are not based on any scientific evidence.He found a series of anecdotal complaints, ranging from nausea and dizziness to irritability and ringing ears, that were attributed to ultrasound exposure. However, there is little data in this field, partly because studies to test exposure on humans could be unethical. So, obtaining data from human subjects will require innovative approaches to research.Leighton will describe his work uncovering the strange history and uncertain future of the use of ultrasonic sound waves during the 175th Meeting of the Acoustical Society of America, to be held May 7-11, 2018, in Minneapolis, Minnesota. He will also discuss why the 2017 incident in Cuba -- when U.S. embassy workers experienced strange symptoms of hearing loss and confusion -- was unlikely to have been caused by ultrasound.""I think 70 years is too long to go with inappropriate guidelines,"" Leighton said. ""If it's public exposure, you're going to have children and newborns exposed as well, and we have no information whatsoever on how safe it is for them.""Story Source:Materials provided by Acoustical Society of America. ",0.117404,0.145637,0.185294,0.218515,0.157043,0.168967,0.181773,0,0
737,"Can 'local acoustic treatment' reduce speech distraction within open-plan offices? If you've ever been distracted by noisy people while attempting to concentrate on work within an open-plan office, 'local acoustic treatment' based on principles of listening to your own voice may be able to provide relief in the future","Overhearing conversations from nearby workstations can be one of the most distracting aspects of working in an open-plan office. To make these environments less noisy, while still providing acoustic support for speaking and listening, researchers are creating small ""acoustic islands"" using high-back chairs and retroreflective ceilings to direct sound to help you hear your own conversations -- not others' -- better.An international standard, ISO 3382-3, specifies measurements of the acoustical properties of a room and it focuses on large-scale acoustic measurements and solutions. These solutions aren't always effective for the short conversational distances in open-plan offices, according to researchers in Australia, who've discovered that open-plan spaces could benefit from additional specialized and local acoustic treatments.During the 175th Meeting of the Acoustical Society of America, being held May 7-11, 2018, in Minneapolis, Minnesota, Manuj Yadav, a postdoctoral research associate at the University of Sydney, will present his and his colleagues' work toward solutions to the speech distraction problem in open-plan offices.""The local acoustic treatment we've studied so far includes the use of high-back chairs -- with or without sound absorption near the head -- and 'retroreflective ceilings' that reflect sound back in the direction of the source, which allows you to hear your own voice reflected back to your ears much louder than is possible with flat or other types of hard ceiling surfaces or absorptive ceilings,"" Yadav said.People tend to ""simply put, lower their voices when they receive some support in the form of reflections from the room and nearby surfaces,"" Yadav said. ""The opposite situation is true in a conversational context, in which people tend to raise their voices if they don't think their voice is audible to the person they're talking to. A somewhat extreme example is everyone progressively raising their voices within a large gathering to talk to someone at less than an arm's length, since they can't hear others or themselves -- the cocktail party effect.""Retroreflective surfaces aren't commonly used in acoustics today, so their use as a ceiling treatment in offices is a novel application. ""We hope that our work encourages more researchers to explore the use of retroreflectors, which are likely to be useful in cases in which support for listening to your own voice is needed in the form of strong reflections from nearby surfaces,"" Yadav said.The researchers' work with high-back chairs is also one of the first efforts to quantify the acoustic effect of chairs or furniture ensembles. ""This may encourage some manufacturers of these products, which are becoming quite popular, to actually measure their acoustic effects,"" Yadav said. ""Unfortunately, today these products are just labeled 'acoustic' for marketing purposes -- without any data or justification.""So far, Yadav and colleagues have found that their local acoustic treatment ""can, in many cases, provide substantial enhancement for speech communication at short distances and reduce the disturbance due to ambient noise when you're trying to concentrate,"" he said. ""This work, however, centers mostly on laboratory-based studies and much more needs to be done in terms of testing with actual people in different communication scenarios, but the effects are there for anyone to hear.""Story Source:Materials provided by Acoustical Society of America. ",0.103847,0.0942503,0.161137,0.193055,0.131232,0.139613,0.15227,0,0
738,A first for quantum physics: Electron orbitals manipulated in diamonds,"While defects in a diamond are mostly undesirable, certain defects are a quantum physicist's best friend, having the potential to store bits of information that could one day be used in a quantum computing system.Applied physicists at Cornell University have demonstrated a technique for engineering some of the key optical properties of those defects, providing a new tool for exploring quantum mechanics.A group of researchers led by Greg Fuchs, professor of applied and engineering physics, have become the first to use vibrations produced by a resonator to help stabilize those optical properties, forcing the diamond's electrons into an excited orbital state. The research is detailed in the paper ""Orbital State Manipulation of a Diamond Nitrogen-Vacancy Center Using a Mechanical Resonator,"" published April 17 in the journal Physical Review Letters.Much like a computer's transistors record binary information by being either ""on"" or ""off,"" the internal states of these atomic-scale diamond defects can also represent bits of information, such as its spin -- an intrinsic form of angular momentum -- being ""up"" or ""down."" But unlike transistors, which only have two states, spin possesses the quantum ability to be up and down at the same time. Used in combination, these quantum states could record and share information exponentially better than transistors, allowing computers to perform certain calculations at once-unimaginable speeds.The challenge: It's difficult to transfer quantum information from one place to another. Physicists have experimented with a number of materials and techniques for doing so, including the use of optical properties inside the atomic defects of diamonds known as nitrogen-vacancy centers.""One thing diamond nitrogen-vacancy centers can be quite good at is communication. So you can have an electron spin, which is a good quantum state, then you can transfer its state into a photon of light,"" said Fuchs, who added that the photon can then carry that bit of information to another defect. ""One of the challenges of doing that is stabilizing it and making it work the way you want. We've provided a new toolbox for engineering that optical transition in way to hopefully make it better.""It was first necessary for the research team to engineer a device that could send vibrational waves through the diamond defect. A gigahertz-frequency mechanical resonator was fabricated from a single-crystal diamond, then sound waves vibrating at about 1 gigahertz were sent through defect.The goal was to use the sound to change the defect's optical transitions, in which the change from one energy state to another results in the emission of a photon. These transitions tend to fluctuate based on various environmental conditions, making it difficult to produce coherent photons to carry information.As an example, randomly fluctuating electric fields can make optical transition wavelength unstable, according to Huiyao Chen, a doctoral student who led the study.""To suppress the effect of these incoherent fluctuations,"" Chen said, ""one thing that we can do is eliminate the coupling between the electron orbital and the unwanted, random electric fields. And that's where the sound waves produced by the resonator come into play.""To know if the experiment worked, the research team used a microscope with a tunable wavelength laser to scan the diamond's nitrogen-vacancy center. When the wavelength of the laser was in resonance with the optical transition, an emitted photon could be seen, a sure indicator that the electrons had reached an excited state. The researchers then studied how the sound waves could alter the orbital states, and thus change the optical transition.Story Source:Materials provided by Cornell University. ",0.0922911,0.123993,0.111355,0.171671,0.163079,0.12517,0.147067,0,0
739,Energy conversion: Optical 'overtones' for solar cells,"Nanosystems Initiative Munich (NIM) scientists from Ludwig-Maximilians-Universitaet (LMU) in Munich have found a new effect regarding the optical excitation of charge carriers in a solar semiconductor. It could facilitate the utilization of infrared light, which is normally lost in solar devices.Semiconductors are nowadays the most prominent materials to convert solar light into usable electric energy. The International Energy Agency (IEA) reported that half a million solar panels were installed every day around the world last year. However, semiconductor-based solar cells still suffer from relatively low energy conversion efficiencies. The reason for that mainly lies in the fact that semiconductors efficiently convert the light from a quite small portion of the solar spectrum into electrical power. The spectral position of this window of light that can be efficiently converted is strongly related to a property of the semiconductor involved (that is, its band-gap). This means that, if the semiconductor is designed to absorb yellow light, longer-wavelength light (such as red and infrared light), will pass through the material without producing currents. Additionally, shorter-wavelength light (green, blue and UV light), that is more energetic than yellow light, will lose its additional amount of energy into heat. Obtaining higher energy conversion efficiencies from semiconductors is therefore still a big challenge.Perovskite nanocrystals for energy conversionTo study these limitations, Aurora Manzi, a PhD student from the Chair for Photonics led by Prof. Jochen Feldmann, has measured the charge carrier density created by the absorption of multiple photons in perovskite nanocrystals, a novel and promising material for photovoltaic applications.""Multiple photon absorption of long-wavelength light with an energy lower than the semiconductor absorption window is usually very inefficient.,"" highlights Manzi, first author of the publication in Nature Communications and a student of the NIM graduate program. ""I was therefore totally surprised to observe that for specific excitation wavelengths the efficiency of this process becomes drastically enhanced. At the beginning this did not make any sense to us!""Light and exciton ""overtones"" in resonanceAfter intense discussions, the team of LMU scientists realized that these resonances occur when multiples of two distinct fundamental frequencies become equal, namely that of the frequency of the primary light oscillation and that of the frequency of the band gap or more precisely of the exciton at the band-gap.One could draw an analogy to resonance or overtone phenomena in acoustics, commonly used in music instruments. When intense red light impinges on nano-structured perovskite nanocrystals, a process similar to the generation of overtones in a guitar string takes place. The fundamental light wavelength generates higher order optical harmonics, that are overtones whose frequencies are integer multiples of the primary light oscillation. When such a ""light overtone"" becomes resonant with an overtone of the excitonic band-gap, the energy exchange is enhanced leading to an increased generation of charge carriers or more precisely of multiple excitons at the band gap.Starting point for further research""The resonances observed are analogous to the physical phenomena taking place in two different strings of a guitar,"" continues Manzi. ""If we associate the first string to the light excitation and the second string to the semiconductor excitonic band-gap, we know from acoustics that they will get into resonance if a certain harmonic of the first string will match another harmonic of the second string.""""The observation of this novel resonance phenomenon for optical excitations in excitonic semiconductors could pave the way for solar cells to more efficiently convert long-wavelength light into usable electric power,"" adds Prof. Feldmann, the leader of the research team. ""This is an exciting new finding with a possible impact for future solar devices. Together with our colleagues from the Research Network ""Solar Technologies Go Hybrid"" (SolTech), we will now try to develop innovative applications by playing with such overtones.""Story Source:Materials provided by Ludwig-Maximilians-Universitet Menchen. ",0.0957562,0.12016,0.11402,0.153303,0.177957,0.14434,0.13948,0,0
740,What makes someone believe or reject science? Quality of recordings,"Separating fact from fiction in the age of alternate facts is becoming increasingly difficult, and now a new study has helped reveal why. Research by Dr Eryn Newman of The Australian National University (ANU) has found that when people listen to recordings of a scientist presenting their work, the quality of audio had a significant impact on whether people believed what they were hearing, regardless of who the researcher was or what they were talking about.Dr Newman, of the ANU Research School of Psychology, said the results showed when it comes to communicating science, style can triumph over substance.""When people are assessing the credibility of information, most of the time people are making a judgement based on how something feels,"" Dr Newman said.""Our results showed that when the sound quality was poor, the participants thought the researcher wasn't as intelligent, they didn't like them as much and found their research less important.""The study used experiments where people viewed video clips of scientists speaking at conferences. One group of participants heard the recordings in clear high-quality audio, while the other group heard the same recordings with poor-quality audio.Participants were then asked to evaluate the researchers and their work. Those who listened to the poorer quality audio consistently evaluated the scientists as less intelligent and their research as less important.In a second experiment, researchers upped the ante and conducted the same experiment using renowned scientists discussing their work on the well-known US Science Friday radio program. This time the recordings included audio of the scientists being introduced with their qualifications and institutional affiliations.""It made no difference,"" she said.""As soon as we reduced the audio quality, all of a sudden the scientists and their research lost credibility.""As with the first experiments, participants thought the research was worse, the scientists were less competent and they also reported finding their work less interesting.Dr Newman said in a time when genuine science is struggling to be heard above fake news and alternate facts, researchers need to consider not only the content of their messages, but features of the delivery.""Another recent study showed false information travels six times faster than real information on Twitter,"" she said.""Our results show that it's not just about who you are and what you are saying, it's about how your work is presented.""A research paper for the study has been published in the journals Science Communication.The study was co-authored by Professor Norbert Schwarz of the University of Southern California.Story Source:Materials provided by Australian National University. ",0.0983462,0.141042,0.176347,0.231921,0.155618,0.149373,0.174226,0,0
741,Microphone for light: Tiny guitar string vibrates 1 billion times when plucked,"Strain can be used to engineer unusual properties at the nanoscale. Researchers in Tobias Kippenberg's lab at EPFL have harnessed this effect to engineer an extremely low loss nanostring. When plucked, the string vibrates for minutes with a period of a microsecond (equivalent to a standard guitar note playing for a month). Using it as an ultrasensitive microphone, the researchers hope to be able to ""hear"" the sound of photons in a laser beam. The work is published in Science.A lesson in stress managementFor a mechanical engineer, stress is usually a nuisance. Properly managed, however, it can also be a powerful tool: An elastic body responds to stress by adjusting the distance between its atoms (strain), which can be used to control the properties of its electrons. One example of such ""elastic strain engineering"" is the modern transistor, whose operating speed is enhanced by stressing its silicon gate material.Stress can also be used to engineer the properties of an elastic body. Stretching a guitar string, for example, will change not only its sound (its vibrational frequency), but also its quality factor (the number of vibrations produced by a single pluck). This effect, known as ""dissipation dilution,"" in undesirable in many musical circles, but in other fields can be a tremendous advantage.Bigger is not always betterOne such field is nanomechanics, where the quality factor an oscillator dictates its utility for applications such as force sensing. Over the last decade, strained nanomechanical oscillators have emerged as an important paradigm owing to their anomalously high quality factors; however, this trend is not as much a design choice as an artifact of large stresses naturally produced at the nanoscale.Armed with a powerful set of tools at EPFL's Center of MicroNanoTechnology, researchers in Kippenberg's lab set about engineering nanomechanical devices with deliberately enhanced stress and dissipation dilution. They found that a string is an ideal geometry for this, although its motion must be localized away from its supports and co-localized with its internal stress profile.To meet these requirements, the researchers patterned the string into a periodic structure in which vibrations could be trapped around a central defect: a phononic crystal. To co-localize strain, the defect is carefully tapered, and the entire pattern is printed onto a string of roughly 10 nm thick and 1 cm long (the equivalent of stretching the Golden Gate bridge across the Pacific ocean).Measurements made on nanostring devices at room temperature reveal localized modes that vibrate at 1 MHz for tens of minutes, corresponding to a quality factor of 800 million. Transposed onto a standard guitar string, an equivalent ""note"" would play for a month.Listening to lightBy dint of their small mass and extreme quality factors, nanostrings similar to those developed in the Kippenberg lab are expected to have an important impact on traditional sensing applications. Operated as force sensors, for example, they are capable of detecting local disturbances at the level of attonewtons, equivalent to the gravitational pull between human beings.One intriguing application is to detect weak light forces. By coupling a nanostring to an optical waveguide, Kippenberg's lab recently demonstrated the ability to ""hear"" the gentle sound of photons flowing in a laser beam (each imparting a tiny radiation pressure force to the string). In a surprising twist, they showed how this measurement could be used to generate a nonclassical state of light known as squeezed light, which can be used to enhance the sensitivity of an optical interferometer.They are now asking a different question: is it possible to use the same light field to ""see"" the vacuum fluctuations of the nanostring (a consequence of its phonone-like nature)? ""Heisenberg's uncertainty principle predicts that the two capabilities are commensurate,"" says Dalziel Wilson, one of the paper's authors. ""Operating at this so-called standard quantum limit offers the possibility of cooling a tangibly-sized mechanical object from room temperature to absoluate zero (its motional ground state), the starting point for myriad quantum experiments.""Story Source:Materials provided by Ecole Polytechnique Federale de Lausanne. ",0.0702963,0.0880851,0.104773,0.136506,0.138273,0.100345,0.130153,0,0
742,3-D printed active metamaterials for sound and vibration control Researchers develop 3-D printed acoustic metamaterials that can be switched on and off remotely using a magnetic field,"Researchers have been pushing the capabilities of materials by carefully designing precise structures that exhibit abnormal properties that can control acoustic or optical waves. However, these metamaterials are constructed in fixed geometries, meaning their unique abilities are always fixed. Now, new 3-D printed metamaterial developed by a team led by University of Southern California researchers can be remotely switched between active control and passive states.USC Viterbi School of Engineering Assistant Professor Qiming Wang and Ph.D. student Kun-Hao Yu, along with MIT Professor Nicholas Fang and University of Missouri Professor Guoliang Huang, have developed 3-D printed metamaterials capable of blocking sound waves and mechanical vibrations. Unlike current metamaterials, these can be turned on or off remotely using a magnetic field. Their materials can be used for noise cancellation, vibration control and sonic cloaking, which can be used to hide objects from acoustic waves.""When you fabricate a structure, the geometry cannot be changed, which means the property is fixed. The idea here is, we can design something very flexible so that you can change it using external controls,"" said Wang, an assistant professor of civil and environmental engineering.Metamaterials can be used to manipulate wave phenomena such as radar, sound and light and have been used to develop technology such as cloaking devices and improved communication systems. The team's metamaterials are able to control environmental sounds and structural vibrations, which have similar waveforms. By 3-D printing a deformable material containing iron particles in a lattice structure, their metamaterials can be compressed using a magnetic field.""You can apply an external magnetic force to deform the structure and change the architecture and the geometry inside it. Once you change the architecture, you change the property,"" Wang said. ""We wanted to achieve this kind of freedom to switch between states. Using magnetic fields, the switch is reversible and very rapid.""The magnetic field compresses the material, but unlike a physical contact force like a metal plate, the material is not constrained. Therefore, when an acoustic or mechanical wave contacts the material, it perturbs it, generating the unique properties that block sound waves and mechanical vibrations of certain frequencies from passing through.The mechanism relies on the abnormal properties of their metamaterials -- negative modulus and negative density. In everyday materials, these are both positive.""Material with a negative modulus or negative density can trap sounds or vibrations within the structure through local resonances so that they cannot transfer through it,"" Yu said.Typically, when you push on an object, it pushes back against you. In contrast, objects with a negative modulus attract you, pulling you towards them as you push. Objects exhibiting a negative density work in a similarly contradictory way. When you push these objects away from you, they instead move toward you.One negative property, either negative modulus or negative density, can work independently to block noise and stop vibrations within certain frequency regimes. However, when working together, the noise or vibration can pass through again. The team is able to maintain versatile control over the metamaterial, switching among double-positive (sound passing), single-negative (sound blocking), and double-negative (sound passing) just by switching the magnetic field.""This is the first time researchers have demonstrated reversible switching among these three phases using remote stimuli,"" Wang said.Future directionsWang believes they may be able to demonstrate another unique property called negative refraction, in which a wave goes through the material and comes back in at an unnatural angle, which according to Wang is, ""anti-physics."" They plan to study this phenomenon further once they are able to fabricate larger structures.""We want to scale down or scale up our fabrication system,"" Wang said. ""This would give us more opportunity to work on a larger range of wavelengths.""With their current system, they can only 3-D print material with a beam diameter between a micron to a millimeter. But size matters. Smaller beams would control higher frequency waves, and larger beams would affect lower frequency waves.""There are indeed a number of possible applications for smartly controlling acoustics and vibrations,"" Yu said. ""Traditional engineering materials may only shield from acoustics and vibrations, but few of them can switch between on and off.""Story Source:Materials provided by University of Southern California. ",0.0892116,0.0852327,0.129071,0.124396,0.124535,0.146777,0.123886,0,0
743,Introducing the latest in textiles: Soft hardware Researchers incorporate optoelectronic diodes into fibers and weave them into washable fabrics,"The latest development in textiles and fibers is a kind of soft hardware that you can wear: cloth that has electronic devices built right into it.Researchers at MIT have now embedded high speed optoelectronic semiconductor devices, including light-emitting diodes (LEDs) and diode photodetectors, within fibers that were then woven at Inman Mills, in South Carolina, into soft, washable fabrics and made into communication systems. This marks the achievement of a long-sought goal of creating ""smart"" fabrics by incorporating semiconductor devices -- the key ingredient of modern electronics -- which until now was the missing piece for making fabrics with sophisticated functionality.This discovery, the researchers say, could unleash a new ""Moore's Law"" for fibers -- in other words, a rapid progression in which the capabilities of fibers would grow rapidly and exponentially over time, just as the capabilities of microchips have grown over decades.The findings are described this week in the journal Nature in a paper by former MIT graduate student Michael Rein; his research advisor Yoel Fink, MIT professor of materials science and electrical engineering and CEO of AFFOA (Advanced Functional Fabrics of America); along with a team from MIT, AFFOA, Inman Mills, EPFL in Lausanne, Switzerland, and Lincoln Laboratory.Optical fibers have been traditionally produced by making a cylindrical object called a ""preform,"" which is essentially a scaled-up model of the fiber, then heating it. Softened material is then drawn or pulled downward under tension and the resulting fiber is collected on a spool.The key breakthrough for producing these new fibers was to add to the preform light-emitting semiconductor diodes the size of a grain of sand, and a pair of copper wires a fraction of a hair's width. When heated in a furnace during the fiber-drawing process, the polymer preform partially liquified, forming a long fiber with the diodes lined up along its center and connected by the copper wires.In this case, the solid components were two types of electrical diodes made using standard microchip technology: light-emitting diodes (LEDs) and photosensing diodes. ""Both the devices and the wires maintain their dimensions while everything shrinks around them"" in the drawing process, Rein says. The resulting fibers were then woven into fabrics, which were laundered 10 times to demonstrate their practicality as possible material for clothing.""This approach adds a new insight into the process of making fibers,"" says Rein, who was the paper's lead author and developed the concept that led to the new process. ""Instead of drawing the material all together in a liquid state, we mixed in devices in particulate form, together with thin metal wires.""One of the advantages of incorporating function into the fiber material itself is that the resulting fiber is inherently waterproof. To demonstrate this, the team placed some of the photodetecting fibers inside a fish tank. A lamp outside the aquarium transmitted music (appropriately, Handel's ""Water Music"") through the water to the fibers in the form of rapid optical signals. The fibers in the tank converted the light pulses -- so rapid that the light appears steady to the naked eye -- to electrical signals, which were then converted into music. The fibers survived in the water for weeks.Though the principle sounds simple, making it work consistently, and making sure that the fibers could be manufactured reliably and in quantity, has been a long and difficult process. Staff at the Advanced Functional Fabric of America Institute, led by Jason Cox and Chia-Chun Chung, developed the pathways to increasing yield, throughput, and overall reliability, making these fibers ready for transitioning to industry. At the same time, Marty Ellis from Inman Mills developed techniques for weaving these fibers into fabrics using a conventional industrial manufacturing-scale loom.""This paper describes a scalable path for incorporating semiconductor devices into fibers. We are anticipating the emergence of a 'Moore's law' analog in fibers in the years ahead,"" Fink says. ""It is already allowing us to expand the fundamental capabilities of fabrics to encompass communications, lighting, physiological monitoring, and more. In the years ahead fabrics will deliver value-added services and will no longer just be selected for aesthetics and comfort.""He says that the first commercial products incorporating this technology will be reaching the marketplace as early as next year -- an extraordinarily short progression from laboratory research to commercialization. Such rapid lab-to-market development was a key part of the reason for creating an academic-industry-government collaborative such as AFFOA in the first place, he says. These initial applications will be specialized products involving communications and safety. ""It's going to be the first fabric communication system. We are right now in the process of transitioning the technology to domestic manufacturers and industry at an unprecedented speed and scale,"" he says.In addition to commercial applications, Fink says the U.S. Department of Defense -- one of AFFOA's major supporters -- ""is exploring applications of these ideas to our women and men in uniform.""Beyond communications, the fibers could potentially have significant applications in the biomedical field, the researchers say. For example, devices using such fibers might be used to make a wristband that could measure pulse or blood oxygen levels, or be woven into a bandage to continuously monitor the healing process.Story Source:Materials provided by Massachusetts Institute of Technology. Original written by David L. Chandler. ",0.0825227,0.112771,0.131045,0.167947,0.162194,0.125896,0.138636,0,0
744,This is what a stretchy circuit looks like,"Researchers in China have made a new hybrid conductive material -- part elastic polymer, part liquid metal -- that can be bent and stretched at will. Circuits made with this material can take most two-dimensional shapes and are also non-toxic. The work appears June 14 in the new interdisciplinary journal iScience.""These are the first flexible electronics that are at once highly conductive and stretchable, fully biocompatible, and able to be fabricated conveniently across size scales with micro-feature precision,"" says senior author Xingyu Jiang, a professor at the National Center for Nanoscience and Technology. ""We believe that they will have broad applications for both wearable electronics and implantable devices.""The material that the researchers fashioned is called a metal-polymer conductor (MPC), so called because it is a combination of two components with very different yet equally desirable properties. The metals in this case are not familiar conductive solids, such as copper, silver, or gold, but rather gallium and indium, which exist as thick, syrupy liquids that still permit electricity to flow. The researchers found that embedding globs of this liquid metal mixture within a supporting network of silicone-based polymer yielded mechanically resilient materials with enough conductivity to support functioning circuits.Up close, the structure of the MPC can be likened to round liquid metal islands floating in a sea of polymer, with a liquid metal mantle underneath to ensure full conductivity. The researchers successfully tried out different MPC formulations in a variety of applications, including in sensors for wearable keyboard gloves and as electrodes for stimulating the passage of DNA through the membranes of live cells.""The applications of the MPC depend on the polymers,"" says first author Lixue Tang, a graduate student in Jiang's research group. ""We cast super-elastic polymers to make MPCs for stretchable circuits. We use biocompatible and biodegradable polymers when we want MPCs for implantable devices. In the future, we could even build soft robots by combining electroactive polymers.""In principle, the authors state that their method for manufacturing MPCs, which involves screen printing and microfluidic patterning, can accommodate any two-dimensional geometry, as well as different thicknesses and electrical properties, depending on the concentrations of the liquid metal inks to be sprayed. This versatility could lead directly to desirable biomedical applications, such as flexible patches for identifying and mitigating heart disease.""We wanted to develop biocompatible materials that could be used to build wearable or implantable devices for diagnosing and treating disease without compromising quality of life, and we believe that this is a first step toward changing the way that cardiovascular diseases and other afflictions are managed,"" says Jiang.Story Source:Materials provided by Cell Press. ",0.0905665,0.158499,0.13312,0.165817,0.1445,0.149305,0.165598,0,0
745,E- textiles control home appliances with the swipe of a finger,"Electronic textiles could allow a person to control household appliances or computers from a distance simply by touching a wristband or other item of clothing -- something that could be particularly helpful for those with limited mobility. Now researchers, reporting in ACS Nano, have developed a new type of e-textile that is self-powered, highly sensitive and washable.E-textiles are not new, but most existing versions have poor air permeability, can't be laundered or are too costly or complex to mass-produce. Jiaona Wang, Hengyu Guo, Congju Li and coworkers wanted to develop an E-textile that overcomes all of these limitations and is highly sensitive to human touch.The researchers made a self-powered triboelectric nanogenerator by depositing an electrode array of conductive carbon nanotubes on nylon fabric. To make the E-textile washable, they incorporated polyurethane into the carbon nanotube ink, which made the nanotubes firmly adhere to the fabric. They covered the array with a piece of silk and fashioned the textile into a wristband. When swiped with a finger in different patterns, the E-textile generated electrical signals that were coupled to computers to control programs, or to household objects to turn on lights, a fan or a microwave from across the room. The E-textile is breathable for human skin, washable and inexpensive to produce on a large scale, the researchers say.Story Source:Materials provided by American Chemical Society. ",0.184969,0.200791,0.199837,0.233916,0.240454,0.235485,0.213899,0,0
746,Knitting electronics with yarn batteries,"When someone thinks about knitting, they usually don't conjure up an image of sweaters and scarves made of yarn that can power watches and lights. But that's just what one group is reporting in ACS Nano. They have developed a rechargeable yarn battery that is waterproof and flexible. It also can be cut into pieces and still work.Most people are familiar with smartwatches, but for wearable electronics to progress, scientists will need to overcome the challenge of creating a device that is deformable, durable, versatile and wearable while still holding and maintaining a charge. One dimensional fiber or yarn has shown promise, since it is tiny, flexible and lightweight. Previous studies have had some success combining one-dimensional fibers with flexible Zn-MnO2 batteries, but many of these lose charge capacity and are not rechargeable. So, Chunyi Zhi and colleagues wanted to develop a rechargeable yarn zinc-ion battery that would maintain its charge capacity, while being waterproof and flexible.The group twisted carbon nanotube fibers into a yarn, then coated one piece of yarn with zinc to form an anode, and another with magnesium oxide to form a cathode. These two pieces were then twisted like a double helix and coated with a polyacrylamide electrolyte and encased in silicone. Upon testing, the yarn zinc-ion battery was stable, had a high charge capacity and was rechargeable and waterproof. In addition, the material could be knitted and stretched. It also could be cut into several pieces, each of which could power a watch. In a proof-of-concept demonstration, eight pieces of the cut yarn battery were woven into a long piece that could power a belt containing 100 light emitting diodes (known as LEDs) and an electroluminescent panel.Story Source:Materials provided by American Chemical Society. ",0.203419,0.23357,0.213793,0.22364,0.23287,0.24961,0.216573,0,0
747,Electric textile lights a lamp when stretched,"Working up a sweat from carrying a heavy load? That is when the textile works at its best. Researchers at Chalmers University of Technology have developed a fabric that converts kinetic energy into electric power, in cooperation with the Swedish School of Textiles in Bores and the research institute Swerea IVF. The greater the load applied to the textile and the wetter it becomes the more electricity it generates. The results are now published in the Nature Partner journal Flexible Electronics.Chalmers researchers Anja Lund and Christian Meller have developed a woven fabric that generates electricity when it is stretched or exposed to pressure. The fabric can currently generate enough power to light an LED, send wireless signals or drive small electric units such as a pocket calculator or a digital watch.The technology is based on the piezoelectric effect, which results in the generation of electricity from deformation of a piezoelectric material, such as when it is stretched. In the study the researchers created a textile by weaving a piezoelectric yarn together with an electrically conducting yarn, which is required to transport the generated electric current.""The textile is flexible and soft and becomes even more efficient when moist or wet,"" Lund says. ""To demonstrate the results from our research we use a piece of the textile in the shoulder strap of a bag. The heavier the weight packed in the bag and the more of the bag that consists of our fabric, the more electric power we obtain. When our bag is loaded with 3 kilos of books, we produce a continuous output of 4 microwatts. That's enough to intermittently light an LED. By making an entire bag from our textile, we could get enough energy to transmit wireless signals.""The piezoelectric yarn is made up of twenty-four fibres, each as thin as a strand of hair. When the fibres are sufficiently moist they become enclosed in liquid and the yarn becomes more efficient, since this improves the electrical contact between the fibres. The technology is based on previous studies by the researchers in which they developed the piezoelectric fibres, to which they have now added a further dimension.""The piezoelectric fibres consist of a piezoelectric shell around an electrically conducting core,"" Lund says. ""The piezoelectric yarn in combination with a commercial conducting yarn constitute an electric circuit connected in series.""Previous work by the researchers on piezoelectric textiles has so far mainly focused on sensors and their ability to generate electric signals through pressure sensitivity. Using the energy to continuously drive electronic components is unique.""Woven textiles from piezoelectric yarns makes the technology easily accessible and it could be useful in everyday life. It's also possible to add more materials to the weave or to use it as a layer in a multi-layer product. It requires some modification, but it's possible,"" Lund says.The researchers consider that the technology is, in principle, ready for larger scale production. It is now mainly up to industrial product developers to find out how to make use of the technology. Despite the advanced technology underlying the material, the cost is relatively low and is comparable with the price of Gore-Tex. Through their collaboration with the Swedish School of Textiles in Bores the researchers have been able to demonstrate that the yarn can be woven in industrial looms and is sufficiently wear-resistant to cope with the harsh conditions of mass production.More about: The textile that generates electricityThe textile consists of piezoelectric yarns woven together with electrically conducting yarns. A piezoelectric yarn is made up of 24 fibres each as thin as a strand of hair, with each fibre having an electrically conducting core surrounded by an insulating and piezoelectric polymer. During manufacture the fabric is exposed to a high electric field, which causes positive and negative charges in the polymer to be separated in an orderly manner. When the textile is then stretched or exposed to pressure, the deformation of the fibres causes a reorganisation of the charge distribution, thus generating an electrical voltage. The electrically conducting yarn is required to form a closed circuit through which an electric current can flow.More about: The piezoelectric effectThe piezoelectric effect allows a material to generate an electrical voltage when it is exposed to pressure or stretching. Piezoelectricity is a result of a displacement of the charge distribution in the material due to deformation, and can be found in biological materials such as bone, protein and DNA and also in other types of materials such as ceramics, plastics and textiles.Video: https://vimeo.com/256577138/422cb3db26Story Source:Materials provided by Chalmers University of Technology. ",0.167279,0.15984,0.172687,0.188709,0.183786,0.241425,0.178299,0,0
748,"Printing of flexible, stretchable silver nanowire circuits ","Researchers at North Carolina State University have developed a new technique that allows them to print circuits on flexible, stretchable substrates using silver nanowires. The advance makes it possible to integrate the material into a wide array of electronic devices.Silver nanowires have drawn significant interest in recent years for use in many applications, ranging from prosthetic devices to wearable health sensors, due to their flexibility, stretchability and conductive properties. While proof-of-concept experiments have been promising, there have been significant challenges to printing highly integrated circuits using silver nanowires.Silver nanoparticles can be used to print circuits, but the nanoparticles produce circuits that are more brittle and less conductive than silver nanowires. But conventional techniques for printing circuits don't work well with silver nanowires; the nanowires often clog the printing nozzles.""Our approach uses electrohydrodynamic printing, which relies on electrostatic force to eject the ink from the nozzle and draw it to the appropriate site on the substrate,"" says Jingyan Dong, co-corresponding author of a paper on the work and an associate professor in NC State's Edward P. Fitts Department of Industrial & Systems Engineering. ""This approach allows us to use a very wide nozzle -- which prevents clogging -- while retaining very fine printing resolution.""""And because our 'ink' consists of a solvent containing silver nanowires that are typically more than 20 micrometers long, the resulting circuits have the desired conductivity, flexibility and stretchability,"" says Yong Zhu, a professor of mechanical engineering at NC State and co-corresponding author of the paper.""In addition, the solvent we use is both nontoxic and water-soluble,"" says Zheng Cui, a Ph.D. student at NC State and lead author of the paper. ""Once the circuit is printed, the solvent can simply be washed off.""What's more, the size of the printing area is limited only by the size of the printer, meaning the technique could be easily scaled up.The researchers have used the new technique to create prototypes that make use of the silver nanowire circuits, including a glove with an internal heater and a wearable electrode for use in electrocardiography. NC State has filed a provisional patent on the technique.""Given the technique's efficiency, direct writing capability, and scalability, we're optimistic that this can be used to advance the development of flexible, stretchable electronics using silver nanowires -- making these devices practical from a manufacturing perspective,"" Zhu says.The work was done with support from the National Science Foundation, under grants CMMI-1728370 and CMMI-1333775.Story Source:Materials provided by North Carolina State University. ",0.133935,0.185448,0.177706,0.209173,0.202824,0.186518,0.195504,0,0
749,Engineers advance capability of wearable tech A new low-cost ultra-stretchable sensor can do more with less,"Creating the perfect wearable device to monitor muscle movement, heart rate and other tiny bio-signals without breaking the bank has inspired scientists to look for a simpler and more affordable tool.Now, a team of researchers at UBC's Okanagan campus have developed a practical way to monitor and interpret human motion, in what may be the missing piece of the puzzle when it comes to wearable technology.What started as research to create an ultra-stretchable sensor transformed into a sophisticated inter-disciplinary project resulting in a smart wearable device that is capable of sensing and understanding complex human motion, explains School of Engineering Professor Homayoun Najjaran.The sensor is made by infusing graphene nano-flakes (GNF) into a rubber-like adhesive pad. Najjaran says they then tested the durability of the tiny sensor by stretching it to see if it can maintain accuracy under strains of up to 350 per cent of its original state. The device went through more than 10,000 cycles of stretching and relaxing while maintaining its electrical stability.""We tested this sensor vigorously,"" says Najjaran. ""Not only did it maintain its form but more importantly it retained its sensory functionality. We have further demonstrated the efficacy of GNF-Pad as a haptic technology in real-time applications by precisely replicating the human finger gestures using a three-joint robotic finger.""The goal was to make something that could stretch, be flexible and a reasonable size, and have the required sensitivity, performance, production cost, and robustness. Unlike an inertial measurement unit -- an electronic unit that measures force and movement and is used in most step-based wearable technologies -- Najjaran says the sensors need to be sensitive enough to respond to different and complex body motions. That includes infinitesimal movements like a heartbeat or a twitch of a finger, to large muscle movements from walking and running.School of Engineering Professor and study co-author Mina Hoorfar says their results may help manufacturers create the next level of health monitoring and biomedical devices.""We have introduced an easy and highly repeatable fabrication method to create a highly sensitive sensor with outstanding mechanical and electrical properties at a very low cost,"" says Hoorfar.To demonstrate its practicality, researchers built three wearable devices including a knee band, a wristband and a glove. The wristband monitored heartbeats by sensing the pulse of the artery. In an entirely different range of motion, the finger and knee bands monitored finger gestures and larger scale muscle movements during walking, running, sitting down and standing up. The results, says Hoorfar, indicate an inexpensive device that has a high-level of sensitivity, selectivity and durability.Story Source:Materials provided by University of British Columbia Okanagan campus. Original written by Patty Wellborn. ",0.099113,0.119922,0.133637,0.196779,0.148808,0.153895,0.18204,0,0
750,Stretchable electronics a 'game changer' for stroke recovery treatment New wearable device for the throat,"A groundbreaking new wearable designed to be worn on the throat could be a game-changer in the field of stroke rehabilitation.Developed in the lab of Northwestern University engineering professor John A. Rogers, in partnership with Shirley Ryan AbilityLab, the sensor is the latest in Rogers' growing portfolio of stretchable electronics that are precise enough for use in advanced medical care and portable enough to be worn outside the hospital, even during extreme exercise.Rogers will present research on the implications of stretchable electronics for stroke recovery treatment on Feb. 17, at the American Association for the Advancement of Science (AAAS) annual meeting in Austin, Texas.Rogers also will discuss his work at the AAAS presentation ""Soft Electronics for the Human Body"" from 4:30 to 5 p.m. CST Feb. 17, at the AAAS meeting. Rogers' talk, to be held in Room F of the Austin Convention Center, is part of the scientific session ""Biomedical Sensors: Advances in Health Monitoring and Disease Treatment.""Rogers' sensors stick directly to the skin, moving with the body and providing detailed health metrics including heart function, muscle activity and quality of sleep.""Stretchable electronics allow us to see what is going on inside patients' bodies at a level traditional wearables simply cannot achieve,"" Rogers said. ""The key is to make them as integrated as possible with the human body.""Rogers' new bandage-like throat sensor measures patients' swallowing ability and patterns of speech. The sensors aid in the diagnosis and treatment of aphasia, a communication disorder associated with stroke.The tools that speech-language pathologists have traditionally used to monitor patients' speech function -- such as microphones -- cannot distinguish between patients' voices and ambient noise.""Our sensors solve that problem by measuring vibrations of the vocal chords,"" Rogers said. ""But they only work when worn directly on the throat, which is a very sensitive area of the skin. We developed novel materials for this sensor that bend and stretch with the body, minimizing discomfort to patients.""Shirley Ryan AbilityLab, a research hospital in Chicago, uses the throat sensor in conjunction with electronic biosensors -- also developed in Rogers' lab -- on the legs, arms and chest to monitor stroke patients' recovery progress. The intermodal system of sensors streams data wirelessly to clinicians' phones and computers, providing a quantitative, full-body picture of patients' advanced physical and physiological responses in real time.""One of the biggest problems we face with stroke patients is that their gains tend to drop off when they leave the hospital,"" said Arun Jayaraman, research scientist at the Shirley Ryan AbilityLab and a wearable technology expert. ""With the home monitoring enabled by these sensors, we can intervene at the right time, which could lead to better, faster recoveries for patients.""Because the sensors are wireless, they eliminate barriers posed by traditional health monitoring devices in clinical settings. Patients can wear them even after they leave the hospital, allowing doctors to understand how their patients are functioning in the real world.""Talking with friends and family at home is a completely different dimension from what we do in therapy,"" said Leora Cherney, research scientist at the Shirley Ryan AbilityLab and an expert in aphasia treatment. ""Having a detailed understanding of patients' communication habits outside of the clinic helps us develop better strategies with our patients to improve their speaking skills and speed up their recovery process.""Jayaraman describes the platform's mobility as a ""gamechanger"" in rehabilitation outcomes measurement.Data from the sensors will be presented in a dashboard that is easy for both clinicians and patients to understand. It will send alerts when patients are underperforming on a certain metric and allow them to set and track progress toward their goals.""We are so grateful for our partnership with the Shirley Ryan AbilityLab,"" Rogers said. ""They are helping us move our technology from the research lab to the real world, where it already is making a positive impact on the lives of patients.""Rogers also is collaborating with the Shirley Ryan AbilityLab to test the sensors on patients with other conditions, such as Parkinson's disease.Story Source:Materials provided by Northwestern University. ",0.129151,0.143487,0.18796,0.275015,0.184658,0.176027,0.223864,0,0
751,"Ultrathin, highly elastic skin display developed Device displays electrocardiogram recorded by skin sensor, holds promise for home healthcare applications","A new ultrathin, elastic display that fits snugly on the skin can show the moving waveform of an electrocardiogram recorded by a breathable, on-skin electrode sensor. Combined with a wireless communication module, this integrated biomedical sensor system -- called ""skin electronics"" -- can transmit biometric data to the cloud.This latest research by a Japanese academic-industrial collaboration, led by Professor Takao Someya at the University of Tokyo's Graduate School of Engineering, is slated for a news briefing and talk at the AAAS Annual Meeting in Austin, Texas on February 17th.Thanks to advances in semiconductor technology, wearable devices can now monitor health by first measuring vital signs or taking an electrocardiogram, and then transmitting the data wirelessly to a smartphone. The readings or electrocardiogram waveforms can be displayed on the screen in real time, or sent to either the cloud or a memory device where the information is stored.The newly-developed skin electronics system aims to go a step further by enhancing information accessibility for people such as the elderly or the infirm, who tend to have difficulty operating and obtaining data from existing devices and interfaces. It promises to help ease the strain on home healthcare systems in aging societies through continuous, non-invasive health monitoring and self-care at home.The new integrated system combines a flexible, deformable display with a lightweight sensor composed of a breathable nanomesh electrode and wireless communication module. Medical data measured by the sensor, such as an electrocardiogram, can either be sent wirelessly to a smartphone for viewing or to the cloud for storage. In the latest research, the display showed a moving electrocardiogram waveform that was stored in memory.The skin display, developed by a collaboration between researchers at the University of Tokyo's Graduate School of Engineering and Dai Nippon Printing (DNP), a leading Japanese printing company, consists of a 16 x 24 array of micro LEDs and stretchable wiring mounted on a rubber sheet.""Our skin display exhibits simple graphics with motion,"" says Someya. ""Because it is made from thin and soft materials, it can be deformed freely.""The display is stretchable by as much as 45 percent of its original length.It is far more resistant to the wear and tear of stretching than previous wearable displays. It is built on a novel structure that minimizes the stress resulting from stretching on the juncture of hard materials, such as the micro LEDs, and soft materials, like the elastic wiring -- a leading cause of damage for other models.It is the first stretchable display to achieve superior durability and stability in air, such that not a single pixel failed in the matrix-type display while attached snugly onto the skin and continuously subjected to the stretching and contracting motion of the body.The nanomesh skin sensor can be worn on the skin continuously for a week without causing any inflammation. Although this sensor, developed in an earlier study, was capable of measuring temperature, pressure and myoelectricity (the electrical properties of muscle), it successfully recorded an electrocardiogram for the first time in the latest research.The researchers applied tried-and-true methods used in the mass production of electronics -- specifically, screen printing the silver wiring and mounting the micro LEDs on the rubber sheet with a chip mounter and solder paste commonly used in manufacturing printed circuit boards. Applying these methods will likely accelerate the commercialization of the display and help keep down future production costs.DNP is looking to bring the integrated skin display to market within the next three years by improving the reliability of the stretchable devices through optimizing its structure, enhancing the production process for high integration, and overcoming technical challenges such as large-area coverage.""The current aging society requires user-friendly wearable sensors for monitoring patient vitals in order to reduce the burden on patients and family members providing nursing care,"" says Someya. ""Our system could serve as one of the long-awaited solutions to fulfill this need, which will ultimately lead to improving the quality of life for many.""Story Source:Materials provided by University of Tokyo. ",0.0838123,0.09472,0.111629,0.169215,0.137029,0.117319,0.132106,0,0
752,"Tissue paper sensors show promise for health care, entertainment, robotics ","University of Washington engineers have turned tissue paper -- similar to toilet tissue -- into a new kind of wearable sensor that can detect a pulse, a blink of an eye and other human movement. The sensor is light, flexible and inexpensive, with potential applications in health care, entertainment and robotics.The technology, described in a paper published in January in the journal Advanced Materials Technologies, shows that by tearing tissue paper that's loaded with nanocomposites and breaking the paper's fibers, the paper acts as a sensor. It can detect a heartbeat, finger force, finger movement, eyeball movement and more, said Jae-Hyun Chung, a UW associate professor of mechanical engineering and senior author of the research.""The major innovation is a disposable wearable sensor made with cheap tissue paper,"" said Chung. ""When we break the specimen, it will work as a sensor.""These small, Band Aid-sized sensors could have a variety of applications in various fields. For example, monitoring a person's gait or the movement of their eyes can be used to inspect brain function or a game player's actions. The sensor could track how a special-needs child walks in a home test, sparing the child the need for hospital visits. Or the sensors could be used in occupational therapy for seniors.""They can use these sensors and after one-time use, they can be thrown away,"" said Chung.In their research, the scientists used paper similar to toilet tissue. The paper -- nothing more than conventional paper towels -- is then doused with carbon nanotube-laced water. Carbon nanotubes are tiny materials that create electrical conductivity. Each piece of tissue paper has both horizontal and vertical fibers, so when the paper is torn, the direction of the tear informs the sensor of what's happened. To trace eye movement, they're attached to a person's reading glasses.For now, the work has been contained to a laboratory, and researchers are hoping to find a suitable commercial use. A provisional patent was filed in December 2017.Story Source:Materials provided by University of Washington. ",0.139994,0.17021,0.183829,0.220791,0.199413,0.18623,0.210701,0,0
753,"Customizable, fabric-like power source for wearable electronics ","Scientists at Nanyang Technological University, Singapore (NTU Singapore) have created a customizable, fabric-like power source that can be cut, folded or stretched without losing its function.Led by Professor Chen Xiaodong, Associate Chair (Faculty) at the School of Materials Science & Engineering, the team reported in the journal Advanced Materials (print edition 8 January) how they have created the wearable power source, a supercapacitor, which works like a fast-charging battery and can be recharged many times.Crucially, they have made their supercapacitor customizable or ""editable,"" meaning its structure and shape can be changed after it is manufactured, while retaining its function as a power source. Existing stretchable supercapacitors are made into predetermined designs and structures, but the new invention can be stretched multi-directionally, and is less likely to be mismatched when it is joined up to other electrical components.The new supercapacitor, when edited into a honeycomb-like structure, has the ability to store an electrical charge four times higher than most existing stretchable supercapacitors. In addition, when stretched to four times its original length, it maintains nearly 98 per cent of the initial ability to store electrical energy, even after 10,000 stretch-and-release cycles.Experiments done by Prof Chen and his team also showed that when the editable supercapacitor was paired with a sensor and placed on the human elbow, it performed better than existing stretchable supercapacitors. The editable supercapacitor was able to provide a stable stream of signals even when the arm was swinging, which are then transmitted wirelessly to external devices, such as one that captures a patient's heart rate.The authors believe that the editable supercapacitor could be easily mass-produced as it would rely on existing manufacturing technologies. Production cost will thus be low, estimated at about SGD$0.13 (USD$0.10) to produce 1 cm2 of the material.The team has filed a patent for the technology Professor Chen said, ""A reliable and editable supercapacitor is important for development of the wearable electronics industry. It also opens up all sorts of possibilities in the realm of the 'Internet-of-Things' when wearable electronics can reliably power themselves and connect and communicate with appliances in the home and other environments.""My own dream is to one day combine our flexible supercapacitors with wearable sensors for health and sports performance diagnostics. With the ability for wearable electronics to power themselves, you could imagine the day when we create a device that could be used to monitor a marathon runner during a race with great sensitivity, detecting signals from both under and over-exertion.""The editable supercapacitor is made of strengthened manganese dioxide nanowire composite material. While manganese dioxide is a common material for supercapacitors, the ultralong nanowire structure, strengthened with a network of carbon nanotubes and nanocellulose fibres, allows the electrodes to withstand the associated strains during the customisation process.The NTU team also collaborated with Dr. Loh Xian Jun, Senior Scientist and Head of the Soft Materials Department at the Institute of Materials Research and Engineering (IMRE), Agency for Science, Technology and Research (A*STAR).Dr. Loh said, ""Customisable and versatile, these interconnected, fabric-like power sources are able to offer a plug-and-play functionality while maintaining good performance. Being highly stretchable, these flexible power sources are promising next-generation 'fabric' energy storage devices that could be integrated into wearable electronics.""Story Source:Materials provided by Nanyang Technological University. ",0.0796694,0.104302,0.119566,0.155365,0.128893,0.136689,0.136369,0,0
754,A ski jacket that actively gets rid of sweat,"Man is a warm-blooded animal. If it gets too hot for him, he can tune down his body temperature. This feat is achieved by an evolutionarily refined ""AC system"" in our skin: the sweat glands. However, evolution did not yet know anything about winter sports, and so our heat balance is thrown into a spin if we want to protect ourselves from the freezing cold while skiing and at the same time sweat unhindered. A technology developed at Empa in St. Gallen in cooperation with the Thalwil-based company Osmotex and other industrial partners is designed to keep athletes warm and dry -- thanks to ""electrical"" textiles.An important component of the HYDRO_BOT technology is a principle that enables plants, for instance, to draw in water from the soil via their roots: osmosis. With the new type of sportswear, this principle is accelerated even further by applying a voltage of around 1.5 volts. To ensure that liquid is actively transported from the inside to the outside by means of electro-osmosis, a polymer membrane with a thickness of only 20 micrometers is used, which is coated on both sides with a noble metal by means of plasma coating. This is achieved by using just under 0.2 grams of gold per ski jacket, which has an impact on the price of the membrane. However, gold has proven to be significantly more durable than silver-coated electrodes.How the jacket worksWhen an electrical voltage is applied to the membrane, salt ions -- and with them the liquid surrounding them -- migrate through tiny pores in the membrane to the outside, from where they are attracted electrically, so to speak. For this purpose, the membrane is equipped with a conventional battery, which can be switched on depending on weather and body activity. ""Even without current, liquid passes through the membrane. However, as soon as an electrical voltage is applied, the pumping effect increases significantly,"" says Dirk Hegemann from Empa's Advanced Fibers lab. The membrane can pump out about 10 liters of liquid per square meter and hour by electro-osmosis.For the final product, though, the electro-osmotic membrane will be integrated into a ski jacket within various functional layers. ""Thanks to our new physical and numerical models, we were able to optimize the textile structure of the HYDRO_BOT technology,"" explains Simon Annaheim from Empa's Biomimetic Membranes and Textiles lab.Experiments in the climate chambers at Empa showed that the electro-osmotic principle not only works in aspects of physics but also meets the physiological requirements of the human body. Here, the anatomically shaped sweat manikin SAM simulates how the human body behaves during exercise. SAM moves, heats up and ejects precisely defined quantities of liquid through 125 tiny nozzles. ""SAM and the data it provided us with enabled us to objectively analyze the wearing comfort and functionality of HYDRO_BOT clothing,"" says Annaheim.Osmotex expects jackets with the HYDRO_BOT technology to be launched on the market for the 2018/19 season. In addition to Empa, Norwegian sportswear manufacturer KJUS and Swiss textile company Schoeller are involved in the further development of the technology. However, a prototype of an electro-osmotic jacket can already be admired at the international sports fair ISPO in Munich from 28 to 31 January 2018, where Osmotex will be presenting it to the public for the first time.Story Source:Materials provided by Empa. ",0.100949,0.168814,0.145991,0.157714,0.132106,0.168052,0.16772,0,0
755,"Fiber OLEDs, thinner than a hair ","Professor Kyung Cheol Choi from the School of Electrical Engineering and his team succeeded in fabricating highly efficient Organic Light-Emitting Diodes (OLEDs) on an ultra-thin fiber.The team expects the technology, which produces high-efficiency, long-lasting OLEDs, can be widely utilized in wearable displays.Existing fiber-based wearable displays' OLEDs show much lower performance compared to those fabricated on planar substrates. This low performance caused a limitation for applying it to actual wearable displays.In order to solve this problem, the team designed a structure of OLEDs compatible to fiber and used a dip-coating method in a three-dimensional structure of fibers. Through this method, the team successfully developed efficient OLEDs that are designed to last a lifetime and are still equivalent to those on planar substrates.The team identified that solution process planar OLEDs can be applied to fibers without any reduction in performance through the technology. This fiber OLEDs exhibited luminance and current efficiency values of over 10,000 cd/m^2(candela/square meter) and 11 cd/A (candela/ampere).The team also verified that the fiber OLEDs withstood tensile strains of up to 4.3% while retaining more than 90% of their current efficiency. In addition, they could be woven into textiles and knitted clothes without causing any problems.Moreover, the technology allows for fabricating OLEDs on fibers with diameters ranging from 300?  down to 90?, thinner than a human hair, which attests to the scalability of the proposed fabrication scheme.Noting that every process is carried out at a low temperature (~105?), fibers vulnerable to high temperatures can also employ this fabrication scheme.Professor Choi said, ""Existing fiber-based wearable displays had limitations for applicability due to their low performance. However, this technology can fabricate OLEDs with high performance on fibers. This simple, low-cost process opens a way to commercialize fiber-based wearable displays.""This research led by a PhD candidate Seonil Kwon was published online in the international journal for nanoscience, Nano Letters, on December 6.Story Source:Materials provided by The Korea Advanced Institute of Science and Technology (KAIST). ",0.146517,0.155003,0.196386,0.224584,0.205208,0.189673,0.200715,0,0
756,Scientists create stretchable battery made entirely out of fabric New microbial fuel cell could be integrated into wearable electronics,"A research team led by faculty at Binghamton University, State University of New York has developed an entirely textile-based, bacteria-powered bio-battery that could one day be integrated into wearable electronics.The team, led by Binghamton University Electrical and Computer Science Assistant Professor Seokheun Choi, created an entirely textile-based biobattery that can produce maximum power similar to that produced by his previous paper-based microbial fuel cells.Additionally, these textile-based biobatteries exhibit stable electricity-generating capability when tested under repeated stretching and twisting cycles.Choi said that this stretchable, twistable power device could establish a standardized platform for textile-based biobatteries and will be potentially integrated into wearable electronics in the future.""There is a clear and pressing need for flexible and stretchable electronics that can be easily integrated with a wide range of surroundings to collect real-time information,"" said Choi. ""Those electronics must perform reliably even while intimately used on substrates with complex and curvilinear shapes, like moving body parts or organs. We considered a flexible, stretchable, miniaturized biobattery as a truly useful energy technology because of their sustainable, renewable and eco-friendly capabilities.""Compared to traditional batteries and other enzymatic fuel cells, microbial fuel cells can be the most suitable power source for wearable electronics because the whole microbial cells as a biocatalyst provide stable enzymatic reactions and a long lifetime, said Choi.Sweat generated from the human body can be a potential fuel to support bacterial viability, providing the long-term operation of the microbial fuel cells.""If we consider that humans possess more bacterial cells than human cells in their bodies, the direct use of bacterial cells as a power resource interdependently with the human body is conceivable for wearable electronics,"" said Choi.This work was supported by the National Science Foundation, the Binghamton University Research Foundation and a Binghamton University ADL (Analytical and Diagnostics Laboratory) Small Grant.Story Source:Materials provided by Binghamton University. ",0.115189,0.172975,0.156168,0.193228,0.163527,0.17286,0.177279,0,0
757,"New motion sensors a major step toward low-cost, high-performance wearable technology ","Researchers from the FAMU-FSU College of Engineering have developed a class of breakthrough motion sensors that could herald a near future of ubiquitous, fully integrated and affordable wearable technology.In a paper published in the journal Materials and Design, engineers from FSU's High-Performance Materials Institute, in collaboration with scientists from Institut National des Sciences Appliquees in Lyon, France, detail the impressive properties and cost-effective manufacturing process of an advanced series of motion sensors made using buckypaper -- razor thin, flexible sheets of pure, exceptionally durable carbon nanotubes.These new buckypaper sensors represent a marked improvement on current industry standards, with most sensors being either too crude or too inflexible to reliably monitor complex structures like the human body.""Current technology is not designed for that,"" said Richard Liang, director of the High-Performance Materials Institute and professor at the FAMU-FSU College of Engineering. ""For sensor technology, you need it to be flexible, you need it to be affordable and you need it to be scalable. This new technology is versatile and the sensors are affordable to print. It's a big innovation that presents many possibilities down the road.""At this stage, potential applications for the printable buckypaper sensors are limited only by the breadth of researchers' imaginations. The low-profile design could be integrated into bedsheets to monitor quality of sleep, shoes to track step count and posture or workout clothes to measure intensity of exercise.Researchers also foresee potential applications beyond the realm of wearable technology. In the field of soft robotics, the material could facilitate advances in the production of responsive, self-correcting artificial muscles.Moreover, the scalable sensors represent another step toward the long-predicted future of an ""internet of things,"" where virtually all of an individual's computers, devices, garments, furniture and appliances are digitally connected to freely exchange information in the cloud.""Most projects don't have this many possible applications,"" said doctoral candidate Joshua DeGraff, the lead author of the study. ""This material could be used in structural health monitoring, wearable technology and everything in between. I'm excited because this is something that can affect a lot of people in their everyday lives.""The novel sensor structure combines a strip of seven micron-thin buckypaper with silver ink electrodes printed from a common, commercially available ink-jet printer.The result is a kind of perfect Goldilocks sensor: not as insensitive as common, flexible metallic sensors, but not as rigid or cumbersome as popular, more sensitive semi-conductor sensors.The wearable buckypaper sensors are an ideal marriage of these competing qualities. They're flexible, seamless and sensitive to subtle movements and strains.""We measure sensors by gauge factor, which indicates how much resistance value changes as a material is strained or bent,"" DeGraff said. ""Our gauge factor has been up to eight times higher than commercial sensors and 75 percent higher than many other carbon nanotube sensors.""As development of the printable sensor technology continues, researchers hope to improve upon the already remarkable thinness of the material so that it can be integrated into comfortable and non restrictive clothing.Additional testing on complex model structures is required to ensure the material's ability to conform to the variable curves and crevices of the human body.While the technology might not be ready for primetime quite yet, researchers are energized by its promising future.""As engineering professors, we want to see the things we create in the lab become successful products,"" Liang said. ""We're not quite there yet, but this is an important step. Consumers want great quality and affordable prices, and this material provides both of those things.""Story Source:Materials provided by Florida State University. Original written by Zachary Boehm. ",0.0700822,0.09455,0.130896,0.156311,0.128359,0.116651,0.139515,0,0
758,"Smart, ultra-thin microfibre sensor for real-time healthcare monitoring and diagnosis ","A research team from National University of Singapore (NUS) has developed a soft, flexible and stretchable microfibre sensor for real-time healthcare monitoring and diagnosis. The novel sensor is highly sensitive and ultra-thin with a diameter of a strand of human hair. It is also simple and cost-effective to mass produce.Wearable and flexible technology has gained significant interest in recent years, leading to tremendous progress in soft and wearable sensors. In tandem with this trend, microfluidic devices using conductive liquid metals have been increasingly employed as wearable pressure and strain sensors. However, current devices have various limitations -- for instance, they may not fit well on the skin or are uncomfortable to wear.""Our novel microfibre sensor can hardly be felt on the skin and conforms extremely well to skin curvatures. Despite being soft and tiny, the sensor is highly sensitive and it also has excellent electrical conductivity and mechanical deformability. We have applied the sensor for real-time monitoring of pulse waveform and bandage pressure. The results are very promising,"" said Professor Lim Chwee Teck from the Department of Biomedical Engineering at NUS Faculty of Engineering, who is the leader of the research team.Real-time monitoring of pulse waveformThe smart microfibre sensor developed by the NUS Engineering team comprises a liquid metallic alloy, which serves as the sensing element, encapsulated within a soft silicone microtube. The sensor measures an individual's pulse waveform in real-time, and the information can be used to determine one's heart rate, blood pressure and stiffness in blood vessels.""Currently, doctors will monitor vital signs like heart rate and blood pressure when patients visit clinics. This requires multiple equipment such as heart rate and blood pressure monitors, which are often bulky and may not provide instantaneous feedback. As our sensor functions like a conductive thread, it can be easily woven into a glove which can be worn by doctors to track vital signs of patients in real-time. This approach offers convenience and saves time for healthcare workers, while patients can enjoy greater comfort,"" added Prof Lim.The microfibre sensor could also be beneficial for patients suffering from atherosclerosis, which is the thickening and stiffening of the arteries caused by the accumulation of fatty streaks. Over time, these streaks accumulate into plaques which may completely block off blood flow or break apart, resulting in organ failure or may trigger a heart attack or stroke.Existing methods of detecting plaque in blood vessels -- such as computerised tomography scans and magnetic resonance imaging -- would require expensive and bulky equipment. Such tests need to be done in hospitals by trained medical professionals.As plaque will change the stiffness of the blood vessel and hence the pulse waveform, the novel sensor developed by the NUS Engineering team could be easily used to detect plaque before it accumulates to a size big enough to block or rupture the blood vessel.Earlier this year, the NUS team published the development of the microfibre sensor and its application for pulse monitoring in scientific journals Proceedings of the National Academy of Sciences (PNAS) and Advanced Materials Technologies, respectively.Bandage pressure monitoringAnother clinical application of the smart microfibre sensor is for the management of venous ulcers, which are caused by poor blood circulation. They occur when the veins in the legs could not push blood back to the heart as well as they should. As blood pools in the veins, there is increased pressure in the veins, causing progressive skin damage over time.Compression therapy is a common treatment for venous ulcer. Depending on the severity of the ulcers, bandages with varying amount of pressure have to be applied on the legs of patients for months to even a year. If the bandage is too tight, it could result in tissue damage, but if the bandage is too loose, the healing could be ineffective. Currently, healthcare workers tend to estimate the pressure in the bandage at the point of application, based on training and experience.As the pressure provided by the bandage could change over time due to movements by the patient, accurate and continuous measurement of the bandage pressure in real-time is therefore important to ensure that healing takes place effectively.Being ultra-thin and highly flexible, the NUS Engineering team's microfibre sensor can be easily woven into bandages to monitor the pressure that is being delivered and maintained. This could potentially improve the effectiveness of the treatment and reduce the time required for healing. In future, patients could also track the bandage pressure using an app, and the information could be shared with doctors who could remotely monitor the progress of the treatment.The team is currently collaborating with the Singapore General Hospital to test the application of the microfibre sensor for bandage pressure monitoring.Commercialisation and further research""Our microfibre sensor is highly versatile, and could potentially be used for a wide range of applications, including healthcare monitoring, smart medical prosthetic devices and artificial skins. Uniquely designed to be durable and washable, our novel invention is highly attractive for promising applications in the emerging field of wearable electronics,"" said Prof Lim.The team has filed a patent for its smart microfibre sensor. Researchers are currently refining the sensor design and reducing the size of its accessories to improve the user-friendliness of the device. The NUS team had recently won the Most Innovative Award at the Engineering Medical Innovation Global Competition held in Taipei in September 2017.While the NUS researchers continue to explore new applications of the microfibre sensor, they are also keen to work with commercial partners to bring their novel sensor to market.Story Source:Materials provided by National University of Singapore. ",0.0869878,0.0933919,0.137386,0.186161,0.128822,0.14487,0.161161,0,0
759,Double-duty textile could warm or cool,"Stanford researchers have developed a reversible fabric that, without expending effort or energy, keeps skin a comfortable temperature whatever the weather.In a paper published Nov. 10 in Science Advances, a team led by Yi Cui, professor of materials science and engineering, created a double-sided fabric based on the same material as everyday kitchen wrap. Their fabric can either warm or cool the wearer, depending which side faces out.This project came out of Cui's interest in energy efficiency and his expertise in manipulating nanoscale materials. He thought if people could be more comfortable in a range of temperatures, they could save energy on air conditioning and central heating.""Why do you need to cool and heat the whole building? Why don't you cool and heat individual people?"" asked Cui.Thirteen percent of all of the energy consumed in the United States is simply dedicated to indoor temperature control. But for every 1 degree Celsius (1.8 degrees Fahrenheit) that a thermostat is turned down, a building can save a whopping 10 percent of its heating energy, and the reverse is true for cooling. Adjusting temperature controls by just a few degrees has major effects on energy consumption.Cooling kitchen wrapOur bodies have many ways of controlling our temperature. When it's cold, the hairs in our skin stand out to trap warm air. Eventually, we may start shivering to produce more radiant heat in our muscles. When it's hot, we release heat as infrared radiation from our skin, and if we're still warm we start to sweat. Water evaporating away from our bodies carries a large amount of heat with it. But those mechanisms only help within a few degrees. Get outside the temperature range to which our bodies can adapt, and we reach for the dial on the heating or air conditioning.In 2016 the team announced a first step toward a solution: fabric that allowed the body's heat to pass through, cooling the skin. Although they were inspired by transparent, water-impermeable kitchen wrap, their new material was opaque, breathable and retained its ability to shuttle infrared radiation away from the body. Compared to a cotton sample, their fabric kept artificial skin 2 C cooler in a laboratory test -- possibly enough to stop a person from ever reaching for a fan or the building thermostat. The team's first textile could save a building full of workers 20 to 30 percent of their total energy budget.Reversible progress""Right around when we figured out cooling, then came the question: Can you do heating?"" said postdoctoral fellow Po-Chun Hsu, who was first author on the recent paper. It was a particularly chilly winter, and he was headed to a conference in Minneapolis with a carry-on bag full of coats. Could he create an article of clothing that would serve him in a crowded warm conference room as well as on the frosty street?Hsu realized that controlling radiation could work both ways. He stacked two layers of material with different abilities to release heat energy, and then sandwiched them between layers of their cooling polyethylene.On one side, a copper coating traps heat between a polyethylene layer and the skin; on the other, a carbon coating releases heat under another layer of polyethylene. Worn with the copper layer facing out, the material traps heat and warms the skin on cool days. With the carbon layer facing out, it releases heat, keeping the wearer cool.Combined, the sandwiched material can increase a person's range of comfortable temperatures over 10 F, and Hsu predicts that the potential range is much larger -- close to 25 F. With inhabitants wearing a textile like that, buildings in some climates might never need air conditioning or central heating at all.Practical polymersThe white-colored fabric isn't quite wearable yet, the team said.""Ideally, when we get to the stuff you want to wear on skin, we'll need to make it into a fiber woven structure,"" said Cui. Woven textiles are stronger, more elastic, more comfortable, and look much more like typical clothing. But good news: They've already started testing to make sure their fabric will be machine washable.""From my perspective, this work really highlights the significant opportunities in combining thermal engineering concepts with nanophotonic structures for creating novel functionalities,"" said Shanhui Fan, a professor of electrical engineering who participated in the work.The team's ambitions are to create an easily manufactured, practical textile that people could use to save huge amounts of energy around the world. And they don't stop there -- Cui, Hsu and Fan envision clothing with medical devices and even entertainment printed right into the fabric.""I think we are only seeing the beginning of many creative ideas that can come out of such combinations,"" Fan said.Story Source:Materials provided by Stanford University. Original written by Vicky Stein. ",0.0722404,0.100035,0.112407,0.137466,0.109541,0.119701,0.138013,0,0
760,Fully integrated circuits printed directly onto fabric,"Researchers have successfully incorporated washable, stretchable and breathable electronic circuits into fabric, opening up new possibilities for smart textiles and wearable electronics. The circuits were made with cheap, safe and environmentally friendly inks, and printed using conventional inkjet printing techniques.The researchers, from the University of Cambridge, working with colleagues in Italy and China, have demonstrated how graphene -- a two-dimensional form of carbon -- can be directly printed onto fabric to produce integrated electronic circuits which are comfortable to wear and can survive up to 20 cycles in a typical washing machine.The new textile electronic devices are based on low-cost, sustainable and scalable inkjet printing of inks based on graphene and other two-dimensional materials, and are produced by standard processing techniques. The results are published in the journal Nature Communications.Based on earlier work on the formulation of graphene inks for printed electronics, the team designed low-boiling point inks, which were directly printed onto polyester fabric. Additionally, they found that modifying the roughness of the fabric improved the performance of the printed devices. The versatility of this process allowed the researchers to design not only single transistors but all-printed integrated electronic circuits combining active and passive components.Most wearable electronic devices that are currently available rely on rigid electronic components mounted on plastic, rubber or textiles. These offer limited compatibility with the skin in many circumstances, are damaged when washed and are uncomfortable to wear because they are not breathable.""Other inks for printed electronics normally require toxic solvents and are not suitable to be worn, whereas our inks are both cheap, safe and environmentally-friendly, and can be combined to create electronic circuits by simply printing different two-dimensional materials on the fabric,"" said Dr Felice Torrisi of the Cambridge Graphene Centre, the paper's senior author.""Digital textile printing has been around for decades to print simple colorants on textiles, but our result demonstrates for the first time that such technology can also be used to print the entire electronic integrated circuits on textiles,"" said co-author Professor Roman Sordan of Politecnico di Milano. ""Although we demonstrated very simple integrated circuits, our process is scalable and there are no fundamental obstacles to the technological development of wearable electronic devices both in terms of their complexity and performance.""""The printed components are flexible, washable and require low power, essential requirements for applications in wearable electronics,"" said PhD student Tian Carey, the paper's first author.The work opens up a number of commercial opportunities for two-dimensional material inks, ranging from personal health and well-being technology, to wearable energy harvesting and storage, military garments, wearable computing and fashion.""Turning textile fibres into functional electronic components can open to an entirely new set of applications from healthcare and wellbeing to the Internet of Things,"" said Torrisi. ""Thanks to nanotechnology, in the future our clothes could incorporate these textile-based electronics, such as displays or sensors and become interactive.""The use of graphene and other related 2D material (GRM) inks to create electronic components and devices integrated into fabrics and innovative textiles is at the centre of new technical advances in the smart textiles industry. The teams at the Cambridge Graphene Centre and Politecnico di Milano are also involved in the Graphene Flagship, an EC-funded, pan-European project dedicated to bringing graphene and GRM technologies to commercial applications.The research was supported by grants from the Graphene Flagship, the European Research Council's Synergy Grant, The Engineering and Physical Science Research Council, The Newton Trust, the International Research Fellowship of the National Natural Science Foundation of China and the Ministry of Science and Technology of China. The technology is being commercialised by Cambridge Enterprise, the University's commercialisation arm.Story Source:Materials provided by University of Cambridge. The original story is licensed under a Creative Commons License. ",0.111354,0.138736,0.150636,0.177138,0.19996,0.142976,0.1567,0,0
761,New SOFT e-textiles could offer advanced protection for soldiers and emergency personnel Smart fabrics get major upgrade through a series of research firsts,"New technology that harnesses electronic signals in a smart fabric could lead to advanced hazardous-material gear that protects against toxic chemicals, according to research from Dartmouth College.The advancement could have big implications for the military, emergency service personnel and other workers that rely on ""haz-mat"" protection to perform their work and keep others safe.In research published in the Journal of the American Chemical Society, the chemistry team of Katherine Mirica and Merry Smith describe the creation of new smart fabrics -- named SOFT, for Self-Organized Framework on Textiles -- in what is noted as the first demonstration of simultaneous detection, capture, pre-concentration and filtration of gases in a wearable that uses conductive, porous materials integrated into soft textiles.According to the study, the SOFT devices have the potential for use in sensing applications ranging from real-time gas detection in wearable systems, to electronically accessible adsorbent layers in protective equipment like gas masks.""By adding this fabric to a protective suit, sensors can alert the user if a chemical is penetrating the hazardous-material gear,"" said Katherine Mirica, an assistant professor of chemistry at Dartmouth College. ""This is not just passive protection, the textile can actively alarm a user if there is a tear or defect in the fabric, or if functional performance is diminished in any other way.""Among other firsts described in the research are flexible, textile-supported electronic sensors based on materials known as metal-organic frameworks, or MOFs. In the study, the authors also describe a ""simple"" approach for integrating these conductive, porous materials into cotton and polyester fabrics to produce the e-textiles.As part of the study, the Dartmouth team demonstrated that the new smart fabric can detect common toxic chemicals. Both the vehicle exhaust pollutant, nitric oxide, and the corrosive poison that reminds most of rotten eggs, hydrogen sulfide, were effectively identified by the SOFT system. In addition to sensing the chemicals, the electronic textiles are capable of capturing and filtering the dangerous toxins.""Metal-organic frameworks are the future of designer materials, just like plastics were in the post-WWII era,"" said Mirica. ""By integrating the MOFs into our SOFT devices, we dramatically enhance the performance of smart fabrics that are essential to safety and security.""The research team also describes a one-step e-textile fabrication method based on MOFs that likely constitutes the first use of direct self-assembly to deposit the conductive material on textiles.In a process that Mirica describes as ""similar to a building framework assembling itself,"" cotton and polyester textiles coated with conductive crystals at the fiber-level are created by direct self-assembly of molecules with organic molecular struts connected by metallic linkers from solution.Wearable electronics are thought to have great potential in areas including homeland security, communication and healthcare. Soldiers, emergency personnel, factory workers and others that risk exposure to toxic chemicals could benefit from the new smart fabrics. The materials could also help medical patients that require monitoring of specific airborne chemicals that come from the environment or even from their own bodies.According to researchers, the SOFT devices featuring MOFs display reliable conductivity, enhanced porosity, flexibility and stability to washing. The fabrics are also stable in heat, have good shelf-lives and retain a full-range of utility under humid conditions.While the technology requires further development before it can be used in state-of-the-art wearable systems, the authors are hopeful that the fabrication method has the potential to be extended into other systems, producing a range of novel, multifunctional e-textiles with highly tunable properties and a host of new applications in wearable and portable devices.Support for this research includes funds provided by Dartmouth College, the Walter and Constance Burke Research Initiation Award, and the Army Research Office Young Investigator Program.Story Source:Materials provided by Dartmouth College. ",0.0738593,0.121918,0.137872,0.170664,0.133322,0.130384,0.150967,0,0
762,"How to store information in your clothes invisibly, without electronics ","A new type of smart fabric developed at the University of Washington could pave the way for jackets that store invisible passcodes and open the door to your apartment or office.The UW computer scientists have created fabrics and fashion accessories that can store data -- from security codes to identification tags -- without needing any on-board electronics or sensors.As described in a paper presented Oct. 25 at the Association for Computing Machinery's User Interface Software and Technology Symposium (UIST 2017), they leveraged previously unexplored magnetic properties of off-the-shelf conductive thread. The data can be read using an instrument embedded in existing smartphones to enable navigation apps.""This is a completely electronic-free design, which means you can iron the smart fabric or put it in the washer and dryer,"" said senior author Shyam Gollakota, associate professor in the Paul G. Allen School of Computer Science & Engineering. ""You can think of the fabric as a hard disk -- you're actually doing this data storage on the clothes you're wearing.""Most people today combine conductive thread -- embroidery thread that can carry an electrical current -- with other types of electronics to create outfits, stuffed animals or accessories that light up or communicate.But the UW researchers realized that this off-the-shelf conductive thread also has magnetic properties that can be manipulated to store either digital data or visual information like letters or numbers. This data can be read by a magnetometer, an inexpensive instrument that measures the direction and strength of magnetic fields and is embedded in most smartphones.""We are using something that already exists on a smartphone and uses almost no power, so the cost of reading this type of data is negligible,"" said Gollakota. In one example, they stored the passcode to an electronic door lock on a patch of conductive fabric sewn to a shirt cuff. They unlocked the door by waving the cuff in front of an array of magnetometers.The UW researchers also created fashion accessories like a tie, belt, necklace and wristband and decoded the data by swiping a smartphone across them.They used conventional sewing machines to embroider fabric with off-the-shelf conductive thread, whose magnetic poles start out in a random order. By rubbing a magnet against the fabric, the researchers were able to physically align the poles in either a positive or negative direction, which can correspond to the 1s and 0s in digital data.Like hotel card keys, the strength of the magnetic signal weakens by about 30 percent over the course of a week, though the fabric can be re-magnetized and re-programmed multiple times. In other stress tests, the fabric patch retained its data even after machine washing, drying and ironing at temperatures of up to 320 degrees Fahrenheit.This is in contrast to many smart garments today that still require on-board electronics or sensors to work. That can be problematic if you get caught in the rain or forget to detach those electronics before throwing them in the washing machine -- a potential barrier to widespread adoption of other wearable technology designs.The team also demonstrated that the magnetized fabric could be used to interact with a smartphone while it is in one's pocket. Researchers developed a glove with conductive fabric sewn into its fingertips, which was used to gesture at the smartphone. Each gesture yields a different magnetic signal that can invoke specific actions like pausing or playing music.""With this system, we can easily interact with smart devices without having to constantly take it out of our pockets,"" said lead author Justin Chan, an Allen School doctoral student.In the team's tests, the phone was able to recognize six gestures -- left flick, right flick, upward swipe, downward swipe, click and back click -- with 90 percent accuracy. Future work is focused on developing custom textiles that generate stronger magnetic fields and are capable of storing a higher density of data.The research was funded by the National Science Foundation, the Alfred P. Sloan Foundation and Google.Story Source:Materials provided by University of Washington. Original written by Jennifer Langston. ",0.0804799,0.0885072,0.141424,0.250599,0.147848,0.141021,0.159674,0,0
763,"In a first for wearable optics, researchers develop stretchy fiber to capture body motion New fiber could offer safer, more reliable alternative to electrical motion sensors","The exciting applications of wearable sensors have sparked a tremendous amount of research and business investment in recent years. Sensors attached to the body or integrated into clothing could allow athletes and physical therapists to monitor their progress, provide a more detailed level of motion capture for computer games or animation, help engineers build robots with a lighter touch or form the basis for new types of real-time health monitors.In Optica, The Optical Society's journal for high impact research, a team led by Changxi Yang of the State Key Laboratory of Precision Measurement Technology and Instruments at Tsinghua University in Beijing offers the first demonstration of optical fibers sturdy enough to sense a wide range of human motion.The new fiber is sensitive and flexible enough that it can detect joint movements, unlike currently used fiber sensors. ""This new technique provides a fiber-optic approach for measuring extremely large deformations,"" said Yang. ""It's wearable, mountable and also possesses intrinsic advantages of optical fibers such as inherent electrical safety and immunity to electromagnetic interference.""Trouble with stretchingOptical fibers have been used for strain sensing on bridges and buildings for years; stretch or bend the fiber a little and light going through it is shifted in a way that can be easily picked up by a monitor. Traditionally optical fibers haven't been the best choice for strain sensing on the human body because they are typically made of plastic or glass, which are stiff and don't bend well. A silica glass fiber, for example, can handle a maximum strain of less than 1 percent, while a bending finger joint would strain it by more than 30 percent.This barrier has meant that most wearable sensor developments so far have been based on electronic sensors. These sensors detect movement by measuring changes in electrical properties such as resistance as the sensor bends. However, these systems are difficult to miniaturize, can lose their electrical charge and are sensitive to electromagnetic interference from devices such as cars and cell phones. A bendable optical fiber could avoid these problems and potentially create wearable devices that are more stable and sustainable than those based on electronics.Simple siliconeWhen the researchers started looking for a fiber that could stand up to the amount of bending and stretching involved in human movements, they first tried fibers made of hydrogel, a soft, jelly-like substance that can hold strains of up to 700 percent. But hydrogel consists mostly of water, and therefore only worked in wet environments. When exposed to air, the fibers quickly dried out and shrank.In a second attempt, Yang and his students, Jingjing Guo and Mengxuan Niu, developed a fiber made of silicone -- specifically a soft polymer called polydimethylsiloxane (PDMS). They created the fiber by putting the liquid silicone into a tube-shaped mold and heating it to 80e C (176e F) for 40 minutes to get it to thicken, then used water pressure to push a thin fiber out of one end of the mold. They put the resulting fibers through an elaborate series of tests, such as repeatedly stretching them out to double their length. Even after 500 stretches, a fiber still returned to its original length.""The fabricated PDMS fibers exhibited excellent mechanical flexibility, and could easily be tied and twisted,"" said Yang. What's more, when the team reduced the diameter of the fibers they produced, from 2 millimeters to 0.5 millimeters, the mechanical strength of the fibers actually increased.To aid in sensing, the researchers mixed a fluorescent dye called Rhodamine B into the silicone. When light shines through the fiber, some of the light is absorbed by the dye -- the more the fiber stretches, the more light the dye absorbs. So simply measuring the transmitted light with a spectroscope provides a measurement of how much the fiber is being stretched or bent, which tells an observer about the movement of any body part it is attached to.The glove testThe researchers tested that idea by gluing their fiber to a rubber glove with epoxy, and then monitoring it as a wearer flexed and extended his fingers. During that movement, they measured a strain in the fiber of 36 percent, in line with what others had measured using electronic sensors.""The remarkable flexibility and stretchability of the PDMS fiber makes it especially attractive for sensing of large strains,"" said Yang, adding that this is the first time researchers have used an optical sensor to capture human motion.The sensor also performed well in situations involving more subtle strains, such as the minute movements of neck muscles as a person breathes or speaks. ""All the results show that the optical strain sensor can be used for monitoring of various human motions and may provide a new approach for exploration of human-machine interfaces,"" said Yang.The team tested how well their fibers sensed strain over longer periods of time and in different environments, such as in water, glycerol and air. They learned that the fibers held up well, although the sensing accuracy did change in different environments, suggesting devices using the optical fiber-based sensors would need to be calibrated for the specific environment they'd be used in.The team illuminated the fiber by attaching it to a halogen lamp, and measured the light passing through it with a spectrometer. To adapt the technology to create a wearable device, Yang said it should be possible to develop a compact light source and spectrometer that can be easily worn on the body.Story Source:Materials provided by The Optical Society. ",0.0812312,0.0970892,0.151688,0.160932,0.147017,0.114485,0.152991,0,0
764,A fashionable chemical and biological threat detector-on-a-ring,"Wearable sensors are revolutionizing the tech-world, capable of tracking processes in the body, such as heart rates. They're even becoming fashionable, with many of them sporting sleek, stylish designs. But wearable sensors also can have applications in detecting threats that are external to the body. Researchers now report in ACS Sensors a first-of-its kind device that can do just that. And to stay fashionable, they've designed it as a ring.According to a global analyst firm called CCS Insight, wearable electronics will be a $34 billion industry by 2020. Wearable chemical sensors currently in development include those made in the form of tattoos, mouth guards, wristbands and headbands, but all of these types of sensors face challenges. For example, a sweat sensor worn on an arm could be useful, but patients would need to produce enough sweat for the device to be successful. There is a demand for sensors that are compact, affordable, noninvasive and can be incorporated into everyday life. But more advanced sensors can be costly and difficult to produce. Joseph Wang and colleagues at the University of California, San Diego wanted to develop a portable, affordable, wearable sensor that would detect external chemical threats.The team designed their sensor as a ring that can be worn on a finger. The ring has two parts, an electrochemical sensor cap for detecting chemical and biological threats, and a circuit board under the cap for processing and sending data wirelessly to a smartphone or laptop. It can perform voltammetric and chronoamperometric measurements, which allow the ring to detect a wide array of chemical threats. The team exposed the prototype to explosives and organophosphate nerve agents, both in vapor and liquid phases. The ring was highly selective and sensitive. Although this ring-based sensor was designed to detect explosives and organophosphate nerve agents, the researchers say the device could be expanded to other hazardous environmental or security agents.The authors acknowledge funding from the Defense Threat Reduction Agency Joint Science and Technology Office for Chemical and Biological Defense.Story Source:Materials provided by American Chemical Society. ",0.143749,0.165236,0.178677,0.250649,0.206701,0.200278,0.204324,0,0
765,Flexible sensors can detect movement in GI tract Ingestible devices could diagnose gastrointestinal slowdown or monitor food intake,"Researchers at MIT and Brigham and Women's Hospital have built a flexible sensor that can be rolled up and swallowed. Upon ingestion, the sensor adheres to the stomach wall or intestinal lining, where it can measure the rhythmic contractions of the digestive tract.Such sensors could help doctors to diagnose gastrointestinal disorders that slow down the passage of food through the digestive tract. They could also be used to detect food pressing on the stomach, helping doctors to monitor food intake by patients being treated for obesity.The flexible devices are based on piezoelectric materials, which generate a current and voltage when they are mechanically deformed. They also incorporate polymers with elasticity similar to that of human skin, so that they can conform to the skin and stretch when the skin stretches.In a study appearing in the Oct. 10 issue of Nature Biomedical Engineering, the researchers demonstrated that the sensor remains active in the stomachs of pigs for up to two days. The flexibility of the device could offer improved safety over more rigid ingestible devices, the researchers say.""Having flexibility has the potential to impart significantly improved safety, simply because it makes it easier to transit through the GI tract,"" says Giovanni Traverso, a research affiliate at MIT's Koch Institute for Integrative Cancer Research, a gastroenterologist and biomedical engineer at Brigham and Women's Hospital, and one of the senior authors of the paper.Canan Dagdeviren, an assistant professor in MIT's Media Lab and the director of the Conformable Decoders research group, is the paper's lead author and one of the corresponding authors. Robert Langer, the David H. Koch Institute Professor and a member of the Koch Institute, is also an author of the paper.Flex those sensorsTraverso and colleagues have previously developed ingestible devices that can be used to monitor vital signs or deliver drugs to the digestive tract. With the goal of developing a more flexible sensor that might offer improved safety, Traverso teamed up with Dagdeviren, who previously developed flexible electronic devices such as a wearable blood pressure sensor and flexible mechanical energy harvesters.To make the new sensor, Dagdeviren first fabricates electronic circuits on a silicon wafer. The circuits contain two electrodes: a gold electrode placed atop a piezolelectric material called PZT, and a platinum electrode on the underside of the PZT. Once the circuit is fabricated, it can be removed from the silicon wafer and printed onto a flexible polymer called polyimide.The ingestible sensor that the researchers designed for this study is 2 by 2.5 centimeters and can be rolled up and placed in a capsule that dissolves after being swallowed.In tests in pigs, the sensors successfully adhered to the stomach lining after being delivered endoscopically. Through external cables, the sensors transmitted information about how much voltage the piezoelectrical sensor generated, from which the researchers could calculate how much the stomach wall was moving, as well as distinguish when food or liquid were ingested.""For the first time, we showed that a flexible, piezoelectric device can stay in the stomach up to two days without any electrical or mechanical degradation,"" Dagdeviren says.Monitoring motilityThis type of sensor could make it easier to diagnose digestive disorders that impair motility of the digestive tract, which can result in difficulty swallowing, nausea, gas, or constipation.Doctors could also use it to help measure the food intake of patients being treated for obesity. ""Having a window into what an individual is actually ingesting at home is helpful, because sometimes it's difficult for patients to really benchmark themselves and know how much is being consumed,"" Traverso says.In future versions of the device, the researchers plan to harvest some of the energy generated by the piezoelectric material to power other features, including additional sensors and wireless transmitters. Such devices would not require a battery, further improving their potential safety.Story Source:Materials provided by Massachusetts Institute of Technology. Original written by Anne Trafton. ",0.0903175,0.137171,0.138566,0.183612,0.167019,0.131557,0.162855,0,0
766,Novel circuit design boosts wearable thermoelectric generators,"Using flexible conducting polymers and novel circuitry patterns printed on paper, researchers have demonstrated proof-of-concept wearable thermoelectric generators that can harvest energy from body heat to power simple biosensors for measuring heart rate, respiration or other factors.Because of their symmetrical fractal wiring patterns, the devices can be cut to the size needed to provide the voltage and power requirements for specific applications. The modular generators could be inkjet printed on flexible substrates, including fabric, and manufactured using inexpensive roll-to-roll techniques.""The attraction of thermoelectric generators is that there is heat all around us,"" said Akanksha Menon, a Ph.D. student in the Woodruff School of Mechanical Engineering at the Georgia Institute of Technology. ""If we can harness a little bit of that heat and turn it into electricity inexpensively, there is great value. We are working on how to produce electricity with heat from the body.""The research, supported by PepsiCo, Inc. and the Air Force Office of Scientific Research, was reported online in the Journal of Applied Physics on September 28th.Thermoelectric generators, which convert thermal energy directly into electricity, have been available for decades, but standard designs use inflexible inorganic materials that are too toxic for use in wearable devices. Power output depends on the temperature differential that can be created between two sides of the generators, which makes depending on body heat challenging. Getting enough thermal energy from a small contact area on the skin increases the challenge, and internal resistance in the device ultimately limits the power output.To overcome that, Menon and collaborators in the laboratory of Assistant Professor Shannon Yee designed a device with thousands of dots composed of alternating p-type and n-type polymers in a closely-packed layout. Their pattern converts more heat per unit area due to large packing densities enabled by inkjet printers. By placing the polymer dots closer together, the interconnect length decreases, which in turn lowers the total resistance and results in a higher power output from the device.""Instead of connecting the polymer dots with a traditional serpentine wiring pattern, we are using wiring patterns based on space filling curves, such as the Hilbert pattern -- a continuous space-filling curve,"" said Kiarash Gordiz, a co-author who worked on the project while he was a Ph.D. student at Georgia Tech. ""The advantage here is that Hilbert patterns allow for surface conformation and self-localization, which provides a more uniform temperature across the device.""The new circuit design also has another benefit: its fractally symmetric design allows the modules to be cut along boundaries between symmetric areas to provide exactly the voltage and power needed for a specific application. That eliminates the need for power converters that add complexity and take power away from the system.""This is valuable in the context of wearables, where you want as few components as possible,"" said Menon. ""We think this could be a really interesting way to expand the use of thermoelectrics for wearable devices.""So far, the devices have been printed on ordinary paper, but the researchers have begun exploring the use of fabrics. Both paper and fabric are flexible, but the fabric could be easily integrated into clothing.""We want to integrate our device into the commercial textiles that people wear every day,"" said Menon. ""People would feel comfortable wearing these fabrics, but they would be able to power something with just the heat from their bodies.""With the novel design, the researchers expect to get enough electricity to power small sensors, in the range of microwatts to milliwatts. That would be enough for simple heart rate sensors, but not more complex devices like fitness trackers or smartphones. The generators might also be useful to supplement batteries, allowing devices to operate for longer periods of time.Among the challenges ahead are protecting the generators from moisture and determining just how close they should be to the skin to transfer thermal energy -- while remaining comfortable for wearers.The researchers use commercially-available p-type materials, and are working with chemists at Georgia Tech to develop better n-type polymers for future generations of devices that can operate with small temperature differentials at room temperatures. Body heat produces differentials as small as five degrees, compared to a hundred degrees for generators used as part of piping and steam lines.""One future benefit of this class of polymer material is the potential for a low-cost and abundant thermoelectric material that would have an inherently low thermal conductivity,"" said Yee, who directs the lab as part of the Woodruff School of Mechanical Engineering. ""The organic electronics community has made tremendous advances in understanding electronic and optical properties of polymer-based materials. We are building upon that knowledge to understand thermal and thermoelectric transport in these polymers to enable new device functionality.""Among the other prospects for the materials being developed are localized cooling devices that reverse the process, using electricity to move thermal energy from one side of a device to another. Cooling just parts of the body could provide the perception of comfort without the cost of large-space air conditioning, Yee said.Story Source:Materials provided by Georgia Institute of Technology. ",0.0419121,0.0645945,0.0748075,0.108683,0.0805948,0.0913952,0.0948448,0,0
767,Paper-based supercapacitor uses metal nanoparticles to boost energy density,"Using a simple layer-by-layer coating technique, researchers from the U.S. and Korea have developed a paper-based flexible supercapacitor that could be used to help power wearable devices. The device uses metallic nanoparticles to coat cellulose fibers in the paper, creating supercapacitor electrodes with high energy and power densities -- and the best performance so far in a textile-based supercapacitor.By implanting conductive and charge storage materials in the paper, the technique creates large surface areas that function as current collectors and nanoparticle reservoirs for the electrodes. Testing shows that devices fabricated with the technique can be folded thousands of times without affecting conductivity.""This type of flexible energy storage device could provide unique opportunities for connectivity among wearable and internet of things devices,"" said Seung Woo Lee, an assistant professor in the Woodruff School of Mechanical Engineering at the Georgia Institute of Technology. ""We could support an evolution of the most advanced portable electronics. We also have an opportunity to combine this supercapacitor with energy-harvesting devices that could power biomedical sensors, consumer and military electronics, and similar applications.""The research, done with collaborators at Korea University, was supported by the National Research Foundation of Korea and reported September 14 in the journal Nature Communications.Energy storage devices are generally judged on three properties: their energy density, power density and cycling stability. Supercapacitors often have high power density, but low energy density -- the amount of energy that can be stored -- compared to batteries, which often have the opposite attributes. In developing their new technique, Lee and collaborator Jinhan Cho from the Department of Chemical and Biological Engineering at Korea University set out to boost energy density of the supercapacitors while maintaining their high power output.The researchers began by dipping paper samples into a beaker of solution containing an amine surfactant material designed to bind the gold nanoparticles to the paper. Next they dipped the paper into a solution containing gold nanoparticles. Because the fibers are porous, the surfactants and nanoparticles enter the fibers and become strongly attached, creating a conformal coating on each fiber.By repeating the dipping steps, the researchers created a conductive paper on which they added alternating layers of metal oxide energy storage materials such as manganese oxide. The ligand-mediated layer-by-layer approach helped minimize the contact resistance between neighboring metal and/or metal oxide nanonparticles. Using the simple process done at room temperatures, the layers can be built up to provide the desired electrical properties.""It's basically a very simple process,"" Lee said. ""The layer-by-layer process, which we did in alternating beakers, provides a good conformal coating on the cellulose fibers. We can fold the resulting metallized paper and otherwise flex it without damage to the conductivity.""Though the research involved small samples of paper, the solution-based technique could likely be scaled up using larger tanks or even a spray-on technique. ""There should be no limitation on the size of the samples that we could produce,"" Lee said. ""We just need to establish the optimal layer thickness that provides good conductivity while minimizing the use of the nanoparticles to optimize the tradeoff between cost and performance.""The researchers demonstrated that their self-assembly technique improves several aspects of the paper supercapacitor, including its areal performance, an important factor for measuring flexible energy-storage electrodes. The maximum power and energy density of the metallic paper-based supercapacitors are estimated to be 15.1mWcm?2 and 267.3 ?Wh cm?2, respectively, substantially outperforming conventional paper or textile supercapacitors.The next steps will include testing the technique on flexible fabrics, and developing flexible batteries that could work with the supercapacitors. The researchers used gold nanoparticles because they are easy to work with, but plan to test less expensive metals such as silver and copper to reduce the cost.During his Ph.D. work, Lee developed the layer-by-layer self-assembly process for energy storage using different materials. With his Korean collaborators, he saw a new opportunity to apply that to flexible and wearable devices with nanoparticles.""We have nanoscale control over the coating applied to the paper,"" he added. ""If we increase the number of layers, the performance continues to increase. And it's all based on ordinary paper.""Story Source:Materials provided by Georgia Institute of Technology. ",0.0695443,0.119838,0.110849,0.135201,0.122121,0.114455,0.121385,0,0
768,New finding of particle physics may help to explain the absence of antimatter,"In the Standard Model of particle physics, there is almost no difference between matter and antimatter. But there is an abundance of evidence that our observable universe is made up only of matter -- if there was any antimatter, it would annihilate with nearby matter to produce very high intensity gamma radiation, which has not been observed. Therefore, figuring out how we ended up with an abundance of only matter is one of the biggest open questions in particle physics.Because of this and other gaps in the Standard Model, physicists are considering theories which add a few extra particles in ways that will help to solve the problem. One of these models is called the Two Higgs Doublet Model, which, despite the name, actually adds four extra particles. This model can be made to agree with all particle physics observations made so far, including ones from the Large Hadron Collider at CERN, but it was unclear whether it could also solve the problem of the matter-antimatter imbalance. The research group, led by a University of Helsinki team, set out to tackle the problem from a different angle. Their findings have now been published in a paper in the Physical Review Letters.About ten picoseconds after the Big Bang -- right about the time the Higgs boson was turning on -- the universe was a hot plasma of particles.""The technique of dimensional reduction lets us replace the theory which describes this hot plasma with a simpler quantum theory with a set of rules that all the particles must follow,"" explains Dr. David Weir, the corresponding author of the article.""It turns out that the heavier, slower-moving particles don't matter very much when these new rules are imposed, so we end up with a much less complicated theory.""This theory can then be studied with computer simulations, which provide a clear picture of what happened. In particular, they can tell us how violently out of equilibrium the universe was when the Higgs boson turned on. This is important for determining whether there was scope for producing the matter-antimatter asymmetry at this time in the history of the universe using the Two Higgs Doublet Model.""Our results showed that it is indeed possible to explain the absence of antimatter and remain in agreement with existing observations,"" Dr. Weir remarks. Importantly, by making use of dimensional reduction, the new approach was completely independent of any previous work in this model.If the Higgs boson turned on in such a violent way, it would have left echoes. As the bubbles of the new phase of the universe nucleated, much like clouds, and expanded until the universe was like an overcast sky, the collisions between the bubbles would have produced lots of gravitational waves. Researchers at the University of Helsinki and elsewhere are now gearing up to look for these gravitational waves at missions such as the European LISA project.Story Source:Materials provided by University of Helsinki. ",0.0914268,0.104139,0.129563,0.18644,0.108867,0.164586,0.158915,0,0
769,Gravitational waves could soon accurately measure universe's expansion New LIGO readings could improve disputed measurement quickly,"Twenty years ago, scientists were shocked to realize that our universe is not only expanding, but that it's expanding faster over time.Pinning down the exact rate of expansion, called the Hubble constant after famed astronomer and UChicago alumnus Edwin Hubble, has been surprisingly difficult. Since then scientists have used two methods to calculate the value, and they spit out distressingly different results. But last year's surprising capture of gravitational waves radiating from a neutron star collision offered a third way to calculate the Hubble constant.That was only a single data point from one collision, but in a new paper published Oct. 17 in Nature, three University of Chicago scientists estimate that given how quickly researchers saw the first neutron star collision, they could have a very accurate measurement of the Hubble constant within five to ten years.""The Hubble constant tells you the size and the age of the universe; it's been a holy grail since the birth of cosmology. Calculating this with gravitational waves could give us an entirely new perspective on the universe,"" said study author Daniel Holz, a UChicago professor in physics who co-authored the first such calculation from the 2017 discovery. ""The question is: When does it become game-changing for cosmology?""In 1929, Edwin Hubble announced that based on his observations of galaxies beyond the Milky Way, they seemed to be moving away from us -- and the farther away the galaxy, the faster it was receding. This is a cornerstone of the Big Bang theory, and it kicked off a nearly century-long search for the exact rate at which this is occurring.To calculate the rate at which the universe is expanding, scientists need two numbers. One is the distance to a faraway object; the other is how fast the object is moving away from us because of the expansion of the universe. If you can see it with a telescope, the second quantity is relatively easy to determine, because the light you see when you look at a distant star gets shifted into the red as it recedes. Astronomers have been using that trick to see how fast an object is moving for more than a century -- it's like the Doppler effect, in which a siren changes pitch as an ambulance passes.'Major questions in calculations'But getting an exact measure of the distance is much harder. Traditionally, astrophysicists have used a technique called the cosmic distance ladder, in which the brightness of certain variable stars and supernovae can be used to build a series of comparisons that reach out to the object in question. ""The problem is, if you scratch beneath the surface, there are a lot of steps with a lot of assumptions along the way,"" Holz said.Perhaps the supernovae used as markers aren't as consistent as thought. Maybe we're mistaking some kinds of supernovae for others, or there's some unknown error in our measurement of distances to nearby stars. ""There's a lot of complicated astrophysics there that could throw off readings in a number of ways,"" he said.The other major way to calculate the Hubble constant is to look at the cosmic microwave background -- the pulse of light created at the very beginning of the universe, which is still faintly detectable. While also useful, this method also relies on assumptions about how the universe works.The surprising thing is that even though scientists doing each calculation are confident about their results, they don't match. One says the universe is expanding almost 10 percent faster than the other. ""This is a major question in cosmology right now,"" said the study's first author, Hsin-Yu Chen, then a graduate student at UChicago and now a fellow with Harvard University's Black Hole Initiative.Then the LIGO detectors picked up their first ripple in the fabric of space-time from the collision of two stars last year. This not only shook the observatory, but the field of astronomy itself: Being able to both feel the gravitational wave and see the light of the collision's aftermath with a telescope gave scientists a powerful new tool. ""It was kind of an embarrassment of riches,"" Holz said.Gravitational waves offer a completely different way to calculate the Hubble constant. When two massive stars crash into each other, they send out ripples in the fabric of space-time that can be detected on Earth. By measuring that signal, scientists can get a signature of the mass and energy of the colliding stars. When they compare this reading with the strength of the gravitational waves, they can infer how far away it is.This measurement is cleaner and holds fewer assumptions about the universe, which should make it more precise, Holz said. Along with Scott Hughes at MIT, he suggested the idea of making this measurement with gravitational waves paired with telescope readings in 2005. The only question is how often scientists could catch these events, and how good the data from them would be.'It's only going to get more interesting'The paper predicts that once scientists have detected 25 readings from neutron star collisions, they'll measure the expansion of the universe within an accuracy of 3 percent. With 200 readings, that number narrows to 1 percent.""It was quite a surprise for me when we got into the simulations,"" Chen said. ""It was clear we could reach precision, and we could reach it fast.""A precise new number for the Hubble constant would be fascinating no matter the answer, the scientists said. For example, one possible reason for the mismatch in the other two methods is that the nature of gravity itself might have changed over time. The reading also might shed light on dark energy, a mysterious force responsible for the expansion of the universe.""With the collision we saw last year, we got lucky -- it was close to us, so it was relatively easy to find and analyze,"" said Maya Fishbach, a UChicago graduate student and the other author on the paper. ""Future detections will be much farther away, but once we get the next generation of telescopes, we should be able to find counterparts for these distant detections as well.""The LIGO detectors are planned to begin a new observing run in February 2019, joined by their Italian counterparts at VIRGO. Thanks to an upgrade, the detectors' sensitivities will be much higher -- expanding the number and distance of astronomical events they can pick up.""It's only going to get more interesting from here,"" Holz said.The authors ran calculations at the University of Chicago Research Computing Center.Story Source:Materials provided by University of Chicago. Original written by Louise Lerner. ",0.0723898,0.0670417,0.100223,0.138751,0.085051,0.121914,0.116946,0,0
770,String theory: Is dark energy even allowed?,"In string theory, a paradigm shift could be imminent. In June, a team of string theorists from Harvard and Caltech published a conjecture which sounded revolutionary: String theory is said to be fundamentally incompatible with our current understanding of ""dark energy"" -- but only with ""dark energy"" can we explain the accelerated expansion of our current universe.Timm Wrase of the Vienna University of Technology quickly realized something odd about this conjecture: it seemed to be incompatible with the existence of the Higgs particle. His calculations, which he carried out together with theorists from Columbia University in New York and the University of Heidelberg, have now been published in Physical Review. At the moment, there are heated discussions about strings and dark energy all around the world. Wrase hopes that this will lead to new breakthroughs in this line of research.The theory for everythingGreat hope is placed in string theory. It is supposed to explain how gravity is related to quantum physics and how we can understand the laws of nature, which describe the entire physical world, from the smallest particles to the largest structure of the cosmos.Often, string theory has been accused of merely providing abstract mathematical results and making too few predictions that can actually be verified in an experiment. Now, however, the string theory community all around the world is discussing a question that is closely related to cosmic experiments measuring the expansion of the universe. In 2011, the Nobel Prize in Physics was awarded for the discovery that the universe is not only constantly growing larger, but that this expansion is actually accelerating.This phenomenon can only be explained by assuming an additional, previously unknown ""dark energy."" This idea originally came from Albert Einstein, who added it as a ""cosmological constant"" to his theory of general relativity. Einstein actually did this to construct a non-expanding universe. When Hubble discovered in 1929 that the universe was in fact expanding, Einstein described this modification of his equations as the biggest blunder of his life. But with the discovery of the accelerated expansion of the cosmos, the cosmological constant has been reintroduced as dark energy into the current standard model of cosmology.Like an apple in the fruit bowl""For a long time, we thought that such a dark energy can be well accommodated in string theory,"" says Timm Wrase from the Institute for Theoretical Physics of the Vienna University of Technology. String theory assumes that there are additional, previously unknown particles that can be described as fields.These fields have a state of minimal energy -- much like an apple lying in a bowl. It will always lie at the very bottom, at the lowest point of the bowl. Everywhere else its energy would be higher, if we want to shift it, we have to exert energy. But that does not mean that the apple at the lowest point has no energy at all. We can put the bowl with the apple on the ground, or on top of the table -- there the apple has more energy but it still cannot move, because it is still in a state of minimal energy in its bowl.""In string theory there are fields which could explain dark energy in a similar way -- locally, they are in a state of minimal energy, but still their energy has a value greater than zero,"" explains Timm Wrase. ""So these fields would provide the so-called dark energy, with which we could explain the accelerated expansion of the universe.""But Cumrun Vafa from Harvard University, one of the world's most renowned string theorists, published an article on June 25, raising many eyebrows. He suggested that such ""bowl-shaped"" fields of positive energy are not possible in string theory.The Higgs field -- a contradictionTimm Wrase of the Vienna University of Technology quickly realized the implications of this claim: ""If that is true, the accelerated expansion of the universe, as we have imagined it so far, is not possible"" he says. ""The accelerated expansion would then have to be described by a field with quite different properties, like a tilted plane on which a ball rolls downhill, losing potential energy."" But in that case, the amount of ""dark energy"" in the universe would change over time, and the accelerated expansion of the universe may one day come to a halt. Gravity could then pull all matter back together and assemble everything at one point, similar to the time of the Big Bang.But Timm Wrase, who had already dealt with similar questions in his doctoral thesis, found that this idea cannot be the whole truth either. ""Cumrun Vafa's conjecture, which prohibits certain types of fields, would also prohibit things that we already know to exist,"" he explains.Wrase was able to show that the Higgs field also has properties that should actually be forbidden by Vafa's conjecture -- and the Higgs field is considered an experimentally proven fact. For its discovery, the 2013 Nobel Prize in Physics was awarded. Wrase uploaded his results to the preprint website Arxiv, quickly sparking a lot of discussions in the string theory community. Now the work has been peer reviewed and published in the journal Physical Review.""This controversy is a good thing for string theory,"" Timm Wrase is convinced. ""Suddenly, a lot of people have completely new ideas which nobody has thought about before."" Wrase and his team are now investigating which fields are allowed in string theory and at which points they violate Vafa's conjecture. ""Maybe that leads us to exciting new insights into the nature of dark energy -- that would be a great success,"" says Wrase.The hypotheses that arise will (at least in part) soon be tested experimentally. In the next few years the accelerated expansion of the universe will be measured more accurately than ever before.Story Source:Materials provided by Vienna University of Technology. ",0.0923806,0.103888,0.134516,0.204485,0.130677,0.16377,0.164336,0,0
771,Searching for errors in the quantum world,"There is likely no other scientific theory that is as well supported as quantum mechanics. For nearly 100 years now, it has repeatedly been confirmed with highly precise experiments, yet physicists still aren't entirely happy. Although quantum mechanics describes events at the microscopic level very accurately, it comes up against its limits with larger objects -- especially objects for which the force of gravity plays a role. Quantum mechanics can't describe the behaviour of planets, for instance, which remains the domain of the general theory of relativity. This theory, in turn, can't correctly describe small-scale processes. Many physicists therefore dream of combining quantum mechanics with the theory of relativity to form a coherent worldview.Toward larger objectsBut how is it possible to combine two theories that, despite both describing the physical processes in their domains very accurately, differ so greatly? One possibility is to conduct quantum physics experiments with increasingly larger objects in the hope that discrepancies will eventually appear that point to possible solutions. But physicists must work within tight constraints. The famous double-slit experiment, for instance, which can be used to show that solid particles simultaneously behave like waves, can't be performed with everyday objects.Thought experiments, on the other hand, can be used to transcend the boundaries of the macroscopic world. That's exactly what Renato Renner, Professor for Theoretical Physics, and his former doctoral student Daniela Frauchiger have now done in a publication that appears in Nature Communications magazine today. Roughly speaking, in their thought experiment, the two consider a hypothetical physicist examining a quantum mechanical object and then use quantum mechanics to calculate what that physicist will observe. According to our current worldview, this indirect observation should yield the same result as direct observation, yet the pair's calculations show that precisely this is not the case. The prediction as to what the physicist will observe is exactly the opposite of what would be measured directly, creating a paradoxical situation.No simple solutionsAlthough the thought experiment is only now being officially published in a scientific journal, it has already become a topic of discussion among experts. As the publication process was repeatedly delayed, various other publications are already addressing the findings -- itself a paradoxical situation, Renner notes.The most common initial reaction of his colleagues in the field is to question the calculations, Renner says, but so far, no one has managed to disprove them. One reviewer conceded that he had meanwhile made five attempts to find an error in the calculations -- without success. Other colleagues presented concrete explanations as to how the paradox can be resolved. Upon closer inspection, though, they always turned out to be ad hoc solutions that don't actually fix the problem.Perplexing conclusionsRenner finds it remarkable that the issue evidently polarises people. He was surprised to note that some of his colleagues reacted very emotionally to his findings -- probably due to the fact that the two obvious conclusions from Renner's and Frauchiger's findings are equally perplexing. The one explanation is that quantum mechanics is apparently not, as was previously thought, universally applicable and thus can't be applied to large objects. But how is it possible for a theory to be inconsistent when it has repeatedly been so clearly confirmed by experiments? The other explanation is that it is evidently not only politics that suffers from a lack of clear facts, but also physics, and that there are other possibilities besides what we deem to be true.Renner has difficulties with both of these interpretations. He rather believes that the paradox will be resolved in some other way: ""When we look back at history, at moments like this, the solution often came from an unexpected direction,"" he explains. The general theory of relativity, for instance, which solved contradictions in Newtonian physics, is based on the realisation that the concept of time as it was commonly understood back then was wrong.""Our job now is to examine whether our thought experiment assumes things that shouldn't be assumed in that form,"" Renner says, ""and who knows, perhaps we will even have to revise our concept of space and time once again."" For Renner, that would definitely be an appealing option: ""It's only when we fundamentally rethink existing theories that we gain deeper insights into how nature really works.""Story Source:Materials provided by ETH Zurich. Original written by Felix Wersten. ",0.050905,0.0686826,0.0966102,0.151438,0.075105,0.103099,0.115443,0,0
772,Quantum leap for Einstein's equivalence principle,"How Einstein's equivalence principle extends to the quantum world has been puzzling physicists for decades, but a team including a University of Queensland researcher has found the key to this question.UQ physicist, Dr Magdalena Zych from the ARC Centre of Excellence for Engineered Quantum Systems, and the University of Vienna's Professor Caslav Brukner have been working to discover if quantum objects interact with gravity only through curved space-time.""Einstein's equivalence principle contends that the total inertial and gravitational mass of any objects are equivalent, meaning all bodies fall in the same way when subject to gravity,"" Dr Zych said.""Physicists have been debating whether the principle applies to quantum particles, so to translate it to the quantum world we needed to find out how quantum particles interact with gravity.""We realised that to do this we had to look at the mass.""Mass is dynamic quantity and can have different values, and in quantum physics, mass of a particle can be in a quantum 'superposition' of two different values.In a state unique to quantum physics, energy and mass can exist in a 'quantum superposition' -- as if they consisted of two different values 'at the same time'.""We realised that we had to look how particles in such quantum states of the mass behave in order to understand how a quantum particle sees gravity in general,"" she said.""Our research found that for quantum particles in quantum superpositions of different masses, the principle implies additional restrictions that are not present for classical particles -- this hadn't been discovered before.''""It means that previous studies that attempted to translate the principle to quantum physics were incomplete because they focused on trajectories of the particles but neglected the mass.""The study opens a door for new experiments that are necessary to test if quantum particles obey the additional restrictions that have been found.Story Source:Materials provided by University of Queensland. ",0.161009,0.190712,0.189826,0.235627,0.17859,0.230812,0.234046,0,0
773,Household phenomenon observed by Leonardo da Vinci finally explained,"An everyday occurrence spotted when we turn on the tap to brush our teeth has baffled engineers for centuries -- why does the water splay when it hits the sink before it heads down the plughole?Famous inventor and painter Leonardo da Vinci documented the phenomenon, now known as a hydraulic jump, back in the 1500s. Hydraulic jumps are harmless in our household sinks but they can cause violent waves, turbulence and whirlpools in deeper water.Since the 1820s scientists have believed that hydraulic jumps occur partly as a result of the gravitational pull. But a paper published in the Journal of Fluid Mechanics has disproved this longstanding theory.Rajesh Bhagat, a Chemical Engineering PhD student at St John's College, University of Cambridge, and first author of the paper, fired jets of water upwards and sideways onto flat surfaces, and witnessed exactly the same hydraulic jumps as those when the water flowed downwards.But what was causing it? Bhagat suspected they could all affected by the same factors -- surface tension and viscosity.By altering these attributes of the water he was able to accurately predict the size of the hydraulic jumps, regardless of which direction the water was moving -- debunking the 200-year-old gravitational theory as the cause of a kitchen sink type hydraulic jump. This kind of hydraulic jump is known as a circular hydraulic jump.Professor Paul Linden, Director of Research at the Department of Applied Mathematics and Theoretical Physics at the University of Cambridge and an author of the paper, described Bhagat's findings as 'ground breaking'.He explained: ""His experiments and theory show that the surface tension of the liquid is the key to the process and has this has never before been recognised even though the problem was discussed by da Vinci and many others since. This work represents a remarkable achievement in our understanding of the dynamics of thin layers of fluid.""Bhagat predicts that his findings could have wide reaching consequences for industries that have high levels of water consumption.He said: ""Knowing how to manipulate the boundary of a hydraulic jump is very important and now with this theory we can easily extend or reduce the boundary.""Understanding this process has big implications and could reduce industrial water use dramatically. The new theory is already being used in practical work in the Chemical Engineering department. People can use this theory to find new ways to clean everything from cars to factory equipment.""Bhagat hopes his research will also be used to find new ways to help us use less water in the average household.Story Source:Materials provided by St John's College, University of Cambridge. The original story is licensed under a Creative Commons License. ",0.105246,0.116903,0.157242,0.173624,0.113953,0.175401,0.17619,0,0
774,Centenary of cosmological constant lambda,"Physicists are now celebrating the 100th anniversary of the cosmological constant. On this occasion, two papers recently published in EPJ H highlight its role in modern physics and cosmology. Although the term was first introduced when the universe was thought to be static, today the cosmological constant has become the main candidate for representing the physical essence believed to be responsible for the accelerated expansion of our universe. Before becoming widely accepted, the cosmological constant was during decades the subject of many discussions about its necessity, its value and its physical essence. Today, there are still unresolved problems in understanding the deep physical nature of the phenomena associated with the cosmological constant.In his paper, Bohdan Novosyadlyj, affiliated with the National University of Lviv, Ukraine, explains how Albert Einstein introduced the cosmological constant in 1917 to make the model of static Universe, then accepted by most scientists, work. Its deep physical meaning, however, escaped Einstein. Following the discovery of evidence for a non-static universe in 1929, Einstein regretted introducing this constant into equations of general relativity. Meanwhile, other scientists tried for decades to understand its physical meaning and establish its magnitude. When evidence of dark energy was observed by Michael Turner in 1998, scientists began to consider alternatives of cosmological constant to model that dark energy.In another paper, Cormac O'Raifeartaigh from Waterford Institute of Technology, Ireland, and colleagues present a detailed analysis of the 100-year history of the cosmological constant. Starting with static models of the universe, the paper explains how the constant became marginalised following the discovery of cosmic expansion. Subsequently, it was revived to address specific cosmic puzzles such as the timespan of expansion, the formation of galaxies and the red-shifts of quasars.More recently, the constant has acquired greater physical meaning, as it has helped to the matching of further recent observations with theory. Specifically, it was helpful to reconcile current theory with the recently-observed phenomenon of dark energy as evidenced by the measurement of present cosmic expansion using the Hubble Space Telescope, the measurement of past expansion using supernova, and the measurement of cosmic microwave background by balloon and satellite.Story Source:Materials provided by Springer. ",0.141254,0.153532,0.190913,0.21771,0.171005,0.223188,0.198803,0,0
775,Einstein proved right in another galaxy,"An international team of astronomers have made the most precise test of gravity outside our own solar system.By combining data taken with NASA's Hubble Space Telescope and the European Southern Observatory's Very Large Telescope, the researchers show that gravity in this galaxy behaves as predicted by Albert Einstein's general theory of relativity, confirming the theory's validity on galactic scales.In 1915 Albert Einstein proposed his general theory of relativity (GR) to explain how gravity works. Since then GR has passed a series of high precision tests within the solar system, but there have been no precise tests of GR on large astronomical scales.It has been known since 1929 that the Universe is expanding, but in 1998 two teams of astronomers showed that the Universe is expanding faster now than it was in the past. This surprising discovery -- which won the Nobel Prize in 2011 -- cannot be explained unless the Universe is mostly made of an exotic component called dark energy. However, this interpretation relies on GR being the correct theory of gravity on cosmological scales. Testing the long distance properties of gravity is important to validate our cosmological model.A team of astronomers, led by Dr Thomas Collett of the Institute of Cosmology and Gravitation at the University of Portsmouth, used a nearby galaxy as a gravitational lens to make a precise test of gravity on astronomical length scales.Dr Collett said: ""General Relativity predicts that massive objects deform space-time, this means that when light passes near another galaxy the light's path is deflected. If two galaxies are aligned along our line of sight this can give rise to a phenomenon, called strong gravitational lensing, where we see multiple images of the background galaxy. If we know the mass of the foreground galaxy, then the amount of separation between the multiple images tells us if General Relativity is the correct theory of gravity on galactic scales.""A few hundred strong gravitational lenses are known, but most are too distant to precisely measure their mass, so they can't be used to accurately test GR. However, the galaxy ESO325-G004 is amongst the closest lenses, at 500 million light years from Earth.Dr Collett continues: ""We used data from the Very Large Telescope in Chile to measure how fast the stars were moving in E325 -- this let us infer how much mass there must be in E325 to hold these stars in orbit. We then compared this mass to the strong lensing image separations that we observed with the Hubble Space telescope and the result was just what GR predicts with 9 per cent precision. This is the most precise extrasolar test of GR to date, from just one galaxy.""""The Universe is an amazing place providing such lenses which we can then use as our laboratories,"" adds team member Professor Bob Nichol, Director of the Institute of Cosmology and Gravitation. ""It is so satisfying to use the best telescopes in the world to challenge Einstein, only to find out how right he was.""The research is published today in the journal Science. The work was funded by the University of Portsmouth and the UK Science and Technologies Funding Council.Story Source:Materials provided by University of Portsmouth. ",0.125604,0.111996,0.150452,0.18139,0.129765,0.176347,0.166871,0,0
776,Atomic clock comparison confirms key assumptions of 'Einstein's elevator',"By comparing different types of remote atomic clocks, physicists at the National Institute of Standards and Technology (NIST) have performed the most accurate test ever of a key principle underlying Albert Einstein's famous theory of general relativity, which describes how gravity relates to space and time.The NIST result, made possible by continual improvements in the world's most accurate atomic clocks, yields a record-low, exceedingly small value for a quantity that Einstein predicted to be zero.As described in a Nature Physics paper posted online June 4, NIST researchers used the solar system as a laboratory for testing Einstein's thought experiment involving Earth as a freefalling elevator. Einstein theorized that all objects located in such an elevator would accelerate at the same rate, as if they were in a uniform gravitational field -- or no gravity at all. Moreover, he predicted, these objects' properties relative to each other would remain constant during the elevator's free-fall.In their experiment, the NIST team regarded Earth as an elevator falling through the Sun's gravitational field. They compared recorded data on the ""ticks"" of two types of atomic clocks located around the world to show they remained in sync over 14 years, even as the gravitational pull on the elevator varied during the Earth's slightly off-kilter orbit around the sun. Researchers compared data from 1999 to 2014 for a total of 12 clocks -- four hydrogen masers (microwave lasers) in the NIST time scale with eight of the most accurate cesium fountain atomic clocks operated by metrology laboratories in the United States, the United Kingdom, France, Germany and Italy.The experiment was designed to test a prediction of general relativity, the principle of local position invariance (LPI), which holds that in a falling elevator, measures of nongravitational effects are independent of time and place. One such measurement compares the frequencies of electromagnetic radiation from atomic clocks at different locations. The researchers constrained the violation of LPI to a value of 0.00000022 plus or minus 0.00000025 -- the most miniscule number yet, consistent with general relativity's predicted result of zero, and corresponding to no violation. This means the ratio of hydrogen to cesium frequencies remained the same as the clocks moved together in the falling elevator.This result has five times less uncertainty than NIST's best previous measurement of the LPI violation, translating to five times greater sensitivity. That earlier 2007 result, from a 7-year comparison of cesium and hydrogen atomic clocks, was 20 times more sensitive than the previous tests.The latest measurement advance is due to improvements in several areas, namely more accurate cesium fountain atomic clocks, better time transfer processes (which enable devices at different locations to compare their time signals), and the latest data for computing the position and velocity of Earth in space, NIST's Bijunath Patla said.""But the main reason we did this work was to highlight how atomic clocks are used to test fundamental physics; in particular, the foundations of general relativity,"" Patla said. ""This is the claim made most often when clockmakers strive for better stability and accuracy. We tie together tests of general relativity with atomic clocks, note the limitations of the current generation of clocks, and present a future outlook for how next-generation clocks will become very relevant.""Further limits on LPI are unlikely to be obtained using hydrogen and cesium clocks, the researchers say, but experimental next-generation clocks based on optical frequencies, which are much higher than the frequencies of hydrogen and cesium clocks, could offer much more sensitive results. NIST already operates a variety of these clocks based on atoms such as ytterbium and strontium.Because so many scientific theories and calculations are intertwined, NIST researchers used their new value for the LPI violation to calculate variations in several fundamental ""constants"" of nature, physical quantities thought to be universal and widely used in physics. Their results for the light quark mass were the best ever, while results for the fine structure constant agreed with previously reported values for any pair of atoms.The work was funded in part by the National Aeronautics and Space Administration.Story Source:Materials provided by National Institute of Standards and Technology (NIST). ",0.116796,0.136614,0.138932,0.208952,0.149636,0.180995,0.174153,0,0
777,A quantum entanglement between two physically separated ultra-cold atomic clouds,"Thee journal Science has echoed a novel experiment in the field of quantum physics in which several members of the Quantum Information Theory and Quantum Metrology research group of the Department of Theoretical Physics and History of Science at the UPV/EHU's Faculty of Science and Technology participated, led by Geza Teth, Ikerbasque Research Professor, and carried out at the University of Hannover. In the experiment, they achieved quantum entanglement between two ultra-cold atomic clouds, known as Bose-Einstein condensates, in which the two ensembles of atoms were spatially separated from each other.Quantum entanglement was discovered by Schredinger and later studied by Einstein and other scientists in the last century. It is a quantum phenomenon that has no counterparts in classical physics. The groups of entangled particles lose their individuality and behave as a single entity. Any change in one of the particles leads to an immediate response in the other, even if they are spatially separated. ""Quantum entanglement is essential in applications such as quantum computing, since it enables certain tasks to be performed much faster than in classical computing,"" explained the leader of the Quantum Information Theory and Quantum Metrology group Geza Toth.Unlike the way in which quantum entanglement between clouds of particles has been created up to now, and which involves using incoherent and thermal clouds of particles, in this experiment they used a cloud of atoms in the Bose-Einstein condensate state. As Teth explained, ""Bose-Einstein condensates are achieved by cooling down the atoms to very low temperatures, close to absolute zero. At that temperature, all the atoms are in a highly coherent quantum state; in a sense, they all occupy the same position in space. In that state quantum entanglement exists between the atoms of the ensemble."" Subsequently, the ensemble was split into two atomic clouds. ""We separated the two clouds from each other by a distance, and we were able to demonstrate that the two parts remained entangled with each other,"" he continued.The demonstration that entanglement can be created between two ensembles in the Bose-Einstein condensate state could lead to an improvement in many fields in which quantum technology is used, such as quantum computing, quantum simulation and quantum metrology, since these require the creation and control of large ensembles of entangled particles. ""The advantage of cold atoms is that it is possible to create highly entangled states containing quantities of particles outnumbering any other physical systems by several orders of magnitude, which could provide a basis for large scale quantum computing,"" said the researcher.Story Source:Materials provided by University of the Basque Country. ",0.132294,0.198526,0.186417,0.241973,0.192508,0.197835,0.231578,0,0
778,Taming the multiverse: Stephen Hawking's final theory about the big bang,"Professor Stephen Hawking's final theory on the origin of the universe, which he worked on in collaboration with Professor Thomas Hertog from KU Leuven, has been published today in the Journal of High Energy Physics.The theory, which was submitted for publication before Hawking's death earlier this year, is based on string theory and predicts the universe is finite and far simpler than many current theories about the big bang say.Professor Hertog, whose work has been supported by the European Research Council, first announced the new theory at a conference at the University of Cambridge in July of last year, organised on the occasion of Professor Hawking's 75th birthday.Modern theories of the big bang predict that our local universe came into existence with a brief burst of inflation -- in other words, a tiny fraction of a second after the big bang itself, the universe expanded at an exponential rate. It is widely believed, however, that once inflation starts, there are regions where it never stops. It is thought that quantum effects can keep inflation going forever in some regions of the universe so that globally, inflation is eternal. The observable part of our universe would then be just a hospitable pocket universe, a region in which inflation has ended and stars and galaxies formed.""The usual theory of eternal inflation predicts that globally our universe is like an infinite fractal, with a mosaic of different pocket universes, separated by an inflating ocean,"" said Hawking in an interview last autumn. ""The local laws of physics and chemistry can differ from one pocket universe to another, which together would form a multiverse. But I have never been a fan of the multiverse. If the scale of different universes in the multiverse is large or infinite the theory can't be tested. ""In their new paper, Hawking and Hertog say this account of eternal inflation as a theory of the big bang is wrong. ""The problem with the usual account of eternal inflation is that it assumes an existing background universe that evolves according to Einstein's theory of general relativity and treats the quantum effects as small fluctuations around this,"" said Hertog. ""However, the dynamics of eternal inflation wipes out the separation between classical and quantum physics. As a consequence, Einstein's theory breaks down in eternal inflation.""""We predict that our universe, on the largest scales, is reasonably smooth and globally finite. So it is not a fractal structure,"" said Hawking.The theory of eternal inflation that Hawking and Hertog put forward is based on string theory: a branch of theoretical physics that attempts to reconcile gravity and general relativity with quantum physics, in part by describing the fundamental constituents of the universe as tiny vibrating strings. Their approach uses the string theory concept of holography, which postulates that the universe is a large and complex hologram: physical reality in certain 3D spaces can be mathematically reduced to 2D projections on a surface.Hawking and Hertog developed a variation of this concept of holography to project out the time dimension in eternal inflation. This enabled them to describe eternal inflation without having to rely on Einstein' theory. In the new theory, eternal inflation is reduced to a timeless state defined on a spatial surface at the beginning of time.""When we trace the evolution of our universe backwards in time, at some point we arrive at the threshold of eternal inflation, where our familiar notion of time ceases to have any meaning,"" said Hertog.Hawking's earlier 'no boundary theory' predicted that if you go back in time to the beginning of the universe, the universe shrinks and closes off like a sphere, but this new theory represents a step away from the earlier work. ""Now we're saying that there is a boundary in our past,"" said Hertog.Hertog and Hawking used their new theory to derive more reliable predictions about the global structure of the universe. They predicted the universe that emerges from eternal inflation on the past boundary is finite and far simpler than the infinite fractal structure predicted by the old theory of eternal inflation.Their results, if confirmed by further work, would have far-reaching implications for the multiverse paradigm. ""We are not down to a single, unique universe, but our findings imply a significant reduction of the multiverse, to a much smaller range of possible universes,"" said Hawking.This makes the theory more predictive and testable.Hertog now plans to study the implications of the new theory on smaller scales that are within reach of our space telescopes. He believes that primordial gravitational waves -- ripples in spacetime -- generated at the exit from eternal inflation constitute the most promising ""smoking gun"" to test the model. The expansion of our universe since the beginning means such gravitational waves would have very long wavelengths, outside the range of the current LIGO detectors. But they might be heard by the planned European space-based gravitational wave observatory, LISA, or seen in future experiments measuring the cosmic microwave background.Story Source:Materials provided by University of Cambridge. The original story is licensed under a Creative Commons License. ",0.155759,0.172732,0.171797,0.230012,0.173398,0.238054,0.222089,0,0
779,New math bridges holography and twistor theory,"The modern-day theoretical physicist faces a taxing uphill climb.""As we learn more, reality becomes ever more subtle; the absolute becomes relative, the fixed becomes dynamical, the definite is laden with uncertainty,"" writes physicist Yasha Neiman.A professor and head of the Quantum Gravity Unit at the Okinawa Institute of Science and Technology Graduate University (OIST), he grapples with this conundrum on a daily basis. Quantum gravity, Neiman's branch of physics, aims to unify quantum mechanics, which describes nature at the scale of atoms and subatomic particles, with Einstein's theory of General Relativity -- the modern theory of gravitation as curvature of space and time. How, he asks, can physicists write equations when the geometry of space itself becomes subject to quantum uncertainty? Quantum gravity, the current frontier in fundamental theory, has proven more difficult to detangle than previous concepts, according to Neiman.""With the concept of space slipping between our fingers, we seek out alternative footholds on which to base our description of the world,"" he writes.This search for alternative footholds is, in essence, a search for a new language to describe reality -- and it is the subject of his most recent work, published in the Journal of High Energy Physics. In the paper, Neiman proposes a new vantage point on the geometry of space and time -- one that builds on well-established approaches in physics, like holography and twistor theory, to reach new ground.Holography is an offshoot of string theory -- the theory that the Universe is made up of one-dimensional objects called strings -- which was developed in the late 1990s. Holography imagines the ends of the Universe as the surface of an infinitely large sphere that forms the boundary of space. Even as geometry fluctuates within this sphere, this ""boundary at infinity"" on the sphere's surface can remain fixed.For the past 20 years, holography has been an invaluable tool for conducting quantum-gravity thought experiments. However, astronomical observations have shown that this approach cannot really apply to our world.""The accelerating expansion of our Universe and the finite speed of light conspire to limit all possible observations, present or future, to a finite -- though very large -- region of space,"" Neiman writes.In such a world, the boundary at infinity, where the holographic picture of the Universe is based, is no longer physically meaningful. A new frame of reference may be needed -- one that does not attempt to find a fixed surface in space, but which leaves space behind altogether.In the 1960s, in an attempt to understand quantum gravity, physicist Roger Penrose proposed such a radical alternative. In Penrose's twistor theory, geometric points are replaced by twistors -- entities that most closely resemble stretched, light ray-like shapes. Within this twistor space, Penrose discovered a highly efficient way to represent fields that travel at the speed of light, such as electromagnetic and gravitational fields. Reality, however, is composed of more than fields -- one needs also to account for the interactions between them, such as the electric force between charges, or, in the more complicated case of General Relativity, gravitational attraction resulting from the energy of the field itself. However, including the interactions of General Relativity into this picture has proven a formidable task.So, then, can we express in twistor language a full-fledged quantum gravitational theory, perhaps simpler than General Relativity, but with both fields and interactions fully taken into account? Yes, according to Neiman.Neiman's model builds on higher spin gravity, a model developed by Mikhail Vasiliev in the 1980s and 90s. Higher spin gravity can be thought of as the ""smaller cousin"" of String Theory, ""too simple to reproduce General Relativity, but very instructive as a playground for ideas,"" as Neiman puts it. In particular, it is perfectly suited for exploring possible bridges between holography and twistor theory.On one hand, as discovered by Igor Klebanov and Alexander Polyakov in 2001, higher spin gravity, just like string theory, can be described holographically: its behavior within space can be captured completely in terms of a boundary at infinity. On the other hand, its equations contain twistor-like variables, even if these are still tied to particular points in ordinary space.From these starting points, Neiman's paper takes an additional step, constructing a mathematical dictionary that ties together the languages of holography and twistor theory.""The underlying math that makes this story tick is all about square roots,"" writes Neiman. ""It's about identifying subtle ways in which a geometric operation, such as a rotation or reflection, can be done 'halfway'. A clever square root is like finding a crack in a solid wall, opening it in two, and revealing a new world.""Using square roots in this way has a long-standing history in math and physics. In fact, the intrinsic shape of all matter particles -- such as electrons and quarks -- as well as twistors, is described by a square root of ordinary directions in space. In a subtle technical sense, Neiman's method for connecting space, its boundary at infinity, and twistor space, boils down to taking such a square root again.Neiman hopes that his proof of concept can pave the way towards a quantum theory of gravity that does not rely on a boundary at infinity.""It will take a lot of creativity to uncover the code of the world,"" says Neiman. ""And there's joy in fumbling around for it.""Story Source:Materials provided by Okinawa Institute of Science and Technology (OIST) Graduate University. Original written by Pola Lem. ",0.064053,0.0636466,0.0946579,0.157905,0.0790819,0.124825,0.122324,0,0
780,Understanding gravity: The nanoscale search for extra dimensions Scientists use high-sensitivity experiments to probe exotic gravitational force,"Scientists have used a pulsed slow neutron beamline to probe the deviation of the inverse square law of gravity below the wavelength of 0.1 nm. The experiment achieved the highest sensitivity for a neutron experiment demonstrated to date, and is a significant step toward determining whether the space we live in is really limited to the three dimensions most are familiar with.Often, practical limits control the experimental measurements that can be made, governing the difference between what we expect to be true based on the most likely predictions of models and calculations, and findings that have been supported by testing. A team of researchers has now used the world's highest intensity neutron beamline facility, at J-PARC in central Japan, to push the limits of sensitivity for the study of gravitational force. The multicenter work probing the nm range was recently published in Physical Review D.Most people are familiar with how things around us interact as a result of gravitational interactions. This behavior, known to follow an inverse square law (ISL), has been well explained by experiments down to less than 1 mm. Gravitational interactions over long-distances have also been supported by data collected from astronomy. However, until now, there has been little experimental evidence to support agreement with the ISL when the often-unpredictable quantum level is approached.""There are numerous effects suggested by accepted theories of gravity over short distance ranges that could be borne out by experiment,"" study author Tatsushi Shima of Osaka University says. ""By successfully extending the search range of an exotic gravity down to short distances of ~0.1 nm, we have been able to demonstrate the highest sensitivity reported to date, producing experimental data that will help to unravel the proposals.""The statistical sensitivity achieved was made possible using the high intensity pulsed neutron beam at the J-PARC facility. The net electromagnetic neutrality of neutrons means that the experiments were not influenced by the electromagnetic background that hampers other approaches to probing short distance ISL deviations. The experiment, based on neutron-noble gas scattering, was the first time-of-flight neutron scattering study.""As the performance of the world's most powerful beamlines improves, we are able to significantly enhance our knowledge and understanding in step,"" study corresponding author Tamaki Yoshioka of Kyushu University says. ""Such iterative improvements can be very revealing. In the case of gravitational interactions we have made substantial steps towards understanding the dimensions of the space around us.""It is hoped that the study, along with future work to improve sensitivity even further, will help shed light on whether the space in which we live is limited to three dimensions.Story Source:Materials provided by Osaka University. ",0.103601,0.122554,0.135471,0.163578,0.126253,0.148096,0.145489,0,0
781,"Physicists lay groundwork to better understand the birth of the universe New research extends fundamental thermodynamic theories to describe microscopic systems at high energies, like the universe at the Big Bang","Sebastian Deffner at UMBC and Anthony Bartolotta at Caltech have developed the first techniques for describing the thermodynamics of very small systems with very high energy -- like the universe at the start of the Big Bang -- which could lead to a better understanding of the birth of the universe and other cosmological phenomena.To date, theoretical physicists have developed theories that explain how parts of the universe work: classical mechanics for objects at everyday sizes and speeds, quantum mechanics for very tiny objects at everyday speeds, special relativity for things that approach the speed of light. But no theory yet has managed to explain the behavior of very small objects that also have very high energy, and ""this final case is very important if you want to understand where the universe comes from,"" says Deffner.Current theories assume that systems are at least locally stable, which doesn't hold for some of the most interesting cases, from the Big Bang to black holes. ""If we want to understand all these cosmological models,"" Deffner says, ""and we want to understand the thermodynamics of the universe, we actually have no means to do that."" That may be about to change.Deffner and Bartolotta's new paper in Physical Review X builds on an explosion of research in the field of quantum stochastic thermodynamics in the last decade. This field describes the laws of thermodynamics -- one of the fundamental pillars of physics -- at a microscopic level for the first time. Importantly, it takes into account how the immediate surroundings affect small systems in a way that differs from how the environment affects larger systems. Deffner and Bartolotta's work extends this field even further to examine tiny systems at very high energies that are changing quickly.That extension ""is really uncharted territory...a completely new idea,"" says Deffner. ""And the reason no one has done it before is because stochastic thermodynamics is only 20 years old. Quantum stochastic thermodynamics is only 10 years old. As a field, we've just learned how to stand. We don't even know how to walk yet.""The work is pioneering, but it fits into a larger ecosystem of existing physics theories. For example, ""Quantum field theories are the most general way of attacking a physics problem,"" broader than special relativity or quantum mechanics, explains Bartolotta. He and Deffner chose to test, using new equations they developed, whether one specific quantum field theory held true in the rare case of a system of extremely small size and extremely high energy -- and it did.In addition, they found that in their model system with high energy and at small scale, the system was much more likely to return multiple particles upon sending in just one, than to come out with the initial particle and no more, which was a huge surprise. It is possible at high energies, like those generated at the Large Hadron Collider in Switzerland, to end up with a different number of particles than you started with, because the famous equation e=mc2 allows mass to be created from energy when the energy is extremely large. Even so, finding the multiple particle result so frequently was shocking.The math to calculate it all was ""incredibly hard,"" Deffner reflects, but the end result, which involved finding a way to compute an infinite number of possibilities, ""was actually quite beautiful,"" says Bartolotta. He shares, ""Our hope is that this paper will now open the doors for other people in other fields that previously couldn't use these techniques to now use them.""Deffner's overall research goal is to broaden the uses of the framework of thermodynamics, ""until we find neat things that no one has ever seen before,"" and his latest paper fits that bill.""I'm still totally blown away by the result,"" Deffner says, but he's also already contemplating possibilities for future research directions. When it comes to untangling the mysteries of the early universe, he notes, ""There's a lot that we have to do next.""Story Source:Materials provided by University of Maryland Baltimore County. ",0.0586259,0.081463,0.0896075,0.15758,0.083209,0.115804,0.135952,0,0
782,New way to 'see' the quantum world,"JILA scientists have invented a new imaging technique that produces rapid, precise measurements of quantum behavior in an atomic clock in the form of near-instant visual art.The technique combines spectroscopy, which extracts information from interactions between light and matter, with high-resolution microscopy.As described in Physical Review Letters, the JILA method makes spatial maps of energy shifts among the atoms in a three-dimensional strontium lattice atomic clock, providing information about each atom's location and energy level, or quantum state.The technique rapidly measures physical effects that are important to atomic clocks, thus improving the clock's precision, and it can add new atomic-level detail to studies of phenomena such as magnetism and superconductivity. In the future, the method may allow scientists to finally see new physics such as the connection between quantum physics and gravity.JILA is operated jointly by the National Institute of Standards and Technology (NIST) and the University of Colorado Boulder.""This technique allows us to write a piece of beautiful 'music' with laser light and atoms, and then map that into a structure and freeze it like a stone so we can look at individual atoms listening to the different tones of the laser, read out directly as an image,"" JILA/NIST Fellow Jun Ye said.The atoms are in a so-called quantum degenerate gas, in which large numbers of atoms interact with each other. This ""quantum many-body"" phenomenon is extending measurement precision to new extremes.To prepare atoms for a beauty shot, researchers use a laser pulse to drive about 10,000 strontium atoms from their low-energy ground state to a high-energy, excited state. Then, a blue laser positioned underneath the lattice is shined upward vertically through the atoms, and a camera takes a picture of the shadow the atoms cast, which is a function of how much light they absorb. Ground-state atoms absorb more light.The resulting images are false-color representations of atoms in the ground state (blue) and excited state (red). The white region represents atoms in a fine mixture of about 50 percent red and 50 percent blue, creating a dappled effect. This occurs because these atoms were initially prepared in a quantum state of superposition, or both ground and excited states simultaneously, and the imaging measurement prompts a collapse into one of the two states, which creates ""noise"" in the image.As a demonstration, the JILA team created a series of images to map small frequency shifts, or fractions of atoms in the excited state, across different regions of the lattice. The ability to make simultaneous comparisons improves precision and speed in measurements of a group of atoms. The researchers reported achieving a record precision in measuring frequency of 2.5 x 10-19 (error of just 0.25 parts per billion billion) in 6 hours. Imaging spectroscopy is expected to greatly improve the precision of the JILA atomic clock, and other atomic clocks generally.Imaging spectroscopy provides information about the local environment of the atoms, similar to the incredible resolution offered by scanning tunneling microscopy. So far, the method has been used to produce two-dimensional images, but it could make 3-D images based on layer-by-layer measurements as is done in tomography, which combines multiple cross-sections of solid objects, Ye said.A sort of artificial crystal, the lattice of atoms could also be used as a magnetic or gravitational sensor to test the interplay between different fields of physics. Ye is most excited about the future possibility of using the atoms in the clock as a gravity sensor, to see how quantum mechanics, which operates on very small spatial scales, interacts with general relativity, the theory of gravity, a macroscopic force.""As the clock gets better in the next 20 years, this little crystal could not only map out how gravity affects frequency, but we could also start to see the interplay of gravity and quantum mechanics,"" Ye said. ""This is a physical effect that no experimental probe has ever measured. This imaging technique could become a very important tool.""The research is supported by NIST, the Defense Advanced Research Projects Agency, the Air Force Office of Scientific Research and the National Science Foundation.Story Source:Materials provided by National Institute of Standards and Technology (NIST). ",0.0741343,0.115404,0.120078,0.151898,0.11368,0.131706,0.134945,0,0
783,Laser-ranged satellite measurement now accurately reflects Earth's tidal perturbations The most precise ever laser satellite measurement method provides new clues to relativity,"Tides on Earth have a far-reaching influence, including disturbing satellites' measurements by affecting their motion. This disturbance can be studied using a model for the gravitational potential of Earth, taking into account the fact that Earth's shape is not spherical. The LAser RElativity Satellite (LARES), is the best ever relevant test particle to move in Earth's gravitational field.In a new study published in EPJ Plus, LARES proves its efficiency for high-precision probing of General Relativity and fundamental physics. By studying Earth's tidal perturbations acting on the LARES, Vahe Gurzadyan from the Center for Cosmology and Astrophysics at Yerevan State University, Armenia, and colleagues demonstrate the value of laser-range satellites for high-precision measurements.Specifically, laser-ranged satellites bring increased accuracy in the study and testing of what is referred to in physics as frame dragging. In this study, the authors collect the observations of Earth's tidal perturbations acting on LARES and compare them with two similar laser-ranged satellites: LAGEOS and LAGEOS 2. The team analysed 3.5 years of LARES laser-ranging data, together with that of the two LAGEOS satellites.To extract frame-dragging from the laser-ranging data for high accuracy, the authors model the main gravitational and non-gravitational orbital perturbations. To do so, the team documented 110 significant Earth tide modes for the LARES satellite using the perturbative methods of celestial mechanics and recent data on the satellite's orbit.Frame-dragging is one of the intriguing phenomena of Einstein's theory of General Relativity. It is an effect on space, and is elastic -- in other words, it will revert back to its original shape and energy state after force is exerted on it-whereby particles exchange energy with it. This has implications for astrophysics.Story Source:Materials provided by Springer. ",0.183902,0.143974,0.19092,0.183193,0.154624,0.235319,0.202385,0,0
784,Leaving Flatland: Quantum Hall effect physics in 4-D,"In literature, the potential existence of extra dimensions was discussed in Edwin Abbott's satirical novel ""Flatland: A Romance of Many Dimensions"" (1884), portraying the Victorian society in 19th century England as a hierarchical two-dimensional world, incapable of realizing its narrow-mindedness due to its lower-dimensional nature. In physics, on the other hand, the possibility that our universe comprises more than three spatial dimensions was first proposed in the wake of Albert Einstein's theory of general relativity in the 1920s. Modern string theory -- trying to reconcile Einstein's ideas with the laws of quantum mechanics -- even postulates up to 10 dimensions.In a completely different context, an international team of researchers led by Professor Immanuel Bloch (LMU/MPQ) and Professor Oded Zilberberg (ETH Zerich) has now demonstrated a way to observe physical phenomena proposed to exist in higher-dimensional systems in analogous real-world experiments. Using ultracold atoms trapped in a periodically modulated two-dimensional superlattice potential, the scientists could observe a dynamical version of a novel type of quantum Hall effect that is predicted to occur in four-dimensional systems. (Nature, 4 January 2018)The Hall effect occurs when charged particles move in a two-dimensional plane in the presence of a magnetic field. The magnetic field generates a Lorentz force, which deflects the particles in the direction orthogonal to their motion. This manifests in the appearance of a transverse Hall voltage. In 1980, Klaus von Klitzing made the remarkable discovery that at low temperatures and very strong magnetic fields this voltage can only take certain quantized values. Moreover, these values are identical irrespective of the specific properties of the experimental sample. This astonishing fact was later shown to be related to the topology of the quantum mechanical wave functions describing the behaviour of electrons at such low energies -- a seminal work for which David Thouless was awarded the Nobel prize in physics in 2016.An important prerequisite for the quantum Hall effect turned out to be the two-dimensional geometry of the sample. It can be proven that in general such a phenomenon cannot take place in three-dimensional systems -- as exemplified by the fact that the direction transverse to the velocity of the particles is not defined uniquely in three dimensions. Thus, it was believed that this effect is special to two dimensions. Yet, 20 years after the initial discovery theoretical physicists postulated that a similar effect could also take place in four-dimensional systems, for which even more remarkable properties including a novel non-linear Hall current were predicted. For a long time, however, this proposal was mostly regarded as a mathematical curiosity -- out of reach for actual experiments -- despite its far-reaching implications. For example, both topological insulators and Weyl semimetals, two of the most prominent discoveries in condensed matter physics in recent years, can be derived from 4D quantum Hall models.In 2013, Oded Zilberberg and collaborators realized that key signatures of the 4D quantum Hall effect should also be visible in special time-dependent systems in two dimensions, so-called topological charge pumps, which constitute a dynamical version of the higher-dimensional model. This insight generalized an idea, which also goes back to David Thouless. In 1983, Thouless showed that a quantized transport of particles can be generated by periodically modulating a 1D system and that this response is mathematically equivalent to the 2D quantum Hall effect. Consequently, by combining two such systems in orthogonal directions, it should be possible to observe the non-linear Hall current predicted in 4D.This has now been achieved by the group of Immanuel Bloch. At first a cloud of atoms is cooled down close to absolute zero and placed in a 2D optical lattice. Such an optical lattice is created by interference of retro-reflected laser beams of a certain wavelength along two orthogonal directions. The resulting potential resembles an egg-carton-like ""crystal of light,"" in which the atoms can move. By adding another laser beam with a different wavelength in each direction, a so-called superlattice is created. The researchers could implement the proposed 2D topological charge pump by introducing a constant tiny angle between the beams of different wavelength along one axis while at the same time dynamically changing the shape of the potential in the orthogonal direction by slightly shifting the wavelength of the additional laser beam.When modulating the potential in time, the atoms predominantly move in the direction of the modulation and do so in a quantized way -- the linear (i.e. 1D) response corresponding to the 2D quantum Hall effect as predicted by Thouless. But in addition to this, the Munich team also observed a slight drift in the transverse direction, even though the lattice potential in this direction remains static throughout the experiment. This transverse motion is the equivalent of the non-linear Hall response -- the essential feature of the 4D Hall effect. By carefully monitoring and analyzing at which positions in the superlattice the atoms are located during this process, the scientists could furthermore demonstrate that this motion is quantized, thereby revealing the quantum nature of the Hall effect in 4D.The results have now been published in the journal Nature together with complementary work by an American research team, which used photonic structures to study the intricate boundary phenomena that accompany this motion as a result of the 4D quantum Hall effect. Together, these papers provide the first experimental glimpse into the physics of higher-dimensional quantum Hall systems, which offer a number of fascinating future prospects. These include fundamental questions for our understanding of the universe like the interplay of quantum correlations and dimensionality, the generation of cosmic magnetic fields and quantum gravity, for which 4D quantum Hall systems have been proposed as toy models.Story Source:Materials provided by Max Planck Institute of Quantum Optics. ",0.0705016,0.0851949,0.092131,0.128376,0.0957893,0.139246,0.118678,0,0
785,Low-altitude skiing can slow down aging -- relativistically,"Most biologists and medical professionals would agree that skiing and running can delay aging. This is essentially because of the direct positive impact on the body due to good exercise and physical fitness. However there is a much more subtle effect in this direction from a mathematician's and physicist's point of view.As most of you already know, time and relativity have a unique connection between them. There was a time in history when hardly anyone from the general public understood Einstein's special theory of relativity formulated in 1905. Initially only some mathematicians and physicists could decipher his equations and understand the concept of time dilation in a moving body. Time dilation means that a clock moving relative to an observer will be measured to tick slower than a clock that is at rest in the observer's own frame of reference.Within few years, more scientists and more people from the general audience started to understand and appreciate the significance of his result. Time dilation, twin paradox experiment, clock cartoons and so on became talk of the day!In 1915, Einstein came up with the general theory of relativity which puzzled academics and laymen even more. Only a handful of mathematicians and physicists at the time could digest his concept of explaining gravity by unifying space and time into a mathematical fabric using tensor calculus. In later years, people started to notice and measure the effect of gravitational fields on the passage of time. In today's world, these are well established and time tested theories which even gain occasional mentions in popular sci-fi comics and sci-fi movies.The dangers of climbing the stairsLast year an article in Norwegian Titan.uio.no focused on this general relativity part. This is to do with the acceleration of passage of time (i.e. time running faster) due to change in gravitational field when you go up a mountain or climb stairs! Technically this is due to the change in curvature of space-time when you move farther away from the centre of Earth. It is commendable that Norway is one of very few countries where they try to include aspects of general theory of relativity in the curricula for training school and college students.The simple summary of the above result is: if someone spends decades of their life consistently on some very high mountain peaks (like Himalayas), one would age faster by some hundreds of microseconds; exact count depending on the specific height and respective times spent (about 100 microseconds per decade at 3000 metres above). An accuracy of order microseconds over a decade-long span is something that is achievable using modern-day Cesium atomic clocks. Hence this is a realistically measurable range in today's times if you plant one clock each on the summit and the base of the mountain.The perks of being a skierHowever, there is an interesting counter effect from the special relativistic side, if you are a Norwegian. A blind assumption that all Norwegians ski a lot plus run a lot is not too far from reality!For a top-end skiing professional, assuming typical speeds of about 100 km/hr and duration of one race of about 2 minutes, one gains about half a picosecond. Assuming an active skiing career spanning a decade (about 1000 races), the person would age lesser by about half a nanosecond.For a recreational skier/runner, decades of running and ski-training would result in gaining a few nanoseconds over an average Norwegian's lifespan (assuming an average recreational skiing/running speed of 12 km/hr, gaining a nanosecond would require about 5000 hours of training, which is easily achievable in an average lifetime of a nordic person).A nanosecond-precision time measurement over decades might sound like an impossible feat at the level of conventional engineering we are familiar with. However, the level of sophistication that exists today at the cutting edge of engineering and instrumentation is quite remarkable.The latest example is the LIGO experiment which won the Nobel prize for physics this year. The experimental set up is a technological marvel capable of detecting ripples in space-time that cause changes of the order of one-ten thousandth the size of an atomic nucleus. Moreover the observation which fetched the Nobel was a collision event that happened 1.3 billion years ago between two black holes with masses each of about 30 times that of our sun. This measurement in itself shows the standards of accuracy and precision levels in modern scientific instrumentation. This applies to the latest atomic clocks too.Measuring time has become easierMore pertinent to this discussion is the advent of optical lattice clocks, that have an accuracy of about one second in 13.8 billion years (i.e. age of universe), or equivalently, about a nanosecond per decade.Clearly, measuring a nanosecond-scale time shift resulting from many years or decades of high-speed skiing or long-distance running is not hopelessly beyond the reach of present-day technology.Astronaut twins are no longer the same ageMoreover this core concept was tested using twin astronaut brothers last year. One of the twins, Scott Kelly, a NASA astronaut, spent 340 days aboard the International Space Station while his twin brother Mark Kelly was enjoying his life on ground (i.e. planet Earth).The International Space Station is about 400 km up from Earth's surface and hence the astronaut will age faster by about 1.1 milliseconds during this period due to space-time curvature effect (as discussed in the case of mountains).However the astronaut was racing at about 28,000 km/hr around Earth during this period (a faster version of skiing). This leads to a counter effect which estimates that the astronaut on the shuttle aged slower by about 9.6 milliseconds due to this high velocity.The net effect is that the astronaut aged slower (or gained a bit of time!) by about 8.5 milliseconds during this period which is a good reason to celebrate!It turns out that, in low-earth orbits, the special relativistic slowdown dominates over the gravitational speedup -- the two effects become roughly equal when the spacecraft's orbital radius is 1.5 times Earth's radius. Any astronaut orbiting at a larger radius would in fact age faster than their colleagues on Earth. A similar calculation has been done for frequent air travellers.Realistically, of course, there are much more serious effects to consider, like increased radiation exposure, zero gravity challenges, bio-rhythm changes and dietary problems on board space shuttles and hence it is not just clocks which matter for health & overall well being of a human!!!But with regular skiing and running (at low altitudes only!), you certainly are slowing down aging (both fitness-wise and relativistically speaking!) without all these ill effects from space! As they say, every little counts!About the authorsAswin Sekhar is an astrophysicist based at CEED, UiO and Amol Patwardhan is a high energy physicist based at the University of Wisconsin, Madison, USA. Sekhar acknowledges the Crater Clock project (235058/F20) based at CEED (through the Centres of Excellence scheme project number 223272 (CEED) funded by the Research Council of Norway) and USIT UNINETT Sigma2 computational resource allocation through Notur.Story Source:Materials provided by University of Oslo, Faculty of Mathematics and Natural Sciences. Original written by Aswin Sekhar and Amol Patwardhan. ",0.0533471,0.0527992,0.0732435,0.117385,0.06501,0.106898,0.0903612,0,0
786,Doing without dark energy Mathematicians propose alternative explanation for cosmic acceleration,"Three mathematicians have a different explanation for the accelerating expansion of the universe that does without theories of ""dark energy."" Einstein's original equations for General Relativity actually predict cosmic acceleration due to an ""instability,"" they argue in paper published recently in Proceedings of the Royal Society A.About 20 years ago, astronomers made a startling discovery: Not only is the universe expanding -- as had been known for decades -- but the expansion is speeding up. To explain this, cosmologists have invoked a mysterious force called ""dark energy"" that serves to push space apart.Shortly after Albert Einstein wrote his equations for General Relativity, which describe gravity, he included an ""antigravity"" factor called the ""cosmological constant"" to balance gravitational attraction and produce a static universe. But Einstein later called the cosmological constant his greatest mistake.When modern cosmologists began to tackle cosmic acceleration and dark energy, they dusted off Einstein's cosmological constant as interchangeable with dark energy, given the new knowledge about cosmic acceleration.That explanation didn't satisfy mathematicians Blake Temple and Zeke Vogler at the University of California, Davis, and Joel Smoller at the University of Michigan, Ann Arbor.""We set out to find the best explanation we could come up with for the anomalous acceleration of the galaxies within Einstein's original theory without dark energy,"" Temple said.The original theory of General Relativity has given correct predictions in every other context, Temple said, and there is no direct evidence of dark energy. So why add a ""fudge factor"" (dark energy or the cosmological constant) to equations that already appear correct? Instead of faulty equations that need to be tweaked to get the right solution, the mathematicians argue that the equations are correct, but the assumption of a uniformly expanding universe of galaxies is wrong, with or without dark energy, because that configuration is unstable.An unstable solutionCosmological models start from a ""Friedmann universe,"" which assumes that all matter is expanding but evenly distributed in space at every time, Temple said.Temple, Smoller and Vogler worked out solutions to General Relativity without invoking dark energy. They argue that the equations show that the Friedmann space-time is actually unstable: Any perturbation -- for example if the density of matter is a bit lower than average -- pushes it over into an accelerating universe.Temple compares this to an upside-down pendulum. When a pendulum is hanging down, it is stable at its lowest point. Turn a rigid pendulum the other way, and it can balance if it is exactly centered -- but any small gust will blow it off.This tells us that we should not expect to measure a Friedmann universe, because it is unstable, Temple said. What we should expect to measure instead are local space-times that accelerate faster. Remarkably, the local space-times created by the instability exhibit precisely the same range of cosmic accelerations as you get in theories of dark energy, he said.What this shows is that the acceleration of the galaxies could have been predicted from the original theory of General Relativity without invoking the cosmological constant/dark energy at all, Temple said.""The math isn't controversial, the instability isn't controversial,"" Temple said. ""What we don't know is, does our Milky Way galaxy lie near the center of a large under-density of matter in the universe.""The paper does include testable predictions that distinguish their model from dark energy models, Temple said.Joel Smoller died in September 2017, while the paper was under review. The work was partially supported by the National Science Foundation.Story Source:Materials provided by University of California - Davis. Original written by Andy Fell. ",0.137349,0.13559,0.206839,0.219732,0.1562,0.224907,0.211256,0,0
787,Blackbody radiation from a warm object attracts polarizable objects,"Our physical attraction to hot bodies is real, according to UC Berkeley physicists.To be clear, they're not talking about sexual attraction to a ""hot"" human body.But the researchers have shown that a glowing object actually attracts atoms, contrary to what most people -- physicists included -- would guess.The tiny effect is much like the effect a laser has on an atom in a device called optical tweezers, which are used to trap and study atoms, a discovery that led to the 1997 Nobel Prize in Physics shared by former UC Berkeley professor Steven Chu, now at Stanford, Claude Cohen-Tannoudji and William D. Phillips.Until three years ago, when a group of Austrian physicists predicted it, no one thought that regular light, or even just the heat given off by a warm object -- the infrared glow you see when looking through night-vision goggles -- could affect atoms in the same way.UC Berkeley physicists, who are expert at measuring minute forces using atom interferometry, designed an experiment to check it out. When they measured the force exerted by the so-called blackbody radiation from a warm tungsten cylinder on a cesium atom, the prediction was confirmed.The attraction is actually 20 times the gravitational attraction between the two objects, but since gravity is the weakest of all the forces, the effect on cesium atoms -- or any atom, molecule or larger object -- is usually too small to worry about.""It's hard to find a scenario where this force would stand out,"" said co-author Victoria Xu, a graduate student in the physics department at UC Berkeley. ""It is not clear it makes a significant effect anywhere. Yet.""As gravity measurements become more precise, though, effects this small need to be taken into account. The next generation of experiments to detect gravitational waves from space may use lab-bench atom interferometers instead of the kilometer-long interferometers now in operation. Interferometers typically combine two light waves to detect tiny changes in the distance they've traveled; atom interferometers combine two matter waves to detect tiny changes in the gravitational field they've experienced.For very precise inertial navigation using atom interferometers, this force would also have to be taken into account.""This blackbody attraction has an impact wherever forces are measured precisely, including precision measurements of fundamental constants, tests of general relativity, measurements of gravity and so on,"" said senior author Holger Meller, an associate professor of physics. Xu, Meller and their UC Berkeley colleagues published their study in the December issue of the journal Nature Physics.Optical tweezersOptical tweezers work because light is a superposition of magnetic and electric fields -- an electromagnetic wave. The electric field in a light beam makes charged particles move. In an atom or a small sphere, this can separate positive charges, like the nucleus, from negative charges, like the electrons. This creates a dipole, allowing the atom or sphere to act like a tiny bar magnet.The electric field in the light wave can then move this induced electric dipole around, just as you can use a bar magnet to shove a piece of iron around.Using more than one laser beam, scientists can levitate an atom or bead to conduct experiments.With weak, incoherent light, like blackbody radiation from a hot object, the effect is much weaker, but still there, Meller's team found.They measured the effect by placing a dilute gas of cold cesium atoms -- cooled to three-millionths of a degree above absolute zero (300 nanoKelvin) -- in a vacuum chamber and launching them upward with a quick pulse of laser light.Half are given an extra kick up towards an inch-long tungsten cylinder glowing at 185 degrees Celsius (365 degrees Fahrenheit), while the other half remain unkicked. When the two groups of cesium atoms fall and meet again, their matter waves interfere, allowing the researchers to measure the phase shift caused by the tungsten-cesium interaction, and thus calculate the attractive force of the blackbody radiation.""People think blackbody radiation is a classic concept in physics -- it was a catalyst for starting the quantum mechanical revolution 100 years ago -- but there are still cool things to learn about it,"" Xu said.Story Source:Materials provided by University of California - Berkeley. Original written by Robert Sanders. ",0.0801802,0.121063,0.106801,0.13176,0.128973,0.130929,0.132918,0,0
788,Possible vestiges of a universe previous to the Big Bang,"Although for five decades, the Big Bang theory has been the best known and most accepted explanation for the beginning and evolution of the Universe, it is hardly a consensus among scientists.Brazilian physicist Juliano Cesar Silva Neves part of a group of researchers who dare to imagine a different origin. In a study recently published in the journal General Relativity and Gravitation, Neves suggests the elimination of a key aspect of the standard cosmological model: the need for a spacetime singularity known as the Big Bang.In raising this possibility, Neves challenges the idea that time had a beginning and reintroduces the possibility that the current expansion was preceded by contraction. ""I believe the Big Bang never happened,"" the physician said, who Works as a researcher at the University of Campinas's Mathematics, Statistics & Scientific Computation Institute (IMECC-UNICAMP) in Sao Paulo State, Brazil.For Neves, the fast spacetime expansion stage does not exclude the possibility of a prior contraction phase. Moreover, the switch from contraction to expansion may not have destroyed all traces of the preceding phase.The article, which reflects the work developed under the Thematic Project ""Physics and geometry of spacetime,"" considers the solutions to the general relativity equations that describe the geometry of the cosmos and then proposes the introduction of a ""scale factor"" that makes the rate at which the Universe is expanding depend not only on time but also on cosmological scale.""In order to measure the rate at which the Universe is expanding with the standard cosmology, the model in which there's a Big Bang, a mathematical function is used that depends only on cosmological time,"" said Neves, who elaborated the idea with Alberto Vazques Saa, a Full Professor at IMECC-UNICAMP and also the supervisor for Neves' postdoctoral project, funded by the Sao Paulo Research Foundation -- FAPESP.With the scale factor, Big Bang itself, or cosmologic singularity, ceases to be a necessary condition for the cosmos to begin universal expansion. A concept from mathematics that expresses indefiniteness, singularity was used by cosmologists to characterize the ""primordial cosmologic singularity"" that happened 13.8 billion years ago, when all the matter and energy from the Universe were compressed into an initial state of infinite density and temperature, where the traditional laws of physics no longer apply.The Big Bang Theory has its origins in the late 1920s when US astronomer Edwin Hubble discovered that almost all galaxies are moving away from each other at ever-faster velocities.From the 1940s onward, scientists guided by Einstein's theory of general relativity constructed a detailed model of the evolution of the Universe since the Big Bang. Such model could lead to three possible outcomes: the infinite expansion of the Universe at ever-higher velocities; the stagnation of the Universe expansion in a permanent basis; or an inverted process of retraction caused by the gravitational attraction exerted by the mass of the Universe, what is known as Big Crunch.""Eliminating the singularity or Big Bang brings back the bouncing Universe on to the theoretical stage of cosmology. The absence of a singularity at the start of spacetime opens up the possibility that vestiges of a previous contraction phase may have withstood the phase change and may still be with us in the ongoing expansion of the Universe,"" Neves said.Neves conceptualizes that ""bouncing cosmology"" is rooted in the hypothesis that Big Crunch would give way to an eternal succession of universes, creating extreme conditions of density and temperature in order to instigate a new inversion in the process, giving way to expansion in another bounce.Vestiges of contractionBlack holes are the starting point of Neves' investigations about ""Bouncing Universe."" ""Who knows, there may be remains of black holes in the ongoing expansion that date from the prior contraction phase and passed intact through the bottleneck of the bounce,"" he said.Consisted of the imploded core remaining after a giant star explodes, black holes are a kind of cosmic object whose core contracted to form a singularity, a point with infinite density and the strongest gravitational attraction known to exist. Nothing escapes from it, not even light.According to Neves, a black hole is not defined by singularity, but rather by an event horizon, a membrane that indicates the point of no return from which nothing escapes the inexorable destiny of being swallowed up and destroyed by the singularity.""Outside the event horizon of a regular black hole, there are no major changes, but inside it, the changes are deep-seated. There's a different spacetime that avoids the formation of a singularity.""The scale factor formulated by Neves and Saa was inspired by US physicist James Bardeen. In 1968, Berdeen used a mathematical trick to modify the solution to the general relativity equations that describe black holes.The trick consisted of thinking of the mass of a black hole not as a constant, as had previously been the case, but as a function that depends on the distance to the center of the black hole. With this change, a different black hole, termed a regular black hole, emerged from the solution to the equations. ""Regular black holes are permitted, since they don't violate general relativity. The concept isn't new and has frequently been revisited in recent decades,"" said Neves.Since the insertion of a mathematical trick into the general relativity equations could prevent the formation of singularities in regular black holes, Neves considered creating a similar artifice to eliminate the singularity in a regular bounce.In modern science, a theory is worthless if cannot be verified, however beautiful and inspiring it may be. How do you test the hypothesis of a Big Bang that did not start with a singularity?""By looking for traces of the events in a contraction phase that may have remained in the ongoing expansion phase. What traces? The candidates include remnants of black holes from a previous phase of universal contraction that may have survived the bounce,"" Neves said.Story Source:Materials provided by Fundaeeo de Amparo e Pesquisa do Estado de Seo Paulo. Original written by Peter Moon. ",0.0835228,0.10762,0.130358,0.204855,0.121835,0.161981,0.17859,0,0
789,Dark matter and dark energy: Do they really exist?,"For close on a century, researchers have hypothesised that the universe contains more matter than can be directly observed, known as ""dark matter."" They have also posited the existence of a ""dark energy"" that is more powerful than gravitational attraction. These two hypotheses, it has been argued, account for the movement of stars in galaxies and for the accelerating expansion of the universe respectively. But -- according to a researcher at the University of Geneva (UNIGE), Switzerland -- these concepts may be no longer valid: the phenomena they are supposed to describe can be demonstrated without them. This research, which is published in The Astrophysical Journal, exploits a new theoretical model based on the scale invariance of the empty space, potentially solving two of astronomy's greatest mysteries.In 1933, the Swiss astronomer Fritz Zwicky made a discovery that left the world speechless: there was, claimed Zwicky, substantially more matter in the universe than we can actually see. Astronomers called this unknown matter ""dark matter,"" a concept that was to take on yet more importance in the 1970s, when the US astronomer Vera Rubin called on this enigmatic matter to explain the movements and speed of the stars. Scientists have subsequently devoted considerable resources to identifying dark matter -- in space, on the ground and even at CERN -- but without success. In 1998 there was a second thunderclap: a team of Australian and US astrophysicists discovered the acceleration of the expansion of the universe, earning them the Nobel Prize for physics in 2011. However, in spite of the enormous resources that have been implemented, no theory or observation has been able to define this black energy that is allegedly stronger than Newton's gravitational attraction. In short, black matter and dark energy are two mysteries that have had astronomers stumped for over 80 years and 20 years respectively.A new model based on the scale invariance of the empty spaceThe way we represent the universe and its history are described by Einstein's equations of general relativity, Newton's universal gravitation and quantum mechanics. The model-consensus at present is that of a big bang followed by an expansion. ""In this model, there is a starting hypothesis that hasn't been taken into account, in my opinion,"" says Andre Maeder, honorary professor in the Department of Astronomy in UNIGE's Faculty of Science. ""By that I mean the scale invariance of the empty space; in other words, the empty space and its properties do not change following a dilatation or contraction."" The empty space plays a primordial role in Einstein's equations as it operates in a quantity known as a ""cosmological constant,"" and the resulting universe model depends on it. Based on this hypothesis, Maeder is now re-examining the model of the universe, pointing out that the scale invariance of the empty space is also present in the fundamental theory of electromagnetism.Do we finally have an explanation for the expansion of the universe and the speed of the galaxies?When Maeder carried out cosmological tests on his new model, he found that it matched the observations. He also found that the model predicts the accelerated expansion of the universe without having to factor in any particle or dark energy. In short, it appears that dark energy may not actually exist since the acceleration of the expansion is contained in the equations of the physics.In a second stage, Maeder focused on Newton's law, a specific instance of the equations of general relativity. The law is also slightly modified when the model incorporates Maeder's new hypothesis. Indeed, it contains a very small outward acceleration term, which is particularly significant at low densities. This amended law, when applied to clusters of galaxies, leads to masses of clusters in line with that of visible matter (contrary to what Zwicky argued in 1933): this means that no dark matter is needed to explain the high speeds of the galaxies in the clusters. A second test demonstrated that this law also predicts the high speeds reached by the stars in the outer regions of the galaxies (as Rubin had observed), without having to turn to dark matter to describe them. Finally, a third test looked at the dispersion of the speeds of the stars oscillating around the plane of the Milky Way. This dispersion, which increases with the age of the relevant stars, can be explained very well using the invariant empty space hypothesis, while there was before no agreement on the origin of this effect.Maeder's discovery paves the way for a new conception of astronomy, one that will raise questions and generate controversy. ""The announcement of this model, which at last solves two of astronomy's greatest mysteries, remains true to the spirit of science: nothing can ever be taken for granted, not in terms of experience, observation or the reasoning of human beings,"" concluded Andre Maeder.Story Source:Materials provided by Universite de Geneve. ",0.0997412,0.0975759,0.126178,0.177071,0.106576,0.163113,0.160163,0,0
790,New quantum materials offer novel route to 3-D electronic devices,"Researchers have shown how the principles of general relativity open the door to novel electronic applications such as a three-dimensional electron lens and electronic invisibility devices. In a new study funded by the Academy of Finland, Aalto University researchers Alex Weststrem and Teemu Ojanen propose a method to go beyond special relativity and simulate Einstein's theory of general relativity in inhomogeneous Weyl semimetals. The theory of Weyl metamaterials combines ideas from solid-state physics, particle physics and cosmology and points a way to fabricate metallic designer materials where charge carriers move like particles in curved space-time.The researchers propose Weyl metamaterials, a generalisation of Weyl semimetals, that enable new types of electronic devices through geometry engineering.""The systems we introduced offer a route to make the charge carriers move as if they were living in a curved geometry, providing a tabletop laboratory for simulating curved-space quantum physics and certain cosmological phenomena,"" Alex Weststrem explains.Weyl semimetals are an example of recently discovered quantum materials that have received a lot of attention. Charge carriers in these materials behave as if they were massless particles moving at the speed of light.""We discovered that Weyl metamaterials may serve as a platform for exotic electronic devices such as the 3D electron lens, where the trajectories of charge carriers are focused much like beams of light in an optical lens,"" Teemu Ojanen says.The electric conduction in Weyl semimetals reflects the physics of Einstein's special theory of relativity. Nevertheless, special relativity also assumes an absence of gravity, which Einstein formulated as a geometry of space-time.The theory of Weyl metamaterials also paves the way for fundamentally new electronics applications, for instance, the development of electronic invisibility devices. The key idea behind the potential applications is an artificially created curved geometry, which bends the motion of charge carriers in a controlled way.""In optics, it's been known for centuries that light always chooses the quickest trajectory. In curved geometry, the quickest path doesn't look like a straight line for those watching from outside. The functionality of optical invisibility devices, where the beams of light bypass a hidden object, is in fact based on the application of curved-space geometry. It would be a breakthrough in fundamental research to achieve a similar functionality in electronic systems,"" Ojanen adds.Story Source:Materials provided by Academy of Finland. ",0.125248,0.129082,0.151141,0.189645,0.175299,0.184193,0.180331,0,0
791,"Solution to mysterious behavior of supercooled water Researchers characterize the unexplained diffusion and viscosity behavior of supercooled water with new, highly accurate computer simulations","When Einstein was working toward his PhD, he was among the first to explain how particles exhibit random motions in fluids. Diffusion is an important physical process and the Stokes-Einstein relationship describes how particles diffuse through a fluid based on the hydrodynamic theory. But mysteriously at low temperatures below the melting temperature, something changes in supercooled liquids and the resulting highly viscous glassy behavior can no longer be explained by the simple Stokes-Einstein relationship.Now, two researchers from Osaka University and Nagoya University have simulated supercooled water in unprecedented detail to explain the anomalous behavior at low temperatures. They recently published their findings in Science Advances.""Most liquids obey the Stokes-Einstein equation over a wide range of temperatures, but some unexpected changes in behavior are found in supercooled water and other glassy materials,"" coauthor Kang Kim, of Osaka University, says. ""Breakdown of Stokes-Einstein behavior suggests some kind of anomalous molecular motions even in a liquid state, but it's not clear what those behaviors are.""The simple Stokes-Einstein relationship is based on arguments about how molecules move randomly at the microscopic level. But in supercooled water, molecules start to slow down irregularly. The researchers showed through simulations that some regions of the water become affected more than others, forming hydrogen bonds heterogeneously with partial solidification.Water molecules move through the viscous supercooled water in jumps related to hydrogen bond breaking. The erratic timing of this kind of movement is not accounted for by the Stokes-Einstein equation.Their simulations allowed them to examine how the supercooled water hydrogen bonding network changed over time. Their modelling clearly showed that an intermittent hydrogen bond breaking timings contributed to the breakdown of Stokes-Einstein behavior.""There are interesting physical implications of these findings as the Stokes-Einstein violation is regarded as a hydrodynamic anomaly of many glassy materials systems,"" Kim says. ""Our simulations help to answer questions about what happens in pure supercooled water and could also help to explain other dynamic behaviors in other technologically important glassy materials.""Story Source:Materials provided by Osaka University. ",0.129742,0.200804,0.194127,0.210072,0.169498,0.183834,0.230328,0,0
792,Breaking the rules: Heavy chemical elements alter theory of quantum mechanics,"A series of complicated experiments involving one of the least understood elements of the Periodic Table has turned some long-held tenets of the scientific world upside down.Florida State University researchers found that the theory of quantum mechanics does not adequately explain how the heaviest and rarest elements found at the end of the table function. Instead, another well-known scientific theory -- Albert Einstein's famous Theory of Relativity -- helps govern the behavior of the last 21 elements of the Periodic Table.This new research is published in the Journal of the American Chemical Society.Quantum mechanics are essentially the rules that govern how atoms behave and fully explain the chemical behavior of most of the elements on the table. But, Thomas Albrecht-Schmitt, the Gregory R. Choppin Professor of Chemistry at FSU, found that these rules are somewhat overridden by Einstein's Theory of Relativity when it comes to the heavier, lesser known elements of the Periodic Table.""It's almost like being in an alternate universe because you're seeing chemistry you simply don't see in everyday elements,"" Albrecht-Schmitt said.The study, which took more than three years to complete, involved the element berkelium, or Bk on the Periodic Table. Through experiments involving almost two dozen researchers across the FSU campus and the FSU-headquartered National High Magnetic Field Laboratory, Albrecht-Schmitt made compounds out of berkelium that started exhibiting unusual chemistry.They weren't following the normal rules of quantum mechanics.Specifically, electrons were not arranging themselves around the berkelium atoms the way that they organize around lighter elements like oxygen, zinc or silver. Typically, scientists would expect to see electrons line up so that they all face the same direction. This controls how iron acts as a magnet, for instance.However, these simple rules do not apply when it comes to elements from berkelium and beyond because some of the electrons line up opposite of the way scientists have long predicted.Albrecht-Schmitt and his team realized that Einstein's Theory of Relativity actually explained what they saw in the berkelium compounds. Under the Theory of Relativity, the faster anything with mass moves, the heavier it gets.Because the nucleus of these heavy atoms is highly charged, the electrons start to move at significant fractions of the speed of light. This causes them to become heavier than normal, and the rules that typically apply to electron behavior start to break down.Albrecht-Schmitt said it was ""exhilarating"" when he and his team began to observe the chemistry.""When you see this interesting phenomenon, you start asking yourself all these questions like how can you make it stronger or shut it down,"" Albrecht-Schmitt said. ""A few years ago, no one even thought you could make a berkelium compound.""Berkelium has been mostly used to help scientists synthesize new elements such as element 117 Tennessine, which was added to the table last year. But little has been done to understand what the element -- or several of its neighbors on the tables -- alone can do and how it functions.The Department of Energy gave Albrecht-Schmitt 13 milligrams of berkelium, roughly 1,000 times more than anyone else has used for major research studies. To do these experiments, he and his team had to move exceptionally fast. The element reduces to half the amount in 320 days, at which point it is not stable enough experiments.Story Source:Materials provided by Florida State University. Original written by Kathleen Haughney. ",0.0668019,0.108531,0.0949454,0.142738,0.0882121,0.125238,0.131593,0,0
793,Nobel Prize in Physics 2017: Gravitational waves,"The Royal Swedish Academy of Sciences has decided to award the Nobel Prize in Physics 2017 with one half to Rainer Weiss, LIGO/VIRGO Collaboration, and the other half jointly to Barry C. Barish, LIGO/VIRGO Collaboration and Kip S. Thorne, LIGO/VIRGO Collaboration ""for decisive contributions to the LIGO detector and the observation of gravitational waves.""Gravitational waves finally capturedOn 14 September 2015, the universe's gravitational waves were observed for the very first time. The waves, which were predicted by Albert Einstein a hundred years ago, came from a collision between two black holes. It took 1.3 billion years for the waves to arrive at the LIGO detector in the USA.The signal was extremely weak when it reached Earth, but is already promising a revolution in astrophysics. Gravitational waves are an entirely new way of observing the most violent events in space and testing the limits of our knowledge.LIGO, the Laser Interferometer Gravitational-Wave Observatory, is a collaborative project with over one thousand researchers from more than twenty countries. Together, they have realised a vision that is almost fifty years old. The 2017 Nobel Laureates have, with their enthusiasm and determination, each been invaluable to the success of LIGO. Pioneers Rainer Weiss and Kip S. Thorne, together with Barry C. Barish, the scientist and leader who brought the project to completion, ensured that four decades of effort led to gravitational waves finally being observed.In the mid-1970s, Rainer Weiss had already analysed possible sources of background noise that would disturb measurements, and had also designed a detector, a laser-based interferometer, which would overcome this noise. Early on, both Kip Thorne and Rainer Weiss were firmly convinced that gravitational waves could be detected and bring about a revolution in our knowledge of the universe.Gravitational waves spread at the speed of light, filling the universe, as Albert Einstein described in his general theory of relativity. They are always created when a mass accelerates, like when an ice-skater pirouettes or a pair of black holes rotate around each other. Einstein was convinced it would never be possible to measure them. The LIGO project's achievement was using a pair of gigantic laser interferometers to measure a change thousands of times smaller than an atomic nucleus, as the gravitational wave passed the Earth.So far all sorts of electromagnetic radiation and particles, such as cosmic rays or neutrinos, have been used to explore the universe. However, gravitational waves are direct testimony to disruptions in spacetime itself. This is something completely new and different, opening up unseen worlds. A wealth of discoveries awaits those who succeed in capturing the waves and interpreting their message.Story Source:Materials provided by Nobel Foundation. ",0.13708,0.135589,0.167272,0.198975,0.180951,0.208869,0.194723,0,0
794,The principle Feynman and Einstein could not find was discovered Why Faraday's law and Lorentz force create the same electromotive force?,"The Faraday's induction formula (flux rule) of electromagnetism says that the electromotive force (emf) created in a conducting circuit is equal to the rate at which the magnetic flux through the conducting circuit changes as it is written on a high school text in physics. This emf can be calculated in two ways: either by using the Lorentz force formula and calculating the force acting on electrons in the moving conductor of the circuit; or via one of Maxwell's equations (Faraday's law) and calculating the change of the magnetic flux penetrating through the circuit. The Lorentz force formula and Maxwell's equations are two distinct physical laws, yet the two methods yield the same results.Why the two results coincide was not known. In other words, the flux rule consists of two physically different laws in classical theories. Interestingly, this problem was also a motivation behind the development of the theory of relativity by Albert Einstein. In 1905, Einstein wrote in the opening paragraph of his first paper on relativity theory, ""It is known that Maxwell's electrodynamics -- as usually understood at the present time -- when applied to moving bodies, leads to asymmetries which do not appear to be inherent in the phenomena."" But Einstein's argument moved away from this problem and formulated special theory of relativity, thus the problem was not solved.Richard Feynman once described this situation in his famous lecture (The Feynman Lectures on Physics, Vol. II, 1964), ""we know of no other place in physics where such a simple and accurate general principle requires for its real understanding an analysis in terms of two different phenomena. Usually such a beautiful generalization is found to stem from a single deep underlying principle. ??????We have to understand the ""rule"" as the combined effects of two quite separate phenomena.""Dr. Hiroyasu Koizumi's study, recently published in the Journal of Superconductivity and Novel Magnetism, has revealed what is the ""single deep underlying principle"" in the ""flux rule"" envisaged by Feynman. It is a duality of the U(1) phase factor added on the wave function; it describes the translational motion of electrons, and also gives a time-dependent gauge potential that induces an effective electric field on the electrons. The former view corresponds to the result obtained by the Lorentz force formula, and the latter to the result using the Maxwell's equation for the Faraday's law.Behind this discovery are two big developments in physics in the 20th century. One is the birth of quantum mechanics, and the other is the establishment of the physical reality of the electromagnetic field as a U(1) gauge field. In the above study, electrons in the conductor are described by the wave functions of quantum mechanics and the magnetic field is expressed as the U(1) gauge field. The gauge field has an arbitrariness called the gauge degree-of-freedom. This arbitrariness can be cast in the U(1) phase factor on the wave function, and can be fixed by the requirement of the energy to be minimum. Then, the duality that the U(1) phase factor can be added to the wave function as the translational motion of electrons allows the ""time-dependent gauge potential"" to emerge. The same gauge fixing has been employed in Dr. Koizumi's study on superconductivity, where the gauge fixing is achieved by the energy minimum requirement under the constraint that the wave function be a single-valued function of the electron coordinates.Dr. Koizumi's work also provides new perspectives on superconductivity and possibly also string theory. Since the most promising qubits for quantum computers are now those using superconductors, the present finding is expected to contribute to the development of quantum computers that may supersede classical computers.Story Source:Materials provided by University of Tsukuba. ",0.0889329,0.104817,0.111046,0.151644,0.111366,0.178092,0.141554,0,0
795,How Einstein's theory of gravitation experienced a Renaissance after World War II Journey into the post-war transformation leading to the return of General Relativity within physics,"Einstein's 1915 theory of gravitation, also known as General Relativity, is now considered one of the pillars of modern physics. It contributes to our understanding of cosmology and of fundamental interactions between particles. But that was not always the case. Between the mid-1920s and the mid-1950s, General Relativity underwent a period of stagnation, during which the theory was mostly considered as a stepping-stone for a superior theory. In a special issue of EPJ H just published, historians of science and physicists actively working on General Relativity and closely related fields share their views on the process, during the post-World War II era, in particular, which saw the ""Renaissance"" of General Relativity, following progressive transformation of the theory into a bona fide physics theory.In this special issue, new insights into the historical process leading to this renaissance point to the extension of the foundation of the original theory, ultimately leading to a global transformation in its character. Contributions from several experts reveals that the theory of 1915 was insufficient to reach firm conclusions without being complemented by intuitions drawn from the resources of pre-relativistic physics. Or, in the case of cosmology, the theory needed to be complemented by philosophical considerations that were hardly generalizable to help solve more mundane problems.As physicist Pascual Jordan puts it, there was a ""mismatch between the simplicity of the physical and epistemological foundations and the annoying complexity of the corresponding thicket of formulae.""A number of contributions in this special issue also explain how the theory underwent a period of successive controversies, leading by the 1960s, to the renaissance of the theory. Subsequently, it became in the 1970s, an important, empirically well-tested branch of theoretical physics related to the new, successful sub-discipline of relativistic astrophysics.Story Source:Materials provided by Springer. ",0.109119,0.142072,0.167093,0.236643,0.136025,0.187498,0.201714,0,0
796,Table-top experiment flips current understanding of solutal convection,"When Yu ""Alex"" Liang started graduate school at The University of Texas at Austin, he was tasked with running a straight-forward experiment to collect data on a well-understood phenomenon in fluid mechanics: how density differences influence fluid flow in a porous medium.It's a scenario that plays out across a wide range of scientific problems. In fact, Liang was planning on applying the experimental data toward a larger project on carbon sequestration. However, the experiment revealed that the physics governing the flow was not as settled as scientists thought.The findings revealed that the primary driver of this type of fluid flow -- called solutal convection -- had been overlooked. What's more, once this driver is accounted for, it completely flips the expected flow outcomes.""To some degree, it is a game changer,"" said Liang, who earned his Ph.D. in petroleum engineering in 2017 and is now working for Hilcorp, a Houston-based energy company. ""Our experiments and simulations show that the convective pattern is controlled by a different process than the one previously thought. People will realize that there are much more in-depth theories to be explored on solutal convection in porous media.""The results of Liang's research were published in the journal Geophysical Researcher Letters in September. His co-authors include Marc Hesse, an associate professor at the UT Jackson School of Geosciences and the Institute for Computational Engineering and Sciences (ICES), David DiCarlo, an associate professor at the UT Hildebrand Department of Petroleum and Geosystems Engineering, and Baole Wen, a postdoctoral fellow at the Jackson School and ICES. DiCarlo and Hesse are Liang's Ph.D. advisors.For decades it has been the scientific consensus that the balance between density-driven currents and diffusion was the primary factor controlling solutal convection in porous media. The thinking went: regions of dense fluid move downward until diffusion eliminates the density difference driving the flow, creating dense fingers that sink toward the bottom. In general, the fingers have to be far enough apart so that diffusion cannot smear them out as the fluid sinks. Therefore, the fingers were generally thought to be further apart in slow flows and closer together in fast flows driven by bigger density differences.Despite the pattern being well-established in computer simulations, Hesse said that the research team was unable to find experimental results that demonstrate this basic behavior. So they developed a simple table-top rig -- a transparent tank filled with glass beads and water -- for observing solutal convection in real time. To start the convection process, the water was topped with a layer of methanol and ethylene-glycol -- a mixture that is overall less dense than water, but gradually becomes denser and sinks as it undergoes convection with the water at the fluid interface.The team was expecting the classic narrow finger pattern to emerge in experiments that used larger diameter beads. Instead, the complete opposite pattern emerged. The finger spacing increased with bead size.""Here's this very basic phenomena, which occurs in all sorts of applications, is a classic example of pattern formation -- and you do the experiments and you get literally the opposite of what everybody expects,"" Hesse said. ""This shows that something is totally off in our basic understanding of this process.""A deeper analysis revealed that the dispersion created by the larger diameter beads had a greater impact on the convective environment than diffusion. While previous theories of solutal convection emphasize diffusion, the spreading of the material in the water in their experiments turned out to be controlled by mechanical dispersion, which leads to the additional mixing of the fluids at the pore scale.""In other words, what you're flowing through is really important,"" he said. ""Dispersion gets bigger with increasing grain size, and that's why the fingers get wider as you do this with bigger beads.""Based on their observations of the table-top experiment, the researchers were able to replicate their findings with a computational model.""The key point is that we analyze the effect of dispersion on convection in subsurface based on our experimental data and then use high-resolution numerical simulations to verify our analysis,"" said Wen.Of course, the natural world is much more complex than a tank filled with beads or a simplified model. The researchers said that scientists have many factors to keep in mind when researching complex phenomena that involve solutal convection, such as CO2 sequestration. But DiCarlo said that these findings show that scientists studying the fundamentals of the process have some new factors to consider.""The work shows that if one wants to predict how CO2 dissolves in the subsurface, or similar dissolution process, the study needs to include dispersion in the correct manner,"" he said. ""All previous studies have ignored dispersion.""Hesse added that the experimental results can help add a dose of reality to computational models that have been getting it wrong for decades.""If your numerical simulation can't even simulate this simple experiment that I'm doing, what confidence do you have that it will do the right thing in an even more complicated setting,"" he said.Story Source:Materials provided by University of Texas at Austin. ",0.049938,0.0857784,0.127106,0.131456,0.0797307,0.0902975,0.125123,0,0
797,US groundwater in peril: Potable supply less than thought,"The U.S. groundwater supply is smaller than originally thought, according to a new research study that includes a University of Arizona hydrologist.The study provides important insights into the depths of underground fresh and brackish water in some of the most prominent sedimentary basins across the U.S.The research by scientists from the University of Saskatchewan, the UA and the University of California, Santa Barbara was published Nov. 14 in Environmental Research Letters.""We found that potable groundwater supplies in the U.S. do not go as deep as previously reported, meaning there is less groundwater for human and agricultural uses,"" said Jennifer McIntosh, a University of Arizona Distinguished Scholar and professor of hydrology and atmospheric sciences.Drilling deeper wells may not be a good long-term solution to compensate for increasing demands on groundwater.""We show that there is potential for contamination of deep fresh and brackish water in areas where the oil and gas industry injects wastewaters into -- or in close depth proximity to -- these aquifers,"" McIntosh said. ""These potable water supplies are already being used up from the 'bottom up' by oil and gas activities.""Groundwater is the primary source of domestic water supply for about half of the people living in the U.S. About 40 percent of all of the water used in the U.S. for irrigated agriculture comes from groundwater,"" McIntosh said. ""In Tucson, Arizona, about half of our drinking water comes from groundwater.""Many rural areas in Arizona and other parts of the U.S. rely exclusively on groundwater for both agricultural and domestic use, she said.To find out how deep potable groundwater extends, the scientists analyzed water chemistry data from the U.S. Geological Survey for 28 key sedimentary basins in the U.S. and looked at the correlation between water well depths and the depth to the transition between fresh and brackish water.Until now, the focus has been on monitoring dropping water tables, said lead author Grant Ferguson, principal investigator of the University of Saskatchewan-led Global Water Futures project.In parts of the western U.S. known to geologists as the Basin and Range Province, fresh groundwater extends down an average of 3,400 feet, McIntosh said. The province includes Nevada, southern Arizona and New Mexico, and extends into parts of California, Utah, Oregon and Idaho.The new research found the average depth of transition from fresh to brackish groundwater in the U.S. overall is about 1,800 feet, which contradicts previous studies suggesting that fresh groundwater extends down to 6,500 feet.Especially in parts of the eastern U.S., the team found the transition from fresh to brackish water occurs at less than 1,000 feet. In such regions, drilling deeper wells is not a long-term solution to the need for additional fresh water, the team wrote.""There are a number of cases where potentially you could go a kilometer or so deep for fresh groundwater, but there are other areas of the United States where in maybe a maximum of 200 or 300 metres you would run into saline groundwater -- essentially you would be done in terms of water resources,"" said Ferguson, an associate professor at the University of Saskatchewan in Saskatoon, Canada.In addition, the injection of water, chemicals or sand that occurs with hydraulic fracturing, or ""fracking,"" or the injection of wastewater may drive waters containing hydrocarbons into adjacent areas that contain potable water.Co-author Debra Perrone, assistant professor of environmental studies at the University of California, Santa Barbara, said, ""In some basins, injection wells are installed shallower than the transition from fresh to brackish water.""Addressing how much separation between groundwater resources and oil and gas activities is needed to protect groundwater will require additional research, the team writes.Based on their findings for the U.S., the authors suggest the amount of fresh groundwater available globally may also be less than previously thought. They note that an estimated more than five billion people live in water-scarce areas, many of which rely on groundwater and where, in some cases, significantly more water has been taken out of a groundwater basin than is coming in.Story Source:Materials provided by University of Arizona. ",0.0955097,0.136841,0.31907,0.191014,0.128243,0.170019,0.184435,0,0
798,"A water treatment breakthrough, inspired by a sea creature ","Inspired by Actinia, a sea organism that ensnares its prey with its tentacles, a team of researchers has developed a method for efficiently treating water.The research, a collaboration of the labs of Yale's Menachem Elimelech and Huazhang Zhao of Peking University, used a material known as a nanocoagulant to rid water of contaminants. By removing a broad range of contaminants in a single step, the discovery promises to significantly improve on the centuries-old use of coagulants for water treatment. The results are published today in Nature Nanotechnology.When added to water, conventional coagulants such as aluminum sulfate and other metallic salts remove larger particles from water by causing them to group together into larger formations and settle. Because these coagulants don't remove smaller particles dissolved in water, additional treatment methods are necessary. Employing multiple technologies for water treatment, however, is costly, energy-intensive and can require a large amount of land. Creating an efficient and easy-to-operate technology to remove all contaminants from water is key to addressing global water scarcity.The research team synthesized a novel, highly stable nanocoagulant different from conventional coagulants in structure, performance and behavior. In additional to removing suspended particles, this nanocoagulant also removes small dissolved contaminants.""The behavior of the nanocoagulant is controlled by its structure,"" said Ryan DuChanois, a Ph.D. student in Elimelech's lab. ""Under certain conditions, the nanocoagulant maintains a structure that allows for it to be stored over time.""Actinia is a sea anemone with a spherical body that has tentacles that retract while resting and extend while catching its prey. With this marine predator as their model, the researchers synthesized the coagulant, using organic and inorganic components to replicate the structure of Actinia.Similar to Actinia, the nanocoagulant has a core-shell structure that turns inside-out in water. The shell destabilizes and enmeshes larger suspended particles, while the exposed core captures the smaller, dissolved ones. It removes a broad spectrum of contaminants, from trace micropollutants to larger particles -- many of which elude conventional methods and pose significant public health concerns.""The ability to remove nitrate was quite surprising, as traditional water coagulants exhibit negligible removal of nitrate,"" said Elimelech, the Roberto C. Goizueta Professor of Chemical & Environmental Engineering. It's also critical to water treatment, since nitrate contamination is associated with 'blue-baby' syndrome, a potentially fatal condition that affects young children in some parts of the world.Because it's a one-step process, professor Huazhang Zhao of Peking University said, the work holds promise for replacing current water treatment methods and greatly reducing the operating costs of water treatment. ""It also opens doors for fabricating 'smart' materials that can transform configuration and function in response to its environment,"" he said.Story Source:Materials provided by Yale University. ",0.0728691,0.121984,0.216769,0.152151,0.11136,0.144328,0.147642,0,0
799,"More than H2O: Technology simultaneously measures 71 elements in water, other liquids ","A new method for simultaneous measurement of 71 inorganic elements in liquids -- including water, beverages, and biological fluids -- makes element testing much faster, more efficient, and more comprehensive than was possible in the past.The researchers studied samples of liquid from a variety of sources worldwide, including tap water from a New York City suburb, snow from Italy and Croatia, rain from Brazil and Pakistan, lake water from Switzerland and Croatia, and seawater from Japan and Brazil. Testing each sample results in a distinct elemental pattern, creating a ""fingerprint"" that can help differentiate between substances or trace a liquid back to its environmental origin.The method -- developed by researchers at the isotope laboratory of NYU College of Dentistry and described in the journal RSC Advances, published by the Royal Society of Chemistry -- may be used to explore and understand the distribution of inorganic elements beyond the few that are typically measured. It has implications for fields such as nutrition, ecology and climate science, and environmental health.An analytical technique called inductively coupled plasma mass spectrometry (ICP-MS) is used to measure elements. Historically, ICP-MS instruments have measured elements sequentially, or one by one, but a new type of ICP-MS instrument at NYU College of Dentistry and roughly two dozen other places around the world has the potential to measure the complete range of inorganic elements all at once.""Because of this new method, our mass spectrometer can simultaneously measure all inorganic elements from lithium to uranium. We're able to measure the elements in far less time, at far less expense, using far less material,"" said Timothy Bromage, professor of biomaterials and of basic science and craniofacial biology at NYU College of Dentistry and the study's senior author.This technological advancement may help to fill gaps in our understanding of element distributions and concentrations in substances like water. For instance, the U.S. Environmental Protection Agency monitors and sets maximum concentration limits for 19 elements in drinking water considered to be health risks, yet many elements known to have health consequences -- such as lithium or tin -- are neither monitored nor regulated.""The elemental mapping of concentration levels in bottled and tap water could help to increase our understanding of 'normal' concentration levels of most elements in water,"" said Bromage.Bromage and his colleagues designed a method for using simultaneous ICP-MS to detect 71 elements of the inorganic spectrum involving a specific set of calibration and internal standards. The method, for which they have a patent pending, routinely detects elements in seconds to several minutes and in samples as small as 1 to 4 milliliters.Bromage and his research team tested the method on waters, beverages, and biological samples. Snow contained the most elements of any water sample: 50 in snow collected in Italy and 42 in a sample from Croatia. ""Such evaluations of snow may represent a new and comprehensive means of surveying atmospheric concentrations of elements and for monitoring element patterns in global airflows,"" Bromage said.When testing tap water, the researchers measured 37 elements when the tap was first turned on but only 34 elements after the water was running for five minutes, suggesting that elements such as iron and zinc may be leaching from household pipes into the water.The researchers also measured elements in bottled water, beer, wine, and milk, as well as in samples of saliva, urine, and blood. Milk was distinguished from the other beverages tested by its high concentrations of titanium, zinc, palladium, and gold.In each sample, Bromage and his team found a distinct ""fingerprint"" or elemental pattern, suggesting that samples can be recognized and differentiated by these patterns. The elemental content of water, for example, typically reflects its natural environment, so understanding the elemental composition can tell us if water had its origins from a source with volcanic rock versus limestone, an alkaline rock. In bottled water, the researchers observed variations that can likely be traced to one being bottled at the source and one being chlorinated for transportation from the source to the bottling plant.Future studies will measure and report on larger samples of water, wine, milk, and other fluids; a study of more than 1,000 wines from 34 countries is in progress. In addition, once elemental patterns for specific environments have been established, the method can be applied to answer questions in fields that relate the present to the past, such as the paleoenvironment and climate change.""Water is an arbiter of how a system actually works. If you sample the water from a pond or river and measure the elements, you are measuring the stuff that becomes incorporated into all life -- water feeds the plants, animals eat the plants, we eat the plants and animals. We could use this knowledge to study human fossils and potentially retrodict what the nature of the region's water was hundreds of thousands or millions of years ago,"" said Bromage.VIDEO: https://youtu.be/maZMSY2KG54Story Source:Materials provided by New York University. ",0.0620977,0.12923,0.155403,0.136647,0.0923714,0.111471,0.132001,0,0
800,Reducing the impact forces of water entry,"When professional divers jump from a springboard, their hands are perpendicular to the water, with wrists pointed upward, as they continue toward their plunge at 30 mph.As they complete what's known as a rip dive, their hands remove water in front of the body, creating a cavity that reduces the initial impact force. The rest of the body, from head to toe, is aligned to shoot through the same cavity created by the hands.Using the hands to create cavities in the water's surface is similar to the concept behind the fluid-structure studies that researchers at Utah State University are conducting using spheres.Utah State researcher Rafsan Rabbi will present his team's research at the American Physical Society's Division of Fluid Dynamics 71st Annual Meeting, which will take place Nov. 18-20 at the Georgia World Congress Center in Atlanta, Georgia.In one study, two consecutive spheres falling into a water tank were tested. The first sphere dropped into the water to create the cavity was made of steel. Five different sizes were used. The diameters ranged from 10 millimeters to 38 millimeters, but they were always smaller than the second sphere.The second sphere, which went through the cavity, was made from 3D-printed Vero plastic. These spheres had a fixed diameter of 50 mm (or about 2 inches). Attached to each of these plastic objects was an accelerometer.The researchers investigated how much acceleration the plastic spheres experienced as they went through the cavity created by the first steel spheres and calculated the force reductions from that. They found that the impact force of the plastic spheres was reduced by the steel ones by 40-60 percent.""We have found that this 'free surface preparation' phenomenon can have a huge impact in reducing the initial high-impact force any object experiences while falling into a body of water,"" Rabbi said.The research could have practical applications, such as lessening the blow of rockets landing unintentionally into the ocean.A second study used Worthington jets instead of cavities to reduce the impact force. Such a jet results from a cavity collapsing on itself. The spacing between the two spheres were fixed in such a way that after the cavity collapses, the second sphere falls through the jet.""This also reduced the impact force dramatically by up to 60 percent,"" said Tadd Truscott, director of the Splash Lab at Utah State who heads the research team. ""Our studies show that prior preparations to the surface create a disturbance in the water surface to help cushion the impact of any falling object.""Story Source:Materials provided by American Physical Society. ",0.155242,0.15094,0.204682,0.186112,0.158383,0.196417,0.205088,0,0
801,Ant-Man and The Wasp: Helping Marvel superheroes breathe,"Marvel comics superheroes Ant-Man and the Wasp -- nom de guerre stars of the eponymous 2018 film -- possess the ability to temporarily shrink down to the size of insects, while retaining the mass and strength of their normal human bodies. But a new study suggests that, when bug-sized, Ant-Man and the Wasp would face serious challenges, including oxygen deprivation.Those challenges, along with their solution-microfluidic technologies, will be described by engineering mechanics graduate student Max Mikel-Stites of Virginia Tech at the American Physical Society's Division of Fluid Dynamics 71st Annual Meeting, which will take place Nov. 18-20 at the Georgia World Congress Center in Atlanta, Georgia.Mikel-Stites and his advisor, Anne Staples, an associate professor in the biomedical engineering and mechanics department at Virginia Tech, normally study biological fluid dynamics, with a particular focus on insect respiration and insect-scale fluid flows. Staples' lab has developed microfluidic devices inspired by insect respiratory systems in which the flow rate and direction of flow through individual channels in the device can be controlled without the use of valves.The work, which will be discussed in a separate presentation at the DFD meeting, could reduce the actuation machinery needed for microfluidic devices used in many different scientific fields, and make them more portable and cost-efficient. ""Applying that perspective to Ant-Man and the Wasp seemed like a straightforward thing to do,"" said Mikel-Stites.In their analysis, the researchers determined that the atmospheric density -- basically, the number of molecules (say, of oxygen) in a given volume of air -- experienced by the bug-sized heroes is reduced to a level nearly identical to that of Mt. Everest's so-called ""death zone,"" where there is not enough oxygen for a human to breathe. ""While the actual atmospheric density is the same for an insect and a human, the subjective atmospheric density experienced by a human who shrinks to insect size changes,"" Mikel-Stites explained. ""For example, a normal-sized person taking a deep breath can expect to inhale some number of oxygen molecules. However, when that person is shrunk down to the size of an ant, despite still needing the same number of oxygen molecules, far fewer are available in a single breath of air.The ""death zone"" begins for a normal-sized human about 8,000 meters above sea level. The shrunken superheroes, the researchers calculated, would feel like they were at an altitude of 7,998 meters, and that would make for a serious -- if not deadly -- case of altitude sickness.""For someone not acclimated, symptoms of altitude sickness range from headache and dizziness to the buildup of fluid in the lungs and brain, and possibly death. This occurs in part because people may respond by trying to breathe more rapidly, to increase their oxygen intake, and because the body is attempting to function with less oxygen than it normally does,"" he said.And that's not the extent of Ant-Man's and the Wasp's problems, the team found. Based on a relationship known as Kleiber's law, which correlates the metabolic rate of an animal to its size, the researchers found that the metabolic rates per unit mass of the superheroes at bug size would increase by approximately two orders of magnitude -- as would their oxygen demands.But all is not lost -- thanks to science. According to Mikel-Stites, the use of microfluidic components such as Knudsen pumps (which are driven by temperature gradients) and microscale gas compressors, could be embedded into the helmets of Ant-Man and the Wasp to help them breathe at the microscale.Story Source:Materials provided by American Physical Society. ",0.0964884,0.108691,0.14304,0.180873,0.107304,0.15966,0.168621,0,0
802,Stealth-cap technology for light-emitting nanoparticles,"A team of scientists from the Helmholtz-Zentrum Dresden-Rossendorf (HZDR), in collaboration with researchers from Monash University Australia, has succeeded in significantly increasing the stability and biocompatibility of special light-transducing nanoparticles. The team has developed the so-called ""upconverting"" nanoparticles that not only convert infrared light into UV-visible light, but also are water-soluble, remain stable in complex body fluids such as blood serum, and can be used to store medications. They have created a tool that could potentially make the fight against cancer significantly more effective. The researchers recently published their results in the journal Angewandte Chemie.Nanoparticles are tiny structures, typically less than 100 nanometers in size, which is about 500 to 1000 times smaller than the thickness of a human hair. Such materials are receiving increasing attention for biomedical applications. If equipped with appropriate properties, they can reach almost any tissue in the human body via the bloodstream -- turning into perfect body probes.It has been known for some years that the distribution of nanoparticles in the body is essentially determined by their size and surface properties. Dr. Tanmaya Joshi at HZDR's Institute for Radiopharmaceutical Cancer Research says, ""Upconverting nanomaterials are of great interest for biomedical imaging."" ""When stimulated with infrared light they can send out bright blue, green, or red signals. If we succeed in navigating such nano-probes to diseased tissues, it can be particularly useful for cancer diagnosis,"" the team's photochemist, Dr. Massimo Sgarzi, added.However, these light upconverters show poor solubility in water or tissue fluids -- a must to have feature before any diagnostic or therapeutic use could be imagined. For the HZDR team this was not a hindrance, but rather a challenge: ""We used a unique polymer mixture to cover the particles,"" says Dr. Joshi, who joined HZDR in 2017 from Monash University, as a Humboldt Fellow. Adding this protective cover makes the light-transducing nanoparticles biocompatible. The biologist Dr. Kristof Zarschler adds: ""The upconverters are now water-soluble and even have a neutral surface charge. Our research shows that this new cover can almost completely prevent the body's own substances (present in the blood serum) from binding to the particles. In other words, the nanoparticles now seem to wear an invisibility cloak. This, we believe, will help to avoid their recognition and elimination by phagocytes of the immune system. ""In order to keep the new nano-probes stable for weeks in a complex biological environment, the scientists photochemically link the components of the protective shell with each other: ""We simply irradiated our nanoparticles with UV light. This creates additional bonds between the molecular components constituting the protective cover -- much alike sewing together the individual parts of the cloak of invisibility with the help of light, ""explains the PhD student, Anne Nsubuga. She further adds, ""This shell is only a few nanometers thick, and may even be used for hiding other substances, for example, cancer drugs, which could be later on released in the tumour and destroy it.""Following this breakthrough, the team now intends to validate their current results in living organisms: ""For this, we first have to carry out strictly regulated and ethically acceptable experiments on animals. Only when our stealth-cap technology works on these without any side effects, their medical potential will be explored in detail and their application on the patients can be considered,"" explains the group leader Dr. Holger Stephan cautiously.Story Source:Materials provided by Helmholtz-Zentrum Dresden-Rossendorf. ",0.0735199,0.138166,0.149843,0.166558,0.116434,0.125566,0.156131,0,0
803,"Tiny structures' construction, drip by drip ","Popping the top on house paint usually draws people to look inside the can. But Princeton researchers have turned their gaze upward, to the underside of the lid, where it turns out that pattern of droplets could inspire new ways to make microscopically small structures.The trick comes in controlling the droplets, which form under competing influences like gravity and surface tension. A new study, published Oct. 26 in the journal Nature Communications, explains how a deeper understanding of these highly dynamic, sometimes unstable forces can be harnessed to cheaply and quickly fabricate objects that normally require a more expensive and time-consuming process.""We've done away with the molds,"" said Pierre-Thomas Brun, assistant professor of chemical and biological engineering at Princeton and the principal investigator for the study. ""We don't need a clean room or any fancy equipment, so engineers have much more freedom in the design process.""Using a silicone common in medical devices, the team poured a thin liquid film over the surface of a plate, about the size of a compact disc, which they then flipped upside down for several minutes while the film cured. Without intervention, the liquid silicone congeals into an irregular array of droplets -- much like the paint under a lid. But by etching the plate with mathematical precision, using lasers to cut the marks, the researchers ""seeded"" the droplets into a lattice of perfect hexagons, each with a uniform dimension.""Gravity wants to pull the fluid down,"" said Joel Marthelot, postdoctoral research associate at Princeton and lead author on the paper. ""Capillary forces want the surface to deform minimally. So there is a competition between these two forces, which gives rise to the length scale of the structure.""More sophisticated versions of the experiment used a centrifuge in place of gravity, which allowed the team to vary the size of the drops with an indefinite range. Instead of plates, in this version they used plastic cylinders that look like clear hockey pucks. The excess fluid spun off and left their predictable pattern of cured drops. The technique worked down to the limit of their machinery, which produced a lattice of structures that were each around 10 microns, a fraction of the width of a human hair. The structures, which are prototypes, simulate the kinds of soft lenses that are a common part in smartphones.""The faster it spins, the smaller the drops,"" Marthelot said, noting that they could make structures even smaller than what they had achieved so far. ""We don't really know the limit of our technique. Only the limit of our centrifuge.""According to Brun, the kinds of mechanical instabilities that cause this behavior are usually regarded by engineers as a kind of nemesis. They are the physical thresholds that determine weight loads or heat capacities. ""In this case,"" he said, ""we took advantage of something that is normally seen as bad. We tamed it and made it functional by turning it into a pathway to fabrication.""The technique can be easily expanded to large-scale manufacturing, the researchers said. As their methods evolve, they plan to create biomimetic devices, like an inflatable compound lens that mimics the eye of an insect, or soft robots that can be used in medical technologies.""One can envision a wide range of potential future application,"" said Jern Dunkel, associate professor of mathematics at the Massachusetts Institute of Technology, ""from drag-reducing or superhydrophobic surfaces to micro-lenses and artificial ciliary carpets.""In addition to Brun and Marthelot, two other researchers contributed to the study: Elizabeth Strong, formerly a student at MIT and now a Ph.D. candidate at the University of Colorado, Boulder; and Pedro M. Reis of the Ecole Polytechnique Federale de Lausanne.Story Source:Materials provided by Princeton University, Engineering School. ",0.0595086,0.0874378,0.115723,0.131467,0.0876392,0.103908,0.119142,0,0
804,Factors affecting turbulence scaling,"Fluids exhibiting scaling behaviour can be found in diverse physical phenomena occurring both in the laboratory and in real-world conditions. For instance, they occur at the critical point when a liquid becomes a vapour, at the phase transition of superfluids, and at the phase separation of binary liquids whose components exhibit two different types of behaviour.Until now, models have not fully taken the effect of external turbulences into account. In a recent study published in EPJ B, Michal Hnati? from eaferik University in Koeice, Slovakia and colleagues investigate the influence of ambient turbulent speed fluctuations in physical systems when they reach a critical point. These fluctuations are found to be the result of a lack of spatial regularity in these systems, or anisotropy, and of the compressibility of fluids. What is unique about this study is that the turbulence introduced in the model is novel and helps to elucidate the extent to which the speed of these fluctuations affects their scaling behaviour.The authors examine the critical behaviour of physical systems, using two distinct models to do so. The first describes the critical dynamics of the system at equilibrium, while the second represents the stage in which the system is no longer at equilibrium and adopts a scaling behaviour -- referred to as directed percolation -- previously used to study systems such as the spread of epidemics, forest fires and population growth. To gain a better understanding of the critical behaviour of the system, the authors chose the difficult approach of integrating the mutual effects of large-scale anisotropy and compressibility into their model; previous models had only taken these effects into account separately. They thus identify four types of scaling regimes which can potentially be observed at macroscopic scale for each model. In closing, the authors show that it is anisotropy which may be the key factor determining different types of emerging scaling behaviour.Story Source:Materials provided by Springer. ",0.127919,0.136384,0.175838,0.193257,0.144793,0.185723,0.174703,0,0
805,Doing the wave: How stretchy fluids react to wavy surfaces,"Viscoelastic fluids are everywhere, whether racing through your veins or through 1,300 kilometers of pipe in the Trans-Alaska Pipeline. Unlike Newtonian fluids, such as oil or water, viscoelastic fluids stretch like a sticky strand of saliva. Chains of molecules inside the fluids grant them this superpower, and scientists are still working to understand how it affects their behavior. Researchers at the Okinawa Institute of Science and Technology Graduate University (OIST) have brought us one step closer by demonstrating how viscoelastic fluids flow over wavy surfaces, and their results are unexpected.""To me it was not intuitive, and I've worked with these fluids for nearly 20 years,"" said Simon Haward, group leader in the Micro/Bio/Nanofluidics Unit and first author of the study. The paper, published in Physics of Fluids on November 5, 2018, is the third in a series of three studies putting new theories of viscoelastic fluids to the test.A phenomenal disappearing actWhen water flows through a smooth tube, its motion is uniform throughout. But when water makes contact with a wavy surface, it breaks like the tide over the seashore. The water reacts to each peak and trough of the disrupting wave, thrown into spiraling swirls known as vortices. The spinning motion, known as vorticity, is most pronounced near the wavy wall and dissipates at a calculable distance away.Scientists have witnessed this scenario unfold countless times in water and other Newtonian fluids. But before now, analogous experiments had never been conducted in viscoelastic fluids, which are predicted to behave much differently. OIST researchers set out to fill that gap in the literature.Recent theoretical work suggests that waves send viscoelastic fluids spinning much like Newtonian fluids, but with one key difference. While the swirling motion induced in Newtonian fluids decays with distance, vortices in viscoelastic fluids can actually become amplified at a specific distance away. This region of amplified action has been dubbed the ""critical layer"" in theory, but hadn't been observed experimentally.""The location of this critical layer depends on the elasticity of the fluid,"" said Haward. The more molecule chains, or polymers, a fluid contains, he said, the more elastic it becomes. The more elastic the fluid, the farther away the critical layer moves from the wavy wall. There comes a point when the fluid is so elastic, and the critical layer so distant, that the spiraling vortices near the wall are no longer affected by it.""Normally, we think if a fluid is more viscoelastic, you'll see more strange effects,"" Haward said. ""But in this case, when the fluid is highly elastic, the observable effect disappears.""Filling critical gaps in knowledgeIn past research, the Micro/Bio/Nanofluidics Unit designed experiments and specialized equipment to catch these critical layers in action. Their efforts resulted in the first experimental evidence of the phenomenon. Now, the researchers have constructed a detailed chart describing how the critical layer shifts when the channel is widened, the wavelength is lengthened or the fluid's flow rate is increased.""It was surprising because the theory seemed counterintuitive, but our experimental results fell into the exact same phase diagram as the theory predicted,"" said Haward. ""Basically, our experiments fully confirmed the theory.""The comprehensive research establishes a strong starting point for future studies of viscoelastic fluids. The fundamental properties of these stretchy fluids have direct implications in the oil industry, medicine and biotechnology, and help shape the world around us. With this study, scientists can now begin to factor the critical layer into their calculations, which may help to improve applications or find new avenues for viscoelastic fluids in their research.Story Source:Materials provided by Okinawa Institute of Science and Technology (OIST) Graduate University. ",0.122344,0.137686,0.161967,0.178138,0.133528,0.166209,0.191754,0,0
806,Don't underestimate the force Researchers discover weak chemical interactions hold together box of infinite possibilities,"Researchers have identified the weak molecular forces that hold together a tiny, self-assembling box with powerful possibilities. The study demonstrates a practical application of a force common in biological systems and advances the pursuit of artificial chemical life.""I want to understand self-assembly systems, which are essential for life. Building artificial self-assembling cubes helps us understand how biological systems function,"" said Professor Shuichi Hiraoka, leader of the laboratory at the University of Tokyo Graduate School of Arts and Sciences where the boxes were designed, built, and analyzed.The formation of DNA and proteins are biological examples of self-assembly, but the forces or processes controlling how these natural molecules come together also remain undefined. Investigations by Hiraoka's team contribute to chemical understanding of how natural molecules might self-assemble and reveal techniques for mimicking those processes in the future.Hiraoka and his team identified the forces holding together the sides of their tiny boxes as van der Waals forces, mainly dispersion forces. These forces are weak attractions between molecules created when electrons temporarily group together on one side of an atom. Geckos can walk up walls in part due to van der Waals forces.Each side of the cube is formed from one molecule that is 2 nanometers in diameter and shaped like a six-pointed snowflake. Each side is about one-four-thousandth the size of a human blood cell. The weak forces holding the sides of the cube together make the box slightly flexible, so it adjusts to best accommodate guest molecules based on their size, shape, and atomic charge. The box can bulge to hold large or long contents and contract to eliminate extra space when hosting guest molecules with negative charge(s).""We do not have the data yet, but the logical conclusion is that long chainlike guest molecules somehow fold to get inside the box,"" said Hiraoka.Researchers build the tiny box out of molecules of hexaphenylbenzene. The individual molecules exist as a dry, white powder. When mixed with water, the molecules spontaneously self-assemble into cubes.""In solution, the six molecules come together so quickly that we cannot observe how they form cubes. The exact process of self-assembly remains a mystery,"" said Hiraoka.A cube that can self-assemble in water has the potential for future biological applications. The hexaphenylbenzene cube also holds together even above the boiling temperature of water, remaining stable up to 130 degrees Celsius (266 degrees Fahrenheit).The six points of the snowflake-shaped hexaphenylbenzene molecules lock together when they assemble into a cube. Researchers describe the design of this molecular box as resembling the Japanese wood joining technique called hozo, where pieces of wood are held together without adhesives or hinges, using only intricate interlocking designs.About the Research In addition to van der Waals forces, other forces also contribute to holding the box together, specifically the hydrophobic effect (exclusion of water molecules) and cation-? interactions (an attraction between molecular bonds and positively charged ions).Hiraoka's team uses physical chemistry techniques including nuclear magnetic resonance spectroscopy to characterize the box. Because the box only forms when mixed in water, researchers cannot use imaging techniques that require solid samples. The motion of the box as it self-assembles and bulges or contracts to host new molecules is not visible with techniques that require stationary samples.Hiraoka's research group first constructed a similar cube in 2008 and has worked to improve the water solubility and thermal stability since then.The study is published in Nature Communications. Collaborators in Japan at Ritsumeikan University and Tokyo Institute of Technology also contributed to the research.Story Source:Materials provided by University of Tokyo. ",0.0857714,0.153333,0.149892,0.153349,0.1216,0.141824,0.14918,0,0
807,"Kilogram, ampere, kelvin and mole redefined International System of Units overhauled in historic vote","Today, in a landmark decision, representatives from 60 countries voted to redefine the International System of Units (SI), changing the world's definition of the kilogram, the ampere, the kelvin and the mole, forever.The decision, made at the General Conference on Weights and Measures in Versailles, France, which is organised by the International Bureau of Weights and Measures (BIPM), means that all SI units will now be defined in terms of constants that describe the natural world. This will assure the future stability of the SI and open the opportunity for the use of new technologies, including quantum technologies, to implement the definitions.The changes, which will come into force on 20 May 2019, will bring an end to the use of physical objects to define measurement units.The definition of the kilogram for more than 130 years, the International Prototype of the Kilogram (IPK), a cylinder of a platinum alloy stored at the BIPM in France, will now be retired. It will be replaced by the Planck constant -- the fundamental constant of quantum physics. While the stability of the IPK could only be confirmed by comparisons with identical copies, a difficult and potentially inaccurate process, the Planck constant is ready for use everywhere and always.""The SI redefinition is a landmark moment in scientific progress,"" said Martin Milton, Director, International Bureau of Weights and Measures (BIPM). ""Using the fundamental constants we observe in nature as a foundation for important concepts, such as mass and time, means that we have a stable foundation from which to advance our scientific understanding, develop new technologies and address some of society's greatest challenges.""""Today marks the culmination of decades of work by measurement scientists around the world, the significance of which is immense,"" said Barry Inglis, Director of the International Committee for Weights and Measures. ""We will now no longer be bound by the limitations of objects in our measurement of the world, but have universality accessible units that can pave the way to even greater accuracy, and even accelerate scientific advancement.""The new definitions impact four of the seven base units of the SI: the kilogram, ampere, kelvin and mole; and all units derived from them, such as the volt, ohm and joule.Although the size of these units will not change (a kilogram will still be a kilogram), the four redefined units will join the second, the metre and the candela to ensure that the set of SI base units will continue to be both stable and useful. The revised SI will maintain its relevance by facilitating technical innovations. Just as the redefinition of the second in 1967 provided the basis for technology that has transformed how we communicate across the globe, through GPS and the internet, the new changes will have wide-reaching impact in science, technology, trade, health and the environment, among many other sectors.",0.0992428,0.11476,0.174689,0.197528,0.123474,0.172247,0.173866,0,0
808,Signs of interactive form of quantum matter observed,"JILA researchers have, for the first time, isolated groups of a few atoms and precisely measured their multi-particle interactions within an atomic clock. The advance will help scientists control interacting quantum matter, which is expected to boost the performance of atomic clocks, many other types of sensors, and quantum information systems.The research is described in a Nature paper posted early online Oct. 31. JILA is jointly operated by the National Institute of Standards and Technology (NIST) and the University of Colorado Boulder.NIST scientists have been predicting ""many body"" physics and its benefits for years, but the new JILA work provides the first quantitative evidence of exactly what happens when packing together a few fermions -- atoms that cannot be in the same quantum state and location at the same time.""We are trying to understand the emergence of complexity when multiple particles -- atoms here -- interact with each other,"" NIST and JILA Fellow Jun Ye said. ""Even though we may understand the rules perfectly on how two atoms interact, when multiple atoms get together there are always surprises. We want to understand the surprises quantitatively.""Today's best tools for measuring quantities such as time and frequency are based on control of individual quantum particles. This is the case even when ensembles of thousands of atoms are used in an atomic clock. These measurements are approaching the so-called standard quantum limit -- a ""wall"" preventing further improvements using independent particles.Harnessing of many-particle interactions could push that wall back or even break through it, because an engineered quantum state could suppress atom collisions and protect quantum states against interference, or noise. In addition, atoms in such systems could be arranged to cancel each other's quantum noise such that sensors would get better as more atoms were added, promising significant leaps in precision and data-carrying capacity.In the new research, the JILA team used their three-dimensioned strontium lattice clock], which offers precise atom control. They created arrays of between one and five atoms per lattice cell, and then used a laser to set the clock ""ticking,"" or switching at a specific frequency between two energy levels in the atoms. JILA's new imaging technique was used to measure the atoms' quantum states.The researchers observed unexpected results when three or more atoms were together in a cell. The results were nonlinear, or unpredicted based on past experience, a hallmark of multi-particle interactions. The researchers combined their measurements with theoretical predictions by NIST colleagues Ana Maria Rey and Paul Julienne to conclude that multi-particle interactions occurred.Specifically, the clock's frequency shifted in unexpected ways when three or more atoms were in a lattice site. The shift is different from what one would expect from summing up various pairs of atoms. For example, five atoms per cell caused a shift of 20 percent compared to what would normally be expected.""Once you get three atoms per cell, the rules change,"" Ye said. This is because the atoms' nuclear spins and electronic configurations play together to determine the overall quantum state, and the atoms can all interact simultaneously instead of in a pair-wise fashion, he said.Multi-particle effects also appeared in crowded lattice cells in the form of an unusual, rapid decay process. Two atoms per triad formed a molecule and one atom remained loose, but all had enough energy to escape the trap. By contrast, a single atom is likely to remain in a cell for a much longer time, Ye said.""What this means is, we can make sure there is only one atom per cell in our atomic clock,"" Ye said. ""Understanding of these processes will allow us to figure out a better path for making improved clocks, as particles inevitably will interact if we pack enough of them nearby to improve signal strength.""The JILA team also found that packing three or more atoms into a cell could result in long-lived, highly entangled states, meaning the atoms' quantum properties were linked in a stable way. This simple method of entangling multiple atoms may be a useful resource for quantum information processing.This research is supported by NIST, the Defense Advanced Research Projects Agency, the Army Research Office, the Air Force Office of Scientific Research, National Science Foundation and National Aeronautics and Space Administration.Story Source:Materials provided by National Institute of Standards and Technology (NIST). ",0.06727,0.124486,0.116637,0.171522,0.112668,0.120269,0.147095,0,0
809,Quantum science turns social,"Researchers developed a versatile remote gaming interface that allowed external experts as well as hundreds of citizen scientists all over the world through multiplayer collaboration and in real time to optimize a quantum gas experiment in a lab at Aarhus University. Surprisingly, both teams quickly used the interface to dramatically improve upon the previous best solutions established after months of careful experimental optimization. Comparing domain experts, algorithms and citizen scientists is a first step towards unravelling how humans solve complex, natural science problems.In a future characterized by algorithms with ever increasing computational power, it becomes essential to understand the difference between human and machine intelligence. This will enable the development of hybrid-intelligence interfaces that optimally exploit the best of both worlds. By making complex research challenges available for contribution by the general public, citizen science does exactly this.Numerous citizen science projects have shown that humans can compete with state-of-the-art algorithms in terms of solving complex, natural science problems. However, these projects have so far not addressed why a collective of citizen scientists can solve such complex problems.An interdisciplinary team of researchers from Aarhus University, Ulm University, and the University of Sussex, Brighton have now taken important first steps in this direction by analyzing the performance and search strategy of both a state-of-the-art computer algorithm and citizen scientists in their real-time optimization of an experimental laboratory setting.In the 'Alice Challenge', Robert Heck and colleagues in their quest for realizing quantum simulations enlisted the help of both experts and citizen scientists by providing live access to their ultra-cold quantum gas experiment.This was made possible by using a novel remote interface created by the team at ScienceAtHome, Aarhus University. By manipulating laser beams and magnetic fields, the task was to cool as many atoms as possible down to extremely cold temperatures just above absolute zero at -273.15eC.Surprisingly, both groups using the remote interface consistently outperformed the best solutions identified by the experimental team over months and years of careful optimization.Why could players without any formal training in experimental physics manage to find surprisingly good solutions? One hint came from an interview with a top-player, a retired Italian microwave systems engineer. He said, that for him participating in the Alice Challenge reminded him a lot of his previous job as an engineer. He never attained a detailed understanding of microwave systems but instead spent years developing an intuition of how to optimize the performance of his ""black-box.""""We humans may develop general optimization skills in our everyday work life that we can efficiently transfer to new settings. If this is true, any research challenge can in fact be turned into a citizen science game,"" said Jacob Sherson, head of the ScienceAtHome project at Aarhus University.It still seems incredible that untrained amateurs using an unintuitive game interface outcompete expert experimentalists. One answer may lie in an old Herbert Simon quote: ""Solving a problem simply means representing it so as to make the solution transparent.""In this view, the players may be performing better not because they have superior skills, but because the interface they are using makes another kind of exploration ""the obvious thing to try out"" compared to the traditional experimental control interface.""The process of developing (fun) interfaces that allow experts and citizen scientists alike to view the complex research problems from different angles, may contain the key to developing future hybrid intelligence systems in which we make optimal use of human creativity"" explained Jacob Sherson.More concretely, the team set up a carefully constructed ""social science in the wild"" study which allowed them to quantitatively characterize the collective search behavior of the players. They concluded that what makes human problem solving unique is how a collective of individuals balance innovative attempts and refine existing solutions based on their previous performance.Carsten Bergenholtz and Oana Vuculescu, Dep. of Management, Aarhus University said, ""These findings provide insight into the unique human ability to collectively solve complex problems. Leveraging this knowledge will allow for the design of hybrid intelligence interfaces that combine the computational strengths of AI with the advantages of human intuition.""Story Source:Materials provided by Aarhus University. ",0.0346334,0.0668455,0.088679,0.249817,0.0710271,0.0783252,0.133062,0,0
810,A burst of 'synchronous' light Sandwich structure of nanocrystals as quantum light source,"Excited photo-emitters can cooperate and radiate simultaneously, a phenomenon called superfluorescence. Researchers from Empa and ETH Zurich, together with colleagues from IBM Research Zurich, have recently been able to create this effect with long-range ordered nanocrystal superlattices. This discovery could enable future developments in LED lighting, quantum sensing, quantum communication and future quantum computing. The study has just been published in the renowned journal Nature.Some materials spontaneously emit light if they are excited by an external source, for instance a laser. This phenomenon is known as fluorescence. However, in several gases and quantum systems a much stronger emission of light can occur, when the emitters within an ensemble spontaneously synchronize their quantum mechanical phase with each other and act together when excited. In this way, the resulting light output can be much more intense than the sum of the individual emitters, leading to an ultrafast and bright emission of light -- superfluorescence. It only occurs, however, when those emitters fulfill stringent requirements, such as having the same emission energy, high coupling strength to the light field and a long coherence time. As such, they are strongly interacting with each other but at the same time are not easily disturbed by their environment. This has not been possible up to now using technologically relevant materials. Colloidal quantum dots could just be the ticket; they are a proven, commercially appealing solution already employed in the most advanced LCD television displays -- and they fulfill all the requirements.Researchers at Empa and ETH Zurich, led by Maksym Kovalenko, together with colleagues from IBM Research Zurich, have now shown that the most recent generation of quantum dots made of lead halide perovskites offer an elegant and practically convenient path to superfluorescence on-demand. For this, the researchers arranged perovskite quantum dots into a three-dimensional superlattice, which enables the coherent collective emission of photons -- thus creating superfluorescence. This provides the basis for sources of entangled multi-photon states, a missing key resource for quantum sensing, quantum imaging and photonic quantum computing.Birds of a feather flock togetherA coherent coupling among quantum dots requires, however, that they all have the same size, shape and composition because ""birds of a feather flock together"" in the quantum universe, too. ""Such long-range ordered superlattices could only be obtained from a highly monodisperse solution of quantum dots, the synthesis of which had been carefully optimized over the last few years,"" said Maryna Bodnarchuk, a senior scientist at Empa. With such ""uniform"" quantum dots of various sizes, the research team could then form superlattices by properly controlling the solvent evaporation.The final proof of superfluorescence came from optical experiments performed at temperatures of around minus 267 degrees Celsius. The researchers discovered that photons were emitted simultaneously in a bright burst: ""This was our 'Eureka! ' moment. The moment we realized that this was a novel quantum light source,"" said Gabriele Raine from ETH Zurich and Empa who was part of the team that carried out the optical experiments.The researchers consider these experiments as a starting point to further exploit collective quantum phenomena with this unique class of material. ""As the properties of the ensemble can be boosted compared to just the sum of its parts, one can go way beyond engineering the individual quantum dots,"" added Michael Becker from ETH Zurich and IBM Research. The controlled generation of superfluorescence and the corresponding quantum light could open new possibilities in LED lighting, quantum sensing, quantum-encrypted communication and future quantum computing.Story Source:Materials provided by Swiss Federal Laboratories for Materials Science and Technology (EMPA). Original written by Cornelia Zogg. ",0.107742,0.146014,0.146861,0.229955,0.158444,0.171817,0.191124,0,0
811,"When quantum particles swirl about, they still obey universal laws Different quantum systems can show the same behaviour, experiments show","Some things are so complicated that it is completely impossible to precisely calculate them. This includes large quantum systems, which consist of many particles, particularly when they are not in an equilibrium state, but changing rapidly. Such examples include the wild particle inferno that occurs in particle accelerators when large atoms collide, or in the early universe, just after the Big Bang, when particles rapidly expanded and subsequently cooled.At TU Wien and Heidelberg University, remarkable rules have been detected in the apparent chaos of disequilibrium processes. This indicates that such processes can be divided into universality classes. Systems belonging to the same class behave identically in many ways. This means that experiments can be carried out with quantum systems that are easy to handle, in order to obtain precise information about other systems that cannot be directly studied in the experiment. These findings have since been published in the journal Nature.Universal rules""Universality classes are known from other areas of physics,"" says Prof. Jerg Schmiedmayer from the Institute of Atomic and Subatomic Physics at TU Wien. ""When you study phase transitions, for example materials very close to the melting point, you can describe certain properties using formulas that are very universal, such as the relationship between the specific heat and the temperature."" The microscopic details of the melting process do not matter. Very different materials can obey the same simple equations.""It is however entirely astounding that universality of this kind can also be found in quantum systems that are far removed from an equilibrium state,"" says Jerg Schmiedmayer. ""At first glance, you wouldn't expect this: why should a quantum system made up of many particles that are changing extremely rapidly obey any universal laws?"" Nevertheless, theoretical work from Jergen Berges and Thomas Gasenzer's groups from Heidelberg University predicted exactly that. These notable predictions have now been verified twice at the same time -- at TU Wien and in Heidelberg.The quick and the slow directionThe experiment in Prof. Schmiedmayer's group at the Vienna Center for Quantum Science and Technology (VCQ) at the Institute of Atomic and Subatomic Physics (TU Wien) is using a very special atom trap. On an atom chip, thousands of rubidium atoms can be trapped and cooled using electromagnetic fields. ""In this process, we generate an atom cloud with a short and a long direction, similar to a cigar,"" explains Sebastian Erne, the lead author of the study.Initially, the atoms move in all directions at the same speed. The atom trap can however be opened in the short (transverse) directions, meaning that those atoms that are moving particularly fast in this direction fly away. This leaves behind only atoms that have a relatively low speed in the transverse directions.""The speed distribution in one direction is changed so quickly that during this time, the speed distribution in the other direction, along the longer axis of the cigar, virtually does not change at all,"" says Sebastian Erne. ""As a result, we produce a state that is far from the thermal equilibrium."" Collisions and interactions then lead to energy exchange between the atoms, which is referred to as 'thermalisation'.""Our experiment demonstrates that the course of this thermalisation follows a universal law and is not dependent on any details,"" says Jerg Schmiedmayer. ""Regardless of how we started the thermalisation, the transition can always be described with the same formula.""It was a similar story for the research team from Heidelberg. There too, they started out with an elongated atom cloud. However, the Heidelberg team did not study the speed but the spin (the intrinsic angular momentum) of the particles. They first controlled the spin directions of the atoms and then observed how these directions change over time due to interactions between the atoms.This change can be described using the same formulas as the one from the other experiment: ""In our case, the physical situation is quite different from that of the TU Wien experiment, but the dynamics also obey universal scaling laws,"" explains Maximilian Prefer (Heidelberg), first author of Heidelberg publication. ""We have found a process that also obeys the universality but belongs to a different universality class. This is great because it confirms our theories very convincingly and suggests that we really are on to something -- a new, fundamental law, "" says Markus Oberthaler (also Heidelberg).Learning from one system about othersUniversality upens up the possibility to obtain important information on quantum systems that are usually inaccessible in a laboratory. ""Nobody can recreate the Big Bang in a laboratory, but if we know the universality class to which it belongs, we can look at other quantum systems in the same class and indirectly investigate universal properties during the Big Bang,"" explains Schmiedmayer. ""Better understanding the behaviour of many-particle quantum systems that are far from equilibrium is one of the most pressing issues in physics today. Even with the best supercomputers, there's no chance of precisely calculating these events, and so our universality classes are a major opportunity to learn something new.""Story Source:Materials provided by Vienna University of Technology. ",0.0709,0.105808,0.0912297,0.147141,0.0910075,0.134449,0.142971,0,0
812,"Physicists create new, simpler-than-ever quantum 'hard drive for light' Physicists have developed a new technique for quantum memories, a crucial element for quantum communication","Physicists at the University of Alberta in Canada have developed a new way to build quantum memories, a method for storing delicate quantum information encoded into pulses of light.""We've developed a new way to store pulses of light -- down to the single-photon level -- in clouds of ultracold rubidium atoms, and to later retrieve them, on-demand, by shining a 'control' pulse of light,"" said Lindsay LeBlanc, assistant professor of physics and Canada Research Chair in Ultracold Gases for Quantum Simulation. LeBlanc conducted this research with postdoctoral fellow Erhan Saglamyurek.Quantum memories are an important component of quantum networks, serving much the same role as hard drives in today's computers. And the interest in storing quantum data efficiently and effectively is only growing, with practical applications including a quantum fibre-optic internet and other methods of secure communication.""This experiment involved taking short pulses of light, in which we could encode quantum information, storing the light in the atoms, and then retrieving the original pulse that carries the same information,"" explained Saglamyurek.The novel method developed by LeBlanc and Saglamyurek, which is best-suited for key applications requiring high-speed operation, also has considerably fewer technical requirements than required in common quantum storage techniques. ""The amount of power needed, for example, is significantly lower than current options, and these reduced requirements make it easier to implement in other labs,"" added Saglamyurek. This discovery will allow for the crucial scaling up of quantum technologies, which has proven the biggest challenge to date in the emerging field.Story Source:Materials provided by University of Alberta. ",0.118402,0.151087,0.139014,0.229901,0.180579,0.184793,0.176755,0,0
813,Precision measurement takes it to the limit,"Griffith University researchers have demonstrated a procedure for making precise measurements of speed, acceleration, material properties and even gravity waves possible, approaching the ultimate sensitivity allowed by laws of quantum physics.Published in Nature Communications, the work saw the Griffith team, led by Professor Geoff Pryde, working with photons (single particles of light) and using them to measure the extra distance travelled by the light beam, compared to its partner reference beam, as it went through the sample being measured -- a thin crystal.The researchers combined three techniques -- entanglement - (a kind of quantum connection that can exist between the photons), passing the beams back and forth along the measurement path, and a specially-designed detection technique.""Every time a photon passes through the sample, it makes a kind of mini-measurement. The total measurement is the combination of all of these mini-measurements,"" said Griffith's Dr Sergei Slussarenko, who oversaw the experiment. ""The more times the photons pass through, the more precise the measurement becomes.""Our scheme will serve as a blueprint for tools that can measure physical parameters with precision that is literally impossible to achieve with the common measurement devices.Lead author of the paper Dr Shakib Daryanoosh said this method can be used to investigate and measure other quantum systems.""These can be very fragile, and every probe photon we send it would disturb it. In this case, using few photons but in the most efficient way possible is critical and our scheme shows how do exactly that,"" he said.While one strategy is to just use as many photons as possible, that's not enough to reach the ultimate performance. For that, it is necessary to also extract the maximum amount of measurement information per photon pass, and that is what the Griffith experiment has achieved, coming far closer?to the so-called Heisenberg limit of precision than any comparable experiment.The remaining error is due experimental imperfection, as the scheme designed by Dr Daryanoosh and Professor Howard Wiseman, is capable of achieving the exact Heisenberg limit, in theory.""The really nice thing about this technique is that it works even when you don't have a good starting guess for the measurement,"" Prof. Wiseman said. ""Previous work has mostly focused a lot on the case where it's possible to make a very good starting approximation, but that's not always possible.""A few extra steps are required before this proof-of-principle demonstration can be harnessed outside the lab.Producing entangled photons is not simple with current technology, and this means it is still much easier to use many photons inefficiently, rather than each set of entangled photons in the best way possible.However, according to the team, the ideas behind this approach can find immediate applications in quantum computing algorithms and research in fundamental science.The scheme can ultimately be extended to a larger number of entangled photons, where the difference of the Heisenberg limit over the usually achievable limit is more significant.Story Source:Materials provided by Griffith University. ",0.0817324,0.104218,0.128962,0.180336,0.129289,0.137671,0.153697,0,0
814,Quantum on the edge: Light shines on new pathway for quantum technology World-first demonstration of topologically protected biphoton states,"Scientists in Australia have for the first time demonstrated the protection of correlated states between paired photons -- packets of light energy -- using the intriguing physical concept of topology. This experimental breakthrough opens a pathway to build a new type of quantum bit, the building blocks for quantum computers.The research, developed in close collaboration with Israeli colleagues, is published today in thee journal, Science, a recognition of the foundational importance of this work.""We can now propose a pathway to build robust entangled states for logic gates using protected pairs of photons,"" said lead author, Dr Andrea Blanco-Redondo at the University of Sydney Nano Institute.Logic gates are the switches needed to operate algorithms written for quantum computers. Classical computational switches are in simple binary forms of zero or one. Quantum switches exist in a state of 'superposition' that combine zero and one.Protecting quantum information long enough so that quantum machines can perform useful calculations is one of the biggest challenges in modern physics. Useful quantum computers will require millions or billions of qubits to process information. So far, the best experimental devices have about 20 qubits.To unleash the potential of quantum technology, scientists need to find a way to protect the entangled superposition of quantum bits -- or qubits -- at the nanoscale. Attempts to achieve this using superconductors and trapped ions have shown promise, but they are highly susceptible to electromagnetic interference, making them devilishly difficult to scale up into useful machines.The use of photons -- packets of light energy -- rather than electrons has been one proposed alternative upon which to build logic gates that can calculate quantum algorithms.Photons, unlike electrons, are well isolated from the thermal and electromagnetic environment. However, scaling quantum devices based on photonic qubits has been limited due to scattering loss and other errors; until now.""What we have done is develop a novel lattice structure of silicon nanowires, creating a particular symmetry that provides unusual robustness to the photons' correlation. The symmetry both helps create and guide these correlated states, known as 'edge modes',"" said Dr Blanco-Redondo, the Messel Research Fellow in the School of Physics.""This robustness stems from the underlying topology, a global property of the lattice that remains unchanged against disorder.""The correlation this produces is needed to build entangled states for quantum gates.Channels, or waveguides, made using silicon nanowires just 500 nanometres wide, were lined up in pairs with a deliberate defect in symmetry through the middle, creating two lattice structures with different topologies and an intervening 'edge'.This topology allows for the creation of special modes in which the photons can pair up -- called 'edge modes'. These modes allow information carried by the paired photons to be transported in a robust fashion that otherwise would have been scattered and lost across a uniform lattice.Dr Blanco-Redondo designed and performed the experiment in the Sydney Nanoscience Hub with Dr Bryn Bell, previously at the University of Sydney and now at the University of Oxford.The photons were created by high-intensity, ultra-short laser pulses, the same underlying technology for which Donna Strickland and Gerard Mourou were awarded the 2018 Nobel Prize in Physics.This research is the latest in the flourishing of discoveries in the past decade on topological states of matter. These topological features offer protection for classical and quantum information in fields as diverse as electromagnetism, condensed matter, acoustics and cold atoms.Microsoft Quantum Laboratories, including the one in Sydney, are pursuing the development of electron-based qubits where quantum information is topologically protected via the knotting of quasiparticles known as Majorana fermions. This is a bit like braiding half electron states induced through the interaction of superconductors and semiconducting metals.Topologically protected states have previously been demonstrated for single photons.However, Dr Blanco-Redondo said: ""Quantum information systems will rely on multiphoton states, highlighting the importance of this discovery for further development.""She said the next step will be to improve protection of the photon entanglement to create robust, scalable quantum logic gates.Professor Stephen Bartlett, a theoretical quantum physicist at Sydney Nano who is unconnected to the study, said: ""Dr Blanco-Redondo's result is exciting at a fundamental level because it shows the existence of protected modes attached to the boundary of a topologically ordered material.""What it means for quantum computing is unclear as it is still early days. But the hope is that the protection offered by these edge modes could be used to protect photons from the types of noise that are problematic for quantum applications.""Story Source:Materials provided by University of Sydney. ",0.0706253,0.125206,0.109152,0.184253,0.148384,0.114445,0.147658,0,0
815,Preparing quantum computers to leave the lab Scientists have 'tamed' some disruptive environmental effects on quantum computers,"A team of scientists, led by Professor Winfried Hensinger at the University of Sussex, has made a major breakthrough concerning one of the biggest problems facing quantum computing: how to reduce the disruptive effects of environmental ""noise"" on the highly sensitive function of a large-scale quantum computer.In the real-world, technological developments need to operate in imperfect conditions; what can be successfully tested in a highly controlled laboratory may fail when presented with realistic environmental factors, such as the fluctuations in voltage from an electronic component or stray electromagnetic fields emitted by everyday electronic equipment.The University of Sussex's Ion Quantum Technology Group have managed to dramatically reduce the effects of such environmental ""noise"" affecting trapped ion quantum computers, reporting their findings in an article that has today, Thursday 1 November 2018, been published in the journal Physical Review Letters. It means the team is one step closer to building a large-scale quantum computer with the capability to solve challenging real-world problems.Small-scale quantum computers currently in existence only contain a handful of quantum bits -- components of quantum computers that store information and can exist in multiple states, also referred to as qubits. As such, current quantum computers are small enough to be operated in a highly controlled environment inside a specialized laboratory. However, such machines do not have the processing power required to solve complex problems because of the limited number of qubits.When built, large-scale quantum computers will be able to solve certain problems that would take even the fastest super computers billions of years to calculate. In order to create a quantum computer that can solve such problems, scientists will need to increase the number of qubits, which in turn will increase the size of the quantum computer. The problem is that the more qubits that are added, the more difficult it becomes to isolate the computer from any realistic ""noise"" that would disrupt the computing processes.Hensinger's team of University of Sussex physicists have made a quantum computing breakthrough that is capable of mitigating some of these problems. They collaborated with theoretical scientist Dr Florian Mintert and colleagues from Imperial College London, who proposed a theory of how one might be able to solve this problem by manipulating the strange quantum effects in use inside a quantum computer. The theory allows -- making use of the strange properties of quantum physics -- the execution of quantum computations in such a way that changes in the initial operational parameters of the machine do not lead to a substantial change in the end result of the computation. This in turn helps to insulate the quantum computer from the effects of environmental 'noise'.Dr Sebastian Weidt, senior scientist in the Sussex Ion Quantum Technology Group, explains the significance: ""Realising this technique may have a profound impact on the ability to develop commercial ion trap quantum computers beyond use in an academic laboratory.""The Sussex team went to work to see whether they could actually implement this theory. They used complicated radio-frequency and microwave signals capable of manipulating the quantum effects inherent in individual charged atoms (ions), to demonstrate this in practical experiments. Their implementation is based on microwave technology, such as that present in mobile phones. Following months of intensive work in the laboratory, the Sussex scientists have managed to make this new method a reality, experimentally demonstrating its capabilities to substantially reduce the effect of ""noise"" on a trapped ion quantum computer.Prof Hensinger, Head of the Ion Quantum Technology Group at the University of Sussex -- which last year unveiled the first blueprint for a large-scale quantum computer -- says: ""With this advance we have made another practical step towards constructing quantum computers that can host millions of qubits. Such machines are capable of solving certain problems that even the fastest supercomputer may take billions of years to calculate and be of great benefit to humanity; they may be able to help us create new pharmaceuticals; find new cures for diseases, such as Dementia; create powerful tools for the financial sector; be of benefit to agriculture, through more efficient fertilizer production, among many other applications. We are only starting to understand the tremendous potential of these machines.""Hensinger's group is now utilising this new technique as they put the final touches to a powerful quantum computer prototype that is currently in their laboratory at the University of Sussex.Hensinger says: ""It's now time to translate academic achievements into the construction of practical machines. We're in a fantastic position to do this at Sussex and my team is working round the clock to make large-scale quantum computing a future reality.""Here is a video of Winfried Hensinger explaining the research: https://www.youtube.com/watch?v=C5FNbv7DTxEStory Source:Materials provided by University of Sussex. Original written by Anna Ford. ",0.06529,0.101757,0.104818,0.207912,0.11685,0.11392,0.140745,0,0
816,"Trapping atoms, not space ships, with tractor beams ","University of Adelaide researchers have delved into the realm of Star Wars and created a powerful tractor beam -- or light-driven energy trap -- for atoms.Rather than sucking space-ships into a space-station, this tractor beam pulls atoms into a microscopic hole at the centre of a special optical fibre. It is opening the way for new quantum experiments that may lead to new secure communications or advanced sensing technologies.Published in the journal Physical Review Applied, the researchers from the University's Institute for Photonics and Advanced Sensing (IPAS) say this is the first time that scientists have been able to demonstrate a highly efficient 'waveguide trap'.The PhD student who developed the technology, Ashby Hilton, says: ""Although tractor beams are green or blue in the movies, in this case the trap is made of invisible infra-red light. The beam grabs hold of atoms that are floating in a chamber that is almost completely emptied of gas -- a little sample of outer space on Earth.""Every atom that enters the tractor beam is pulled into the fibre -- there is no escape. And once sucked into the interior of the optical fibre the atoms can be held for long periods of time. Our experiments show that we can very precisely control light to produce exactly the right conditions to control atoms.""The tractor beam works by the infra-red light interacting with the atoms to create a change in energy which drives the atoms to the most intense part of the light beam.Lead researcher Dr Philip Light says: ""What is really exciting is that now we have the possibility to do quantum experiments on these trapped atoms. Our first experiments intend to use these stored atoms as elements of a quantum memory. We hope that our work may eventually form part of absolutely secure communications channel that is of obvious high interest to defence, intelligence and industry.""The researchers are now moving onto the next stage in which the tractor beam is formed from a hollow cone of light rather than a solid beam of light. In this new configuration the atoms will be held at the centre of the light cone where it is perfectly dark.""This is an extremely powerful idea -- we can move and manipulate the atoms, but are able to shield the atoms from the disruptive effect of intense light,"" says Dr Light. The researchers have essentially created a quantum funnel which allows them to guide and trap atoms for longer without disrupting their delicate quantum state.IPAS Director Professor Andre Luiten says: ""Our researchers are manipulating and measuring individual atoms and molecules to sense the world around us. This new era of quantum sensing is opening up diverse new possibilities from attempting to detect disease through finding particular molecules in the breath, to assisting miners and defence by detecting anomalous magnetic fields associated with mineral deposits or covert submarine activity.""Story Source:Materials provided by University of Adelaide. ",0.124363,0.169115,0.172012,0.188718,0.156035,0.17146,0.180985,0,0
817,Berkeley computer theorists show path to verifying that quantum beats classical Classical computers have a hard time understanding 'quantum accents',"As multiple research groups around the world race to build a scalable quantum computer, questions remain about how the achievement of quantum supremacy will be verified.Quantum supremacy is the term that describes a quantum computer's ability to solve a computational task that would be prohibitively difficult for any classical algorithm. It is considered a critical milestone in quantum computing, but because the very nature of quantum activity defies traditional corroboration, there have been parallel efforts to find a way to prove that quantum supremacy has been achieved.Researchers at the University of California, Berkeley, have just weighed in by giving a leading practical proposal known as random circuit sampling (RCS) a qualified seal of approval with the weight of complexity theoretic evidence behind it. Random circuit sampling is the technique Google has put forward to prove whether or not it has achieved quantum supremacy with a 72-qubit computer chip called Bristlecone, unveiled earlier this year.The UC Berkeley computer theorists published their proof of RCS as a verification method in a paper published Monday, Oct. 29, in the journal Nature Physics.""The need for strong evidence for quantum supremacy is underappreciated, but it's important to pin this down,"" said study principal investigator Umesh Vazirani, Roger A. Strauch Professor of Electrical Engineering and Computer Science at UC Berkeley. ""Besides being a milestone on the way to useful quantum computers, quantum supremacy is a new kind of physics experiment to test quantum mechanics in a new regime. The basic question that must be answered for any such experiment is how confident can we be that the observed behavior is truly quantum and could not have been replicated by classical means. That is what our results address.""The other investigators on this paper are Adam Bouland and Bill Fefferman, both postdoctoral research fellows, and Chinmay Nirkhe, a Ph.D. student, all in Vazirani's theoretical computing research group.Investment in quantum is heating upThe paper comes amid accelerated activity in government, academia and industry in quantum informational science. Congress is considering the National Quantum Initiative Act, and last month, the U.S. Department of Energy and the National Science Foundation announced nearly $250 million in grants to support research in quantum science and technologies.At the same time, the Lawrence Berkeley National Laboratory and UC Berkeley announced the formation of Berkeley Quantum, a partnership designed to accelerate and expand innovation in quantum information science.The stakes are high as international competition in quantum research heats up and the need for increasingly complex computations grows. With true quantum computing, problems that are impractical for even the fastest supercomputers to date could be relatively efficient to solve. It would be a game-changer in cryptography, simulations of molecular and chemical interactions and machine learning.Quantum computers are not confined by the traditional 0s and 1s of a traditional computer's bits. Instead, quantum bits, or qubits, can encode 0s, 1s and any quantum superposition of the two to create multiple states simultaneously.When Google unveiled Bristlecone, it said the empirical proof of its quantum supremacy would come through random circuit sampling, a technique in which the device would use random settings to behave like a random quantum circuit. To be convincing, there would also need to be strong evidence that there is no classical algorithm running on a classical computer that could simulate a random quantum circuit, at least in a reasonable amount of time.Detecting quantum accentsVazirani's team referred to an analogy between the output of the random quantum circuit and a string of random syllables in English: even if the syllables don't form coherent sentences or words, they will still possess an English ""accent"" and will be recognizably different from Greek or Sanskrit.They showed that producing a random output with a ""quantum accent"" is indeed hard for a classical computer through a technical complexity theoretic construct called ""worst-to-average-case reduction.""The next step was to verify that a quantum device was actually speaking with a quantum accent. This relies on the Goldilocks principle -- a 50-qubit machine is large enough to be powerful, but small enough to be simulated by a classical supercomputer. If it's possible to verify that a 50-qubit machine speaks with a quantum accent, then that would provide strong evidence that a 100-qubit machine, which would be prohibitively hard to simulate classically, would do so, as well.But even if a classical supercomputer were programmed to speak with a quantum accent, would it be able to recognize a native speaker? The only way to verify the output of the speaker is by a statistical test, said the Berkeley researchers. Google researchers are proposing to measure the degree of matching by a metric called ""cross-entropy difference."" A cross-entropy score of 1 would be an ideal match.The alleged quantum device may be regarded as behaving like an ideal quantum circuit with random noise added. Fefferman and Bouland say the cross-entropy score will certify the authenticity of the quantum accent provided the noise always adds entropy to the output. This is not always the case -- for example if the noise process preferentially erases 0s over 1s, it can actually reduce the entropy.""If Google's random circuits are generated by a process that allows such erasures, then the cross-entropy would not be a valid measure of quantum supremacy,"" said Bouland. ""That's partly why it will be very important for Google to pin down how its device deviates from a real random quantum circuit.""These results are an echo of work that Vazirani did in 1993 with his student Ethan Bernstein when they presented the first formal evidence that quantum computers would exponentially speed up certain computations. This opened the door to quantum algorithms by showing that quantum computers violate the Extended Church-Turing thesis, a foundational principle in computer science.Peter Shor of Bell Labs took their work one step further by showing that a very important practical problem, integer factorization, could be exponentially sped up by a quantum computer.""This sequence provides a template for the race to build working quantum computers,"" said Vazirani. ""Quantum supremacy is an experimental violation of the Extended Church-Turing thesis. Once that is achieved, the next challenge will be to design quantum computers that can solve practically useful problems.""The Army Research Office, National Science Foundation, Vannevar Bush Faculty Fellowship and the National Institute of Standards and Technology helped support this research.Story Source:Materials provided by University of California - Berkeley. Original written by Sarah Yang. ",0.0841848,0.120904,0.12709,0.278685,0.140145,0.145453,0.192304,0,0
818,Novel quantum dots enhance cell imaging,"A team of researchers from the University of Illinois at Urbana-Champaign and Mayo Clinic have engineered a new type of molecular probe that can measure and count RNA in cells and tissue without organic dyes. The probe is based on the conventional fluorescence in situ hybridization (FISH) technique, but it relies on compact quantum dots to illuminate molecules and diseased cells rather than fluorescent dyes.Over the last 50 years, FISH has evolved into a multi-billion-dollar industry because it effectively images and counts DNA and RNA in single cells. However, FISH has its limitations due to the delicate nature of the dyes. For example, the dyes rapidly deteriorate and are not very good at imaging in three dimensions. In addition, conventional FISH can only read out a couple of RNA or DNA sequences at a time.""By replacing dyes with quantum dots, there are no stability issues whatsoever and we can count numerous RNAs with higher fidelity than before,"" said Andrew Smith, an associate professor of Bioengineering and member of the research team. ""Moreover, we uncovered a fundamental limit to the size of a molecular label in cells, revealing new design rules for analysis in cells.""In their latest paper, published October 26, 2018, in the online edition of Nature Communications, Smith and his team identified an optimal size for quantum dots in order to effectively work with the FISH protocol. This discovery enabled quantum dot-based FISH to match the labeling accuracies currently obtained with organic dyes.The team created unique quantum dots that are made of a zinc, selenium, cadmium, and mercury alloy and are coated with polymers. ""The core of the dot dictates the wavelength of emission, and the shell dictates how much light will be given off,"" said Smith, who is also affiliated with the Micro + Nanotechnology Lab, Carle Illinois College of Medicine, and Department of Materials Science and Engineering at the University of Illinois.These dots can emit color independent of the size of the particle, which is not the case for conventional quantum dots. The dots are also small enough (7 nanometers) to fit on a probe that can maneuver between proteins and DNA in a cell, making them more comparable in size to the dyes used in conventional FISH probes.In experiments with HeLa cells and prostate cancer cells, the researchers found that dye-based FISH cell counts declined rapidly in minutes. The quantum dot-based FISH method provided long-term luminescence to allow counting of RNA for more than 10 minutes, making it possible to acquire 3D cell imaging.""This is important because images of cells and tissues are acquired slice-by-slice in sequence, so later slices that are labeled with dyes are depleted before they can be imaged,"" said Smith.This research is part of the Mayo Illinois Alliance in which engineers from Illinois work directly with clinicians and biologists from Mayo Clinic to solve outstanding medical challenges. The Mayo Clinic Biomarker Discovery Group is working to develop FISH-based diagnostics for tumor biopsies in order to improve the accuracy of cancer diagnosis, to select personalized treatments, and to improve prognoses. The QD-FISH methodology was developed to meet this need for biomarker panels requiring the analysis of numerous genetic changes in cells in tumors for which only small amounts of specimens may be available when acquired with a fine needle, as is often the case in prostate cancer.This research was funded by the Mayo-Illinois Alliance, National Institutes of Health, and the National Science Foundation. The full title of the paper is ""Enhanced mRNA FISH with Compact Quantum Dots.""In addition to Smith, the following Illinois Bioengineering researchers contributed to the work: Yang Liu, Phuong Le, Sung Jun Lim, Liang Ma, and Suresh Sarkar. Mayo Clinic collaborators include: Dr. Farhad Kosari, Stephen Murphy, John Cheville, and George Vasmatzis.Story Source:Materials provided by University of Illinois College of Engineering. Original written by Laura Schmitt. ",0.114971,0.195938,0.172586,0.233793,0.166951,0.164509,0.201136,0,0
819,Scalable platform for on-chip quantum emitters,"Household lightbulbs give off a chaotic torrent of energy, as trillions of miniscule light particles -- called photons -- reflect and scatter in all directions. Quantum light sources, on the other hand, are like light guns that fire single photons one by one, each time they are triggered, enabling them to carry hack-proof digital information -- technology attractive to industries such as finance and defense.Now, researchers at Stevens Institute of Technology and Columbia University have developed a scalable method for creating large numbers of these quantum light sources on a chip with unprecedented precision that not only could pave the way for the development of unbreakable cryptographic systems but also quantum computers that can perform complex calculations in seconds that would take normal computers years to finish.""The search for scalable quantum light sources has been going on for 20 years, and more recently has become a national priority,"" says Stefan Strauf, who led the work and is also director of Stevens' Nanophotonic Lab. ""This is the first time anyone has achieved a level of spatial control combined with high efficiency on a chip that is scalable, all of which are needed to realize quantum technologies.""The work, to be reported in the Oct. 29 advance online issue of Nature Nanotechnology, describes a new method for creating quantum light sources on demand in any desired location on a chip, by stretching an atom-thin film of semiconducting material over nanocubes made of gold. Like taut cling-wrap, the film stretches over the corners of the nanocubes, imprinting defined locations where single-photon emitters form.Past research has tested methods for producing quantum emitters in defined locations, but these designs were not scalable or efficient at triggering single photons frequently enough to be practically useful. Strauf and his team changed all that by becoming the first to combine spatial control and scalability with the ability to efficiently emit photons on demand.To achieve these capabilities, Strauf's team designed a unique approach where the gold nanocube serves a dual purpose: it imprints the quantum emitter on the chip and it acts as an antenna around it. By creating the quantum emitters in between the gold nanocube and mirror, Strauf left a five-nanometer narrow gap -- 20,000 times smaller than the width of a sheet of paper.""This tiny space between the mirror and nanocube creates an optical antenna that funnels all the photons into that five-nanometer gap, thereby concentrating all the energy"" says Strauf. ""Essentially, it provides the necessary boost for the single photons to be emitted rapidly from the defined location and in the desired direction.""To further improve the efficiency of the quantum light sources, Strauf teamed up with Katayun Barmak and James Hone, of Columbia University, who developed a technique for growing semiconductor crystals that are nearly free of defects. Using these unique crystals, Stevens' graduate student Yue Luo built rows of quantum emitters on a chip by stretching the atom-thin material over the nanocubes. The nanoantennas are formed by attaching the mirror, on the bottom side of the nanocube.The result: a record-high firing of 42 million single photons per second; in other words, every second trigger created a photon on demand, compared to only one in 100 triggers previously.Though tiny, the emitters are remarkably tough. ""They're astonishingly stable,"" Strauf says. ""We can cool them and warm them and disassemble the resonator and reassemble it, and they still work."" Most quantum emitters must be kept chilled to -273eC but the new technology works up to -70eC. ""We're not yet at room temperature,"" says Strauf, ""but current experiments show that it's feasible to get there.""Story Source:Materials provided by Stevens Institute of Technology. ",0.0812047,0.120984,0.108348,0.172836,0.128871,0.147108,0.153251,0,0
820,"Half moons and pinch points: Same physics, different energy ","When physicists send neutrons shooting through a frustrated magnet, the particles spray out the other side in signature patterns. The designs appear because, even at low temperatures, atoms in a frustrated metal oscillate in time with each other. One distinctive pattern, known as a ""pinch point,"" resembles a bow-tie and holds wide acclaim in the world of spin liquids. Pinch points are often accompanied by unsung crescent patterns called ""half moons,"" but the physics linking the phenomena has never been clarified.Now, researchers at the Okinawa Institute of Science and Technology Graduate University (OIST) have revealed that pinch points and half moons are one and the same -- simply signatures of the same physics at different energy levels. Their unified theory, published October 12, 2018, as a Physical Review B Rapid Communication, is the first to explain the underlying physics driving the often paired phenomena.""The theory itself is kind of simple,"" said Han Yan, a graduate student in the Theory of Quantum Matter Unit at OIST and first author of the study. ""From the same theory that gives you the pinch point at lower energy, you can calculate what happens at higher energy -- and you get a pair of half moons.""If you zoom in close to a frustrated magnet, each atom making up the material seems to spin erratically. In reality, however, these atoms take part in a beautifully coordinated dance, turning in time with each other so that their magnetic pulls ultimately cancel out. This ballet is difficult to observe directly, so instead, physicists search for telltale clues that the performance is taking place.An experimental technique called neutron scattering allows scientists to gather these clues. Neutrons carry no electric charge, but they do act as a localized source of magnetism. Individual atoms also act as tiny magnets, complete with their own north and south poles. When sent whizzing through a material, a neutron's speed and direction is thrown off by the atoms it passes, and thus it is ""scattered.""The pattern of the scattering tells physicists how atoms are behaving inside a material. For instance, if neutrons scatter helter-skelter, physicists infer that the atoms within a material are aligned randomly. If neutrons scatter in a hallmark bow-tie, they infer that the atoms are twirling in tandem, as they would in a frustrated magnet.Pinch points appear when an equal number of atomic magnets, or ""spins,"" are pointing ""out"" as pointing ""in"" in any region of the frustrated magnet. This equilibrium renders the material non-magnetic and maintains it at a minimal level of energy.Half moons appear when a frustrated magnet has energy beyond this minimal level, and thus violates the local conservation law which requires an equal number of spins be pointed out as in. In essence, half moons are pinch points set on a curve. The greater the curvature, the stronger the violation, the more energy the system is using. The OIST researchers uncovered this relationship in their calculations and later put it to the test.The researchers tested their unified theory in a simulated system where pinch points and half moons can be observed together, known as a Heisenberg antiferro-magnet on a kagome lattice. They also applied their equations to recent observations of the frustrated magnet Nd2Zr2O7 and found that their theory explained the appearance of the two patterns in application, as well.""Pinch points and half moons come from the same underlying physics -- one from respecting the local conservation law and the other from violating it,"" said Yan. ""When you put them together, they form a whole picture of the overall phenomenology.""In the future, the unified theory of half moons and pinch points should prove useful in both theoretical and applied physics, and perhaps beyond.""From a certain point of view, each condensed matter system is unto itself a different universe,"" said Yan. ""It's a great intellectual curiosity to find these universes, with their own strange laws of nature, but it also relates to daily life. People are trying to identify the particularly useful laws in these mini-universes so we might use them to our advantage.""Story Source:Materials provided by Okinawa Institute of Science and Technology (OIST) Graduate University. ",0.0881913,0.121545,0.112351,0.160568,0.103443,0.178622,0.139236,0,0
821,Watching nanoparticles: Researchers make movies of photochemistry in single nanoparticles,"When Michal Vadai's experiment worked for the first time, she jumped out of her seat.Vadai, a postdoctoral fellow at Stanford University, had spent months designing and troubleshooting a new tool that could greatly expand the capability of an advanced microscope at the Stanford Nano Shared Facilities. Despite heavy skepticism from the microscopy community, she and her fellow researchers were attempting a union between light microscopy and transmission electron microscopy that, if successful, would reveal a single particle undergoing a light-activated reaction.""I cannot stress how exciting it was to make it work the first time. It was a huge technological challenge,"" said Vadai, who is in the lab of Jennifer Dionne, associate professor of materials science and engineering. ""The first time we got the beginning of an experimental result, we were shouting out loud. It was very, very exciting that we could see and control what was happening to this nanoparticle with light.""This research, published Nov. 7 in Nature Communications, focuses on a photocatalytic reaction where energy from visible light initiates a chemical reaction in nanocubes of palladium. Each of these cubes is about 30 nanometers on each side -- roughly the size of a cold virus.Scientists know a lot about photocatalysis based on large groups of nanoparticles, but the new technique allows researchers to study what occurs in individual nanoparticles. Beyond photocatalysis, this technique could someday be used to study almost any interaction of light and matter with a resolution of about 2 nanometers, even those that occur in living cells.Carefully cradled nanocubesTransmission electron microscopy forms an image by beaming electrons through a thin slice of material. This process reveals structures in intricate detail but it doesn't allow scientists to observe materials as they change under different light conditions, like light receptors in the eye, materials used in solar cells or, as in this case, palladium nanocubes for catalysis. The new setup melds the resolution of electron microscopy with the color of light microscopy.""One of the biggest achievements of this paper is the technique itself,"" Dionne said. ""We bring light of various 'colors' to the electron microscope. Our measurements are direct -- one can visibly see the photochemical reaction as it unfolds within the nanoparticle.""The new technique involved a custom-designed specimen holder to which the sample was loaded. Surrounding that were mirrors to focus the light from two optical fibers with a gap for the electron beam. The entire design had to fit in a very limited space: a 5 mm gap in the microscope.To test the setup, the researchers pumped the specimen's chamber with hydrogen. Looking through the electron microscope, they could confirm the palladium nanocube changed its phase as it filled up with hydrogen. The experiment was structured so that the nanocubes would remain in this hydrogen-filled phase until the researchers turned on the light. Once illuminated, they watched as a water-like wave flowed gracefully across the particle -- the hydrogen leaving the palladium. It was a light-based reaction seen by an electron microscope and a jump-for-joy-worthy success.Individual differencesNanoparticles are often produced and studied in large numbers, which means we know they vary in size, shape or position -- but we know little about how those variations affect performance.""If you really want to dive down to the fundamental physics of what's happening, you need to look at single particles because we know individual differences matter,"" said Vadai. ""It's like a mystery and you have to get a good look at one clue to be able to solve that.""The initial experiments were largely designed to show that the technique could work, but still revealed something new about the nanocubes. For one, the reaction happens 10 times faster in the light than in the dark. The researchers could also see how each step of the reaction -- the hydrogen leaving the nanocube, the lattice structure of the nanocube rearranging -- is affected by different wavelengths of light.It also turns out that the corner of the nanocube closest to the light absorbing region -- in this case, a nearby gold disk -- is most reactive. Understanding how and why that happened could potentially allow for product-selective reactions based on the catalyst geometry.With the success of this proof of concept, the lab is on to the next steps. For example, the researchers aim to add spectroscopy capabilities, which means they could evaluate the light generated from these reactions in order to analyze the chemistry in greater detail.""If you're talking about a single particle, you usually have to fight to see these weak signals,"" Vadai said. ""Looking forward, this will be a complete suite of tools that you can use to study interaction of light and matter in the nanoscale in real time, at very high resolution, at a single-particle level.""Co-authors of the paper are Daniel K. Angell, Fariah Hayee and Katherine Sytwu of Stanford. Dionne is also a member of Stanford Bio-X, an affiliate of the Precourt Institute for Energy and a member of the Wu Tsai Neurosciences Institute at Stanford.This research was funded by Chi-Chang Kao at SLAC National Accelerator Laboratory, a postdoctoral fellowship from the TomKat Center for Sustainable Energy at Stanford University, the Gabilan Stanford Graduate Fellowship and the National Science Foundation.Story Source:Materials provided by Stanford University. Original written by Taylor Kubota. ",0.0601459,0.1336,0.110023,0.136917,0.106975,0.104047,0.128574,0,0
822,Laser blasting antimatter into existence Scientists hit trapped electrons with gamma-ray beams to generate a cascade of matter-antimatter pairs,"Antimatter is an exotic material that vaporizes when it contacts regular matter. If you hit an antimatter baseball with a bat made of regular matter, it would explode in a burst of light. It is rare to find antimatter on Earth, but it is believed to exist in the furthest reaches of the universe. Amazingly, antimatter can be created out of thin air -- scientists can create blasts of matter and antimatter simultaneously using light that is extremely energetic.How do scientists make antimatter? When electrons, negatively charged subatomic particles, move back and forth they give off light. If they move very fast, they give off a lot of light. A great way to make them move back and forth is to blast them with powerful laser pulses. The electrons become almost as fast as light, and they generate beams of gamma-rays. Gamma-rays are like X-rays, such as those at doctor's offices or airport security lines, but are much smaller and have even more energy. The light beam is very sharp, about the thickness of a sewing needle even a few feet away from its source.When gamma-rays made by electrons run into each other, they can create matter-antimatter pairs -- an electron and a positron. Now, scientists have developed a new trick to create these matter-antimatter pairs even more efficiently.""We developed an 'optical trap' which keeps the electrons from moving too far after they emit gamma-rays,"" said Marija Vranic from the University of Lisbon, who will be presenting her work at the American Physical Society Division of Plasma Physics meeting in Portland, Ore. ""They get trapped where they can be hit again by the powerful laser pulses. This generates more gamma-rays, which creates even more pairs of particles.""This process repeats, and the number of pairs grows very fast in what is called a ""cascade."" The process continues until the particles that have been created are very dense.Cascades are thought to occur naturally in faraway corners of the universe. For example, rapidly rotating neutron stars called pulsars have extremely strong magnetic fields, a trillion times stronger than the magnetic fields on Earth, that can produce cascades.Studying cascades in the laboratory could shed light on mysteries related to astrophysical plasmas in extreme conditions. These beams can also have industrial and medical applications for non-invasive high-contrast imaging. Further research is necessary to make the sources cheaper and more efficient, so that they can become widely available.Story Source:Materials provided by American Physical Society. ",0.153778,0.159665,0.177354,0.177897,0.18479,0.218246,0.17986,0,0
823,Breakthrough opens door to smartphone-powered $100 ultrasound machine,"Engineers at the University of British Columbia have developed a new ultrasound transducer, or probe, that could dramatically lower the cost of ultrasound scanners to as little as $100. Their patent-pending innovation -- no bigger than a Band-Aid -- is portable, wearable and can be powered by a smartphone.Conventional ultrasound scanners use piezoelectric crystals to create images of the inside of the body and send them to a computer to create sonograms. Researchers replaced the piezoelectric crystals with tiny vibrating drums made of polymer resin, called polyCMUTs (polymer capacitive micro-machined ultrasound transducers), which are cheaper to manufacture.""Transducer drums have typically been made out of rigid silicon materials that require costly, environment-controlled manufacturing processes, and this has hampered their use in ultrasound,"" said study lead author Carlos Gerardo, a PhD candidate in electrical and computer engineering at UBC. ""By using polymer resin, we were able to produce polyCMUTs in fewer fabrication steps, using a minimum amount of equipment, resulting in significant cost savings.""Sonograms produced by the UBC device were as sharp as or even more detailed than traditional sonograms produced by piezoelectric transducers, said co-author Edmond Cretu, professor of electrical and computer engineering.""Since our transducer needs just 10 volts to operate, it can be powered by a smartphone, making it suitable for use in remote or low-power locations,"" he added. ""And unlike rigid ultrasound probes, our transducer has the potential to be built into a flexible material that can be wrapped around the body for easier scanning and more detailed views -- without dramatically increasing costs.""Co-author Robert Rohling, also a professor of electrical and computer engineering, said the next step in the research is to develop a wide range of prototypes and eventually test their device in clinical applications.""You could miniaturize these transducers and use them to look inside your arteries and veins. You could stick them on your chest and do live continuous monitoring of your heart in your daily life. It opens up so many different possibilities,"" said Rohling.Story Source:Materials provided by University of British Columbia. ",0.123789,0.171793,0.161013,0.209006,0.182015,0.187256,0.182931,0,0
824,Cell-like nanorobots clear bacteria and toxins from blood,"Engineers at the University of California San Diego have developed tiny ultrasound-powered robots that can swim through blood, removing harmful bacteria along with the toxins they produce. These proof-of-concept nanorobots could one day offer a safe and efficient way to detoxify and decontaminate biological fluids.Researchers built the nanorobots by coating gold nanowires with a hybrid of platelet and red blood cell membranes. This hybrid cell membrane coating allows the nanorobots to perform the tasks of two different cells at once -- platelets, which bind pathogens like MRSA bacteria (an antibiotic-resistant strain of Staphylococcus aureus), and red blood cells, which absorb and neutralize the toxins produced by these bacteria. The gold body of the nanorobots responds to ultrasound, which gives them the ability to swim around rapidly without chemical fuel. This mobility helps the nanorobots efficiently mix with their targets (bacteria and toxins) in blood and speed up detoxification.The work, published May 30 in Science Robotics, combines technologies pioneered by Joseph Wang and Liangfang Zhang, professors in the Department of NanoEngineering at the UC San Diego Jacobs School of Engineering. Wang's team developed the ultrasound-powered nanorobots, and Zhang's team invented the technology to coat nanoparticles in natural cell membranes.""By integrating natural cell coatings onto synthetic nanomachines, we can impart new capabilities on tiny robots such as removal of pathogens and toxins from the body and from other matrices,"" said Wang. ""This is a proof-of-concept platform for diverse therapeutic and biodetoxification applications.""""The idea is to create multifunctional nanorobots that can perform as many different tasks at once,"" said co-first author Berta Esteban-Fernendez de evila, a postdoctoral scholar in Wang's research group at UC San Diego. ""Combining platelet and red blood cell membranes into each nanorobot coating is synergistic -- platelets target bacteria, while red blood cells target and neutralize the toxins those bacteria produce.""The coating also protects the nanorobots from a process known as biofouling -- when proteins collect onto the surface of foreign objects and prevent them from operating normally.Researchers created the hybrid coating by first separating entire membranes from platelets and red blood cells. They then applied high-frequency sound waves to fuse the membranes together. Since the membranes were taken from actual cells, they contain all their original cell surface protein functions. To make the nanorobots, researchers coated the hybrid membranes onto gold nanowires using specific surface chemistry.The nanorobots are about 25 times smaller than the width of a human hair. They can travel up to 35 micrometers per second in blood when powered by ultrasound. In tests, researchers used the nanorobots to treat blood samples contaminated with MRSA and their toxins. After five minutes, these blood samples had three times less bacteria and toxins than untreated samples.The work is still at an early stage. Researchers note that the ultimate goal is not to use the nanorobots specifically for treating MRSA infections, but more generally for detoxifying biological fluids. Future work includes tests in live animals. The team is also working on making nanorobots out of biodegradable materials instead of gold.Story Source:Materials provided by University of California - San Diego. ",0.131371,0.224825,0.206465,0.229551,0.192122,0.172881,0.238866,0,0
825,Scientists design bacteria to reflect 'sonar' signals for ultrasound imaging New technology may one day allow doctors to image therapeutic bacterial cells in patients,"In the 1966 science fiction film Fantastic Voyage, a submarine is shrunken down and injected into a scientist's body to repair a blood clot in his brain. While the movie may be still be fiction, researchers at Caltech are making strides in this direction: they have, for the first time, created bacterial cells with the ability to reflect sound waves, reminiscent of how submarines reflect sonar to reveal their locations.The ultimate goal is to be able to inject therapeutic bacteria into a patient's body -- for example, as probiotics to help treat diseases of the gut or as targeted tumor treatments -- and then use ultrasound machines to hit the engineered bacteria with sound waves to generate images that reveal the locations of the microbes. The pictures would let doctors know if the treatments made it to the right place in the body and were working properly.""We are engineering the bacterial cells so they can bounce sound waves back to us and let us know their location the way a ship or submarine scatters sonar when another ship is looking for it,"" says Mikhail Shapiro, assistant professor of chemical engineering, Schlinger Scholar, and Heritage Medical Research Institute Investigator. ""We want to be able to ask the bacteria, 'Where are you and how are you doing?' The first step is to learn to visualize and locate the cells, and the next step is to communicate with them.""The results will be published in the January 4 issue of the journal Nature. The lead author is Raymond Bourdeau, a former postdoctoral scholar in Shapiro's lab.The idea of using bacteria as medicine is not new. Probiotics have been developed to treat conditions of the gut, such as irritable bowel disease, and some early studies have shown that bacteria can be used to target and destroy cancer cells. But visualizing these bacterial cells as well as communicating with them -- both to gather intel on what's happening in the body and give the bacteria instructions about what to do next -- is not yet possible. Imaging techniques that rely on light -- such as taking pictures of cells tagged with a ""reporter gene"" that codes for green fluorescent protein -- only work in tissue samples removed from the body. This is because light cannot penetrate into deeper tissues like the gut, where the bacterial cells would reside.Shapiro wants to solve this problem with ultrasound techniques because sound waves can travel deeper into bodies. He says he had a eureka moment about six years ago when he learned about gas-filled protein structures in water-dwelling bacteria that help regulate the organisms' buoyancy. Shapiro hypothesized that these structures, called gas vesicles, could bounce back sound waves in ways that make them distinguishable from other types of cells. Indeed, Shapiro and his colleagues demonstrated that the gas vesicles can be imaged with ultrasound in the guts and other tissues of mice.The team's next goal was to transfer the genes for making gas vesicles from the water-dwelling bacteria into a different type of bacteria -- Escherichia coli, which is commonly used in microbial therapeutics, such as probiotics.""We wanted to teach the E. coli bacteria to make the gas vesicles themselves,"" says Shapiro. ""I've been wanting to do this ever since we realized the potential of gas vesicles, but we hit some roadblocks along the way. When we finally got the system to work, we were ecstatic.""One of the challenges the team hit involved the transfer of the genetic machinery for gas vesicles into E. coli. They first tried to transfer gas-vesicle genes isolated from a water-dwelling bacterium called Anabaena flos-aquae, but this didn't work -- the E. coli failed to make the vesicles. They tried again using gas-vesicle genes from a closer relative of E. coli, a bacterium called Bacillus megaterium. This didn't succeed either, because the resulting gas vesicles were too small to efficiently scatter sound waves. Finally, the team tried a mix of genes from both species -- and it worked. The E. coli made gas vesicles on their own.The gas vesicle genes code for proteins that act like either bricks or cranes in building the final vesicle structure -- some of the proteins are the building blocks of the vesicles while some help in actually assembling the structures. ""Essentially, we figured out that we need the bricks from Anabaena flos-aquae and the cranes from Bacillus megaterium in order for the E. coli to be able to make gas vesicles,"" says Bourdeau.Subsequent experiments from the team demonstrated that the engineered E. coli could indeed be imaged and located within the guts of mice using ultrasound.""This is the first acoustic reporter gene for use in ultrasound imaging,"" says Shapiro. ""We hope it will ultimately do for ultrasound what green fluorescent protein has done for light-based imaging techniques, which is to really revolutionize the imaging of cells in ways there were not possible before.""The researchers say the technology should be available soon to scientists who do research in animals, although it will take many more years to develop the method for use in humans.The Nature study, titled ""Acoustic reporter genes for noninvasive imaging of microbes in mammalian hosts,"" was funded by the National Institutes of Health, the Canadian Institute of Health Research, the Burroughs Wellcome Fund, the Packard Fellowship, the Pew Scholarship, the Heritage Medical Research Institute, the National Science Foundation, and the Natural Sciences and Engineering Research Council of Canada. Other Caltech authors include research technician Audrey Lee Gosselin, graduate students Anupama Lakshmanan, Arash Farhadi, Sripriya Ravindra Kumar, and former undergraduate student Suchita Nety (BS '17).Story Source:Materials provided by California Institute of Technology. Original written by Whitney Clavin. ",0.0856691,0.136404,0.164417,0.193404,0.128678,0.130511,0.166493,0,0
826,Revealing hidden information in sound waves,"By essentially turning down the pitch of sound waves, University of Michigan engineering researchers have devised a way to unlock greater amounts of data from acoustic fields than ever before.That additional information could boost performance of passive sonar and echolocation systems for detecting and tracking adversaries in the ocean, medical imaging devices, seismic surveying systems for locating oil and mineral deposits, and possibly radar systems as well.""Acoustic fields are unexpectedly richer in information than is typically thought,"" said David Dowling, a professor in U-M's Department of Mechanical Engineering.He likens his approach to solving the problem of human sensory overload.Sitting in a room with your eyes closed, you would have little trouble locating someone speaking to you at normal volume without looking. Speech frequencies are right in the comfort zone for human hearing.Now, imagine yourself in the same room when a smoke alarm goes off. That annoying screech is generated by sound waves at higher frequencies, and in the midst of them, it would be difficult for you to locate the source of the screech without opening your eyes for additional sensory information. The higher frequency of the smoke alarm sound creates directional confusion for the human ear.""The techniques my students and I have developed will allow just about any signal to be shifted to a frequency range where you're no longer confused,"" said Dowling, whose research is primarily funded by the U.S. Navy.Navy sonar arrays on submarines and surface ships deal with a similar kind of confusion as they search for vessels on the ocean surface and below the waves. The ability to detect and locate enemy ships at sea is a crucial task for naval vessels.Sonar arrays are typically designed to record sounds in specific frequency ranges. Sounds with frequencies higher than an array's intended range may confuse the system; it might be able to detect the presence of an important contact but still be unable to locate it.Any time sound is recorded, a microphone takes the role of the human ear, sensing sound amplitude as it in varies in time. Through a mathematical calculation known as a Fourier transform, sound amplitude versus time can be converted to sound amplitude versus frequency.With the recorded sound translated into frequencies, Dowling puts his technique to use. He mathematically combines any two frequencies within the signal's recorded frequency range, to reveal information outside that range at a new, third frequency that is the sum or difference of the two input frequencies.""This information at the third frequency is something that we haven't traditionally had before,"" he said.In the case of a Navy vessel's sonar array, that additional information could allow an adversary's ship or underwater asset to be reliably located from farther away or with recording equipment that was not designed to receive the recorded signal. In particular, tracking the distance and depth of an adversary from hundreds of miles away -- far beyond the horizon -- might be possible.And what's good for the Navy may also be good for medical professionals investigating areas of the body that are hardest to reach, such as inside the skull. Similarly, remote seismic surveys that parse through the earth seeking oil or mineral deposits could also be improved.""The science that goes into biomedical ultrasound and the science that goes into Navy sonar are nearly identical,"" Dowling said. ""The waves that I study are scalar, or longitudinal, waves. Electromagnetic waves are transverse, but those follow similar equations. Also, seismic waves can be both transverse and longitudinal, but again they follow similar equations.""There's a lot of potential scientific common ground, and room to expand these ideas.""Story Source:Materials provided by University of Michigan. ",0.101346,0.0819416,0.121595,0.14579,0.133706,0.140719,0.12819,0,0
827,Ultrasound releases drug to alter activity in targeted brain areas in rats,"Stanford University School of Medicine scientists have developed a noninvasive way of delivering drugs to within a few millimeters of a desired point in the brain.The method, tested in rats, uses focused ultrasound to jiggle drug molecules loose from nanoparticle ""cages"" that have been injected into the bloodstream.In a proof-of-principle study, the researchers showed that pharmacologically active amounts of a fast-acting drug could be released from these cages in small areas of the rats' brains targeted by a beam of focused ultrasound. The drug went to work immediately, reducing neural activity in the targeted area -- but only while the ultrasound device was active and only where the ultrasound intensity exceeded a certain threshold. By modifying the strength and duration of the beam, the investigators could fine-tune the neural inhibition.While the drug used in this study was propofol, an anesthetic commonly used in surgery, in principle the same approach could work for many drugs with widely differing pharmacological actions and psychiatric applications, and even for some chemotherapeutic drugs used to combat cancer.By turning up the ultrasound intensity and monitoring brainwide metabolic activity, the researchers could also observe the drug's secondary effects on distant downstream brain regions receiving input from the targeted area, said Raag Airan, MD, PhD, an assistant professor of neuroradiology. In this way, the researchers were able to noninvasively map out the connections among disparate circuits in the living brain.A paper describing the study's findings will be published online Nov. 7 in Neuron. Airan is the senior author. Lead authorship is shared by Jeffrey Wang, a student in the MD-PhD program, and postdoctoral scholar Muna Aryal, PhD.A kindred technology known as optogenetics, pioneered by Karl Deisseroth, MD, PhD, a Stanford professor of bioengineering and of psychiatry and behavioral sciences under whom Airan completed his PhD work a decade ago, uses invasive gene delivery to render specified classes of nerve cells vulnerable to precise experimental manipulation. Airan's approach employs noninvasive pharmacological methods to achieve similar control of neural activity.""This important work establishes that ultrasonic drug uncaging appears to have the required precision to tune the brain's activity via targeted drug application,"" said Deisseroth, who wasn't involved in the study. ""The powerful new technique could be used to test optogenetically inspired ideas, derived initially from rodent studies, in large animals -- and perhaps soon in clinical trials.""'We're optimistic'The new technology could not only speed advances in neuroscientific research but move rapidly into clinical practice, Airan said. ""While this study was done in rats, each component of our nanoparticle complex has been approved for at least investigational human use by the Food and Drug Administration, and focused ultrasound is commonly employed in clinical procedures at Stanford,"" he said. ""So, we're optimistic about this procedure's translational potential.""Harmless at the low intensities routinely used for imaging bodily tissues, high-intensity focused ultrasound is approved for the ablation, or deliberate destruction, of certain tissues, including portions of a central brain structure called the thalamus to treat the condition known as essential tremor.For the new study, ""we turned down the dials"" on the ultrasound device, Airan said. The intensity of the ultrasound used in these experiments was about 1/10th to 1/100th of the intensity used in clinical ablation procedures. The ultrasound in these experiments was delivered in a series of short staccato pulses separated by periods of rest, giving the targeted brain tissue plenty of time to cool off between pulses. Rats exposed numerous times to the experimental protocol showed no evidence of tissue damage from it.The nanoparticles, which Airan has been perfecting for several years, are biocompatible, biodegradable, liquid-filled spheres averaging 400 nanometers (about 15-millionths of an inch) in diameter. Their surfaces consist of a copolymer matrix in which the drug of choice is encaged. Roughly 3 million molecules of a drug typically dot the surface of one of these nanoparticles.Each nanoparticle encloses a droplet of a substance called perfluorocarbon. Buffeted by ultrasound waves at the right frequency, these liquid cores begin shaking and expanding until the copolymer matrix coating the surface ruptures, setting the trapped drug molecules free. Propofol, like all psychoactive drugs, easily diffuses through the otherwise formidable blood-brain barrier. But having crossed this barrier, the drug is quickly soaked up by brain tissue, so that it never gets farther than about a half-millimeter from the capillary where it's been released.Airan and his colleagues injected these particles intravenously into experimental rats and explored focused ultrasound's potential for targeted drug delivery.Initially, they measured nerve cells' activity in the visual cortex, an area in the back of the brain that's activated by visual stimuli, in response to flashes of light aimed at the rats' eyes. Focusing the ultrasound beam on that brain area, they watched electrical activity there plunge while the beam was being transmitted, then recover within about 10 seconds after the device was shut off. This drop-off in the visual cortex's electrical activity, which is what you'd expect from the release of an anesthetic there, grew more pronounced with increasing ultrasound intensity, and didn't occur at all when the rats had been injected instead with drug-free nanoparticles.In contrast, activity in the motor cortex, a brain area not involved in vision, in response to light flashes directed at the rats' eyes was not diminished when ultrasound was applied there. But ultrasound targeting the lateral geniculate nucleus, a brain area that relays visual information to the visual cortex, did reduce electrical activity in the visual cortex. This showed that propofol release in one brain structure can produce secondary effects in another, distant region receiving inputs from that structure.Brainwide metabolic responseNext, Airan's team monitored the brainwide metabolic response to focused ultrasound by using positron emission tomography to measure brainwide uptake of a radioactive analog of glucose -- glucose is the brain's chief energy source -- in the rats. When the injected nanoparticles were blanks, there was no effect in ultrasound-exposed areas. But with propofol-loaded nanoparticles, the metabolism dropped, meaning there was reduced neural activity in these ultrasound-exposed regions. This inhibition increased with increasing ultrasound intensity. Cranking the ultrasound level high enough also triggered selectively diminished activity in distant brain regions known to receive inputs from the ultrasound-exposed area.""We hope to use this technology to noninvasively predict the results of excising or inactivating a particular small volume of brain tissue in patients slated for neurosurgery,"" said Airan. ""Will inactivating or removing that small piece of tissue achieve the desired effect -- for example, stopping epileptic seizure activity? Will it cause any unexpected side effects?""Other study co-authors are postdoctoral scholar Qian Zhong, PhD, and medical student Daivik Vyas.The work was funded by the National Institutes of Health (grants RF1MH114252 and U54CA199075), the Stanford Center for Cancer Nanotechnology Excellence, the Foundation of the American Society for Neuroradiology, the Wallace H. Coulter Foundation, the Dana Foundation and the Wu Tsai Neurosciences Institute.Stanford's Office of Technology Licensing has filed patent applications on intellectual property associated with the new technology.Stanford's Department of Radiology also supported the work.Story Source:Materials provided by Stanford Medicine. Original written by Bruce Goldman. ",0.0933964,0.154896,0.153851,0.231345,0.141615,0.146022,0.191598,0,0
828,Wearable ultrasound patch monitors blood pressure deep inside body,"A new wearable ultrasound patch that non-invasively monitors blood pressure in arteries deep beneath the skin could help people detect cardiovascular problems earlier on and with greater precision. In tests, the patch performed as well as some clinical methods to measure blood pressure.Applications include real-time, continuous monitoring of blood pressure changes in patients with heart or lung disease, as well as patients who are critically ill or undergoing surgery. The patch uses ultrasound, so it could potentially be used to non-invasively track other vital signs and physiological signals from places deep inside the body.A team of researchers led by the University of California San Diego describe their work in a paper published Sept. 11 in Nature Biomedical Engineering.""Wearable devices have so far been limited to sensing signals either on the surface of the skin or right beneath it. But this is like seeing just the tip of the iceberg,"" said Sheng Xu, a professor of nanoengineering at the UC San Diego Jacobs School of Engineering and the corresponding author of the study. ""By integrating ultrasound technology into wearables, we can start to capture a whole lot of other signals, biological events and activities going on way below the surface in a non-invasive manner.""""We are adding a third dimension to the sensing range of wearable electronics,"" said Xu, who is also affiliated with the Center for Wearable Sensors at UC San Diego.The new ultrasound patch can continuously monitor central blood pressure in major arteries as deep as four centimeters (more than one inch) below the skin.Physicians involved with the study say the technology would be useful in various inpatient procedures.""This has the potential to be a great addition to cardiovascular medicine,"" said Dr. Brady Huang, a co-author on the paper and radiologist at UC San Diego Health. ""In the operating room, especially in complex cardiopulmonary procedures, accurate real-time assessment of central blood pressure is needed -- this is where this device has the potential to supplant traditional methods.""A convenient alternative to clinical methodsThe device measures central blood pressure -- which differs from the blood pressure that's measured with an inflatable cuff strapped around the upper arm, known as peripheral blood pressure. Central blood pressure is the pressure in the central blood vessels, which send blood directly from the heart to other major organs throughout the body. Medical experts consider central blood pressure more accurate than peripheral blood pressure and also say it's better at predicting heart disease.Measuring central blood pressure isn't typically done in routine exams, however. The state-of-the-art clinical method is invasive, involving a catheter inserted into a blood vessel in a patient's arm, groin or neck and guiding it to the heart.A non-invasive method exists, but it can't consistently produce accurate readings. It involves holding a pen-like probe, called a tonometer, on the skin directly above a major blood vessel. To get a good reading, the tonometer must be held steady, at just the right angle and with the right amount of pressure each time. But this can vary between tests and different technicians.""It's highly operator-dependent. Even with the proper technique, if you move the tonometer tip just a millimeter off, the data get distorted. And if you push the tonometer down too hard, it'll put too much pressure on the vessel, which also affects the data,"" said co-first author Chonghe Wang, a nanoengineering graduate student at UC San Diego. Tonometers also require the patient to sit still -- which makes continuous monitoring difficult -- and are not sensitive enough to get good readings through fatty tissue.The UC San Diego-led team has developed a convenient alternative -- a soft, stretchy ultrasound patch that can be worn on the skin and provide accurate, precise readings of central blood pressure each time, even while the user is moving. And it can still get a good reading through fatty tissue.The patch was tested on a male subject, who wore it on the forearm, wrist, neck and foot. Tests were performed both while the subject was stationary and during exercise. Recordings collected with the patch were more consistent and precise than recordings from a commercial tonometer. The patch recordings were also comparable to those collected with a traditional ultrasound probe.Making ultrasound wearable""A major advance of this work is it transforms ultrasound technology into a wearable platform. This is important because now we can start to do continuous, non-invasive monitoring of major blood vessels deep underneath the skin, not just in shallow tissues,"" said Wang.The patch is a thin sheet of silicone elastomer patterned with what's called an ""island-bridge"" structure -- an array of small electronic parts (islands) that are each connected by spring-shaped wires (bridges). Each island contains electrodes and devices called piezoelectric transducers, which produce ultrasound waves when electricity passes through them. The bridges connecting them are made of thin, spring-like copper wires. The island-bridge structure allows the entire patch to conform to the skin and stretch, bend and twist without compromising electronic function.The patch uses ultrasound waves to continuously record the diameter of a pulsing blood vessel located as deep as four centimeters below the skin. This information then gets translated into a waveform using customized software. Each peak, valley and notch in the waveform, as well as the overall shape of the waveform, represents a specific activity or event in the heart. These signals provide a lot of detailed information to doctors assessing a patient's cardiovascular health. They can be used to predict heart failure, determine if blood supply is fine, etc.Next stepsResearchers note that the patch still has a long way to go before it reaches the clinic. Improvements include integrating a power source, data processing units and wireless communication capability into the patch.""Right now, these capabilities have to be delivered by wires from external devices. If we want to move this from benchtop to bedside, we need to put all these components on board,"" said Xu.The team is looking to collaborate with experts in data processing and wireless technologies for the next phase of the project.Story Source:Materials provided by University of California - San Diego. ",0.0862312,0.107908,0.144939,0.180539,0.131468,0.132653,0.161764,0,0
829,"Novel biomedical imaging system System combines optical, ultrasound technology","Purdue University researchers are developing a novel biomedical imaging system that combines optical and ultrasound technology to improve diagnosis of life-threatening diseases.Photoacoustic tomography is a noninvasive technique that works by converting absorbed optical energy into acoustic signal. Pulsed light is sent into body tissue, creating a small increase in temperature that causes the tissue to expand and create an acoustic response that can be detected by an ultrasound transducer. The ultrasound data is used to visualize the tissue.""The nice thing about photoacoustic tomography is the compositional information,"" said Craig Goergen, an assistant professor in Purdue's Weldon School of Biomedical Engineering. ""It provides information about where blood and lipid are located, along with other essential information.""The ultimate goal is to enhance the clinical care of patients.The results of a study describing an adjustable photoacoustic probe with improved light delivery and image quality were published Tuesday (Aug. 28) in the journal Photoacoustics.The system provides real-time compositional information of body tissue without the need for contrast agents and with better depth penetration compared with conventional optical techniques.Photoacoustic tomography can be used to detect or monitor a myriad of diseases, including cardiovascular disease, diabetes, and cancer. Those are diseases that the Centers for Disease Control and Prevention lists as among the most common, costly, and preventable of all health problems. Heart disease and cancer each account for one in every four deaths a year in the United States, and more than 30 million Americans, or more than 9 percent of the population, have diabetes. The cost of those three diseases a year in the United States is more than $718 billion a year, according to the CDC.""That means there will be a great need for medical imaging. Trying to diagnose these diseases at an earlier time can lead to improved patient care,"" Goergen said. ""We are in the process now of trying to use this enhanced imaging approach to a variety of different applications to see what it can be used for.""Among other potential uses for photoacoustic tomography is the mapping of lipid deposition within an arterial wall that can cause other health problems, measuring cardiac tissue damage and tumor biopsies. Using photoacoustic tomography for intraoperative tumor biopsies could help surgeons make sure they remove all the cancer from a patient, Goergen said.One of the challenges of photoacoustic tomography is improving the penetration depth and signal-to-noise ratio past optical absorbers. The researchers believe creating optical manipulation techniques to maximize photon density could provide a solution. As a result, they have created a motorized photoacoustic holder that allows users to easily maneuver the aim of the device and tune the depth where light is focused, improving the light penetration depth and signal-to-noise ratio.Story Source:Materials provided by Purdue University. ",0.0875387,0.137489,0.168152,0.220824,0.149807,0.145022,0.187035,0,0
830,Laser-sonic scanner aims to replace mammograms for finding breast cancer Imaging an entire breast in 15 seconds,"For women over 40, mammography is a necessary yet annoying procedure to endure every year or two. The technique, while valuable for reducing breast cancer deaths, is less than ideal because it exposes patients to X-ray radiation and requires their breasts to be painfully squished between plates. The plates flatten the breast so the X-rays can more easily pass through it and produce a clear image.Early detection has been shown to increase breast cancer survival rates, but many women avoid having their mammograms taken as often as they should because of the discomfort involved. A 2013 study found that as many as half of women who were avoiding their mammograms cited pain as the reason why.Mammography also has trouble with breasts, such as those in young women, that are ""radiographically dense,"" or somewhat opaque to X-rays. And mammography tends to overdiagnose, causing around half of women to receive a false-positive diagnosis at some point in their lives.Caltech researchers say they have developed something better: a laser-sonic scanner that can find tumors in as little as 15 seconds by shining pulses of light into the breast. The scanning system, known as photoacoustic computed tomography, or PACT, was developed in the lab of Lihong Wang, Caltech's Bren Professor of Medical Engineering and Electrical Engineering.PACT works by shining a near-infrared laser pulse into the breast tissue. The laser light diffuses through the breast and is absorbed by oxygen-carrying hemoglobin molecules in the patient's red blood cells, causing the molecules to vibrate ultrasonically. Those vibrations travel through the tissue and are picked up by an array of 512 tiny ultrasonic sensors around the skin of the breast. The data from those sensors are used to assemble an image of the breast's internal structures in a process that is similar to ultrasound imaging, though much more precise. PACT can provide a clear view of structures as small as a quarter of a millimeter at a depth of 4 centimeters. Mammograms cannot provide soft-tissue contrast with the level of detail in PACT images, Wang says.Because the laser light at the currently used wavelength is so strongly absorbed by hemoglobin, PACT can construct images that primarily show the blood vessels present in the tissue being scanned. That's useful for finding cancer because many tumors grow their own blood vessels, surrounding themselves with dense networks of vascular tissue. Those vessels provide the tumors with large amounts of blood and allow the tumors to grow quickly.During a PACT scan, the patient lies face down on a table that has a recess containing ultrasonic sensors and the laser. One breast at a time is placed in the recess, and the laser shines into it from underneath. Since the scan is quick, taking only 15 seconds, the patient can easily hold their breath while being scanned, and a clearer image can be developed.""This is the only single-breath-hold technology that gives us high-contrast, high-resolution, 3-D images of the entire breast,"" Wang says.The speed with which a PACT scan can be performed gives it advantages over other imaging techniques. For example, magnetic resonance imaging (MRI) scans can take 45 minutes. MRI scans are also expensive and sometimes requires contrast agents to be injected into the patient's blood.""Gadolinium, for example, is a common contrast agent for MRI which is not totally innocuous,"" Wang says. ""Gadolinium may cause nausea and vomiting, and can remain in the brain for years, with unknown long-term effects. In comparison, PACT is entirely safe; the blood serves as an intrinsic contrast agent, and the laser exposure is well within the safety limits.""In a recent pilot study, the researchers scanned the breasts of eight women using PACT, and correctly identified eight of the nine breast tumors that were present.The PACT technology has already been licensed by a company, of which Wang is a founder and shareholder, that plans to commercialize it and conduct large-scale clinical studies, Wang says.""Our goal is to build a dream machine for breast screening, diagnosis, monitoring, and prognosis without any harm to the patient,"" he says. ""We want it to be fast, painless, safe, and inexpensive.""Wang says future research may focus on using PACT for imaging other parts of the body. It could give medical personnel the ability to assess the health of the vascular tissue in the extremities of a patient with diabetes, for example. Diabetes damages blood vessels and, if not properly diagnosed and treated, can cause tissue death, particularly in the feet. PACT is also able to differentiate between oxygenated and non-oxygenated blood; Wang says that could further improve the device's ability to find tumors, since cancer tissue, with its high oxygen demands, deoxygenates blood more rapidly than healthy tissue.Story Source:Materials provided by California Institute of Technology. ",0.077752,0.111391,0.137806,0.184239,0.12307,0.131379,0.168708,0,0
831,Cancer: Microbubbles delivered via gas embolotherapy could cut off blood supply and deliver drugs,"Embolization -- the use of various techniques to cut off the blood vessels that feed tissue growth -- has gained traction over the past few decades to treat cancerous tumors.This method of starving tumors of blood supply and nutrients is less invasive than surgery and typically involves injecting drugs (chemoembolization) or lodging nanoscopic beads directly into blood vessels.Recently, scientists have explored another version of embolization, called gas embolotherapy. During this process, the blood supply is cut off using acoustic droplet vaporization (ADV), which uses microscopic gas bubbles induced by exposure to ultrasonic waves.A team of researchers from China and France has discovered that these bubbles could also be used as potential drug delivery systems. The researchers reported their findings this week in Applied Physics Letters, from AIP Publishing.""We have found that gas embolotherapy has great potential to not only starve tumors by shutting off blood flow, but also to be used as a source of targeted drug delivery,"" said Yi Feng, associate professor of biomedical engineering at Xi'an Jiaotong University and first co-author of the paper.In gas embolotherapy, researchers inject droplets, from tens to hundreds of nanometers in diameter, into feeder vessels surrounding the tumor. Microscopic gas bubbles are then formed from the droplets through ultrasound, growing large enough to block the feeder vessels (e.g., arterioles). ADV is the process of transforming droplets into bubbles.In previous work, the researchers expected to use ADV to starve the tumor by blocking blood flow in the arterioles. To their surprise, they found that the bubbles not only blocked the arterioles, but other gas bubbles made their way into the capillaries, resulting in vessel rupture and more leaky microvasculature.In their latest work, gas embolotherapy through ADV was performed to further explore the dynamics of the bubbles inside the capillaries. Testing was conducted ex vivo on rat tissue that attaches the intestines to the abdominal wall. It involved bubble formation in droplets of dodecafluoropentane containing a bovine serum which were injected into the blood flow.Produced in the dodecafluoropentane through ultrasound, the bubbles accumulated, sometimes merging, as they lodged themselves in the capillaries. At one point, the researchers observed a local vessel invagination (a pouchlike cavity), which they believe was caused by the interaction between the bubble and vessel and led to a capillary rupture.Their findings could provide a one-two punch for cancer treatment -- shutting off blood flow from the arterioles and delivering drugs through the capillaries. In addition, chemotherapy drugs could be kept localized for longer periods of time because blood flow has been shut down, reducing drug dosage.""In cancer therapy research, scientists are always interested in answering two questions: how to kill the cancer effectively and how to reduce the side effects of chemotherapeutic drugs,"" said Mingxi Wan, professor of biomedical engineering also at Xi'an Jiaotong University and corresponding author of the paper. ""We have found that gas embolotherapy has the potential to successfully address both of these areas.""With an ultrasound imaging method in place, according to Feng, gas embolotherapy has a good chance of becoming a standard practice. Wan and Feng's team is working with another group of researchers in the same lab, and such an imaging system is being built to apply the ADV gas embolotherapy method in rats.Story Source:Materials provided by American Institute of Physics. ",0.105446,0.164898,0.166066,0.22487,0.151377,0.158376,0.218281,0,0
832,3D images of cancer cells in the body,"Making tumour cells glow: Medical physicists at Martin Luther University Halle-Wittenberg (MLU) have developed a new method that can generate detailed three-dimensional images of the body's interior. This can be used to more closely investigate the development of cancer cells in the body. The research group presents its findings in Communication Physics, a journal published by the Nature Publishing Group.Clinicians and scientists are in need of a better understanding of cancer cells and their properties in order to provide targeted cancer treatment. Individual cancer cells are often examined in test tubes before the findings are tested in living organisms. ""Our aim is to visualise cancer cells inside the living body to find out how they function, how they spread and how they react to new therapies,"" says medical physicist Professor Jan Laufer from MLU. He specialises in the field of photoacoustic imaging, a process that uses ultrasound waves generated by laser beams to produce high-resolution, three-dimensional images of the body's interior.""The problem is that tumour cells are transparent. This makes it difficult to use optical methods to examine tumours in the body,"" explains Laufer whose research group has developed a new method to solve this problem: First the scientists introduce a specific gene into the genome of the cancer cells. ""Once inside the cells, the gene produces a phytochrome protein, which originates from plants and bacteria. There it serves as a light sensor,"" Laufer continues. In the next step, the researchers illuminate the tissue with short pulses of light at two different wavelengths using a laser. Inside the body, the light pulses are absorbed and converted into ultrasonic waves. These waves can then be measured outside the organism and two images of the body's interior can be reconstructed based on this data.""The special feature of phytochrome proteins is that they alter their structure and thus also their absorption properties depending on the wavelength of the laser beams. This results in changes to the amplitude of the ultrasound waves that are generated in the tumour cells. None of the other tissue components, for example blood vessels, have this property -- their signal remains constant,"" Laufer says. By calculating the difference between the two images, a high-resolution, three-dimensional image of the tumour cells is created, which is free of the otherwise overwhelming background contrast.The development of Halle's medical physicists can be applied to a wide range of applications in the preclinical research and the life sciences. In addition to cancer research, the method can be used to observe cellular and genetic processes in living organisms.Story Source:Materials provided by Martin-Luther-Universitet Halle-Wittenberg. ",0.0860172,0.143256,0.152883,0.194901,0.153378,0.123105,0.175249,0,0
833,"Why we need erasable MRI scans New technology could allow an MRI contrast agent to 'blink off,' helping doctors diagnose disease","Magnetic resonance imaging, or MRI, is a widely used medical tool for taking pictures of the insides of our body. One way to make MRI scans easier to read is through the use of contrast agents -- magnetic dyes injected into the blood or given orally to patients that then travel to organs and tissues, making them easier to see. Recently, researchers have begun to develop next-generation contrast agents, such as magnetic nanoparticles, that can be directed specifically to sites of interest, such as tumors.But there remains a problem with many of these agents: they are sometimes difficult to distinguish from our bodies' tissues, which give off their own MRI signals. For example, a researcher reading an MRI scan may not know with certainty if a dark patch near a tumor represents a contrast agent bound to the tumor, or is an unrelated signal from surrounding tissue.Caltech's Mikhail Shapiro, assistant professor of chemical engineering, thinks he has a solution. He and his team are working on ""erasable"" contrast agents that would have the ability to blink off, on command, thereby revealing their location in the body.""We're developing MRI contrast agents that can be erased with ultrasound, allowing you to turn them off,"" says Shapiro, who is also a Schlinger Scholar and Heritage Medical Research Institute Investigator. ""It's the same principle behind blinking bicycle lights. Having the lights turn on and off makes them easier to see, only in our case we just blink off the contrast agent once.""The new research was published in the February 26 advanced issue of Nature Materials, and is on the cover of the May print edition out this month. The lead author is George Lu, a postdoctoral scholar in Shapiro's lab.""Clearly visualizing MRI contrast agents is a longtime, lingering problem in the field,"" says Lu. ""With our new study, we are showing how it could be possible to erase the contrast agent, making it much easier to read MRI scans properly.""The new technology relies on nanoscale structures called gas vesicles, which are naturally produced by some microbes. Gas vesicles consist of a protein shell with a hollow interior and are used by the microbes as flotation devices to regulate access to light and nutrients. Previously, Shapiro and his colleagues demonstrated how gas vesicles could someday enable imaging of therapeutic bacteria and other cells in people's bodies using ultrasound. The idea would be to engineer cells of interest -- such as bacterial cells used to treat gut conditions -- to produce the gas vesicles. Because the hollow chambers of the vesicles bounce back sound waves in distinctive ways, the vesicles (and the cells producing them) would be easy to distinguish from surrounding tissue.It turns out that the gas vesicles also stand out in MRI scans because the air in their chambers reacts differently to magnetic fields compared to the aqueous tissues around them. This results in a local darkening of MRI images where the gas vesicles are present.In the new study, performed in mice, the researchers showed that gas vesicles could indeed be used as MRI contrast agents -- the gas vesicles were detected in certain tissues and organs, such as the brain and liver. What's more, the gas vesicles could be turned off. When hit with ultrasound waves of a high enough pressure, the cylindrical structures ""collapsed like crushed soda cans,"" Shapiro says, and their magnetic signals went away.Previous studies of gas vesicles have shown that these protein structures can be genetically modified to target receptors on specific cells, such as tumor cells. Populations of gas vesicles can also be engineered differently -- for example, one group might target a tumor while another would stay in the blood stream to outline blood vessels. This would allow doctors and researchers to visualize two types of tissue at once.""We have previously shown that we can genetically engineer the gas vesicles in a variety of ways for use in ultrasound imaging,"" says Shapiro. ""Now they have a whole new application in MRI.""Story Source:Materials provided by California Institute of Technology. ",0.117903,0.180902,0.169092,0.224202,0.162227,0.183192,0.209034,0,0
834,"Smallest volume, most efficient wireless nerve stimulator ","In 2016, University of California, Berkeley, engineers demonstrated the first implanted, ultrasonic neural dust sensors, bringing closer the day when a Fitbit-like device could monitor internal nerves, muscles or organs in real time. Now, Berkeley engineers have taken neural dust a step forward by building the smallest volume, most efficient wireless nerve stimulator to date.The device, called StimDust, short for stimulating neural dust, adds more sophisticated electronics to neural dust without sacrificing the technology's tiny size or safety, greatly expanding the range of neural dust applications. The researchers' goal is to have StimDust implanted in the body through minimally invasive procedures to monitor and treat disease in a real-time, patient-specific approach. StimDust is just 6.5 cubic millimeters in volume and is powered wirelessly by ultrasound, which the device then uses to power nerve stimulation at an efficiency of 82 percent.""StimDust is the smallest deep-tissue stimulator that we are aware of that's capable of stimulating almost all of the major therapeutic targets in the peripheral nervous system,"" said Rikky Muller, co-lead of the work and assistant professor of electrical engineering and computer sciences at Berkeley. ""This device represents our vision of having tiny devices that can be implanted in minimally invasive ways to modulate or stimulate the peripheral nervous system, which has been shown to be efficacious in treating a number of diseases.""The research will be presented April 10 at the IEEE Custom Integrated Circuits Conference in San Diego. The research team was co-led by one of neural dust's inventors, Michel Maharbiz, a professor of electrical engineering and computer sciences at Berkeley.The creation of neural dust at Berkeley, led by Maharbiz and Jose Carmena, a Berkeley professor of electrical engineering and computer sciences and a member of the Helen Wills Neuroscience Institute, has opened the door for wireless communication to the brain and peripheral nervous system through tiny implantable devices inside the body that are powered by ultrasound. Engineering teams around the world are now using the neural dust platform to build devices that can be charged wirelessly by ultrasound.Maharbiz came up with the idea to use ultrasound for powering and communicating with very small implants. Together with Berkeley professors Elad Alon and Jan Rabaey, the group then developed the technical framework to demonstrate the scaling power of ultrasound for implantable devices.Early engineering work by D.J. Seo, a Berkeley Ph.D. student who was co-advised by Alon and Maharbiz, followed by experimental validations by Ryan Neely, another Berkeley Ph.D. student, advised by Carmena, set the foundations of the neural dust vision. In the years since neural dust's invention, ultrasound has proven to be among the most promising technologies for powering and communicating implantable devices.Muller came to Berkeley in 2016 and has been a key driver of neural dust innovation. Her research group specializes in bidirectional electronic interfaces with human body, specifically in the brain and peripheral nervous system. Her team has been working on ways to use the power that can be transmitted to neural dust. In StimDust, her lab has taken the neural dust platform and built a more effective stimulator that can wrap around a nerve cuff and can also record, transmit and receive data. They did this by designing a custom integrated circuit to transfer ultrasound charge to the nerve in a well-controlled, safe and efficient way.StimDust is about an order of magnitude smaller than any active device with similar capabilities that the research team is aware of. The components of StimDust include a single piezocrystal, which is the antenna of the system, a 1-millimeter integrated circuit and one charge storage capacitor. StimDust has electrodes on the bottom, which make contact with a nerve through a cuff that wraps around the nerve. In addition to the device, Muller's team designed a custom wireless protocol that gives them a large range of programmability while maintaining efficiency. The entire device is powered by just 4 microwatts and has a mass of 10 milligrams.After testing StimDust on the benchtop, the research team implanted it in a live rodent to test it in a realistic environment. Through a cuff around the sciatic nerve, the research team was able to control hind leg motion, record the stimulation activity and measure how much force was exerted on the hind leg muscle as it was stimulated. The researchers then gradually increased stimulation and mapped the response of the hind leg muscle so they could know exactly how much stimulation was needed for a desired muscle recruitment, a kind of sophisticated analysis required of medical devices.Muller hopes that her work can lead to applications of StimDust to treat diseases such as heart irregularities, chronic pain, asthma or epilepsy.""One of the big visions of my group is to create these very efficient bidirectional interfaces with the nervous system and couple that with intelligence to really understand the signals of disease and then to be able to treat disease in an intelligent, methodical way,"" Muller said. There's an incredible opportunity for healthcare applications that can really be transformative.""Story Source:Materials provided by University of California - Berkeley. Original written by Brett Israel. ",0.0885578,0.12541,0.140748,0.267744,0.154395,0.156195,0.187139,0,0
835,Flexible ultrasound patch could make it easier to inspect damage in odd-shaped structures,"Researchers have developed a stretchable, flexible patch that could make it easier to perform ultrasound imaging on odd-shaped structures, such as engine parts, turbines, reactor pipe elbows and railroad tracks -- objects that are difficult to examine using conventional ultrasound equipment.The ultrasound patch is a versatile and more convenient tool to inspect machine and building parts for defects and damage deep below the surface. A team of researchers led by engineers at the University of California San Diego published the study in the Mar. 23 issue of Science Advances.The new device overcomes a limitation of today's ultrasound devices, which are difficult to use on objects that don't have perfectly flat surfaces. Conventional ultrasound probes have flat and rigid bases, which can't maintain good contact when scanning across curved, wavy, angled and other irregular surfaces. That's a considerable limitation, said Sheng Xu, a professor of nanoengineering at the UC San Diego Jacobs School of Engineering and the study's corresponding author. ""Nonplanar surfaces are prevalent in everyday life,"" he said.""Elbows, corners and other structural details happen to be the most critical areas in terms of failure -- they are high stress areas,"" said Francesco Lanza di Scalea, a professor of structural engineering at UC San Diego and co-author of the study. ""Conventional rigid, flat probes aren't ideal for imaging internal imperfections inside these areas.""Gel, oil or water is typically used to create better contact between the probe and the surface of the object it's examining. But too much of these substances can filter some of the signals. Conventional ultrasound probes are also bulky, making them impractical for inspecting hard-to-access parts.""If a car engine has a crack in a hard-to-reach location, an inspector will need to take apart the entire engine and immerse the parts in water to get a full 3D image,"" Xu said.Now, a UC San Diego-led team has developed a soft ultrasound probe that can work on odd-shaped surfaces without water, gel or oil.The probe is a thin patch of silicone elastomer patterned with what's called an ""island-bridge"" structure. This is essentially an array of small electronic parts (islands) that are each connected by spring-like structures (bridges). The islands contain electrodes and devices called piezoelectric transducers, which produce ultrasound waves when electricity passes through them. The bridges are spring-shaped copper wires that can stretch and bend, allowing the patch to conform to nonplanar surfaces without compromising its electronic functions.Researchers tested the device on an aluminum block with a wavy surface. The block contained defects two to six centimeters beneath the surface. Researchers placed the probe at various spots on the wavy surface, collected data and then reconstructed the images using a customized data processing algorithm. The probe was able to image the 2-millimeter-wide holes and cracks inside the block.""It would be neat to be able to stick this ultrasound probe onto an engine, airplane wing or different parts of a bridge to continuously monitor for any cracks,"" said Hongjie Hu, a materials science and engineering Ph.D. student at UC San Diego and co-first author of the study.The device is still at the proof-of-concept stage. It does not yet provide real-time imaging. It also needs to be connected to a power source and a computer to process data. ""In the future, we hope to integrate both power and a data processing function into the soft ultrasound probe to enable wireless, real-time imaging and videoing,"" Xu said.This work was supported in part by the National Institutes of Health (grant R21EB025521) and funding from Clinical and Translational Science Awards (UL1TR001442). Additional support was provided by the UC San Diego Center for Healthy Aging, a grant from the U.S. Federal Railroad Administration (FR-RRD-0027-11) and the National Science Foundation (CMMI-1362144).Story Source:Materials provided by University of California - San Diego. Original written by Liezel Labios. ",0.0970324,0.129301,0.172352,0.185238,0.137789,0.14398,0.161588,0,0
836,Diagnosing breast cancer using red light. New design vastly improves sensitivity of optical mammography instruments,"Optical Mammography, or OM, which uses harmless red or infrared light, has been developed for use in conjunction with X-rays for diagnosis or monitoring in cases demanding repeated imaging where high amounts of ionizing radiation should be avoided. At the OSA Biophotonics Congress: Biomedical Optics meeting, held 3-6 April in Hollywood, Florida, USA, researchers from Milan, Italy, will report an advance in instrument development that increases the sensitivity of OM by as much as 1000-fold.In 2012, the most recent year for which data is available, more than 1.7 million women worldwide were diagnosed with breast cancer. Many of these diagnoses are made using X-ray mammography. Although standard and widely used, X-ray imaging for breast cancer suffers from both low sensitivity (50-75%) and the use of ionizing radiation that cannot be considered completely safe.The newly-developed instrument replaces two photomultiplier tubes (PMTs) of existing instruments with an eight-channel probe involving silicon photomultipliers (SiPMs) and a multichannel time-to-digital converter. These changes eliminate a time-wasting pre-scan step that was required to avoid damage to the PMTs. In addition to increased sensitivity, the new instrument is both more robust and cheaper.While X-ray mammography is widely used and is still the recommended method for routine screenings, its use is limited by the patient's age, weight or body mass index, whether or not hormone replacement therapy is being used, and other issues. In addition, its accuracy -- particularly when used in younger women -- has been called into question. Other imaging techniques, such as MRI and ultrasound, are sometimes suggested, but neither is an effective replacement for X-ray mammography.Optical imaging methods, on the other hand, have attracted increasing interest for breast cancer diagnosis since both visible and infrared light are highly sensitive to blood volume and oxygenation. Tumors are characterized by a high volume of blood due to the increased vascularization that occurs as tumors grow. OM can be used to measure blood volume, oxygenation, lipid, water and collagen content for a suspicious area identified through standard X-ray imaging. Collagen measurements are particularly important since this species is known to be involved in the onset and progression of breast cancer.One major disadvantage to OM imaging is the poor spatial resolution that has been achieved to date. Breast cancer tumors larger than 1 centimeter are very dangerous and more likely to lead to death, so a successful screening technique must be able to resolve smaller lesions. This remains a problem with OM imaging as a stand-alone technique, but combining OM with other imaging methods shows some promise.A possible advantage to OM, however, is that only gentle pressure need be applied to the breast tissue, in stark contrast to the standard technique for X-ray imaging. In fact, breast compression tends to reduce blood volume in the tissue, which would interfere with the OM image, so some three-dimensional OM detectors being developed use no compression at all but, rather, surround the breast tissue with rings of light sources and detectors.While poor spatial resolution of OM methods remains a challenge, the method does show promise for use in pre-surgical chemotherapy. As Edoardo Ferocino, Politecnico di Milano, Italy, co-author of the work explains, ""This technique is able to provide information on the outcome of chemotherapy just weeks after beginning treatment, or possibly even sooner."" Ferocino's group is planning clinical studies to explore the use of OM to monitor and predict the outcome of chemotherapy.The investigators in Milan are working with a larger consortium on a project known as SOLUS, ""Smart Optical and Ultrasound Diagnostics of Breast Cancer."" This project is funded by the European Union through the Horizon 2020 Research and Innovation Program and aims to combine optical imaging methods with ultrasound to improve specificity in the diagnosis of breast cancer.Story Source:Materials provided by The Optical Society. ",0.0907216,0.119239,0.172803,0.201613,0.151709,0.13528,0.171742,0,0
837,Ultrasound to enhance cancer drug delivery. Sound waves drive drug therapies to cancer cells and minimize toxicity,"For decades, ultrasound has been used to image organs such as the heart and kidneys, check blood flow, monitor the development of fetuses, reduce pain and even break up kidney stones.Now, a Norwegian biotech company called Phoenix Solutions AS is working with the Translational Genomics Research Institute (TGen), a Phoenix, Arizona-based biomedical research facility, to test the use of these pulsed sound waves to direct and focus cancer drug therapies.In laboratory tests, TGen will help analyze the effectiveness of a technology called Acoustic Cluster Therapy (ACT), a unique approach to targeting cancer cells by concentrating the delivery of chemotherapies, making them more effective and potentially reducing their toxicity.Humanscan Co. Ltd., a South Korean manufacture of ultrasound diagnostic imaging, is developing clinically applicable hardware optimized for ACT, which TGen will then validate.Phoenix Solutions is using funding from Innovation Norway to conduct this research in advance of pancreatic cancer clinical trials planned later this year.""We are very pleased to receive this grant, which will enable us to develop and validate an optimal ultrasound platform for clinical use of ACT. In their respective fields, Humanscan and TGen both represent the cutting edge of science, and we are confident this will contribute to the clinical success of our program,"" said Dr. Per Sontum, CEO of Phoenix Solutions.TGen is a world leader in the development of novel therapeutics for the treatment of pancreatic cancer. The TGen team led the clinical development of one of the current standard-of-care regimens for this disease -- nab-paclitaxel plus gemcitabine. TGen currently is involved in 13 pancreatic cancer clinical programs.""We are pleased that this research program has become a reality, and look forward to working with ACT. The concept represents a novel approach to targeted drug delivery and looks very promising,"" said Dr. Haiyong Han, a Professor in TGen's Molecular Medicine Division and head of the Basic Research Unit in TGen's Pancreatic Cancer Program.Among the advantages of ultrasound technology: it is generally painless; non-invasive; does not require needles, injections or incisions; and patients are not exposed to ionizing radiation, making the procedure safer than diagnostic techniques such as X-rays and CT scans.Humanscan CEO Sungmin Rhim said, ""We are excited to enter into this collaboration with Phoenix (Solutions) and participate in the development of ACT. Ultrasound mediated, targeted drug delivery is an emerging therapy approach with great potential and we are delighted to be in the forefront of this development.""Phoenix Solutions also is considering this technology for use in addressing other types of cancer, including: liver, prostate and triple-negative breast cancer; and other diseases, including those involving inflammation and the central nervous system.Story Source:Materials provided by The Translational Genomics Research Institute. ",0.105006,0.154248,0.168625,0.251722,0.149402,0.174764,0.209086,0,0
838,Clinically-validated 3-D printed stethoscope,"A team of researchers have developed an open-source, clinically validated template for a 3D printed stethoscope for use in areas of the world with limited access to medical supplies -- places where a stethoscope could mean the difference between life and death.""As far as we know this is the first open-source medical device that has been clinically validated and is widely available,"" said Dr. Tarek Loubani, associate professor at Western's Schulich School of Medicine & Dentistry, associate scientist at Lawson Health Research Institute and an emergency room physician at London Health Sciences Centre.Loubani spent time working as an ER physician in hospitals in Gaza during wartime when medical supplies were often scarce. ""We wanted physicians and allied health care professionals to be able to have something that was high quality. This study found that the acoustic quality was the same in our stethoscope as in a premium brand stethoscope.""The idea to 3D print a stethoscope was born while playing with a toy stethoscope and noticing it performed its function quite well. That led Loubani and a team of engineers to design an open-access template for a 3D printed stethoscope that could be created using recycled plastic. Now, the team's stethoscope has been clinically validated, and their results are published online today in the journal PLOS ONE.The stethoscope, called the Glia model, was made using free open source software to keep costs low and allow others to easily access the code. With the Glia template, the stethoscope can be made in less than three hours and costs less than $3 to produce. Anyone with a 3D printer and access to ABS -- a plastic used to make garden chairs and Lego -- can create the device. The results of the study show it has the same acoustic quality as the best stethoscopes on the market.""Use of the open source approach in every aspect of this project contributes powerfully to the body of medical device research,"" said Gabriella Coleman, PhD, noted scholar on technology and open source software. ""This research gives a guide for others to create medical-grade open access devices that can reduce costs and ultimately save lives.""The device is currently in clinical use by physicians and allied health professionals in Gaza and is also being trialled clinically at the London Health Sciences Centre, in London, Ontario.Loubani says stethoscopes may not be seen as vital for diagnosis and treatment in places such as London, where physicians rely heavily on ultrasound, CT and other diagnostic technologies. However, in war torn and low-income communities, the stethoscope is a necessary tool. ""Stethoscope utility goes up as other resources go down. In London, if someone gets shot, I can use an ultrasound to look inside and see if there is a life-threatening air pocket called a pneumothorax. In Gaza, ultrasounds are not available in emergency departments, or are dilapidated, so the stethoscope becomes an inexpensive tool that allows us to make life-saving decisions.""The hope now is to create templates for other medical devices that can be made or improved on-site in locations with scarce resources.Story Source:Materials provided by University of Western Ontario. Original written by Crystal Mackay. ",0.0820217,0.113152,0.151734,0.194906,0.129937,0.13537,0.150351,0,0
839,Atomic structure of ultrasound material not what anyone expected,"Lead magnesium niobate (PMN) is a prototypical ""relaxor"" material, used in a wide variety of applications, from ultrasound to sonar. Researchers have now used state-of-the-art microscopy techniques to see exactly how atoms are arranged in PMN -- and it's not what anyone expected.""This work gives us information we can use to better understand how and why PMN behaves the way it does -- and possibly other relaxor materials as well,"" says James LeBeau, an associate professor of materials science and engineering at North Carolina State University and corresponding author of a paper on the work.""What we've found is that the arrangement of atoms in PMN gradually shift along a gradient, from areas of high order to areas of low order; this happens throughout the material,"" LeBeau says. ""That's substantially different than what conventional wisdom predicted, which was there would be alternating areas of high order and no order, right next to each other.""This information can be fed into computational models to provide new insights into how PMN's atomic structure influences its characteristics.""This won't happen overnight, but we're optimistic that this may be a step toward the development of processes that create PMN materials with microstructures tailored to emphasize the most desirable characteristics for ultrasound, sonar or other applications,"" LeBeau says.""It could also potentially offer insights into the role of atomic structure in other relaxor materials, providing similar long-term benefits for the entire class of materials.""Story Source:Materials provided by North Carolina State University. ",0.0941519,0.148814,0.159381,0.211949,0.127035,0.166289,0.184331,0,0
840,New method for waking up devices,"As smartphone users know all too well, a sleeping device can still suck the life out of a battery. One solution for extending the battery life of wireless devices under development by researchers at Stanford University is to add a wake-up receiver that can turn on a shut-off device at a moment's notice.Angad Rekhi, a graduate student in the Arbabian lab at Stanford, and Amin Arbabian, assistant professor of electrical engineering, have developed a wake-up receiver that turns on a device in response to incoming ultrasonic signals -- signals outside the range that humans can hear. By working at a significantly smaller wavelength and switching from radio waves to ultrasound, this receiver is much smaller than similar wake-up receivers that respond to radio signals, while operating at extremely low power and with extended range. The group is presenting the work at the International Solid-State Circuits Conference on Feb. 14.This wake-up receiver has many potential applications, particularly in designing the next generation of networked devices, including so-called ""smart"" devices that can communicate directly with one another without human intervention.""As technology advances, people use it for applications that you could never have thought of. The internet and the cellphone are two great examples of that,"" said Rekhi. ""I'm excited to see how people will use wake-up receivers to enable the next generation of the Internet of Things.""The power tradeoffOnce attached to a device, a wake-up receiver listens for a unique ultrasonic pattern that tells it when to turn the device on. It needs only a very small amount of power to maintain this constant listening, so it still saves energy overall while extending the battery life of the larger device. A well-designed wake-up receiver also allows the device to be turned on from a significant distance.Arbabian said the designing these electronic devices posed a number of challenges. ""Scaling down wake-up receivers in size and power consumption while maintaining or extending range is a fundamental challenge,"" he said. ""But this challenge is worth pursuing, because solving this problem can enable scalable networks of wake-up receivers working in our everyday environment.""In order to miniaturize the wake-up receiver and drive down the amount of power it consumes, the researchers made use of the highly sensitive ultrasonic transducers provided by the Khuri-Yakub lab at Stanford, which convert analog sound input to electrical signals. With that technology, the researchers designed a system that can detect a wake-up signature with as little as 1 nanowatt of signal power, about 1 billionth the power it takes to light a single old-fashioned Christmas bulb.Given the increased interest in networked devices, researchers and industry organizations are starting to define what features and techniques will become standard. Regardless of whether this ultrasound wake-up receiver is among these standard designs, it is likely wake-up receivers of some kind will be integrated into commercial applications soon, Rekhi said.Connecting the futureThis work branched off from a previous Arbabian lab creation, a tiny chip dubbed the ""ant-sized radio"" that can send and receive signals over radio waves without a battery. The ant-sized radio has the advantage of being wirelessly powered but needs to remain relatively close to the transmitter with which it communicates. The group has since published a way of using ultrasound to extend the powering range of devices like the ant-sized radio, but that distance is still limiting.By comparison, the ultrasound wake-up receiver requires a battery but has much greater range than the wirelessly powered devices, while still maintaining a long lifetime due to extremely low power draw. These two technologies -- wireless power and wake-up receivers -- would likely serve different purposes but both hint at a turning point in devices that make up the internet of things.In light of a long-promised future where interconnected, autonomous, widespread, unobtrusive technologies make lives easier, the networked devices available now, like video doorbells and app-enabled lights, seem like rather subtle advances, the researchers said. They believe technologies like theirs could help span the gap between the internet of things as we know it and the internet of things at its best -- whatever that may be.This research was funded by the Department of Defense and the Air Force Office of Scientific Research.Story Source:Materials provided by Stanford University. ",0.106053,0.106823,0.118501,0.217347,0.172256,0.144002,0.144739,0,0
841,NIR light may identify breast cancer patients who will benefit most from chemotherapy,"A new optical imaging system developed at Columbia University uses red and near-infrared light to identify breast cancer patients who will respond to chemotherapy. The imaging system may be able to predict response to chemotherapy as early as two weeks after beginning treatment.Findings from a first pilot study of the new imaging system -- a noninvasive method of measuring blood flow dynamics in response to a single breath hold -- were published today in Radiology.The optical imaging system was developed in the laboratory of Andreas Hielscher, professor of biomedical engineering and electrical engineering at Columbia Engineering and professor of radiology at Columbia University Irving Medical Center.""There is currently no method that can predict treatment outcome of chemotherapy early on in treatment, so this is a major advance,"" says Hielscher, co-leader of the study, who is also a member of the Breast Cancer Program at the Herbert Irving Comprehensive Cancer Center at NewYork-Presbyterian/Columbia University Irving Medical Center. His dynamic optical tomographic breast imaging system generates 3D images of both breasts simultaneously. The images enable the researchers to look at blood flow in the breasts, see how the vasculature changes, and how the blood interacts with the tumor. He adds,""This helps us distinguish malignant from healthy tissue and tells us how the tumor is responding to chemotherapy earlier than other imaging techniques can.""Neoadjuvant chemotherapy, given for five to six months before surgery, is the standard treatment for some women with newly diagnosed invasive, but operable, breast cancer. The aim of neoadjuvant chemotherapy is to eliminate active cancer cells -- producing a complete response -- before surgery. Those who achieve a complete response have a lower risk of cancer recurrence than those who do not. However, fewer than half of women treated with neoadjuvant chemotherapy achieve a complete response.""Patients who respond to neoadjuvant chemotherapy have better outcomes than those who do not, so determining early in treatment who is going to be more likely to have a complete response is important,"" says Dawn Hershman, MD, leader of the Breast Cancer Program at the Herbert Irving Comprehensive Cancer Center at NewYork-Presbyterian/Columbia and co-leader of the study. ""If we know early that a patient is not going to respond to the treatment they are getting, it may be possible to change treatment and avoid side effects.""The researchers had suspected that looking at the vasculature system in breasts might hold a clue. Breast tumors have a denser network of blood vessels than those found in a healthy breast. Blood flows freely through healthy breasts, but in breasts with tumors, blood gets soaked up by the tumor, inhibiting blood flow. Chemotherapy drugs kill cancer cells, but they also affect the vasculature inside the tumor. The team thought they might be able to pick optical clues of these vascular changes, since blood is a strong absorber of light.The researchers analyzed imaging data from 34 patients with invasive breast cancer between June 2011 and March 2016. The patients comfortably positioned their breasts in the optical system, where, unlike mammograms, there was no compression.The investigators captured a series of images during a breath hold of at least 15 seconds, which inhibited the backflow of blood through the veins but not the inflow through the arteries. Additional images were captured after the breath was released, allowing the blood to flow out of the veins in the breasts. Images were obtained before and two weeks after starting chemotherapy. The researchers then compared the images with the patients' outcomes after five months of chemotherapy. They found that various aspects of the blood inflow and outflow could be used to distinguish between patients who respond and those who do not respond to therapy. For example, the rate of blood outflow can be used to correctly identify responders in 92.3 percent of patients, while the initial increase of blood concentration inside the tumor can be used to identify non-responders in 90.5 percent of patients.""If we can confirm these results in the larger study that we are planning to begin soon, this imaging system may allow us to personalize breast cancer treatment and offer the treatment that is most likely to benefit individual patients,"" says Hershman, who is also a professor of medicine and epidemiology at Columbia University Irving Medical Center.Researchers are also studying other imaging technologies for breast cancer treatment monitoring, such as MRI, X-ray imaging, and ultrasound, but Hielscher notes that these have not yet shown as much promise as this new technology.""X-ray imaging uses damaging radiation and so is not well-suited for treatment monitoring, which requires imaging sessions every two to three weeks,"" he says. ""MRIs are expensive and take a long time, from 30-90 minutes, to perform. Because our system takes images in less than 10 minutes and uses harmless light, it can be performed more frequently than MRI.""Hielscher and Hershman are currently refining and optimizing the imaging system and planning a larger, multicenter clinical trial. They hope to commercialize their technology in the next three to five years.Story Source:Materials provided by Columbia University School of Engineering and Applied Science. ",0.0982513,0.130169,0.178596,0.243258,0.149841,0.153955,0.201145,0,0
842,3-D nanoscale imaging made possible,"Imaging at the nanoscale is important to a plethora of modern applications in materials science, physics, biology, medicine and other fields. Limitations of current techniques are, e.g. their resolution, imaging speed or the inability to look behind opaque objects with arbitrary shapes.However, imaging like this would be useful e.g. for investigating spongy electrodes, thus helping to increase capacity and charging speed of next generation batteries.In a research article ""3D Nano-scale Imaging by Plasmonic Brownian Microscopy"" published today in Nanophotonics, the team around Prof. Xiang Zhang from the University of California in Berkeley demonstrate a method with stunning properties.""We wanted to overcome limitations of current nano imaging techniques and are excited to have found a way to image complex 3d nanostructures even with intricate internal structures such as cavities,"" explains Prof. Zhang.Nanoparticles are immersed in a fluid surrounding the object under investigation. By exploiting their special properties when interacting with light, each of the particles acts as a light source, thereby probing the object from all sides, also behind overhangs and inside any cavities. With a resolution of 30nm in all direction, this new technique offers true 3D imaging at the nanoscale.Besides applications in the technology sector, Plasmonic Brownian Microscopy may also be used to map out the biological machinery inside single cells, especially those with intricate internal structures. This would help to further our understanding of the basic mechanisms of living organisms and may give rise to new medical solutions.Story Source:Materials provided by De Gruyter Open. ",0.107687,0.172289,0.170192,0.220696,0.161441,0.164955,0.204493,0,0
843,The force is strong: Amputee controls individual prosthetic fingers,"Luke Skywalker's bionic hand is a step closer to reality for amputees in this galaxy. Researchers at the Georgia Institute of Technology have created an ultrasonic sensor that allows amputees to control each of their prosthetic fingers individually. It provides fine motor hand gestures that aren't possible with current commercially available devices.The first amputee to use it, a musician who lost part of his right arm five years ago, is now able to play the piano for the first time since his accident. He can even strum the Star Wars theme song.""Our prosthetic arm is powered by ultrasound signals,"" said Gil Weinberg, the Georgia Tech College of Design professor who leads the project. ""By using this new technology, the arm can detect which fingers an amputee wants to move, even if they don't have fingers.""Jason Barnes is the amputee working with Weinberg. The 28-year-old was electrocuted during a work accident in 2012, forcing doctors to amputate his right arm just below the elbow. Barnes no longer has his hand and most of his forearm but does have the muscles in his residual limb that control his fingers.Barnes' everyday prosthesis is similar to the majority of devices on the market. It's controlled by electromyogram (EMG) sensors attached to his muscles. He switches the arm into various modes by pressing buttons on the arm. Each mode has two programmed moves, which are controlled by him either flexing or contracting his forearm muscles. For example, flexing allows his index finger and thumb to clamp together; contracting closes his fist.""EMG sensors aren't very accurate,"" said Weinberg, director of Georgia Tech's Center for Music Technology. ""They can detect a muscle movement, but the signal is too noisy to infer which finger the person wants to move. We tried to improve the pattern detection from EMG for Jason but couldn't get finger-by-finger control.""But then the team looked around the lab and saw an ultrasound machine. They partnered with two other Georgia Tech professors -- Minoru Shinohara (College of Sciences) and Levent Degertekin (Woodruff School of Mechanical Engineering) -- and attached an ultrasound probe to the arm. The same kind of probe doctors use to see babies in the womb could watch how Barnes' muscles moved.""That's when we had a eureka moment,"" said Weinberg.When Barnes tries to move his amputated ring finger, the muscle movements differ from those seen when he tries to move any other digit. Weinberg and the team fed each unique movement into an algorithm that can quickly determine which finger Barnes wants to move. The ultrasound signals and machine learning can detect continuous and simultaneous movements of each finger, as well as how much force he intends to use.""It's completely mind-blowing,"" said Barnes. ""This new arm allows me to do whatever grip I want, on the fly, without changing modes or pressing a button. I never thought we'd be able to do this.""This is the second device Weinberg's lab has built for Barnes. His first love is the drums, so the team fitted him with a prosthetic arm with two drumsticks in 2014. He controlled one of the sticks. The other moved on its own by listening to the music in the room and improvising.The device gave him the chance to drum again. The robotic stick could play faster than any drummer in the world. Worldwide attention has sent Barnes and Weinberg's robots around the globe for concerts across four continents. They've also played at the Kennedy Center in Washington, D.C. and Moogfest.That success pushed Weinberg to take the next step and create something that gives Barnes the dexterity he's lacked since 2012.""If this type of arm can work on music, something as subtle and expressive as playing the piano, this technology can also be used for many other types of fine motor activities such as bathing, grooming and feeding,"" said Weinberg. ""I also envision able-bodied persons being able to remotely control robotic arms and hands by simply moving their fingers.""Video: https://www.youtube.com/watch?v=HjW1kIt5iQgStory Source:Materials provided by Georgia Institute of Technology. Original written by Jason Maderer. ",0.121093,0.128867,0.155233,0.234502,0.154701,0.199965,0.205046,0,0
844,Ultrasound imaging needle to transform heart surgery,"Heart tissue can be imaged in real-time during keyhole procedures using a new optical ultrasound needle developed by researchers at UCL and Queen Mary University of London (QMUL).The revolutionary technology has been successfully used for minimally invasive heart surgery in pigs, giving an unprecedented, high-resolution view of soft tissues up to 2.5 cm in front of the instrument, inside the body.Doctors currently rely on external ultrasound probes combined with pre-operative imaging scans to visualise soft tissue and organs during keyhole procedures as the miniature surgical instruments used do not support internal ultrasound imaging.For the study, published today in Light: Science & Applications, the team of surgeons, engineers, physicists and material chemists designed and built the optical ultrasound technology to fit into existing single-use medical devices, such as a needle.""The optical ultrasound needle is perfect for procedures where there is a small tissue target that is hard to see during keyhole surgery using current methods and missing it could have disastrous consequences,"" said Dr Malcolm Finlay, study co-lead and consultant cardiologist at QMUL and Barts Heart Centre.""We now have real-time imaging that allows us to differentiate between tissues at a remarkable depth, helping to guide the highest risk moments of these procedures. This will reduce the chances of complications occurring during routine but skilled procedures such as ablation procedures in the heart. The technology has been designed to be completely compatible with MRI and other current methods, so it could also be used during brain or fetal surgery, or with guiding epidural needles.""The team developed the all-optical ultrasound imaging technology for use in a clinical setting over four years. They made sure it was sensitive enough to image centimetre-scale depths of tissues when moving; it fitted into the existing clinical workflow and worked inside the body.""This is the first demonstration of all-optical ultrasound imaging in a clinically realistic environment. Using inexpensive optical fibres, we have been able to achieve high resolution imaging using needle tips under 1 mm. We now hope to replicate this success across a number of other clinical applications where minimally invasive surgical techniques are being used,"" explained study co-lead, Dr Adrien Desjardins (Wellcome EPSRC Centre for Interventional and Surgical Sciences at UCL).The technology uses a miniature optical fibre encased within a customised clinical needle to deliver a brief pulse of light which generates ultrasonic pulses. Reflections of these ultrasonic pulses from tissue are detected by a sensor on a second optical fibre, giving real-time ultrasound imaging to guide surgery.One of the key innovations was the development of a black flexible material that included a mesh of carbon nanotubes enclosed within clinical grade silicone precisely applied to an optical fibre. The carbon nanotubes absorb pulsed laser light, and this absorption leads to an ultrasound wave via the photoacoustic effect.A second innovation was the development of highly sensitive optical fibre sensors based on polymer optical microresonators for detecting the ultrasound waves. This work was undertaken in a related UCL study led by Dr James Guggenheim (UCL Medical Physics & Biomedical Engineering) and recently published in Nature Photonics.""The whole process happens extremely quickly, giving an unprecedented real-time view of soft tissue. It provides doctors with a live image with a resolution of 64 microns, which is the equivalent of only nine red blood cells, and its fantastic sensitivity allows us to readily differentiate soft tissues,"" said study co-author, Dr Richard Colchester (UCL Medical Physics & Biomedical Engineering).Story Source:Materials provided by University College London. ",0.109052,0.154301,0.156042,0.197473,0.179613,0.152827,0.18799,0,0
845,Neutrophil-inspired propulsion,"When white blood cells are summoned to combat invasive bacteria, they move along blood vessels in a specific fashion, i.e., like a ball propelled by the wind, they roll along the vascular wall to reach their point of deployment. Since white blood cells can anchor themselves to the vasculature, they are capable of moving against the direction of the blood flow.This type of behaviour of the white blood cells served as an inspiration for the postdoc, Daniel Ahmed, who was working in Professor Bradley Nelson's research group at ETH Zurich. In the lab, Ahmed and hi co-workers developed a novel system that enables aggregates composed of magnetised particles to roll along a channel in a combined acoustic and magnetic field. In addition, researchers of Jerg Dual's group have developed numerical and theoretical studies of the project. Their work was published recently in the journal, Nature Communications.The strategy of the devices' transport mechanism is both simple and ingenious, i.e., the scientists put commercially-available, biocompatible magnetic particles into an artificial vasculature. When a rotating magnetic field is applied, these particles self-assemble into aggregates and start to spin around their own axes. When the researchers apply ultrasound at a specific frequency and pressure, the aggregates migrate towards the wall and start rolling along the boundaries. The rolling motion is initiated once the microparticles achieve a minimum size of six micrometres, which is 1/10 the diameter of a human hair. When researchers turn off the magnetic field, the aggregates disassociate into their constituent parts and disperse in the fluid stream.Feasible in living tissueTo date, Ahmed has only tested this system in artificial channels. However, he believes that the method is feasible for use in living organisms. He stated, ""The ultimate goal is to use this kind of transport mechanism to deliver drugs in hard-to-reach sites within the body and integrate it with imaging modalities,"" he says. He is thinking about tumours that only can be reached via narrow capillaries but could be killed using rolling micro-therapeutics and their active substances.In vivo imaging is an important challenge in the field of micro and nanorobotics. The ultrasound and magnetic imaging technique is well established in clinical practice. Currently, there are several in vivo imaging techniques, e.g., magnetic resonance imaging (MRI) and magnetic particle imaging (MPI). Both can be used to track the aggregates of the superparamagnetic particles used in the study. The MPI that uses clinically-approved, iron oxide-based MRI contrast agents is capable of 3D, high-resolution imaging in real time. The researchers look forward to functionalizing nanodrugs with the iron oxide particles to allow mapping of the vasculature and the simultaneous transportation of nanodrugs.Improve resolution of ultrasound imagingThe mechanism developed via ultrasound is another potential application. For example, superparamagnetic particles and chemotherapeutics drugs can be incorporated in the polymeric shelled microbubbles. Bubbles are used as a contrast medium that could be distributed via a rolling motion into difficult-to-reach areas of the body. This could improve the resolution of ultrasound imaging.""In this study we demonstrated propulsion using self-assembling micro-aggregates, but that's only the beginning,"" Daniel Ahmed commented. The next step will be to examine how magnetic microrollers behave under flow conditions with auxiliary particles, such as red and white blood cells, and whether it is possible to persuade the magnetic particles to move against the flow as well. He also wants to test his system in vivo in animal models.Story Source:Materials provided by ETH Zurich. Original written by Peter Reegg. ",0.0942641,0.127781,0.15091,0.172324,0.110746,0.177134,0.162657,0,0